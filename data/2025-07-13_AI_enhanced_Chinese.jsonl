{"id": "2507.07683", "pdf": "https://arxiv.org/pdf/2507.07683", "abs": "https://arxiv.org/abs/2507.07683", "authors": ["Jude Haris", "Jos\u00e9 Cano"], "title": "Accelerating Transposed Convolutions on FPGA-based Edge Devices", "categories": ["cs.AR", "cs.DC", "cs.LG"], "comment": "Accepted to 35th International Conference on Field-Programmable Logic\n  and Applications (FPL) 2025", "summary": "Transposed Convolutions (TCONV) enable the up-scaling mechanism within\ngenerative Artificial Intelligence (AI) models. However, the predominant\nInput-Oriented Mapping (IOM) method for implementing TCONV has complex output\nmapping, overlapping sums, and ineffectual computations. These inefficiencies\nfurther exacerbate the performance bottleneck of TCONV and generative models on\nresource-constrained edge devices. To address this problem, in this paper we\npropose MM2IM, a hardware-software co-designed accelerator that combines Matrix\nMultiplication (MatMul) with col2IM to process TCONV layers on\nresource-constrained edge devices efficiently. Using the SECDA-TFLite design\ntoolkit, we implement MM2IM and evaluate its performance across 261 TCONV\nproblem configurations, achieving an average speedup of 1.9x against a\ndual-thread ARM Neon optimized CPU baseline. We then evaluate the performance\nof MM2IM on a range of TCONV layers from well-known generative models achieving\nup to 4.2x speedup, and compare it against similar resource-constrained TCONV\naccelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate\nMM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x\nenergy reduction against the CPU baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMM2IM\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u77e9\u9635\u4e58\u6cd5\u548ccol2IM\u65b9\u6cd5\u4f18\u5316TCONV\u5c42\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u73b0\u6709TCONV\u5b9e\u73b0\u65b9\u6cd5\uff08IOM\uff09\u5b58\u5728\u8f93\u51fa\u6620\u5c04\u590d\u6742\u3001\u8ba1\u7b97\u91cd\u53e0\u548c\u65e0\u6548\u8ba1\u7b97\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8fb9\u7f18\u8bbe\u5907\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51faMM2IM\u52a0\u901f\u5668\uff0c\u7ed3\u5408\u77e9\u9635\u4e58\u6cd5\u548ccol2IM\u65b9\u6cd5\uff0c\u5229\u7528SECDA-TFLite\u5de5\u5177\u5305\u5b9e\u73b0\u5e76\u8bc4\u4f30\u3002", "result": "\u5728261\u79cdTCONV\u914d\u7f6e\u4e2d\u5e73\u5747\u52a0\u901f1.9\u500d\uff0c\u5728\u77e5\u540d\u751f\u6210\u6a21\u578b\u4e2d\u6700\u9ad8\u52a0\u901f4.2\u500d\uff0c\u80fd\u6548\u63d0\u5347\u663e\u8457\u3002", "conclusion": "MM2IM\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u4f18\u5316TCONV\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.07223", "pdf": "https://arxiv.org/pdf/2507.07223", "abs": "https://arxiv.org/abs/2507.07223", "authors": ["Myoungsoo Jung"], "title": "Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure", "categories": ["cs.DC", "cs.AR", "B.4.3; C.0; C.2.1; C.2.2"], "comment": null, "summary": "Modern AI workloads such as large language models (LLMs) and\nretrieval-augmented generation (RAG) impose severe demands on memory,\ncommunication bandwidth, and resource flexibility. Traditional GPU-centric\narchitectures struggle to scale due to growing inter-GPU communication\noverheads. This report introduces key AI concepts and explains how Transformers\nrevolutionized data representation in LLMs. We analyze large-scale AI hardware\nand data center designs, identifying scalability bottlenecks in hierarchical\nsystems. To address these, we propose a modular data center architecture based\non Compute Express Link (CXL) that enables disaggregated scaling of memory,\ncompute, and accelerators. We further explore accelerator-optimized\ninterconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink\nFusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance\ndata transfers while preserving memory coherence. We also propose a\nhierarchical memory model that combines local and pooled memory, and evaluate\nlightweight CXL implementations, HBM, and silicon photonics for efficient\nscaling. Our evaluations demonstrate improved scalability, throughput, and\nflexibility in AI infrastructure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCXL\u7684\u6a21\u5757\u5316\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\uff0c\u7ed3\u5408XLink\u548c\u5206\u5c42\u5185\u5b58\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3AI\u5de5\u4f5c\u8d1f\u8f7d\u5728\u5185\u5b58\u548c\u901a\u4fe1\u5e26\u5bbd\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u4ee3AI\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982LLMs\u548cRAG\uff09\u5bf9\u5185\u5b58\u3001\u901a\u4fe1\u5e26\u5bbd\u548c\u8d44\u6e90\u7075\u6d3b\u6027\u63d0\u51fa\u4e86\u6781\u9ad8\u8981\u6c42\uff0c\u4f20\u7edfGPU\u67b6\u6784\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\uff0c\u7ed3\u5408CXL\u548cXLink\u6280\u672f\uff0c\u8bbe\u8ba1\u5206\u5c42\u5185\u5b58\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u8f7b\u91cf\u7ea7CXL\u5b9e\u73b0\u3001HBM\u548c\u7845\u5149\u5b50\u6280\u672f\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86AI\u57fa\u7840\u8bbe\u65bd\u7684\u53ef\u6269\u5c55\u6027\u3001\u541e\u5410\u91cf\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u901a\u8fc7CXL\u548cXLink\u7684\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6269\u5c55\u74f6\u9888\uff0c\u4e3a\u672a\u6765\u6570\u636e\u4e2d\u5fc3\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.07186", "pdf": "https://arxiv.org/pdf/2507.07186", "abs": "https://arxiv.org/abs/2507.07186", "authors": ["Itay Itzhak", "Yonatan Belinkov", "Gabriel Stanovsky"], "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "CoLM 2025", "summary": "Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u56e0\u679c\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u8ba4\u77e5\u504f\u89c1\u7684\u6765\u6e90\uff0c\u53d1\u73b0\u504f\u89c1\u4e3b\u8981\u6e90\u4e8e\u9884\u8bad\u7ec3\u800c\u975e\u5fae\u8c03\u6216\u968f\u673a\u566a\u58f0\u3002", "motivation": "\u63a2\u8ba8LLMs\u4e2d\u8ba4\u77e5\u504f\u89c1\u7684\u6765\u6e90\uff0c\u660e\u786e\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u8bad\u7ec3\u968f\u673a\u6027\u5bf9\u504f\u89c1\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u5b9e\u9a8c\uff1a1\uff09\u591a\u6b21\u5fae\u8c03\u6a21\u578b\u4ee5\u7814\u7a76\u8bad\u7ec3\u968f\u673a\u6027\u5bf9\u504f\u89c1\u7684\u5f71\u54cd\uff1b2\uff09\u5f15\u5165\u8de8\u8c03\u6362\uff08cross-tuning\uff09\u65b9\u6cd5\uff0c\u4ea4\u6362\u6307\u4ee4\u6570\u636e\u96c6\u4ee5\u9694\u79bb\u504f\u89c1\u6765\u6e90\u3002", "result": "\u8bad\u7ec3\u968f\u673a\u6027\u5f15\u5165\u4e00\u5b9a\u53d8\u5f02\u6027\uff0c\u4f46\u504f\u89c1\u4e3b\u8981\u7531\u9884\u8bad\u7ec3\u51b3\u5b9a\uff1b\u76f8\u540c\u9884\u8bad\u7ec3\u9aa8\u5e72\u7684\u6a21\u578b\u504f\u89c1\u6a21\u5f0f\u66f4\u76f8\u4f3c\u3002", "conclusion": "\u7406\u89e3\u5fae\u8c03\u6a21\u578b\u7684\u504f\u89c1\u9700\u8003\u8651\u9884\u8bad\u7ec3\u6765\u6e90\uff0c\u4e3a\u672a\u6765\u8bc4\u4f30\u548c\u51cf\u5c11LLMs\u504f\u89c1\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2507.07114", "pdf": "https://arxiv.org/pdf/2507.07114", "abs": "https://arxiv.org/abs/2507.07114", "authors": ["Erez Weintraub", "Ron Banner", "Ariel Orda"], "title": "Distributed Training under Packet Loss", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "State-of-the-art language and vision models are routinely trained across\nthousands of GPUs, often spanning multiple data-centers, yet today's\ndistributed frameworks still assume reliable connections (e.g., InfiniBand or\nRoCE). The resulting acknowledgment traffic and retransmissions inflate tail\nlatencies and limit scalability. Leveraging unreliable connections will reduce\nlatency but may sacrifice model accuracy and convergence once packets are\ndropped. A principled, end-to-end solution that preserves accuracy and\nconvergence guarantees under genuine packet loss has previously been missing.\nWe address this critical gap by introducing a novel distributed training\nframework capable of operating over unreliable connections, offering unbiased\ngradient aggregation and bounded parameter drift without modifying model code\nor optimizers. The key insight is a two-stage defense against missing messages:\n(i) Unbiased gradient aggregation: each worker reconstructs a consistent\ngradient estimate from whatever packets arrive, guaranteeing expectation-level\ncorrectness; and (ii) Bounded-drift parameter broadcasts: we prove the\ninter-worker model discrepancy remains O(1) even after arbitrarily many\niterations, preventing the unbounded divergence typical of asynchronous setups.\nAnalytical bounds are matched by experiments on the LLAMA2 7B model with 64\nGPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.\nThis work bridges the gap between communication-efficient datacenter protocols\nand the accuracy and generalization guarantees demanded by modern large-model\ntraining, enabling robust, high-throughput learning on commodity or wide-area\nnetworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\uff0c\u53ef\u5728\u4e0d\u53ef\u9760\u8fde\u63a5\u4e0b\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u548c\u6536\u655b\u6027\uff0c\u901a\u8fc7\u65e0\u504f\u68af\u5ea6\u805a\u5408\u548c\u6709\u754c\u53c2\u6570\u6f02\u79fb\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u6846\u67b6\u4f9d\u8d56\u53ef\u9760\u8fde\u63a5\uff0c\u5bfc\u81f4\u5c3e\u5ef6\u8fdf\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u800c\u4e0d\u53ef\u9760\u8fde\u63a5\u53ef\u80fd\u727a\u7272\u6a21\u578b\u51c6\u786e\u6027\u548c\u6536\u655b\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u9632\u5fa1\u673a\u5236\uff1a\u65e0\u504f\u68af\u5ea6\u805a\u5408\u548c\u6709\u754c\u53c2\u6570\u5e7f\u64ad\uff0c\u786e\u4fdd\u68af\u5ea6\u4f30\u8ba1\u6b63\u786e\u6027\u548c\u6a21\u578b\u53c2\u6570\u4e00\u81f4\u6027\u3002", "result": "\u5728LLAMA2 7B\u6a21\u578b\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u5bb9\u5fcd10%\u4e22\u5305\u7387\u4ec5\u5bfc\u81f40.8%\u56f0\u60d1\u5ea6\u53d8\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86\u9ad8\u6548\u901a\u4fe1\u534f\u8bae\u4e0e\u73b0\u4ee3\u5927\u6a21\u578b\u8bad\u7ec3\u9700\u6c42\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u652f\u6301\u5728\u666e\u901a\u6216\u5e7f\u57df\u7f51\u7edc\u4e0a\u8fdb\u884c\u9c81\u68d2\u5b66\u4e60\u3002"}}
{"id": "2507.07108", "pdf": "https://arxiv.org/pdf/2507.07108", "abs": "https://arxiv.org/abs/2507.07108", "authors": ["Zhiwei Hu", "V\u00edctor Guti\u00e9rrez-Basulto", "Zhiliang Xiang", "Ru Li", "Jeff Z. Pan"], "title": "Multi-level Mixture of Experts for Multimodal Entity Linking", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Accepted at KDD 2025", "summary": "Multimodal Entity Linking (MEL) aims to link ambiguous mentions within\nmultimodal contexts to associated entities in a multimodal knowledge base.\nExisting approaches to MEL introduce multimodal interaction and fusion\nmechanisms to bridge the modality gap and enable multi-grained semantic\nmatching. However, they do not address two important problems: (i) mention\nambiguity, i.e., the lack of semantic content caused by the brevity and\nomission of key information in the mention's textual context; (ii) dynamic\nselection of modal content, i.e., to dynamically distinguish the importance of\ndifferent parts of modal information. To mitigate these issues, we propose a\nMulti-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:\n(i) the description-aware mention enhancement module leverages large language\nmodels to identify the WikiData descriptions that best match a mention,\nconsidering the mention's textual context; (ii) the multimodal feature\nextraction module adopts multimodal feature encoders to obtain textual and\nvisual embeddings for both mentions and entities; (iii)-(iv) the intra-level\nmixture of experts and inter-level mixture of experts modules apply a switch\nmixture of experts mechanism to dynamically and adaptively select features from\nrelevant regions of information. Extensive experiments demonstrate the\noutstanding performance of MMoE compared to the state-of-the-art. MMoE's code\nis available at: https://github.com/zhiweihu1103/MEL-MMoE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\uff08MEL\uff09\u65b9\u6cd5MMoE\uff0c\u901a\u8fc7\u591a\u7ea7\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u63d0\u53ca\u6b67\u4e49\u548c\u6a21\u6001\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MEL\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3\u63d0\u53ca\u6b67\u4e49\u548c\u6a21\u6001\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "MMoE\u5305\u542b\u63cf\u8ff0\u611f\u77e5\u63d0\u53ca\u589e\u5f3a\u3001\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u3001\u4ee5\u53ca\u4e24\u7ea7\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u52a8\u6001\u9009\u62e9\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMMoE\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MMoE\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86MEL\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2507.07188", "pdf": "https://arxiv.org/pdf/2507.07188", "abs": "https://arxiv.org/abs/2507.07188", "authors": ["Jens Rupprecht", "Georg Ahnert", "Markus Strohmaier"], "title": "Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses", "categories": ["cs.CL", "cs.AI", "cs.CY", "J.4"], "comment": "18 pages, 17 figures", "summary": "Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown response biases are poorly understood. This paper investigates the\nresponse robustness of LLMs in normative survey contexts -- we test nine\ndiverse LLMs on questions from the World Values Survey (WVS), applying a\ncomprehensive set of 11 perturbations to both question phrasing and answer\noption structure, resulting in over 167,000 simulated interviews. In doing so,\nwe not only reveal LLMs' vulnerabilities to perturbations but also reveal that\nall tested models exhibit a consistent \\textit{recency bias} varying in\nintensity, disproportionately favoring the last-presented answer option. While\nlarger models are generally more robust, all models remain sensitive to\nsemantic variations like paraphrasing and to combined perturbations. By\napplying a set of perturbations, we reveal that LLMs partially align with\nsurvey response biases identified in humans. This underscores the critical\nimportance of prompt design and robustness testing when using LLMs to generate\nsynthetic survey data.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u793e\u4f1a\u79d1\u5b66\u8c03\u67e5\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u5176\u5bf9\u95ee\u9898\u8868\u8ff0\u548c\u7b54\u6848\u9009\u9879\u7684\u6270\u52a8\u654f\u611f\uff0c\u5e76\u8868\u73b0\u51fa\u660e\u663e\u7684\u201c\u8fd1\u56e0\u504f\u5dee\u201d\u3002", "motivation": "\u8bc4\u4f30LLMs\u4f5c\u4e3a\u4eba\u7c7b\u53d7\u8bd5\u8005\u4ee3\u7406\u7684\u53ef\u9760\u6027\u53ca\u5176\u5bf9\u5df2\u77e5\u54cd\u5e94\u504f\u5dee\u7684\u6613\u611f\u6027\u3002", "method": "\u6d4b\u8bd5\u4e86\u4e5d\u79cd\u4e0d\u540c\u7684LLMs\uff0c\u5e94\u752811\u79cd\u6270\u52a8\uff08\u5305\u62ec\u95ee\u9898\u8868\u8ff0\u548c\u7b54\u6848\u9009\u9879\u7ed3\u6784\u7684\u53d8\u5316\uff09\uff0c\u6a21\u62df\u4e86\u8d85\u8fc7167,000\u6b21\u8bbf\u8c08\u3002", "result": "\u6240\u6709\u6a21\u578b\u5747\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u8fd1\u56e0\u504f\u5dee\uff0c\u4e14\u5bf9\u8bed\u4e49\u53d8\u5316\uff08\u5982\u6539\u5199\uff09\u548c\u7ec4\u5408\u6270\u52a8\u654f\u611f\u3002\u8f83\u5927\u6a21\u578b\u66f4\u7a33\u5065\uff0c\u4f46\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u63d0\u793a\u8bbe\u8ba1\u548c\u9c81\u68d2\u6027\u6d4b\u8bd5\u5728\u5229\u7528LLMs\u751f\u6210\u5408\u6210\u8c03\u67e5\u6570\u636e\u65f6\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.07116", "pdf": "https://arxiv.org/pdf/2507.07116", "abs": "https://arxiv.org/abs/2507.07116", "authors": ["Juan Cano-Benito", "Andrea Cimmino", "Sven Hertling", "Heiko Paulheim", "Ra\u00fal Garc\u00eda-Castro"], "title": "Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces", "categories": ["cs.DC", "cs.AI", "cs.ET"], "comment": null, "summary": "Data spaces are emerging as decentralised infrastructures that enable\nsovereign, secure, and trustworthy data exchange among multiple participants.\nTo achieve semantic interoperability within these environments, the use of\nsemantic web technologies and knowledge graphs has been proposed. Although\ndistributed ledger technologies (DLT) fit as the underlying infrastructure for\ndata spaces, there remains a significant gap in terms of the efficient storage\nof semantic data on these platforms. This paper presents a systematic\nevaluation of semantic data storage across different types of DLT (public,\nprivate, and hybrid), using a real-world knowledge graph as an experimental\nbasis. The study compares performance, storage efficiency, resource\nconsumption, and the capabilities to update and query semantic data. The\nresults show that private DLTs are the most efficient for storing and managing\nsemantic content, while hybrid DLTs offer a balanced trade-off between public\nauditability and operational efficiency. This research leads to a discussion on\nthe selection of the most appropriate DLT infrastructure based on the data\nsovereignty requirements of decentralised data ecosystems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u7c7b\u578bDLT\uff08\u516c\u6709\u3001\u79c1\u6709\u3001\u6df7\u5408\uff09\u5728\u8bed\u4e49\u6570\u636e\u5b58\u50a8\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u79c1\u6709DLT\u6548\u7387\u6700\u9ad8\uff0c\u6df7\u5408DLT\u5728\u516c\u5f00\u5ba1\u8ba1\u4e0e\u64cd\u4f5c\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u7a7a\u95f4\u4e2d\u8bed\u4e49\u6570\u636e\u5728DLT\u4e0a\u9ad8\u6548\u5b58\u50a8\u7684\u7f3a\u53e3\uff0c\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u751f\u6001\u7cfb\u7edf\u7684\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u5b9e\u9a8c\u57fa\u7840\uff0c\u6bd4\u8f83\u4e0d\u540cDLT\u7684\u6027\u80fd\u3001\u5b58\u50a8\u6548\u7387\u3001\u8d44\u6e90\u6d88\u8017\u53ca\u8bed\u4e49\u6570\u636e\u66f4\u65b0\u4e0e\u67e5\u8be2\u80fd\u529b\u3002", "result": "\u79c1\u6709DLT\u5728\u8bed\u4e49\u5185\u5bb9\u5b58\u50a8\u548c\u7ba1\u7406\u4e0a\u6700\u6709\u6548\uff0c\u6df7\u5408DLT\u5728\u516c\u5f00\u5ba1\u8ba1\u4e0e\u64cd\u4f5c\u6548\u7387\u95f4\u63d0\u4f9b\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u751f\u6001\u7cfb\u7edf\u57fa\u4e8e\u6570\u636e\u4e3b\u6743\u9700\u6c42\u9009\u62e9\u5408\u9002DLT\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2507.07125", "pdf": "https://arxiv.org/pdf/2507.07125", "abs": "https://arxiv.org/abs/2507.07125", "authors": ["Cristina Mata", "Kanchana Ranasinghe", "Michael S. Ryoo"], "title": "CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings", "categories": ["cs.CV", "eess.IV"], "comment": "ECCV 2024", "summary": "Unsupervised domain adaptation (UDA) involves learning class semantics from\nlabeled data within a source domain that generalize to an unseen target domain.\nUDA methods are particularly impactful for semantic segmentation, where\nannotations are more difficult to collect than in image classification. Despite\nrecent advances in large-scale vision-language representation learning, UDA\nmethods for segmentation have not taken advantage of the domain-agnostic\nproperties of text. To address this, we present a novel Covariance-based\nPixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn\ndomain-invariant features in an image segmentation encoder. The text embeddings\nare generated through our LLM Domain Template process, where an LLM is used to\ngenerate source and target domain descriptions that are fed to a frozen CLIP\nmodel and combined. In experiments on four benchmarks we show that a model\ntrained using CoPT achieves the new state of the art performance on UDA for\nsegmentation. The code can be found at https://github.com/cfmata/CoPT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoPT\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u6280\u672f\uff0c\u63d0\u5347\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002", "motivation": "\u8bed\u4e49\u5206\u5272\u7684\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709UDA\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u6587\u672c\u7684\u9886\u57df\u65e0\u5173\u7279\u6027\u3002", "method": "\u901a\u8fc7LLM\u751f\u6210\u9886\u57df\u63cf\u8ff0\uff0c\u7ed3\u5408CLIP\u6a21\u578b\u751f\u6210\u6587\u672c\u5d4c\u5165\uff0c\u63d0\u51faCovariance-based Pixel-Text loss\uff08CoPT\uff09\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u7279\u5f81\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoPT\u5b9e\u73b0\u4e86UDA\u5206\u5272\u7684\u6700\u65b0\u6027\u80fd\u3002", "conclusion": "CoPT\u901a\u8fc7\u6587\u672c\u5d4c\u5165\u663e\u8457\u63d0\u5347\u4e86UDA\u5206\u5272\u7684\u6548\u679c\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.07229", "pdf": "https://arxiv.org/pdf/2507.07229", "abs": "https://arxiv.org/abs/2507.07229", "authors": ["Krithika Ramesh", "Daniel Smolyak", "Zihao Zhao", "Nupoor Gandhi", "Ritu Agarwal", "Margr\u00e9t Bjarnad\u00f3ttir", "Anjalie Field"], "title": "SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains", "categories": ["cs.CL"], "comment": null, "summary": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development.", "AI": {"tldr": "SynthTextEval\u662f\u4e00\u4e2a\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u5408\u6210\u6587\u672c\u7684\u5de5\u5177\u5305\uff0c\u652f\u6301\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u5305\u62ec\u5b9e\u7528\u6027\u3001\u516c\u5e73\u6027\u3001\u9690\u79c1\u98ce\u9669\u7b49\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u5982\u533b\u7597\u548c\u6cd5\u5f8b\u3002", "motivation": "\u5408\u6210\u6587\u672c\u5728AI\u5f00\u53d1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4ee5\u786e\u4fdd\u5176\u53ef\u884c\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u3002", "method": "\u63d0\u4f9b\u591a\u7ef4\u5ea6\u8bc4\u4f30\u5de5\u5177\uff0c\u652f\u6301\u7528\u6237\u4e0a\u4f20\u6216\u751f\u6210\u5408\u6210\u6587\u672c\uff0c\u5e76\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5de5\u5177\u5305\u5728\u533b\u7597\u548c\u6cd5\u5f8b\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u529f\u80fd\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\uff0cSynthTextEval\u65e8\u5728\u63d0\u5347\u5408\u6210\u6587\u672c\u7684\u53ef\u884c\u6027\u548cAI\u5f00\u53d1\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2507.07117", "pdf": "https://arxiv.org/pdf/2507.07117", "abs": "https://arxiv.org/abs/2507.07117", "authors": ["Jit Gupta", "Andrew Li", "Tarun Banka", "Ariel Cohen", "T. Sridhar", "Raj Yavatkar"], "title": "Collective Communication Profiling of Modern-day Machine Learning Workloads", "categories": ["cs.DC", "cs.AI", "cs.NI"], "comment": "Poser, USENIX NSDI 2025, April 2025, Philadelphia, PA, USA", "summary": "Machine Learning jobs, carried out on large number of distributed high\nperformance systems, involve periodic communication using operations like\nAllReduce, AllGather, and Broadcast. These operations may create high bandwidth\nand bursty traffic patterns, leading to network congestion and packet loss,\nthus impacting the performance of these jobs. Hence it is imperative to analyze\nthese patterns, which can be helpful in provisioning network resources\ndepending on the type of machine learning workloads. In this poster we carry\nout extensive analysis of the collective communication behavior seen in a wide\nvariety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we\ninstrument Nvidia Collective Communication Library logging functionality for\nricher context about the collectives and workloads. We adjust configuration\nparameters that influence collective communication behavior, such as\nparallelism, number of nodes, and model type. This overview presents and\ndiscusses some of the results on the collective communication behavior for the\nopen source DeepSeek V3 inferencing model, which includes operation type and\ncount, transfer sizes per operation, and request size distribution. Our\nanalysis shows that it makes sense to rethink current collective communication\nframeworks and network topologies so as to accommodate the effect of network\nanomalies on the mentioned workloads.", "AI": {"tldr": "\u5206\u6790\u673a\u5668\u5b66\u4e60\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u96c6\u4f53\u901a\u4fe1\u884c\u4e3a\u5bf9\u7f51\u7edc\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u6846\u67b6\u548c\u62d3\u6251\u7684\u5efa\u8bae\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u96c6\u4f53\u901a\u4fe1\u64cd\u4f5c\u53ef\u80fd\u5bfc\u81f4\u7f51\u7edc\u62e5\u585e\u548c\u4e22\u5305\uff0c\u5f71\u54cd\u6027\u80fd\uff0c\u9700\u5206\u6790\u8fd9\u4e9b\u6a21\u5f0f\u4ee5\u4f18\u5316\u7f51\u7edc\u8d44\u6e90\u914d\u7f6e\u3002", "method": "\u901a\u8fc7Nvidia Collective Communication Library\u8bb0\u5f55\u548c\u5206\u6790\u591a\u79cd\u6a21\u578b\uff08\u5982DeepSeek\u3001GPT\u3001Llama\uff09\u7684\u901a\u4fe1\u884c\u4e3a\uff0c\u8c03\u6574\u5e76\u884c\u5ea6\u3001\u8282\u70b9\u6570\u548c\u6a21\u578b\u7c7b\u578b\u7b49\u53c2\u6570\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u96c6\u4f53\u901a\u4fe1\u6846\u67b6\u548c\u7f51\u7edc\u62d3\u6251\u9700\u91cd\u65b0\u8bbe\u8ba1\uff0c\u4ee5\u9002\u5e94\u7f51\u7edc\u5f02\u5e38\u5bf9\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5f71\u54cd\u3002", "conclusion": "\u5efa\u8bae\u91cd\u65b0\u601d\u8003\u96c6\u4f53\u901a\u4fe1\u6846\u67b6\u548c\u7f51\u7edc\u62d3\u6251\uff0c\u4ee5\u4f18\u5316\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2507.07139", "pdf": "https://arxiv.org/pdf/2507.07139", "abs": "https://arxiv.org/abs/2507.07139", "authors": ["Renyang Liu", "Guanlin Li", "Tianwei Zhang", "See-Kiong Ng"], "title": "Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Recent advances in image generation models (IGMs), particularly\ndiffusion-based architectures such as Stable Diffusion (SD), have markedly\nenhanced the quality and diversity of AI-generated visual content. However,\ntheir generative capability has also raised significant ethical, legal, and\nsocietal concerns, including the potential to produce harmful, misleading, or\ncopyright-infringing content. To mitigate these concerns, machine unlearning\n(MU) emerges as a promising solution by selectively removing undesirable\nconcepts from pretrained models. Nevertheless, the robustness and effectiveness\nof existing unlearning techniques remain largely unexplored, particularly in\nthe presence of multi-modal adversarial inputs.\n  To bridge this gap, we propose Recall, a novel adversarial framework\nexplicitly designed to compromise the robustness of unlearned IGMs. Unlike\nexisting approaches that predominantly rely on adversarial text prompts, Recall\nexploits the intrinsic multi-modal conditioning capabilities of diffusion\nmodels by efficiently optimizing adversarial image prompts with guidance from a\nsingle semantically relevant reference image. Extensive experiments across ten\nstate-of-the-art unlearning methods and diverse tasks show that Recall\nconsistently outperforms existing baselines in terms of adversarial\neffectiveness, computational efficiency, and semantic fidelity with the\noriginal textual prompt. These findings reveal critical vulnerabilities in\ncurrent unlearning mechanisms and underscore the need for more robust solutions\nto ensure the safety and reliability of generative models. Code and data are\npublicly available at \\textcolor{blue}{https://github.com/ryliu68/RECALL}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRecall\u7684\u5bf9\u6297\u6027\u6846\u67b6\uff0c\u65e8\u5728\u6d4b\u8bd5\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08IGMs\uff09\u5728\u53bb\u5b66\u4e60\u6280\u672f\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u5c3d\u7ba1\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u5185\u5bb9\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u53ef\u80fd\u4ea7\u751f\u6709\u5bb3\u6216\u4fb5\u6743\u5185\u5bb9\u7684\u95ee\u9898\u5f15\u53d1\u4e86\u4f26\u7406\u548c\u6cd5\u5f8b\u62c5\u5fe7\u3002\u53bb\u5b66\u4e60\u6280\u672f\uff08MU\uff09\u88ab\u63d0\u51fa\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u8bba\u6587\u63d0\u51faRecall\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5bf9\u6297\u6027\u56fe\u50cf\u63d0\u793a\uff08\u800c\u975e\u4ec5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\uff09\u6765\u6d4b\u8bd5\u53bb\u5b66\u4e60\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u6761\u4ef6\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRecall\u5728\u5bf9\u6297\u6548\u679c\u3001\u8ba1\u7b97\u6548\u7387\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u53bb\u5b66\u4e60\u673a\u5236\u7684\u6f0f\u6d1e\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u53bb\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.07248", "pdf": "https://arxiv.org/pdf/2507.07248", "abs": "https://arxiv.org/abs/2507.07248", "authors": ["Minseon Kim", "Jean-Philippe Corbeil", "Alessandro Sordoni", "Francois Beaulieu", "Paul Vozila"], "title": "Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings", "categories": ["cs.CL"], "comment": null, "summary": "As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u7597\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u8bc4\u4f30\u534f\u8bae\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u7f3a\u4e4f\u533b\u7597\u9886\u57df\u7279\u5b9a\u5b89\u5168\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u968f\u7740LLMs\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u5c24\u5176\u662f\u5bf9\u60a3\u8005\u548c\u4e34\u5e8a\u533b\u751f\u7684\u6f5c\u5728\u5f71\u54cd\u3002\u73b0\u6709\u8bc4\u4f30\u591a\u5173\u6ce8\u901a\u7528\u9886\u57df\uff0c\u7f3a\u4e4f\u533b\u7597\u9886\u57df\u7684\u9488\u5bf9\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u9488\u5bf9\u60a3\u8005\u548c\u4e34\u5e8a\u533b\u751f\u89c6\u89d2\u7684\u5b89\u5168\u8bc4\u4f30\u534f\u8bae\uff0c\u6784\u5efa\u4e86\u5305\u542b466\u4e2a\u6837\u672c\u7684PatientSafetyBench\uff0c\u5e76\u5728MediPhi\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u7ea2\u961f\u6d4b\u8bd5\u3002", "result": "\u9996\u6b21\u4ece\u60a3\u8005\u3001\u4e34\u5e8a\u533b\u751f\u548c\u666e\u901a\u7528\u6237\u4e09\u4e2a\u89c6\u89d2\u5b9a\u4e49\u4e86\u533b\u7597LLMs\u7684\u5b89\u5168\u8bc4\u4f30\u6807\u51c6\uff0c\u4e3a\u533b\u7597\u9886\u57df\u7684\u5b89\u5168\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533b\u7597LLMs\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u66f4\u5b89\u5168\u7684\u533b\u7597\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2507.07120", "pdf": "https://arxiv.org/pdf/2507.07120", "abs": "https://arxiv.org/abs/2507.07120", "authors": ["Nidhi Bhatia", "Ankit More", "Ritika Borkar", "Tiyasa Mitra", "Ramon Matas", "Ritchie Zhao", "Maximilian Golub", "Dheevatsa Mudigere", "Brian Pharris", "Bita Darvish Rouhani"], "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.", "AI": {"tldr": "Helix Parallelism\u662f\u4e00\u79cd\u6df7\u5408\u6267\u884c\u7b56\u7565\uff0c\u901a\u8fc7KV\u5e76\u884c\u548cTP/EP\u5e76\u884c\u4f18\u5316LLM\u7684\u63a8\u7406\u6548\u7387\uff0c\u51cf\u5c11\u5ef6\u8fdf\u5e76\u652f\u6301\u66f4\u5927\u6279\u6b21\u3002", "motivation": "\u968f\u7740LLM\u7684KV\u5386\u53f2\u589e\u957f\uff0c\u5b9e\u65f6\u81ea\u56de\u5f52\u89e3\u7801\u9762\u4e34FFN\u6743\u91cd\u8bbf\u95ee\u548c\u957fKV\u7f13\u5b58\u8bfb\u53d6\u7684\u74f6\u9888\uff0c\u4f20\u7edfTP\u65b9\u6cd5\u6548\u7387\u6709\u9650\u3002", "method": "\u63d0\u51faHelix Parallelism\uff0c\u7ed3\u5408KV\u5e76\u884c\u548cTP/EP\u5e76\u884c\uff0c\u5f15\u5165\u8f7b\u91cf\u901a\u4fe1\u6b65\u9aa4\u548cHelix HOP-B\u6280\u672f\u4ee5\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "result": "Helix\u5728\u56fa\u5b9a\u6279\u6b21\u4e0b\u51cf\u5c11TTL\u8fbe1.5\u500d\uff0c\u652f\u630132\u500d\u66f4\u5927\u6279\u6b21\uff0c\u63d0\u5347DeepSeek-R1\u7684\u541e\u5410-\u5ef6\u8fdfPareto\u6548\u7387\u3002", "conclusion": "Helix\u4f7f\u8d85\u957f\u5e8f\u5217\u5b9e\u65f6\u63a8\u7406\u6210\u4e3a\u53ef\u80fd\uff0c\u663e\u8457\u4f18\u5316\u4e86GPU\u6548\u7387\u548c\u5ef6\u8fdf\u8868\u73b0\u3002"}}
{"id": "2507.07148", "pdf": "https://arxiv.org/pdf/2507.07148", "abs": "https://arxiv.org/abs/2507.07148", "authors": ["Getamesay Haile Dagnaw", "Yanming Zhu", "Muhammad Hassan Maqsood", "Wencheng Yang", "Xingshuai Dong", "Xuefei Yin", "Alan Wee-Chung Liew"], "title": "Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Explainable artificial intelligence (XAI) has become increasingly important\nin biomedical image analysis to promote transparency, trust, and clinical\nadoption of DL models. While several surveys have reviewed XAI techniques, they\noften lack a modality-aware perspective, overlook recent advances in multimodal\nand vision-language paradigms, and provide limited practical guidance. This\nsurvey addresses this gap through a comprehensive and structured synthesis of\nXAI methods tailored to biomedical image analysis.We systematically categorize\nXAI methods, analyzing their underlying principles, strengths, and limitations\nwithin biomedical contexts. A modality-centered taxonomy is proposed to align\nXAI methods with specific imaging types, highlighting the distinct\ninterpretability challenges across modalities. We further examine the emerging\nrole of multimodal learning and vision-language models in explainable\nbiomedical AI, a topic largely underexplored in previous work. Our\ncontributions also include a summary of widely used evaluation metrics and\nopen-source frameworks, along with a critical discussion of persistent\nchallenges and future directions. This survey offers a timely and in-depth\nfoundation for advancing interpretable DL in biomedical image analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7efc\u8ff0\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u6a21\u6001\u611f\u77e5\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709XAI\u7efc\u8ff0\u7f3a\u4e4f\u5bf9\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7279\u5b9a\u9700\u6c42\u7684\u5173\u6ce8\uff0c\u4e14\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7bXAI\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u539f\u7406\u3001\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u6a21\u6001\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684XAI\u65b9\u6cd5\u5206\u7c7b\uff0c\u603b\u7ed3\u4e86\u8bc4\u4f30\u6307\u6807\u548c\u5f00\u6e90\u6846\u67b6\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u53ef\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ca\u65f6\u4e14\u6df1\u5165\u7684\u57fa\u7840\u3002"}}
{"id": "2507.07280", "pdf": "https://arxiv.org/pdf/2507.07280", "abs": "https://arxiv.org/abs/2507.07280", "authors": ["Mariah Bradford", "Nikhil Krishnaswamy", "Nathaniel Blanchard"], "title": "The Impact of Background Speech on Interruption Detection in Collaborative Groups", "categories": ["cs.CL"], "comment": "Long Paper AIED 2025", "summary": "Interruption plays a crucial role in collaborative learning, shaping group\ninteractions and influencing knowledge construction. AI-driven support can\nassist teachers in monitoring these interactions. However, most previous work\non interruption detection and interpretation has been conducted in\nsingle-conversation environments with relatively clean audio. AI agents\ndeployed in classrooms for collaborative learning within small groups will need\nto contend with multiple concurrent conversations -- in this context,\noverlapping speech will be ubiquitous, and interruptions will need to be\nidentified in other ways. In this work, we analyze interruption detection in\nsingle-conversation and multi-group dialogue settings. We then create a\nstate-of-the-art method for interruption identification that is robust to\noverlapping speech, and thus could be deployed in classrooms. Further, our work\nhighlights meaningful linguistic and prosodic information about how\ninterruptions manifest in collaborative group interactions. Our investigation\nalso paves the way for future works to account for the influence of overlapping\nspeech from multiple groups when tracking group dialog.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u534f\u4f5c\u5b66\u4e60\u4e2d\u6253\u65ad\u884c\u4e3a\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u7ec4\u5bf9\u8bdd\u73af\u5883\u4e2d\u8bc6\u522b\u6253\u65ad\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6559\u5ba4\u573a\u666f\u3002", "motivation": "\u534f\u4f5c\u5b66\u4e60\u4e2d\u7684\u6253\u65ad\u884c\u4e3a\u5bf9\u77e5\u8bc6\u6784\u5efa\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u57fa\u4e8e\u5355\u5bf9\u8bdd\u73af\u5883\uff0c\u800c\u6559\u5ba4\u4e2d\u591a\u7ec4\u5bf9\u8bdd\u7684\u91cd\u53e0\u8bed\u97f3\u95ee\u9898\u9700\u8981\u89e3\u51b3\u3002", "method": "\u5206\u6790\u4e86\u5355\u5bf9\u8bdd\u548c\u591a\u7ec4\u5bf9\u8bdd\u73af\u5883\u4e2d\u7684\u6253\u65ad\u68c0\u6d4b\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5bf9\u91cd\u53e0\u8bed\u97f3\u9c81\u68d2\u7684\u6253\u65ad\u8bc6\u522b\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u6559\u5ba4\u73af\u5883\u7684\u6253\u65ad\u8bc6\u522b\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u6253\u65ad\u5728\u534f\u4f5c\u4e92\u52a8\u4e2d\u7684\u8bed\u8a00\u548c\u97f5\u5f8b\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u5728\u591a\u7ec4\u5bf9\u8bdd\u4e2d\u8ddf\u8e2a\u6253\u65ad\u884c\u4e3a\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u91cd\u53e0\u8bed\u97f3\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.07130", "pdf": "https://arxiv.org/pdf/2507.07130", "abs": "https://arxiv.org/abs/2507.07130", "authors": ["Zihan Zhang", "Leon Wong", "Blesson Varghese"], "title": "Ampere: Communication-Efficient and High-Accuracy Split Federated Learning", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "A Federated Learning (FL) system collaboratively trains neural networks\nacross devices and a server but is limited by significant on-device computation\ncosts. Split Federated Learning (SFL) systems mitigate this by offloading a\nblock of layers of the network from the device to a server. However, in doing\nso, it introduces large communication overheads due to frequent exchanges of\nintermediate activations and gradients between devices and the server and\nreduces model accuracy for non-IID data. We propose Ampere, a novel\ncollaborative training system that simultaneously minimizes on-device\ncomputation and device-server communication while improving model accuracy.\nUnlike SFL, which uses a global loss by iterative end-to-end training, Ampere\ndevelops unidirectional inter-block training to sequentially train the device\nand server block with a local loss, eliminating the transfer of gradients. A\nlightweight auxiliary network generation method decouples training between the\ndevice and server, reducing frequent intermediate exchanges to a single\ntransfer, which significantly reduces the communication overhead. Ampere\nmitigates the impact of data heterogeneity by consolidating activations\ngenerated by the trained device block to train the server block, in contrast to\nSFL, which trains on device-specific, non-IID activations. Extensive\nexperiments on multiple CNNs and transformers show that, compared to\nstate-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up\nto 13.26% while reducing training time by up to 94.6%, (ii) reduces\ndevice-server communication overhead by up to 99.1% and on-device computation\nby up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for\nvarious non-IID degrees highlighting superior performance when faced with\nheterogeneous data.", "AI": {"tldr": "Ampere\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u51cf\u5c11\u8bbe\u5907\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86Split Federated Learning (SFL)\u7684\u95ee\u9898\u3002", "motivation": "SFL\u7cfb\u7edf\u867d\u7136\u51cf\u5c11\u4e86\u8bbe\u5907\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5f15\u5165\u4e86\u5927\u91cf\u901a\u4fe1\u5f00\u9500\u5e76\u964d\u4f4e\u4e86\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u7684\u6a21\u578b\u51c6\u786e\u6027\u3002Ampere\u65e8\u5728\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Ampere\u91c7\u7528\u5355\u5411\u5757\u95f4\u8bad\u7ec3\u548c\u8f7b\u91cf\u7ea7\u8f85\u52a9\u7f51\u7edc\u751f\u6210\u65b9\u6cd5\uff0c\u51cf\u5c11\u68af\u5ea6\u4f20\u8f93\u548c\u4e2d\u95f4\u4ea4\u6362\uff0c\u5e76\u901a\u8fc7\u6574\u5408\u6fc0\u6d3b\u6570\u636e\u6765\u5e94\u5bf9\u6570\u636e\u5f02\u8d28\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAmpere\u5728\u6a21\u578b\u51c6\u786e\u6027\u3001\u8bad\u7ec3\u65f6\u95f4\u3001\u901a\u4fe1\u5f00\u9500\u548c\u8bbe\u5907\u8ba1\u7b97\u65b9\u9762\u663e\u8457\u4f18\u4e8eSFL\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "Ampere\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5904\u7406\u975eIID\u6570\u636e\u65f6\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.07151", "pdf": "https://arxiv.org/pdf/2507.07151", "abs": "https://arxiv.org/abs/2507.07151", "authors": ["Zongmeng Zhang", "Wengang Zhou", "Jie Zhao", "Houqiang Li"], "title": "Robust Multimodal Large Language Models Against Modality Conflict", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u63d0\u51fa\u6a21\u6001\u51b2\u7a81\u662f\u5bfc\u81f4\u5e7b\u89c9\u7684\u539f\u56e0\uff0c\u5e76\u6784\u5efa\u4e86MMMC\u6570\u636e\u96c6\u3002\u901a\u8fc7\u4e09\u79cd\u65b9\u6cd5\uff08\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u7f13\u89e3\u5e7b\u89c9\uff0c\u5b9e\u9a8c\u8868\u660e\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u5c3d\u7ba1MLLMs\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6613\u4ea7\u751f\u5e7b\u89c9\u3002\u8bba\u6587\u4ece\u6a21\u6001\u51b2\u7a81\u7684\u89d2\u5ea6\u7814\u7a76\u8fd9\u4e00\u73b0\u8c61\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5b9a\u4e49\u4e86\u6a21\u6001\u51b2\u7a81\u5e76\u6784\u5efaMMMC\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4e09\u79cd\u65b9\u6cd5\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7f13\u89e3\u6a21\u6001\u51b2\u7a81\u5bfc\u81f4\u7684\u5e7b\u89c9\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u7a33\u5b9a\u4e14\u6709\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86\u6a21\u6001\u51b2\u7a81\u5bf9MLLMs\u5e7b\u89c9\u7684\u5f71\u54cd\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.07307", "pdf": "https://arxiv.org/pdf/2507.07307", "abs": "https://arxiv.org/abs/2507.07307", "authors": ["Anirban Saha Anik", "Xiaoying Song", "Elliott Wang", "Bryan Wang", "Bengisu Yarimbas", "Lingzi Hong"], "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation\n(RAG) have demonstrated powerful capabilities in generating counterspeech\nagainst misinformation. However, current studies rely on limited evidence and\noffer less control over final outputs. To address these challenges, we propose\na Multi-agent Retrieval-Augmented Framework to generate counterspeech against\nhealth misinformation, incorporating multiple LLMs to optimize knowledge\nretrieval, evidence enhancement, and response refinement. Our approach\nintegrates both static and dynamic evidence, ensuring that the generated\ncounterspeech is relevant, well-grounded, and up-to-date. Our method\noutperforms baseline approaches in politeness, relevance, informativeness, and\nfactual accuracy, demonstrating its effectiveness in generating high-quality\ncounterspeech. To further validate our approach, we conduct ablation studies to\nverify the necessity of each component in our framework. Furthermore, human\nevaluations reveal that refinement significantly enhances counterspeech quality\nand obtains human preference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9488\u5bf9\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u7684\u53cd\u9a73\u8a00\u8bba\uff0c\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u548c\u52a8\u6001\u8bc1\u636e\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f9d\u8d56\u6709\u9650\u8bc1\u636e\u4e14\u5bf9\u6700\u7ec8\u8f93\u51fa\u63a7\u5236\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u53cd\u9a73\u8a00\u8bba\u7684\u751f\u6210\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u4e2aLLM\u4f18\u5316\u77e5\u8bc6\u68c0\u7d22\u3001\u8bc1\u636e\u589e\u5f3a\u548c\u54cd\u5e94\u7cbe\u70bc\uff0c\u6574\u5408\u9759\u6001\u4e0e\u52a8\u6001\u8bc1\u636e\u3002", "result": "\u65b9\u6cd5\u5728\u793c\u8c8c\u6027\u3001\u76f8\u5173\u6027\u3001\u4fe1\u606f\u91cf\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u548c\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u53cd\u9a73\u8a00\u8bba\uff0c\u4e14\u7cbe\u70bc\u8fc7\u7a0b\u663e\u8457\u63d0\u5347\u8d28\u91cf\u5e76\u83b7\u4eba\u7c7b\u504f\u597d\u3002"}}
{"id": "2507.07144", "pdf": "https://arxiv.org/pdf/2507.07144", "abs": "https://arxiv.org/abs/2507.07144", "authors": ["Hongyi Xie", "Min Zhou", "Qiao Yu", "Jialiang Yu", "Zhenli Sheng", "Hong Xie", "Defu Lian"], "title": "M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure", "categories": ["cs.DC"], "comment": null, "summary": "As cloud services become increasingly integral to modern IT infrastructure,\nensuring hardware reliability is essential to sustain high-quality service.\nMemory failures pose a significant threat to overall system stability, making\naccurate failure prediction through the analysis of memory error logs (i.e.,\nCorrectable Errors) imperative. Existing memory failure prediction approaches\nhave notable limitations: rule-based expert models suffer from limited\ngeneralizability and low recall rates, while automated feature extraction\nmethods exhibit suboptimal performance. To address these limitations, we\npropose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction\nframework designed to enhance the reliability and availability of cloud\ninfrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level\nbinary matrix representations and introduces a Binary Spatial Feature Extractor\n(BSFE) to automatically extract high-order features at both DIMM-level and\nbit-level. Building upon the BSFE outputs, we develop a dual-path temporal\nmodeling architecture: 1) a time-patch module that aggregates multi-level\nfeatures within observation windows, and 2) a time-point module that employs\ninterpretable rule-generation trees trained on bit-level patterns. Experiments\non both benchmark datasets and real-world deployment show the superiority of\nM$^2$-MFP as it outperforms existing state-of-the-art methods by significant\nmargins. Code and data are available at this repository:\nhttps://github.com/hwcloud-RAS/M2-MFP.", "AI": {"tldr": "M$^2$-MFP\u662f\u4e00\u79cd\u591a\u5c3a\u5ea6\u5206\u5c42\u5185\u5b58\u6545\u969c\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u5668\u548c\u53cc\u8def\u5f84\u65f6\u95f4\u5efa\u6a21\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5b58\u6545\u969c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5185\u5b58\u6545\u969c\u5bf9\u4e91\u57fa\u7840\u8bbe\u65bd\u7684\u7a33\u5b9a\u6027\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u6027\u5dee\u548c\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u53ef\u7ea0\u6b63\u9519\u8bef\u8f6c\u6362\u4e3a\u591a\u7ea7\u4e8c\u8fdb\u5236\u77e9\u9635\u8868\u793a\uff0c\u5f15\u5165\u4e8c\u8fdb\u5236\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u5668\uff08BSFE\uff09\uff0c\u5e76\u5f00\u53d1\u53cc\u8def\u5f84\u65f6\u95f4\u5efa\u6a21\u67b6\u6784\uff08\u65f6\u95f4\u5757\u6a21\u5757\u548c\u65f6\u95f4\u70b9\u6a21\u5757\uff09\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0cM$^2$-MFP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "M$^2$-MFP\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5185\u5b58\u6545\u969c\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u4e91\u57fa\u7840\u8bbe\u65bd\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.07153", "pdf": "https://arxiv.org/pdf/2507.07153", "abs": "https://arxiv.org/abs/2507.07153", "authors": ["Antonella Barisic Kulas", "Frano Petric", "Stjepan Bogdan"], "title": "Aerial Maritime Vessel Detection and Identification", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Preprint. ICUAS 2025", "summary": "Autonomous maritime surveillance and target vessel identification in\nenvironments where Global Navigation Satellite Systems (GNSS) are not available\nis critical for a number of applications such as search and rescue and threat\ndetection. When the target vessel is only described by visual cues and its last\nknown position is not available, unmanned aerial vehicles (UAVs) must rely\nsolely on on-board vision to scan a large search area under strict\ncomputational constraints. To address this challenge, we leverage the YOLOv8\nobject detection model to detect all vessels in the field of view. We then\napply feature matching and hue histogram distance analysis to determine whether\nany detected vessel corresponds to the target. When found, we localize the\ntarget using simple geometric principles. We demonstrate the proposed method in\nreal-world experiments during the MBZIRC2023 competition, integrated into a\nfully autonomous system with GNSS-denied navigation. We also evaluate the\nimpact of perspective on detection accuracy and localization precision and\ncompare it with the oracle approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728GNSS\u4e0d\u53ef\u7528\u73af\u5883\u4e0b\u901a\u8fc7\u65e0\u4eba\u673a\u89c6\u89c9\u81ea\u4e3b\u8bc6\u522b\u76ee\u6807\u8239\u53ea\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408YOLOv8\u68c0\u6d4b\u3001\u7279\u5f81\u5339\u914d\u548c\u8272\u8c03\u76f4\u65b9\u56fe\u5206\u6790\u3002", "motivation": "\u5728GNSS\u4e0d\u53ef\u7528\u73af\u5883\u4e2d\uff0c\u81ea\u4e3b\u8bc6\u522b\u76ee\u6807\u8239\u53ea\u5bf9\u4e8e\u641c\u6551\u548c\u5a01\u80c1\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528YOLOv8\u68c0\u6d4b\u8239\u53ea\uff0c\u7ed3\u5408\u7279\u5f81\u5339\u914d\u548c\u8272\u8c03\u76f4\u65b9\u56fe\u8ddd\u79bb\u5206\u6790\u8bc6\u522b\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u539f\u7406\u5b9a\u4f4d\u3002", "result": "\u5728MBZIRC2023\u7ade\u8d5b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc4\u4f30\u4e86\u89c6\u89d2\u5bf9\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728GNSS\u4e0d\u53ef\u7528\u73af\u5883\u4e0b\u80fd\u6709\u6548\u8bc6\u522b\u548c\u5b9a\u4f4d\u76ee\u6807\u8239\u53ea\u3002"}}
{"id": "2507.07414", "pdf": "https://arxiv.org/pdf/2507.07414", "abs": "https://arxiv.org/abs/2507.07414", "authors": ["Fardin Rastakhiz"], "title": "GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Time, cost, and energy efficiency are critical considerations in\nDeep-Learning (DL), particularly when processing long texts. Transformers,\nwhich represent the current state of the art, exhibit quadratic computational\ncomplexity relative to input length, making them inefficient for extended\ndocuments. This study introduces a novel model architecture that combines Graph\nNeural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated\nwith a real-time, end-to-end graph generation mechanism. The model processes\ncompact batches of character-level inputs without requiring padding or\ntruncation. To enhance performance while maintaining high speed and efficiency,\nthe model incorporates information from Large Language Models (LLMs), such as\ntoken embeddings and sentiment polarities, through efficient dictionary\nlookups. It captures local contextual patterns using CNNs, expands local\nreceptive fields via lattice-based graph structures, and employs small-world\ngraphs to aggregate document-level information. The generated graphs exhibit\nstructural properties indicative of meaningful semantic organization, with an\naverage clustering coefficient of approximately 0.45 and an average shortest\npath length ranging between 4 and 5. The model is evaluated across multiple\ntext classification tasks, including sentiment analysis and\nnews-categorization, and is compared against state-of-the-art models.\nExperimental results confirm the proposed model's efficiency and competitive\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GNN\u548cCNN\u7684\u65b0\u6a21\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u957f\u6587\u672c\uff0c\u89e3\u51b3\u4e86Transformer\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3Transformer\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\u7684\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u7ed3\u5408GNN\u548cCNN\uff0c\u5229\u7528\u5b9e\u65f6\u56fe\u751f\u6210\u673a\u5236\u5904\u7406\u5b57\u7b26\u7ea7\u8f93\u5165\uff0c\u96c6\u6210LLM\u4fe1\u606f\uff0c\u901a\u8fc7CNN\u6355\u6349\u5c40\u90e8\u6a21\u5f0f\uff0c\u56fe\u7ed3\u6784\u6269\u5c55\u611f\u53d7\u91ce\u3002", "result": "\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u9ad8\u6548\u4e14\u7ade\u4e89\u529b\u5f3a\uff0c\u751f\u6210\u7684\u56fe\u5177\u6709\u8bed\u4e49\u7ec4\u7ec7\u7279\u6027\u3002", "conclusion": "\u65b0\u6a21\u578b\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u957f\u6587\u672c\u5904\u7406\u3002"}}
{"id": "2507.07154", "pdf": "https://arxiv.org/pdf/2507.07154", "abs": "https://arxiv.org/abs/2507.07154", "authors": ["Desheng Li", "Chaoliang Liu", "Zhiyong Xiao"], "title": "CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of polyps from colonoscopy images is crucial for the\nearly diagnosis and treatment of colorectal cancer. Most existing deep\nlearning-based polyp segmentation methods adopt an Encoder-Decoder\narchitecture, and some utilize multi-task frameworks that incorporate auxiliary\ntasks such as classification to enhance segmentation performance. However,\nthese approaches often require additional labeled data and rely on task\nsimilarity, which can limit their generalizability. To address these\nchallenges, we propose CL-Polyp, a contrastive learning-enhanced polyp\nsegmentation network. Our method leverages contrastive learning to improve the\nencoder's ability to extract discriminative features by contrasting positive\nand negative sample pairs derived from polyp images. This self-supervised\nstrategy enhances visual representation without requiring additional\nannotations. In addition, we introduce two lightweight and effective modules:\nthe Modified Atrous Spatial Pyramid Pooling (MASPP) module for better\nmulti-scale feature fusion, and the Channel Concatenate and Element Add (CA)\nmodule to fuse low-level and upsampled features for improved boundary\nreconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,\nCVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp\nconsistently outperforms state-of-the-art methods. Specifically, it improves\nthe IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,\nrespectively, validating its effectiveness in clinical polyp segmentation\ntasks.", "AI": {"tldr": "CL-Polyp\u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u606f\u8089\u5206\u5272\u7f51\u7edc\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u7b56\u7565\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u606f\u8089\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u6807\u6ce8\u6570\u636e\u548c\u4efb\u52a1\u76f8\u4f3c\u6027\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u63d0\u51faCL-Polyp\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7f16\u7801\u5668\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u7ed3\u5408MASPP\u6a21\u5757\u548cCA\u6a21\u5757\u4f18\u5316\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u8fb9\u754c\u91cd\u5efa\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cIoU\u6307\u6807\u5728Kvasir-SEG\u548cCVC-ClinicDB\u4e0a\u5206\u522b\u63d0\u53470.011\u548c0.020\u3002", "conclusion": "CL-Polyp\u5728\u4e34\u5e8a\u606f\u8089\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.07419", "pdf": "https://arxiv.org/pdf/2507.07419", "abs": "https://arxiv.org/abs/2507.07419", "authors": ["Hieu Tran", "Zonghai Yao", "Won Seok Jang", "Sharmin Sultana", "Allen Chang", "Yuan Zhang", "Hong Yu"], "title": "MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning", "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors. arXiv admin note: text\n  overlap with arXiv:2406.09205", "summary": "Generative AI has demonstrated strong potential in healthcare, from clinical\ndecision support to patient-facing chatbots that improve outcomes. A critical\nchallenge for deployment is effective human-AI communication, where content\nmust be both personalized and understandable. We introduce MedReadCtrl, a\nreadability-controlled instruction tuning framework that enables LLMs to adjust\noutput complexity without compromising meaning. Evaluations of nine datasets\nand three tasks across medical and general domains show that MedReadCtrl\nachieves significantly lower readability instruction-following errors than\nGPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains\non unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).\nExperts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low\nliteracy levels. These gains reflect MedReadCtrl's ability to restructure\nclinical content into accessible, readability-aligned language while preserving\nmedical intent, offering a scalable solution to support patient education and\nexpand equitable access to AI-enabled care.", "AI": {"tldr": "MedReadCtrl\u662f\u4e00\u4e2a\u53ef\u8bfb\u6027\u63a7\u5236\u7684\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u5e2e\u52a9LLMs\u5728\u4e0d\u635f\u5931\u610f\u4e49\u7684\u60c5\u51b5\u4e0b\u8c03\u6574\u8f93\u51fa\u590d\u6742\u5ea6\uff0c\u663e\u8457\u4f18\u4e8eGPT-4\uff0c\u5e76\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u9886\u57df\u4e2d\u751f\u6210AI\u5728\u4eba\u7c7b-AI\u6c9f\u901a\u4e2d\u7684\u6311\u6218\uff0c\u786e\u4fdd\u5185\u5bb9\u65e2\u4e2a\u6027\u5316\u53c8\u6613\u4e8e\u7406\u89e3\u3002", "method": "\u5f15\u5165MedReadCtrl\u6846\u67b6\uff0c\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u63a7\u5236\u8f93\u51fa\u590d\u6742\u5ea6\uff0c\u8bc4\u4f30\u4e86\u4e5d\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u4efb\u52a1\u3002", "result": "MedReadCtrl\u5728\u53ef\u8bfb\u6027\u6307\u4ee4\u8ddf\u968f\u9519\u8bef\u7387\u4e0a\u663e\u8457\u4f4e\u4e8eGPT-4\uff0c\u4e34\u5e8a\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u4e13\u5bb6\u66f4\u504f\u597d\u5176\u8f93\u51fa\u3002", "conclusion": "MedReadCtrl\u80fd\u591f\u5c06\u4e34\u5e8a\u5185\u5bb9\u8f6c\u5316\u4e3a\u6613\u4e8e\u7406\u89e3\u7684\u8bed\u8a00\uff0c\u540c\u65f6\u4fdd\u7559\u533b\u5b66\u610f\u56fe\uff0c\u4e3a\u60a3\u8005\u6559\u80b2\u548cAI\u533b\u7597\u516c\u5e73\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07352", "pdf": "https://arxiv.org/pdf/2507.07352", "abs": "https://arxiv.org/abs/2507.07352", "authors": ["Lo\u00efc Pottier", "Konstantia Georgouli", "Timothy S. Carpenter", "Fikret Aydin", "Jeremy O. B. Tempkin", "Dwight V. Nissley", "Frederick H. Streitz", "Thomas R. W. Scogland", "Peer-Timo Bremer", "Felice C. Lightstone", "Helgi I. Ing\u00f3lfsson"], "title": "Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Computational models have become one of the prevalent methods to model\ncomplex phenomena. To accurately model complex interactions, such as detailed\nbiomolecular interactions, scientists often rely on multiscale models comprised\nof several internal models operating at difference scales, ranging from\nmicroscopic to macroscopic length and time scales. Bridging the gap between\ndifferent time and length scales has historically been challenging but the\nadvent of newer machine learning (ML) approaches has shown promise for tackling\nthat task. Multiscale models require massive amounts of computational power and\na powerful workflow management system. Orchestrating ML-driven multiscale\nstudies on parallel systems with thousands of nodes is challenging, the\nworkflow must schedule, allocate and control thousands of simulations operating\nat different scales. Here, we discuss the massively parallel Multiscale\nMachine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow\nmanagement infrastructure, that can orchestrate thousands of molecular dynamics\n(MD) simulations operating at different timescales, spanning from millisecond\nto nanosecond. More specifically, we introduce a novel version of MuMMI called\n\"mini-MuMMI\". Mini-MuMMI is a curated version of MuMMI designed to run on\nmodest HPC systems or even laptops whereas MuMMI requires larger HPC systems.\nWe demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions\nand discuss the different challenges behind the generalization of multiscale\nworkflows and how mini-MuMMI can be leveraged to target a broader range of\napplications outside of MD and RAS-RAF interactions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u591a\u5c3a\u5ea6\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u57fa\u7840\u8bbe\u65bdMuMMI\u53ca\u5176\u8f7b\u91cf\u7248mini-MuMMI\uff0c\u7528\u4e8e\u7ba1\u7406\u5927\u89c4\u6a21\u5e76\u884c\u591a\u5c3a\u5ea6\u6a21\u62df\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728RAS-RAF\u819c\u76f8\u4e92\u4f5c\u7528\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u591a\u5c3a\u5ea6\u6a21\u578b\u5728\u590d\u6742\u73b0\u8c61\u5efa\u6a21\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8de8\u5c3a\u5ea6\u6a21\u62df\u7684\u8ba1\u7b97\u7ba1\u7406\u548c\u8d44\u6e90\u9700\u6c42\u4e00\u76f4\u662f\u6311\u6218\u3002\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51faMuMMI\u53ca\u5176\u8f7b\u91cf\u7248mini-MuMMI\uff0c\u7528\u4e8e\u7ba1\u7406\u548c\u534f\u8c03\u5927\u89c4\u6a21\u5e76\u884c\u591a\u5c3a\u5ea6\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u652f\u6301\u4ece\u6beb\u79d2\u5230\u7eb3\u79d2\u7684\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u3002", "result": "mini-MuMMI\u5728RAS-RAF\u819c\u76f8\u4e92\u4f5c\u7528\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u5b9e\u7528\u6027\uff0c\u5e76\u5177\u5907\u6269\u5c55\u5230\u5176\u4ed6\u5e94\u7528\u7684\u6f5c\u529b\u3002", "conclusion": "MuMMI\u548cmini-MuMMI\u4e3a\u591a\u5c3a\u5ea6\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406\u5de5\u5177\uff0c\u672a\u6765\u53ef\u62d3\u5c55\u81f3\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u9886\u57df\u3002"}}
{"id": "2507.07157", "pdf": "https://arxiv.org/pdf/2507.07157", "abs": "https://arxiv.org/abs/2507.07157", "authors": ["Arshak Rezvani", "Ali Akbari", "Kosar Sanjar Arani", "Maryam Mirian", "Emad Arasteh", "Martin J. McKeown"], "title": "Interpretable EEG-to-Image Generation with Semantic Prompts", "categories": ["cs.CV", "cs.LG", "eess.SP"], "comment": "Actionable Interpretability Workshop (non-archival) at the 42\n  International Conference on Machine Learning", "summary": "Decoding visual experience from brain signals offers exciting possibilities\nfor neuroscience and interpretable AI. While EEG is accessible and temporally\nprecise, its limitations in spatial detail hinder image reconstruction. Our\nmodel bypasses direct EEG-to-image generation by aligning EEG signals with\nmultilevel semantic captions -- ranging from object-level to abstract themes --\ngenerated by a large language model. A transformer-based EEG encoder maps brain\nactivity to these captions through contrastive learning. During inference,\ncaption embeddings retrieved via projection heads condition a pretrained latent\ndiffusion model for image generation. This text-mediated framework yields\nstate-of-the-art visual decoding on the EEGCVPR dataset, with interpretable\nalignment to known neurocognitive pathways. Dominant EEG-caption associations\nreflected the importance of different semantic levels extracted from perceived\nimages. Saliency maps and t-SNE projections reveal semantic topography across\nthe scalp. Our model demonstrates how structured semantic mediation enables\ncognitively aligned visual decoding from EEG.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7EEG\u4fe1\u53f7\u4e0e\u591a\u7ea7\u8bed\u4e49\u63cf\u8ff0\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u7ed5\u8fc7\u76f4\u63a5\u56fe\u50cf\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u4ece\u8111\u4fe1\u53f7\u89e3\u7801\u89c6\u89c9\u4f53\u9a8c\u7684\u5148\u8fdb\u6280\u672f\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528EEG\u4fe1\u53f7\u89e3\u7801\u89c6\u89c9\u4f53\u9a8c\uff0c\u5c3d\u7ba1EEG\u5728\u7a7a\u95f4\u7ec6\u8282\u4e0a\u6709\u9650\uff0c\u4f46\u5176\u65f6\u95f4\u7cbe\u786e\u6027\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06EEG\u4fe1\u53f7\u6620\u5c04\u5230\u591a\u7ea7\u8bed\u4e49\u63cf\u8ff0\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5728EEGCVPR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u89e3\u7801\u6548\u679c\uff0c\u63ed\u793a\u4e86EEG\u4fe1\u53f7\u4e0e\u8bed\u4e49\u63cf\u8ff0\u7684\u795e\u7ecf\u8ba4\u77e5\u8def\u5f84\u5173\u8054\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u4e2d\u4ecb\uff0c\u5b9e\u73b0\u4e86\u4e0e\u8ba4\u77e5\u5bf9\u9f50\u7684EEG\u89c6\u89c9\u89e3\u7801\uff0c\u5c55\u793a\u4e86\u8bed\u4e49\u5c42\u6b21\u5728\u89e3\u7801\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.07421", "pdf": "https://arxiv.org/pdf/2507.07421", "abs": "https://arxiv.org/abs/2507.07421", "authors": ["Zonghai Yao", "Youxia Zhao", "Avijit Mitra", "David A. Levy", "Emily Druhl", "Jack Tsai", "Hong Yu"], "title": "SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data", "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors", "summary": "Eviction is a significant yet understudied social determinants of health\n(SDoH), linked to housing instability, unemployment, and mental health. While\neviction appears in unstructured electronic health records (EHRs), it is rarely\ncoded in structured fields, limiting downstream applications. We introduce\nSynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop\nannotation, and automated prompt optimization (APO) to extract eviction\nstatuses from clinical notes. Using this pipeline, we created the largest\npublic eviction-related SDoH dataset to date, comprising 14 fine-grained\ncategories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on\nSynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other\nSDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),\nGPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling\ncost-effective deployment across various model sizes. The pipeline reduces\nannotation effort by over 80%, accelerates dataset creation, enables scalable\neviction detection, and generalizes to other information extraction tasks.", "AI": {"tldr": "SynthEHR-Eviction\u662f\u4e00\u4e2a\u7ed3\u5408LLMs\u3001\u4eba\u5de5\u6807\u6ce8\u548c\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8a\u8bb0\u5f55\u4e2d\u63d0\u53d6\u9a71\u9010\u72b6\u6001\uff0c\u521b\u5efa\u4e86\u6700\u5927\u7684\u516c\u5171\u9a71\u9010\u76f8\u5173SDoH\u6570\u636e\u96c6\uff0c\u6027\u80fd\u4f18\u4e8eGPT-4o\u548cBioBERT\u3002", "motivation": "\u9a71\u9010\u662f\u5065\u5eb7\u7684\u793e\u4f1a\u51b3\u5b9a\u56e0\u7d20\uff08SDoH\uff09\u4e4b\u4e00\uff0c\u4f46\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u5316\u6570\u636e\uff0c\u9650\u5236\u4e86\u5e94\u7528\u3002", "method": "\u7ed3\u5408LLMs\u3001\u4eba\u5de5\u6807\u6ce8\u548c\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff08APO\uff09\u6784\u5efa\u7ba1\u9053\uff0c\u63d0\u53d6\u4e34\u5e8a\u8bb0\u5f55\u4e2d\u7684\u9a71\u9010\u72b6\u6001\uff0c\u5e76\u521b\u5efa\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u3002", "result": "\u5fae\u8c03\u540e\u7684LLMs\u5728\u9a71\u9010\u548c\u5176\u4ed6SDoH\u4efb\u52a1\u4e0a\u5206\u522b\u8fbe\u523088.8%\u548c90.3%\u7684Macro-F1\u5206\u6570\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u540c\u65f6\u964d\u4f4e\u6807\u6ce8\u6210\u672c80%\u3002", "conclusion": "\u8be5\u7ba1\u9053\u9ad8\u6548\u3001\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\uff0c\u4e3a\u9a71\u9010\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.07400", "pdf": "https://arxiv.org/pdf/2507.07400", "abs": "https://arxiv.org/abs/2507.07400", "authors": ["Zaifeng Pan", "Ajjkumar Patel", "Zhengding Hu", "Yipeng Shen", "Yue Guan", "Wan-Lu Li", "Lianhui Qin", "Yida Wang", "Yufei Ding"], "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows", "categories": ["cs.DC", "cs.MA"], "comment": null, "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.", "AI": {"tldr": "KVFlow\u662f\u4e00\u79cd\u9488\u5bf9\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u4f18\u5316\u7684KV\u7f13\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u4ee3\u7406\u7684\u672a\u6765\u4f7f\u7528\u60c5\u51b5\uff0c\u51cf\u5c11\u7f13\u5b58\u672a\u547d\u4e2d\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u4f7f\u7528LRU\u7b56\u7565\u7ba1\u7406KV\u7f13\u5b58\uff0c\u65e0\u6cd5\u9884\u6d4b\u4ee3\u7406\u7684\u672a\u6765\u4f7f\u7528\uff0c\u5bfc\u81f4\u9891\u7e41\u7f13\u5b58\u672a\u547d\u4e2d\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "method": "KVFlow\u901a\u8fc7Agent Step Graph\u62bd\u8c61\u4ee3\u7406\u6267\u884c\u8ba1\u5212\uff0c\u5e76\u4f7f\u7528\u6b65\u9aa4\u5230\u6267\u884c\u503c\u6307\u5bfc\u7ec6\u7c92\u5ea6\u7f13\u5b58\u9a71\u9010\u7b56\u7565\uff0c\u540c\u65f6\u5f15\u5165\u91cd\u53e0KV\u9884\u53d6\u673a\u5236\u3002", "result": "KVFlow\u5728\u5355\u5de5\u4f5c\u6d41\u7a0b\u548c\u5e76\u53d1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5206\u522b\u5b9e\u73b0\u4e861.83\u500d\u548c2.19\u500d\u7684\u52a0\u901f\u3002", "conclusion": "KVFlow\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u7684\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u7f13\u5b58\u672a\u547d\u4e2d\u548c\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2507.07202", "pdf": "https://arxiv.org/pdf/2507.07202", "abs": "https://arxiv.org/abs/2507.07202", "authors": ["Mohamed Elmoghany", "Ryan Rossi", "Seunghyun Yoon", "Subhojyoti Mukherjee", "Eslam Bakr", "Puneet Mathur", "Gang Wu", "Viet Dac Lai", "Nedim Lipka", "Ruiyi Zhang", "Varun Manjunatha", "Chien Nguyen", "Daksh Dangi", "Abel Salinas", "Mohammad Taesiri", "Hongjie Chen", "Xiaolei Huang", "Joe Barrow", "Nesreen Ahmed", "Hoda Eldardiry", "Namyong Park", "Yu Wang", "Jaemin Cho", "Anh Totti Nguyen", "Zhengzhong Tu", "Thien Nguyen", "Dinesh Manocha", "Mohamed Elhoseiny", "Franck Dernoncourt"], "title": "A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality", "categories": ["cs.CV"], "comment": null, "summary": "Despite the significant progress that has been made in video generative\nmodels, existing state-of-the-art methods can only produce videos lasting 5-16\nseconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16\nseconds struggle to maintain consistent character appearances and scene layouts\nthroughout the narrative. In particular, multi-subject long videos still fail\nto preserve character consistency and motion coherence. While some methods can\ngenerate videos up to 150 seconds long, they often suffer from frame redundancy\nand low temporal diversity. Recent work has attempted to produce long-form\nvideos featuring multiple characters, narrative coherence, and high-fidelity\ndetail. We comprehensively studied 32 papers on video generation to identify\nkey architectural components and training strategies that consistently yield\nthese qualities. We also construct a comprehensive novel taxonomy of existing\nmethods and present comparative tables that categorize papers by their\narchitectural designs and performance characteristics.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u5173\u952e\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ea\u80fd\u751f\u62105-16\u79d2\u7684\u77ed\u89c6\u9891\uff0c\u4e14\u957f\u89c6\u9891\u96be\u4ee5\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u548c\u573a\u666f\u8fde\u8d2f\u6027\uff0c\u591a\u4e3b\u9898\u957f\u89c6\u9891\u95ee\u9898\u66f4\u7a81\u51fa\u3002", "method": "\u7efc\u5408\u7814\u7a76\u4e8632\u7bc7\u89c6\u9891\u751f\u6210\u8bba\u6587\uff0c\u8bc6\u522b\u5173\u952e\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6784\u5efa\u5206\u7c7b\u6cd5\u5e76\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u5173\u952e\u7ec4\u4ef6\u548c\u7b56\u7565\uff0c\u5e76\u5c55\u793a\u4e86\u5206\u7c7b\u8868\u548c\u6027\u80fd\u6bd4\u8f83\u3002", "conclusion": "\u7814\u7a76\u4e3a\u957f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u6790\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2507.07439", "pdf": "https://arxiv.org/pdf/2507.07439", "abs": "https://arxiv.org/abs/2507.07439", "authors": ["Matthieu Boileau", "Philippe Helluy", "Jeremy Pawlus", "Svitlana Vyetrenko"], "title": "Towards Interpretable Time Series Foundation Models", "categories": ["cs.CL", "cs.AI"], "comment": "International Conference on Machine Leaning (ICML) 2025 Workshop on\n  Foundation Models for Structured Data", "summary": "In this paper, we investigate the distillation of time series reasoning\ncapabilities into small, instruction-tuned language models as a step toward\nbuilding interpretable time series foundation models. Leveraging a synthetic\ndataset of mean-reverting time series with systematically varied trends and\nnoise levels, we generate natural language annotations using a large multimodal\nmodel and use these to supervise the fine-tuning of compact Qwen models. We\nintroduce evaluation metrics that assess the quality of the distilled reasoning\n- focusing on trend direction, noise intensity, and extremum localization - and\nshow that the post-trained models acquire meaningful interpretive capabilities.\nOur results highlight the feasibility of compressing time series understanding\ninto lightweight, language-capable models suitable for on-device or\nprivacy-sensitive deployment. This work contributes a concrete foundation\ntoward developing small, interpretable models that explain temporal patterns in\nnatural language.", "AI": {"tldr": "\u7814\u7a76\u5c06\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u5c0f\u578b\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u4ee5\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\uff0c\u9002\u7528\u4e8e\u8bbe\u5907\u7aef\u6216\u9690\u79c1\u654f\u611f\u573a\u666f\u3002", "method": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6ce8\u91ca\uff0c\u76d1\u7763\u5fae\u8c03\u7d27\u51d1Qwen\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u83b7\u5f97\u6709\u610f\u4e49\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u538b\u7f29\u65f6\u95f4\u5e8f\u5217\u7406\u89e3\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u4e3a\u5f00\u53d1\u5c0f\u578b\u3001\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u63d0\u4f9b\u4e86\u5177\u4f53\u57fa\u7840\u3002"}}
{"id": "2507.07671", "pdf": "https://arxiv.org/pdf/2507.07671", "abs": "https://arxiv.org/abs/2507.07671", "authors": ["Jovan Prodanov", "Bla\u017e Bertalani\u010d", "Carolina Fortuna", "Shih-Kai Chou", "Matja\u017e Branko Juri\u010d", "Ramon Sanchez-Iborra", "Jernej Hribar"], "title": "Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems", "categories": ["cs.DC"], "comment": "Accepted at IEEE Cloud 2025", "summary": "Modern edge-cloud systems face challenges in efficiently scaling resources to\nhandle dynamic and unpredictable workloads. Traditional scaling approaches\ntypically rely on static thresholds and predefined rules, which are often\ninadequate for optimizing resource utilization and maintaining performance in\ndistributed and dynamic environments. This inefficiency hinders the\nadaptability and performance required in edge-cloud infrastructures, which can\nonly be achieved through the newly proposed in-place scaling. To address this\nproblem, we propose the Multi-Agent Reinforcement Learning-based In-place\nScaling Engine (MARLISE) that enables seamless, dynamic, reactive control with\nin-place resource scaling. We develop our solution using two Deep Reinforcement\nLearning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization\n(PPO). We analyze each version of the proposed MARLISE solution using dynamic\nworkloads, demonstrating their ability to ensure low response times of\nmicroservices and scalability. Our results show that MARLISE-based approaches\noutperform heuristic method in managing resource elasticity while maintaining\nmicroservice response times and achieving higher resource efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u8d44\u6e90\u6269\u5c55\u5f15\u64ce\uff08MARLISE\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u8fb9\u7f18\u4e91\u7cfb\u7edf\u4e2d\u8d44\u6e90\u6269\u5c55\u7684\u6311\u6218\uff0c\u901a\u8fc7DQN\u548cPPO\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u8d44\u6e90\u7ba1\u7406\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u9608\u503c\u548c\u9884\u5b9a\u4e49\u89c4\u5219\u7684\u8d44\u6e90\u6269\u5c55\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8fb9\u7f18\u4e91\u57fa\u7840\u8bbe\u65bd\u7684\u6027\u80fd\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08DQN\u548cPPO\uff09\u5f00\u53d1MARLISE\u5f15\u64ce\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u53cd\u5e94\u5f0f\u7684\u8d44\u6e90\u6269\u5c55\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMARLISE\u5728\u52a8\u6001\u8d1f\u8f7d\u4e0b\u80fd\u663e\u8457\u964d\u4f4e\u5fae\u670d\u52a1\u54cd\u5e94\u65f6\u95f4\u5e76\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\uff0c\u4f18\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "MARLISE\u4e3a\u8fb9\u7f18\u4e91\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u52a8\u6001\u8d44\u6e90\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2507.07230", "pdf": "https://arxiv.org/pdf/2507.07230", "abs": "https://arxiv.org/abs/2507.07230", "authors": ["Priyank Pathak", "Yogesh S. Rawat"], "title": "Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement", "categories": ["cs.CV"], "comment": "ICCV'25 paper", "summary": "Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals\nacross different locations and times, irrespective of clothing. Existing\nmethods often rely on additional models or annotations to learn robust,\nclothing-invariant features, making them resource-intensive. In contrast, we\nexplore the use of color - specifically foreground and background colors - as a\nlightweight, annotation-free proxy for mitigating appearance bias in ReID\nmodels. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that\nleverages color information directly from raw images or video frames. CSCI\nefficiently captures color-related appearance bias ('Color See') while\ndisentangling it from identity-relevant ReID features ('Color Ignore'). To\nachieve this, we introduce S2A self-attention, a novel self-attention to\nprevent information leak between color and identity cues within the feature\nspace. Our analysis shows a strong correspondence between learned color\nembeddings and clothing attributes, validating color as an effective proxy when\nexplicit clothing labels are unavailable. We demonstrate the effectiveness of\nCSCI on both image and video ReID with extensive experiments on four CC-ReID\ndatasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for\nimage-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID\nwithout relying on additional supervision. Our results highlight the potential\nof color as a cost-effective solution for addressing appearance bias in\nCC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u989c\u8272\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5CSCI\uff0c\u7528\u4e8e\u89e3\u51b3\u8863\u7269\u66f4\u6362\u91cd\u8bc6\u522b\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6216\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u6a21\u578b\u6216\u6807\u6ce8\uff0c\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u63a2\u7d22\u989c\u8272\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002", "method": "CSCI\u5229\u7528RGB\u4fe1\u606f\uff0c\u901a\u8fc7S2A\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5206\u79bb\u989c\u8272\u4e0e\u8eab\u4efd\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u56fe\u50cfReID\u63d0\u53472.9%-5.0%\uff0c\u89c6\u9891ReID\u63d0\u53471.0%-2.5%\u3002", "conclusion": "\u989c\u8272\u662f\u89e3\u51b3\u8863\u7269\u66f4\u6362\u91cd\u8bc6\u522b\u4e2d\u5916\u89c2\u504f\u89c1\u7684\u6709\u6548\u4e14\u7ecf\u6d4e\u7684\u65b9\u6848\u3002"}}
{"id": "2507.07441", "pdf": "https://arxiv.org/pdf/2507.07441", "abs": "https://arxiv.org/abs/2507.07441", "authors": ["Yu Xia", "Yiran Jenny Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model (LLM) agents are commonly tuned with supervised\nfinetuning on ReAct-style expert trajectories or preference optimization over\npairwise rollouts. Most of these methods focus on imitating specific expert\nbehaviors or promoting chosen reasoning thoughts and actions over rejected\nones. However, without reasoning and comparing over alternatives actions, LLM\nagents finetuned with these methods may over-commit towards seemingly plausible\nbut suboptimal actions due to limited action space exploration. To address\nthis, in this paper we propose Self-taught ActioN Deliberation (SAND)\nframework, enabling LLM agents to explicitly deliberate over candidate actions\nbefore committing to one. To tackle the challenges of when and what to\ndeliberate given large action space and step-level action evaluation, we\nincorporate self-consistency action sampling and execution-guided action\ncritique to help synthesize step-wise action deliberation thoughts using the\nbase model of the LLM agent. In an iterative manner, the deliberation\ntrajectories are then used to finetune the LLM agent itself. Evaluating on two\nrepresentative interactive agent tasks, SAND achieves an average 20%\nimprovement over initial supervised finetuning and also outperforms\nstate-of-the-art agent tuning approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAND\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5730\u8ba9LLM\u4ee3\u7406\u5728\u884c\u52a8\u524d\u8fdb\u884c\u5019\u9009\u884c\u52a8\u5ba1\u8bae\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u884c\u52a8\u7a7a\u95f4\u63a2\u7d22\u4e0d\u8db3\u800c\u5bfc\u81f4\u7684\u6b21\u4f18\u884c\u52a8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u76d1\u7763\u5fae\u8c03\u6216\u504f\u597d\u4f18\u5316\uff09\u53ef\u80fd\u5bfc\u81f4LLM\u4ee3\u7406\u8fc7\u5ea6\u4f9d\u8d56\u770b\u4f3c\u5408\u7406\u4f46\u6b21\u4f18\u7684\u884c\u52a8\uff0c\u7f3a\u4e4f\u5bf9\u66ff\u4ee3\u884c\u52a8\u7684\u63a2\u7d22\u548c\u6bd4\u8f83\u3002", "method": "SAND\u6846\u67b6\u901a\u8fc7\u81ea\u6d3d\u884c\u52a8\u91c7\u6837\u548c\u6267\u884c\u5f15\u5bfc\u7684\u884c\u52a8\u8bc4\u4ef7\uff0c\u5408\u6210\u9010\u6b65\u884c\u52a8\u5ba1\u8bae\u601d\u8def\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u5ba1\u8bae\u8f68\u8ff9\u8fed\u4ee3\u5fae\u8c03LLM\u4ee3\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u4ee3\u8868\u6027\u4ea4\u4e92\u4ee3\u7406\u4efb\u52a1\u4e2d\uff0cSAND\u6bd4\u521d\u59cb\u76d1\u7763\u5fae\u8c03\u5e73\u5747\u63d0\u534720%\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u8c03\u4f18\u65b9\u6cd5\u3002", "conclusion": "SAND\u6846\u67b6\u901a\u8fc7\u884c\u52a8\u5ba1\u8bae\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89e3\u51b3\u884c\u52a8\u7a7a\u95f4\u63a2\u7d22\u4e0d\u8db3\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.07932", "pdf": "https://arxiv.org/pdf/2507.07932", "abs": "https://arxiv.org/abs/2507.07932", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan", "Qiang Guan", "Hailong Jiang"], "title": "KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling", "categories": ["cs.DC"], "comment": "8 pages, 6 figures", "summary": "Autoscaling GPU inference workloads in Kubernetes remains challenging due to\nthe reactive and threshold-based nature of default mechanisms such as the\nHorizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty\ntraffic patterns and lack integration with GPU-level metrics. We present KIS-S,\na unified framework that combines KISim, a GPU-aware Kubernetes Inference\nSimulator, with KIScaler, a Proximal Policy Optimization (PPO)-based\nautoscaler. KIScaler learns latency-aware and resource-efficient scaling\npolicies entirely in simulation, and is directly deployed without retraining.\nExperiments across four traffic patterns show that KIScaler improves average\nreward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and\ngeneralizes without retraining. Our work bridges the gap between reactive\nautoscaling and intelligent orchestration for scalable GPU-accelerated\nenvironments.", "AI": {"tldr": "KIS-S\u6846\u67b6\u7ed3\u5408GPU\u611f\u77e5\u7684Kubernetes\u63a8\u7406\u6a21\u62df\u5668\u548c\u57fa\u4e8ePPO\u7684\u81ea\u52a8\u6269\u5c55\u5668\uff0c\u663e\u8457\u63d0\u5347GPU\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3Kubernetes\u4e2dGPU\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u81ea\u52a8\u6269\u5c55\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u548c\u7a81\u53d1\u6d41\u91cf\u4e0b\uff0c\u9ed8\u8ba4\u673a\u5236\uff08\u5982HPA\uff09\u8868\u73b0\u4e0d\u4f73\u4e14\u7f3a\u4e4fGPU\u7ea7\u6307\u6807\u96c6\u6210\u3002", "method": "\u63d0\u51faKIS-S\u6846\u67b6\uff0c\u7ed3\u5408KISim\uff08GPU\u611f\u77e5\u6a21\u62df\u5668\uff09\u548cKIScaler\uff08\u57fa\u4e8ePPO\u7684\u81ea\u52a8\u6269\u5c55\u5668\uff09\uff0c\u5728\u6a21\u62df\u4e2d\u5b66\u4e60\u5ef6\u8fdf\u611f\u77e5\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u6269\u5c55\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cKIScaler\u5e73\u5747\u5956\u52b1\u63d0\u534775.2%\uff0cP95\u5ef6\u8fdf\u964d\u4f4e6.7\u500d\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\u3002", "conclusion": "KIS-S\u586b\u8865\u4e86\u53cd\u5e94\u5f0f\u81ea\u52a8\u6269\u5c55\u4e0e\u667a\u80fd\u7f16\u6392\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u7684GPU\u52a0\u901f\u73af\u5883\u3002"}}
{"id": "2507.07242", "pdf": "https://arxiv.org/pdf/2507.07242", "abs": "https://arxiv.org/abs/2507.07242", "authors": ["Johannes Merz", "Lucien Fostier"], "title": "Automated Video Segmentation Machine Learning Pipeline", "categories": ["cs.CV"], "comment": null, "summary": "Visual effects (VFX) production often struggles with slow, resource-intensive\nmask generation. This paper presents an automated video segmentation pipeline\nthat creates temporally consistent instance masks. It employs machine learning\nfor: (1) flexible object detection via text prompts, (2) refined per-frame\nimage segmentation and (3) robust video tracking to ensure temporal stability.\nDeployed using containerization and leveraging a structured output format, the\npipeline was quickly adopted by our artists. It significantly reduces manual\neffort, speeds up the creation of preliminary composites, and provides\ncomprehensive segmentation data, thereby enhancing overall VFX production\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u89c6\u9891\u5206\u5272\u6d41\u7a0b\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u5b9e\u4f8b\u63a9\u7801\uff0c\u663e\u8457\u63d0\u5347VFX\u751f\u4ea7\u6548\u7387\u3002", "motivation": "\u89e3\u51b3VFX\u5236\u4f5c\u4e2d\u63a9\u7801\u751f\u6210\u6162\u4e14\u8d44\u6e90\u5bc6\u96c6\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6587\u672c\u63d0\u793a\u7684\u7075\u6d3b\u5bf9\u8c61\u68c0\u6d4b\u3001\u9010\u5e27\u56fe\u50cf\u5206\u5272\u548c\u9c81\u68d2\u89c6\u9891\u8ddf\u8e2a\uff0c\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u52a0\u5feb\u4e86\u521d\u6b65\u5408\u6210\u7684\u521b\u5efa\u901f\u5ea6\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5206\u5272\u6570\u636e\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u4e86VFX\u5236\u4f5c\u7684\u6548\u7387\uff0c\u5e76\u5feb\u901f\u88ab\u827a\u672f\u5bb6\u91c7\u7528\u3002"}}
{"id": "2507.07451", "pdf": "https://arxiv.org/pdf/2507.07451", "abs": "https://arxiv.org/abs/2507.07451", "authors": ["Hongzhi Zhang", "Jia Fu", "Jingyuan Zhang", "Kai Fu", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning", "categories": ["cs.CL"], "comment": "https://github.com/Kwai-Klear/RLEP", "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present \\emph{RLEP}\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.", "AI": {"tldr": "RLEP\u6846\u67b6\u901a\u8fc7\u91cd\u653e\u5df2\u9a8c\u8bc1\u7684\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u5e76\u63d0\u9ad8\u6700\u7ec8\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u7b56\u7565\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u5148\u6536\u96c6\u5df2\u9a8c\u8bc1\u8f68\u8ff9\uff0c\u518d\u5728\u8bad\u7ec3\u4e2d\u91cd\u653e\u8fd9\u4e9b\u8f68\u8ff9\uff0c\u6df7\u5408\u65b0\u751f\u6210\u7684rollouts\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\uff08\u5982AIME-2024\u4ece38.2%\u63d0\u5347\u81f339.9%\uff09\u3002", "conclusion": "RLEP\u901a\u8fc7\u91cd\u653e\u9ad8\u8d28\u91cf\u7ecf\u9a8c\uff0c\u6709\u6548\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "1602.03104", "pdf": "https://arxiv.org/pdf/1602.03104", "abs": "https://arxiv.org/abs/1602.03104", "authors": ["Ayan Dutta", "Prithviraj Dasgupta", "Carl Nelson"], "title": "A Graph Isomorphism-based Decentralized Algorithm for Modular Robot Configuration Formation", "categories": ["cs.RO", "cs.DC", "cs.DS"], "comment": null, "summary": "We consider the problem of configuration formation in modular robot systems\nwhere a set of modules that are initially in different configurations and\nlocated at different locations are required to assume appropriate positions so\nthat they can get into a new, user-specified, target configuration. We propose\na novel algorithm based on graph isomorphism, where the modules select\nlocations or spots in the target configuration using a utility-based framework,\nwhile retaining their original configuration to the greatest extent possible,\nto reduce the time and energy required by the modules to assume the target\nconfiguration. We have shown analytically that our proposed algorithm is\ncomplete and guarantees a Pareto-optimal allocation. Experimental simulations\nof our algorithm with different number of modules in different initial\nconfigurations and located initially at different locations, show that the\nplanning time of our algorithm is nominal (order of msec. for 100 modules). We\nhave also compared our algorithm against a market-based allocation algorithm\nand shown that our proposed algorithm performs better in terms of time and\nnumber of messages exchanged.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u540c\u6784\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u914d\u7f6e\u5f62\u6210\u95ee\u9898\uff0c\u65e8\u5728\u51cf\u5c11\u65f6\u95f4\u548c\u80fd\u91cf\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6a21\u5757\u4ece\u4e0d\u540c\u521d\u59cb\u914d\u7f6e\u548c\u4f4d\u7f6e\u79fb\u52a8\u5230\u76ee\u6807\u914d\u7f6e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6548\u7528\u7684\u6846\u67b6\u548c\u56fe\u540c\u6784\u7b97\u6cd5\uff0c\u6a21\u5757\u9009\u62e9\u76ee\u6807\u914d\u7f6e\u4e2d\u7684\u4f4d\u7f6e\uff0c\u540c\u65f6\u5c3d\u91cf\u4fdd\u7559\u539f\u59cb\u914d\u7f6e\u3002", "result": "\u7b97\u6cd5\u5177\u6709\u5b8c\u6574\u6027\u548c\u5e15\u7d2f\u6258\u6700\u4f18\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u89c4\u5212\u65f6\u95f4\u77ed\uff08100\u6a21\u5757\u4ec5\u9700\u6beb\u79d2\u7ea7\uff09\uff0c\u4f18\u4e8e\u5e02\u573a\u5206\u914d\u7b97\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u65f6\u95f4\u548c\u901a\u4fe1\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u914d\u7f6e\u5f62\u6210\u3002"}}
{"id": "2507.07262", "pdf": "https://arxiv.org/pdf/2507.07262", "abs": "https://arxiv.org/abs/2507.07262", "authors": ["Shehreen Azad", "Yogesh S Rawat"], "title": "DisenQ: Disentangling Q-Former for Activity-Biometrics", "categories": ["cs.CV"], "comment": "Accepted in ICCV 2025", "summary": "In this work, we address activity-biometrics, which involves identifying\nindividuals across diverse set of activities. Unlike traditional person\nidentification, this setting introduces additional challenges as identity cues\nbecome entangled with motion dynamics and appearance variations, making\nbiometrics feature learning more complex. While additional visual data like\npose and/or silhouette help, they often struggle from extraction inaccuracies.\nTo overcome this, we propose a multimodal language-guided framework that\nreplaces reliance on additional visual data with structured textual\nsupervision. At its core, we introduce \\textbf{DisenQ} (\\textbf{Disen}tangling\n\\textbf{Q}-Former), a unified querying transformer that disentangles\nbiometrics, motion, and non-biometrics features by leveraging structured\nlanguage guidance. This ensures identity cues remain independent of appearance\nand motion variations, preventing misidentifications. We evaluate our approach\non three activity-based video benchmarks, achieving state-of-the-art\nperformance. Additionally, we demonstrate strong generalization to complex\nreal-world scenario with competitive performance on a traditional video-based\nidentification benchmark, showing the effectiveness of our framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8bed\u8a00\u5f15\u5bfc\u6846\u67b6DisenQ\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u76d1\u7763\u89e3\u8026\u751f\u7269\u7279\u5f81\u3001\u8fd0\u52a8\u548c\u975e\u751f\u7269\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u6d3b\u52a8\u751f\u7269\u7279\u5f81\u8bc6\u522b\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u8eab\u4efd\u8bc6\u522b\u5728\u591a\u6837\u5316\u6d3b\u52a8\u4e2d\u9762\u4e34\u8eab\u4efd\u7ebf\u7d22\u4e0e\u8fd0\u52a8\u52a8\u6001\u548c\u5916\u89c2\u53d8\u5316\u4ea4\u7ec7\u7684\u6311\u6218\uff0c\u73b0\u6709\u89c6\u89c9\u6570\u636e\u65b9\u6cd5\u56e0\u63d0\u53d6\u4e0d\u51c6\u786e\u800c\u53d7\u9650\u3002", "method": "\u5f15\u5165DisenQ\uff08\u89e3\u8026Q-Former\uff09\uff0c\u5229\u7528\u7ed3\u6784\u5316\u8bed\u8a00\u6307\u5bfc\u89e3\u8026\u7279\u5f81\uff0c\u907f\u514d\u8eab\u4efd\u7ebf\u7d22\u53d7\u5916\u89c2\u548c\u8fd0\u52a8\u53d8\u5316\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u6d3b\u52a8\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u4f20\u7edf\u89c6\u9891\u8bc6\u522b\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u6709\u6548\u89e3\u51b3\u4e86\u6d3b\u52a8\u751f\u7269\u7279\u5f81\u8bc6\u522b\u7684\u590d\u6742\u6027\uff0c\u5c55\u793a\u4e86\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.07484", "pdf": "https://arxiv.org/pdf/2507.07484", "abs": "https://arxiv.org/abs/2507.07484", "authors": ["Kaiqu Liang", "Haimin Hu", "Xuandong Zhao", "Dawn Song", "Thomas L. Griffiths", "Jaime Fern\u00e1ndez Fisac"], "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page, code & data: https://machine-bullshit.github.io", "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u2018\u673a\u5668\u5e9f\u8bdd\u2019\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u771f\u5b9e\u6027\u7f3a\u5931\u7684\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u2018\u5e9f\u8bdd\u6307\u6570\u2019\u548c\u5206\u7c7b\u6cd5\uff0c\u8bc4\u4f30\u4e86RLHF\u548cCoT\u63d0\u793a\u5bf9\u5e9f\u8bdd\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76LLM\u4e2d\u771f\u5b9e\u6027\u7f3a\u5931\u7684\u666e\u904d\u73b0\u8c61\uff0c\u63d0\u51fa\u2018\u673a\u5668\u5e9f\u8bdd\u2019\u4f5c\u4e3a\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u63ed\u793a\u5176\u673a\u5236\u3002", "method": "\u5f15\u5165\u5e9f\u8bdd\u6307\u6570\u548c\u56db\u79cd\u5e9f\u8bdd\u5206\u7c7b\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u65b0\u8bbe\u8ba1\u7684BullshitEval\uff09\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "RLHF\u663e\u8457\u52a0\u5267\u5e9f\u8bdd\uff0cCoT\u63d0\u793a\u653e\u5927\u7279\u5b9a\u5f62\u5f0f\uff1b\u653f\u6cbb\u8bed\u5883\u4e2d\u5e9f\u8bdd\u666e\u904d\uff0c\u4ee5\u2018\u6a21\u7cca\u63aa\u8f9e\u2019\u4e3a\u4e3b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86AI\u5bf9\u9f50\u7684\u7cfb\u7edf\u6027\u6311\u6218\uff0c\u4e3a\u63d0\u5347LLM\u771f\u5b9e\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.07589", "pdf": "https://arxiv.org/pdf/2507.07589", "abs": "https://arxiv.org/abs/2507.07589", "authors": ["Arpana Sinhal", "Anay Sinhal", "Amit Sinhal"], "title": "Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Healthcare professionals, particularly nurses, face elevated occupational\nstress, a concern amplified during the COVID-19 pandemic. While wearable\nsensors offer promising avenues for real-time stress monitoring, existing\nstudies often lack comprehensive datasets and robust analytical frameworks.\nThis study addresses these gaps by introducing a multimodal dataset comprising\nphysiological signals, electrodermal activity, heart rate and skin temperature.\nA systematic literature review identified limitations in prior stress-detection\nmethodologies, particularly in handling class imbalance and optimizing model\ngeneralizability. To overcome these challenges, the dataset underwent\npreprocessing with the Synthetic Minority Over sampling Technique (SMOTE),\nensuring balanced representation of stress states. Advanced machine learning\nmodels including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were\nevaluated and combined into a Stacking Classifier to leverage their collective\npredictive strengths. By using a publicly accessible dataset and a reproducible\nanalytical pipeline, this work advances the development of deployable\nstress-monitoring systems, offering practical implications for safeguarding\nhealthcare workers' mental health. Future research directions include expanding\ndemographic diversity and exploring edge-computing implementations for low\nlatency stress alerts.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u533b\u62a4\u4eba\u5458\u538b\u529b\u76d1\u6d4b\u4e2d\u7684\u6570\u636e\u96c6\u4e0d\u8db3\u548c\u65b9\u6cd5\u5c40\u9650\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u53ef\u90e8\u7f72\u7684\u538b\u529b\u76d1\u6d4b\u7cfb\u7edf\u3002", "motivation": "\u533b\u62a4\u4eba\u5458\uff08\u5c24\u5176\u662f\u62a4\u58eb\uff09\u9762\u4e34\u9ad8\u804c\u4e1a\u538b\u529b\uff0cCOVID-19\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5168\u9762\u6570\u636e\u96c6\u548c\u9c81\u68d2\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528SMOTE\u9884\u5904\u7406\u6570\u636e\u96c6\u4ee5\u5e73\u8861\u538b\u529b\u72b6\u6001\uff0c\u8bc4\u4f30\u5e76\u7ec4\u5408\u968f\u673a\u68ee\u6797\u3001XGBoost\u548cMLP\u6a21\u578b\u4e3aStacking Classifier\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u5206\u6790\u6d41\u7a0b\u548c\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4e3a\u538b\u529b\u76d1\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u4e3a\u533b\u62a4\u4eba\u5458\u5fc3\u7406\u5065\u5eb7\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6301\uff0c\u672a\u6765\u53ef\u6269\u5c55\u4eba\u53e3\u591a\u6837\u6027\u548c\u63a2\u7d22\u8fb9\u7f18\u8ba1\u7b97\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u8b66\u62a5\u3002"}}
{"id": "2507.07274", "pdf": "https://arxiv.org/pdf/2507.07274", "abs": "https://arxiv.org/abs/2507.07274", "authors": ["Ananya Raval", "Aravind Narayanan", "Vahid Reza Khazaie", "Shaina Raza"], "title": "LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted at ASONAM'25", "summary": "Large Multimodal Models (LMMs) are typically trained on vast corpora of\nimage-text data but are often limited in linguistic coverage, leading to biased\nand unfair outputs across languages. While prior work has explored multimodal\nevaluation, less emphasis has been placed on assessing multilingual\ncapabilities. In this work, we introduce LinguaMark, a benchmark designed to\nevaluate state-of-the-art LMMs on a multilingual Visual Question Answering\n(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages\nand five social attributes. We evaluate models using three key metrics: Bias,\nAnswer Relevancy, and Faithfulness. Our findings reveal that closed-source\nmodels generally achieve the highest overall performance. Both closed-source\n(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform\ncompetitively across social attributes, and Qwen2.5 demonstrates strong\ngeneralization across multiple languages. We release our benchmark and\nevaluation code to encourage reproducibility and further research.", "AI": {"tldr": "LinguaMark\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d611\u79cd\u8bed\u8a00\u548c5\u79cd\u793e\u4f1a\u5c5e\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u504f\u89c1\u3001\u7b54\u6848\u76f8\u5173\u6027\u548c\u5fe0\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709LMMs\u5728\u8bed\u8a00\u8986\u76d6\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u8f93\u51fa\u504f\u89c1\u548c\u4e0d\u516c\u5e73\uff0c\u4f46\u591a\u8bed\u8a00\u80fd\u529b\u7684\u8bc4\u4f30\u8f83\u5c11\u88ab\u5173\u6ce8\u3002", "method": "\u63d0\u51faLinguaMark\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b6,875\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u504f\u89c1\u3001\u7b54\u6848\u76f8\u5173\u6027\u548c\u5fe0\u5b9e\u6027\u4e09\u4e2a\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\u548cGemini2.5\uff09\u6574\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u5f00\u6e90\u6a21\u578b\uff08\u5982Gemma3\u548cQwen2.5\uff09\u5728\u67d0\u4e9b\u65b9\u9762\u8868\u73b0\u7ade\u4e89\u6027\uff0cQwen2.5\u5728\u591a\u8bed\u8a00\u6cdb\u5316\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "LinguaMark\u4e3a\u591a\u8bed\u8a00LMMs\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.07495", "pdf": "https://arxiv.org/pdf/2507.07495", "abs": "https://arxiv.org/abs/2507.07495", "authors": ["Mihir Parmar", "Palash Goyal", "Xin Liu", "Yiwen Song", "Mingyang Ling", "Chitta Baral", "Hamid Palangi", "Tomas Pfister"], "title": "PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving", "categories": ["cs.CL", "cs.AI"], "comment": "15 Pages", "summary": "Recently, decomposing complex problems into simple subtasks--a crucial part\nof human-like natural planning--to solve the given problem has significantly\nboosted the performance of large language models (LLMs). However, leveraging\nsuch planning structures during post-training to boost the performance of\nsmaller open-source LLMs remains underexplored. Motivated by this, we introduce\nPLAN-TUNING, a unified post-training framework that (i) distills synthetic task\ndecompositions (termed \"planning trajectories\") from large-scale LLMs and (ii)\nfine-tunes smaller models via supervised and reinforcement-learning objectives\ndesigned to mimic these planning processes to improve complex reasoning. On\nGSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by\nan average $\\sim7\\%$. Furthermore, plan-tuned models show better generalization\ncapabilities on out-of-domain datasets, with average $\\sim10\\%$ and $\\sim12\\%$\nperformance improvements on OlympiadBench and AIME 2024, respectively. Our\ndetailed analysis demonstrates how planning trajectories improves complex\nreasoning capabilities, showing that PLAN-TUNING is an effective strategy for\nimproving task-specific performance of smaller LLMs.", "AI": {"tldr": "PLAN-TUNING\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4efb\u52a1\u5206\u89e3\uff08\u89c4\u5212\u8f68\u8ff9\uff09\u5e76\u5fae\u8c03\u5c0f\u6a21\u578b\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5229\u7528\u4efb\u52a1\u5206\u89e3\uff08\u7c7b\u4f3c\u4eba\u7c7b\u89c4\u5212\uff09\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u76ee\u524d\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u89c4\u5212\u8f68\u8ff9\uff0c\u901a\u8fc7\u76d1\u7763\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5c0f\u6a21\u578b\u3002", "result": "\u5728GSM8k\u548cMATH\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u53477%\uff0c\u5728OlympiadBench\u548cAIME 2024\u4e0a\u5206\u522b\u63d0\u534710%\u548c12%\u3002", "conclusion": "PLAN-TUNING\u6709\u6548\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u6cdb\u5316\u4f18\u52bf\u3002"}}
{"id": "2507.07297", "pdf": "https://arxiv.org/pdf/2507.07297", "abs": "https://arxiv.org/abs/2507.07297", "authors": ["Chengfei Wu", "Ronald Seoh", "Bingxuan Li", "Liqiang Zhang", "Fengrong Han", "Dan Goldwasser"], "title": "MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in large vision-language models have led to impressive\nperformance in visual question answering and multimodal reasoning. However, it\nremains unclear whether these models genuinely perform grounded visual\nreasoning or rely on superficial patterns and dataset biases. In this work, we\nintroduce MagiC, a comprehensive benchmark designed to evaluate grounded\nmultimodal cognition, assessing not only answer accuracy but also the quality\nof step-by-step reasoning and its alignment with relevant visual evidence. Our\nbenchmark includes approximately 5,500 weakly supervised QA examples generated\nfrom strong model outputs and 900 human-curated examples with fine-grained\nannotations, including answers, rationales, and bounding box groundings. We\nevaluate 15 vision-language models ranging from 7B to 70B parameters across\nfour dimensions: final answer correctness, reasoning validity, grounding\nfidelity, and self-correction ability. MagiC further includes diagnostic\nsettings to probe model robustness under adversarial visual cues and assess\ntheir capacity for introspective error correction. We introduce new metrics\nsuch as MagiScore and StepSense, and provide comprehensive analyses that reveal\nkey limitations and opportunities in current approaches to grounded visual\nreasoning.", "AI": {"tldr": "MagiC\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u6a21\u6001\u8ba4\u77e5\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5173\u6ce8\u7b54\u6848\u51c6\u786e\u6027\u3001\u63a8\u7406\u8d28\u91cf\u4e0e\u89c6\u89c9\u8bc1\u636e\u7684\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u65b0\u6307\u6807\u5206\u6790\u6a21\u578b\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8bc4\u4f30\u5176\u662f\u5426\u4f9d\u8d56\u8868\u9762\u6a21\u5f0f\u6216\u6570\u636e\u96c6\u504f\u5dee\u3002", "method": "\u6784\u5efa\u5305\u542b5,500\u4e2a\u5f31\u76d1\u7763QA\u548c900\u4e2a\u4eba\u5de5\u6807\u6ce8\u793a\u4f8b\u7684\u57fa\u51c6\uff0c\u8bc4\u4f3015\u4e2a\u6a21\u578b\u5728\u56db\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u8bca\u65ad\u8bbe\u7f6e\u548c\u65b0\u6307\u6807\u3002", "result": "MagiC\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u673a\u4f1a\u3002", "conclusion": "MagiC\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u5de5\u5177\u3002"}}
{"id": "2507.07498", "pdf": "https://arxiv.org/pdf/2507.07498", "abs": "https://arxiv.org/abs/2507.07498", "authors": ["Keqin Bao", "Nuo Chen", "Xiaoyuan Li", "Binyuan Hui", "Bowen Yu", "Fuli Feng", "Junyang Lin", "Xiangnan He", "Dayiheng Liu"], "title": "Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTeaR\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u7b5b\u9009\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\u662f\u7814\u7a76\u91cd\u70b9\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u4ee3\u7801\u7ed3\u6784\uff0c\u5bfc\u81f4\u8fc7\u62df\u5408\u3002", "method": "TeaR\u7ed3\u5408\u6570\u636e\u7b5b\u9009\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4f18\u5316\u6a21\u578b\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTeaR\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5982Qwen2.5-7B\u63d0\u534735.9%\u3002", "conclusion": "TeaR\u6709\u6548\u63d0\u5347LLM\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.07317", "pdf": "https://arxiv.org/pdf/2507.07317", "abs": "https://arxiv.org/abs/2507.07317", "authors": ["Sherry X. Chen", "Yi Wei", "Luowei Zhou", "Suren Kumar"], "title": "ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation", "categories": ["cs.CV"], "comment": "International Conference on Computer Vision (ICCV) 2025", "summary": "Recent advances in instruction-guided image editing underscore the need for\neffective automated evaluation. While Vision-Language Models (VLMs) have been\nexplored as judges, open-source models struggle with alignment, and proprietary\nmodels lack transparency and cost efficiency. Additionally, no public training\ndatasets exist to fine-tune open-source VLMs, only small benchmarks with\ndiverse evaluation schemes. To address this, we introduce ADIEE, an automated\ndataset creation approach which is then used to train a scoring model for\ninstruction-guided image editing evaluation. We generate a large-scale dataset\nwith over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified\nto decode a numeric score from a custom token. The resulting scorer outperforms\nall open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a\n0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,\nand improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench\nand 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the\nstate-of-the-art. The scorer can act as a reward model, enabling automated best\nedit selection and model fine-tuning. Notably, the proposed scorer can boost\nMagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43\n(+8.98%).", "AI": {"tldr": "ADIEE\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6570\u636e\u96c6\u521b\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u8bc4\u5206\u6a21\u578b\u4ee5\u8bc4\u4f30\u6307\u4ee4\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u6548\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f00\u6e90\u6a21\u578b\u5bf9\u9f50\u56f0\u96be\uff0c\u4e13\u6709\u6a21\u578b\u4e0d\u900f\u660e\u4e14\u6210\u672c\u9ad8\uff0c\u4e14\u7f3a\u4e4f\u516c\u5f00\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7ADIEE\u751f\u6210\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08\u8d8510\u4e07\u6837\u672c\uff09\uff0c\u5e76\u5fae\u8c03LLaVA-NeXT-8B\u6a21\u578b\u4ee5\u89e3\u7801\u81ea\u5b9a\u4e49\u4ee4\u724c\u7684\u6570\u503c\u5206\u6570\u3002", "result": "\u8bc4\u5206\u6a21\u578b\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5f00\u6e90\u548cGemini-Pro 1.5\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u76f8\u5173\u6027\u53ca\u6bd4\u8f83\u51c6\u786e\u6027\u3002", "conclusion": "ADIEE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6307\u4ee4\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u95ee\u9898\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u63d0\u5347\u7f16\u8f91\u6548\u679c\u3002"}}
{"id": "2507.07499", "pdf": "https://arxiv.org/pdf/2507.07499", "abs": "https://arxiv.org/abs/2507.07499", "authors": ["Hein Htet", "Amgad Ahmed Ali Ibrahim", "Yutaka Sasaki", "Ryoji Asahi"], "title": "Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature", "categories": ["cs.CL", "physics.data-an"], "comment": "28 pages, 12 figures, 6 tables", "summary": "The oxygen reduction reaction (ORR) catalyst plays a critical role in\nenhancing fuel cell efficiency, making it a key focus in material science\nresearch. However, extracting structured information about ORR catalysts from\nvast scientific literature remains a significant challenge due to the\ncomplexity and diversity of textual data. In this study, we propose a named\nentity recognition (NER) and relation extraction (RE) approach using DyGIE++\nwith multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,\nto extract ORR catalyst-related information from the scientific literature,\nwhich is compiled into a fuel cell corpus for materials informatics\n(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12\ncritical entities and two relationship types between pairs of the entities. Our\nmethodology involves data annotation, integration, and fine-tuning of\ntransformer-based models to enhance information extraction accuracy. We assess\nthe impact of different BERT variants on extraction performance and investigate\nthe effects of annotation consistency. Experimental evaluations demonstrate\nthat the fine-tuned PubMedBERT model achieves the highest NER F1-score of\n82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.\nFurthermore, the comparison with human annotators highlights the reliability of\nfine-tuned models for ORR catalyst extraction, demonstrating their potential\nfor scalable and automated literature analysis. The results indicate that\ndomain-specific BERT models outperform general scientific models like BlueBERT\nfor ORR catalyst extraction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDyGIE++\u548c\u591a\u79cdBERT\u53d8\u4f53\u7684NER\u4e0eRE\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6ORR\u50ac\u5316\u5242\u4fe1\u606f\uff0c\u6784\u5efa\u4e86FC-CoMIcs\u6570\u636e\u96c6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u9886\u57df\u7279\u5b9aBERT\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "ORR\u50ac\u5316\u5242\u5bf9\u63d0\u5347\u71c3\u6599\u7535\u6c60\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ece\u590d\u6742\u591a\u6837\u7684\u6587\u732e\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528DyGIE++\u7ed3\u5408MatSciBERT\u548cPubMedBERT\u7b49\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fdb\u884c\u6570\u636e\u6807\u6ce8\u3001\u6574\u5408\u548c\u5fae\u8c03\uff0c\u4ee5\u63d0\u5347\u4fe1\u606f\u63d0\u53d6\u51c6\u786e\u6027\u3002", "result": "\u5fae\u8c03\u7684PubMedBERT\u5728NER\u4efb\u52a1\u4e2dF1-score\u8fbe82.19%\uff0cMatSciBERT\u5728RE\u4efb\u52a1\u4e2dF1-score\u4e3a66.10%\uff0c\u9886\u57df\u7279\u5b9a\u6a21\u578b\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u6587\u732e\u5206\u6790\u7684\u6f5c\u529b\uff0c\u9886\u57df\u7279\u5b9aBERT\u6a21\u578b\u5728ORR\u50ac\u5316\u5242\u63d0\u53d6\u4e2d\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2507.07333", "pdf": "https://arxiv.org/pdf/2507.07333", "abs": "https://arxiv.org/abs/2507.07333", "authors": ["Hui Pang", "Sunil Hadap", "Violetta Shevchenko", "Rahul Suresh", "Amin Banitalebi-Dehkordi"], "title": "Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory", "categories": ["cs.CV", "I.4.9"], "comment": "Presented at the workshop Three questions about virtual try-on at\n  CVPR 2025", "summary": "Augmented reality is revolutionizing beauty industry with virtual try-on\n(VTO) applications, which empowers users to try a wide variety of products\nusing their phones without the hassle of physically putting on real products. A\ncritical technical challenge in foundation VTO applications is the accurate\nsynthesis of foundation-skin tone color blending while maintaining the\nscalability of the method across diverse product ranges. In this work, we\npropose a novel method to approximate well-established Kubelka-Munk (KM) theory\nfor faster image synthesis while preserving foundation-skin tone color blending\nrealism. Additionally, we build a scalable end-to-end framework for realistic\nfoundation makeup VTO solely depending on the product information available on\ne-commerce sites. We validate our method using real-world makeup images,\ndemonstrating that our framework outperforms other techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKubelka-Munk\u7406\u8bba\u7684\u5feb\u901f\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u865a\u62df\u8bd5\u5986\uff0c\u63d0\u5347\u80a4\u8272\u4e0e\u7c89\u5e95\u878d\u5408\u7684\u771f\u5b9e\u611f\uff0c\u5e76\u6784\u5efa\u4e86\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u865a\u62df\u8bd5\u5986\u4e2d\u80a4\u8272\u4e0e\u7c89\u5e95\u989c\u8272\u878d\u5408\u7684\u771f\u5b9e\u6027\u548c\u65b9\u6cd5\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u8fd1\u4f3cKubelka-Munk\u7406\u8bba\u4ee5\u52a0\u901f\u56fe\u50cf\u5408\u6210\uff0c\u6784\u5efa\u4f9d\u8d56\u7535\u5546\u4ea7\u54c1\u4fe1\u606f\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002", "result": "\u5728\u771f\u5b9e\u5316\u5986\u56fe\u50cf\u4e0a\u9a8c\u8bc1\uff0c\u6846\u67b6\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u865a\u62df\u8bd5\u5986\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u771f\u5b9e\u7684\u80a4\u8272\u4e0e\u7c89\u5e95\u878d\u5408\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.07505", "pdf": "https://arxiv.org/pdf/2507.07505", "abs": "https://arxiv.org/abs/2507.07505", "authors": ["Varin Sikka", "Vishal Sikka"], "title": "Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages; to be submitted to AAAI-26 after reviews", "summary": "With widespread adoption of transformer-based language models in AI, there is\nsignificant interest in the limits of LLMs capabilities, specifically so-called\nhallucinations, occurrences in which LLMs provide spurious, factually incorrect\nor nonsensical information when prompted on certain subjects. Furthermore,\nthere is growing interest in agentic uses of LLMs - that is, using LLMs to\ncreate agents that act autonomously or semi-autonomously to carry out various\ntasks, including tasks with applications in the real world. This makes it\nimportant to understand the types of tasks LLMs can and cannot perform. We\nexplore this topic from the perspective of the computational complexity of LLM\ninference. We show that LLMs are incapable of carrying out computational and\nagentic tasks beyond a certain complexity, and further that LLMs are incapable\nof verifying the accuracy of tasks beyond a certain complexity. We present\nexamples of both, then discuss some consequences of this work.", "AI": {"tldr": "LLMs\u5728\u590d\u6742\u8ba1\u7b97\u548c\u4ee3\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u80fd\u529b\u9650\u5236\uff0c\u65e0\u6cd5\u9a8c\u8bc1\u8d85\u51fa\u5176\u590d\u6742\u6027\u8303\u56f4\u7684\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740LLMs\u5728AI\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7814\u7a76\u5176\u80fd\u529b\u9650\u5236\uff08\u5982\u5e7b\u89c9\u95ee\u9898\uff09\u548c\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u53d8\u5f97\u91cd\u8981\u3002", "method": "\u4ece\u8ba1\u7b97\u590d\u6742\u6027\u7684\u89d2\u5ea6\u5206\u6790LLM\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u5c55\u793a\u5176\u5c40\u9650\u6027\u3002", "result": "LLMs\u65e0\u6cd5\u6267\u884c\u6216\u9a8c\u8bc1\u8d85\u51fa\u7279\u5b9a\u590d\u6742\u6027\u7684\u8ba1\u7b97\u548c\u4ee3\u7406\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4efb\u52a1\u8bbe\u8ba1\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2507.07340", "pdf": "https://arxiv.org/pdf/2507.07340", "abs": "https://arxiv.org/abs/2507.07340", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "title": "Entity Re-identification in Visual Storytelling via Contrastive Reinforcement Learning", "categories": ["cs.CV", "I.2; I.4; I.5; I.7"], "comment": "7 pages", "summary": "Visual storytelling systems, particularly large vision-language models,\nstruggle to maintain character and object identity across frames,\n  often failing to recognize when entities in different images represent the\nsame individuals or objects,\n  leading to inconsistent references and referential hallucinations.\n  This occurs because models lack explicit training on when to establish entity\nconnections across frames.\n  We propose a contrastive reinforcement learning approach that trains models\nto discriminate between coherent image sequences\n  and stories from unrelated images.\n  We extend the Story Reasoning dataset with synthetic negative examples to\nteach appropriate entity connection behavior.\n  We employ Direct Preference Optimization with a dual-component reward\nfunction that promotes grounding and re-identification of entities\n  in real stories while penalizing incorrect entity connections in synthetic\ncontexts.\n  Using this contrastive framework, we fine-tune Qwen Storyteller (based on\nQwen2.5-VL 7B).\n  Evaluation shows improvements in grounding mAP from 0.27 to 0.31 (+14.8%), F1\nfrom 0.35 to 0.41 (+17.1%).\n  Pronoun grounding accuracy improved across all pronoun types except ``its'',\n  and cross-frame character and object persistence increased\n  across all frame counts, with entities appearing in 5 or more frames\nadvancing from 29.3% to 33.3% (+13.7%).\n  Well-structured stories, containing the chain-of-thought and grounded story,\nincreased from 79.1% to 97.5% (+23.3%).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u8d1f\u4f8b\u8bad\u7ec3\u6a21\u578b\u533a\u5206\u8fde\u8d2f\u56fe\u50cf\u5e8f\u5217\u4e0e\u65e0\u5173\u56fe\u50cf\uff0c\u63d0\u5347\u89c6\u89c9\u6545\u4e8b\u7cfb\u7edf\u4e2d\u89d2\u8272\u548c\u5bf9\u8c61\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6545\u4e8b\u7cfb\u7edf\u5728\u8de8\u5e27\u8bc6\u522b\u540c\u4e00\u89d2\u8272\u6216\u5bf9\u8c61\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u5f15\u7528\u4e0d\u4e00\u81f4\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u539f\u56e0\u662f\u7f3a\u4e4f\u660e\u786e\u7684\u8de8\u5e27\u5b9e\u4f53\u8fde\u63a5\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6269\u5c55Story Reasoning\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u53cc\u7ec4\u5206\u5956\u52b1\u51fd\u6570\u4f18\u5316Direct Preference Optimization\uff0c\u5fae\u8c03Qwen Storyteller\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5b9e\u4f53\u8bc6\u522b\u548c\u8fde\u8d2f\u6027\u663e\u8457\u63d0\u5347\uff0c\u5982grounding mAP\u63d0\u9ad814.8%\uff0cF1\u63d0\u9ad817.1%\uff0c\u8de8\u5e27\u5b9e\u4f53\u6301\u7eed\u6027\u589e\u52a013.7%\u3002", "conclusion": "\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u6545\u4e8b\u7cfb\u7edf\u4e2d\u5b9e\u4f53\u4e00\u81f4\u6027\u548c\u6545\u4e8b\u8fde\u8d2f\u6027\u3002"}}
{"id": "2507.07509", "pdf": "https://arxiv.org/pdf/2507.07509", "abs": "https://arxiv.org/abs/2507.07509", "authors": ["Yuanchen Shi", "Longyin Zhang", "Fang Kong"], "title": "Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "10pages,8 figures", "summary": "The growing need for psychological support due to increasing pressures has\nexposed the scarcity of relevant datasets, particularly in non-English\nlanguages. To address this, we propose a framework that leverages limited\nreal-world data and expert knowledge to fine-tune two large language models:\nDialog Generator and Dialog Modifier. The Generator creates large-scale\npsychological counseling dialogues based on predefined paths, which guide\nsystem response strategies and user interactions, forming the basis for\neffective support. The Modifier refines these dialogues to align with\nreal-world data quality. Through both automated and manual review, we construct\nthe Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K\ndialogues across 13 groups, 16 psychological problems, 13 causes, and 12\nsupport focuses. Additionally, we introduce the Comprehensive Agent Dialogue\nSupport System (CADSS), where a Profiler analyzes user characteristics, a\nSummarizer condenses dialogue history, a Planner selects strategies, and a\nSupporter generates empathetic responses. The experimental results of the\nStrategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate\nthat CADSS achieves state-of-the-art performance on both CPsDD and ESConv\ndatasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6709\u9650\u771f\u5b9e\u6570\u636e\u548c\u4e13\u5bb6\u77e5\u8bc6\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u4e2d\u6587\u5fc3\u7406\u652f\u6301\u5bf9\u8bdd\u6570\u636e\u96c6\uff08CPsDD\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u7efc\u5408\u4ee3\u7406\u5bf9\u8bdd\u652f\u6301\u7cfb\u7edf\uff08CADSS\uff09\uff0c\u5728\u7b56\u7565\u9884\u6d4b\u548c\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u5fc3\u7406\u652f\u6301\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u5fc3\u7406\u652f\u6301\u9700\u6c42\u3002", "method": "\u901a\u8fc7Dialog Generator\u548cDialog Modifier\u751f\u6210\u548c\u4f18\u5316\u5fc3\u7406\u8f85\u5bfc\u5bf9\u8bdd\uff0c\u6784\u5efaCPsDD\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1CADSS\u7cfb\u7edf\uff0c\u5305\u542bProfiler\u3001Summarizer\u3001Planner\u548cSupporter\u6a21\u5757\u3002", "result": "CPsDD\u5305\u542b68K\u5bf9\u8bdd\uff0c\u8986\u76d613\u7ec4\u300116\u79cd\u5fc3\u7406\u95ee\u9898\u300113\u79cd\u539f\u56e0\u548c12\u79cd\u652f\u6301\u91cd\u70b9\uff1bCADSS\u5728CPsDD\u548cESConv\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u5fc3\u7406\u652f\u6301\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.07374", "pdf": "https://arxiv.org/pdf/2507.07374", "abs": "https://arxiv.org/abs/2507.07374", "authors": ["Haotian Wang", "Aoran Xiao", "Xiaoqin Zhang", "Meng Yang", "Shijian Lu"], "title": "PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Generalizable depth completion enables the acquisition of dense metric depth\nmaps for unseen environments, offering robust perception capabilities for\nvarious downstream tasks. However, training such models typically requires\nlarge-scale datasets with metric depth labels, which are often labor-intensive\nto collect. This paper presents PacGDC, a label-efficient technique that\nenhances data diversity with minimal annotation effort for generalizable depth\ncompletion. PacGDC builds on novel insights into inherent ambiguities and\nconsistencies in object shapes and positions during 2D-to-3D projection,\nallowing the synthesis of numerous pseudo geometries for the same visual scene.\nThis process greatly broadens available geometries by manipulating scene scales\nof the corresponding depth maps. To leverage this property, we propose a new\ndata synthesis pipeline that uses multiple depth foundation models as scale\nmanipulators. These models robustly provide pseudo depth labels with varied\nscene scales, affecting both local objects and global layouts, while ensuring\nprojection consistency that supports generalization. To further diversify\ngeometries, we incorporate interpolation and relocation strategies, as well as\nunlabeled images, extending the data coverage beyond the individual use of\nfoundation models. Extensive experiments show that PacGDC achieves remarkable\ngeneralizability across multiple benchmarks, excelling in diverse scene\nsemantics/scales and depth sparsity/patterns under both zero-shot and few-shot\nsettings. Code: https://github.com/Wang-xjtu/PacGDC.", "AI": {"tldr": "PacGDC\u662f\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u6280\u672f\uff0c\u901a\u8fc7\u5408\u6210\u4f2a\u51e0\u4f55\u6570\u636e\u589e\u5f3a\u6df1\u5ea6\u8865\u5168\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u8865\u5168\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u75282D\u52303D\u6295\u5f71\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5408\u6210\u591a\u79cd\u4f2a\u51e0\u4f55\u6570\u636e\uff0c\u7ed3\u5408\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u548c\u63d2\u503c/\u91cd\u5b9a\u4f4d\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u5e94\u4e0d\u540c\u573a\u666f\u548c\u6df1\u5ea6\u7a00\u758f\u6027\u3002", "conclusion": "PacGDC\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u8865\u5168\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2507.07518", "pdf": "https://arxiv.org/pdf/2507.07518", "abs": "https://arxiv.org/abs/2507.07518", "authors": ["Mikey Elmers", "Koji Inoue", "Divesh Lala", "Tatsuya Kawahara"], "title": "Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems", "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "Turn-taking is a fundamental component of spoken dialogue, however\nconventional studies mostly involve dyadic settings. This work focuses on\napplying voice activity projection (VAP) to predict upcoming turn-taking in\ntriadic multi-party scenarios. The goal of VAP models is to predict the future\nvoice activity for each speaker utilizing only acoustic data. This is the first\nstudy to extend VAP into triadic conversation. We trained multiple models on a\nJapanese triadic dataset where participants discussed a variety of topics. We\nfound that the VAP trained on triadic conversation outperformed the baseline\nfor all models but that the type of conversation affected the accuracy. This\nstudy establishes that VAP can be used for turn-taking in triadic dialogue\nscenarios. Future work will incorporate this triadic VAP turn-taking model into\nspoken dialogue systems.", "AI": {"tldr": "\u7814\u7a76\u5c06\u8bed\u97f3\u6d3b\u52a8\u9884\u6d4b\uff08VAP\uff09\u5e94\u7528\u4e8e\u4e09\u5143\u5bf9\u8bdd\u4e2d\u7684\u8f6e\u6d41\u53d1\u8a00\u9884\u6d4b\uff0c\u53d1\u73b0\u4e09\u5143\u5bf9\u8bdd\u8bad\u7ec3\u7684VAP\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u7814\u7a76\u591a\u5173\u6ce8\u4e8c\u5143\u5bf9\u8bdd\uff0c\u800c\u4e09\u5143\u5bf9\u8bdd\u4e2d\u7684\u8f6e\u6d41\u53d1\u8a00\u9884\u6d4b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728\u65e5\u8bed\u4e09\u5143\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u591a\u4e2aVAP\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u58f0\u5b66\u6570\u636e\u9884\u6d4b\u672a\u6765\u8bed\u97f3\u6d3b\u52a8\u3002", "result": "\u4e09\u5143\u5bf9\u8bdd\u8bad\u7ec3\u7684VAP\u6a21\u578b\u5728\u6240\u6709\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5bf9\u8bdd\u7c7b\u578b\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "VAP\u53ef\u7528\u4e8e\u4e09\u5143\u5bf9\u8bdd\u4e2d\u7684\u8f6e\u6d41\u53d1\u8a00\u9884\u6d4b\uff0c\u672a\u6765\u5c06\u6574\u5408\u5230\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2507.07379", "pdf": "https://arxiv.org/pdf/2507.07379", "abs": "https://arxiv.org/abs/2507.07379", "authors": ["Hong Xu", "Shireen Y. Elhabian"], "title": "Adaptive Particle-Based Shape Modeling for Anatomical Surface Correspondence", "categories": ["cs.CV"], "comment": null, "summary": "Particle-based shape modeling (PSM) is a family of approaches that\nautomatically quantifies shape variability across anatomical cohorts by\npositioning particles (pseudo landmarks) on shape surfaces in a consistent\nconfiguration. Recent advances incorporate implicit radial basis function\nrepresentations as self-supervised signals to better capture the complex\ngeometric properties of anatomical structures. However, these methods still\nlack self-adaptivity -- that is, the ability to automatically adjust particle\nconfigurations to local geometric features of each surface, which is essential\nfor accurately representing complex anatomical variability. This paper\nintroduces two mechanisms to increase surface adaptivity while maintaining\nconsistent particle configurations: (1) a novel neighborhood correspondence\nloss to enable high adaptivity and (2) a geodesic correspondence algorithm that\nregularizes optimization to enforce geodesic neighborhood consistency. We\nevaluate the efficacy and scalability of our approach on challenging datasets,\nproviding a detailed analysis of the adaptivity-correspondence trade-off and\nbenchmarking against existing methods on surface representation accuracy and\ncorrespondence metrics.", "AI": {"tldr": "PSM\u65b9\u6cd5\u901a\u8fc7\u7c92\u5b50\u914d\u7f6e\u91cf\u5316\u5f62\u72b6\u53d8\u5f02\u6027\uff0c\u4f46\u7f3a\u4e4f\u81ea\u9002\u5e94\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u673a\u5236\u63d0\u5347\u9002\u5e94\u6027\u5e76\u4fdd\u6301\u4e00\u81f4\u6027\uff1a\u90bb\u57df\u5bf9\u5e94\u635f\u5931\u548c\u6d4b\u5730\u7ebf\u5bf9\u5e94\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709PSM\u65b9\u6cd5\u65e0\u6cd5\u81ea\u52a8\u8c03\u6574\u7c92\u5b50\u914d\u7f6e\u4ee5\u9002\u5e94\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\uff0c\u9650\u5236\u4e86\u590d\u6742\u89e3\u5256\u53d8\u5f02\u7684\u51c6\u786e\u8868\u793a\u3002", "method": "\u5f15\u5165\u90bb\u57df\u5bf9\u5e94\u635f\u5931\u548c\u6d4b\u5730\u7ebf\u5bf9\u5e94\u7b97\u6cd5\uff0c\u63d0\u5347\u9002\u5e94\u6027\u5e76\u4fdd\u6301\u7c92\u5b50\u914d\u7f6e\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5206\u6790\u4e86\u9002\u5e94\u6027\u4e0e\u5bf9\u5e94\u6027\u7684\u6743\u8861\u3002", "conclusion": "\u65b0\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86PSM\u7684\u9002\u5e94\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7c92\u5b50\u914d\u7f6e\u7684\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.07539", "pdf": "https://arxiv.org/pdf/2507.07539", "abs": "https://arxiv.org/abs/2507.07539", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Aboubacar Tuo", "Adrian Popescu"], "title": "CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text", "categories": ["cs.CL", "cs.AI"], "comment": "Notebook for the CheckThat! Lab at CLEF 2025", "summary": "This paper presents a competitive approach to multilingual subjectivity\ndetection using large language models (LLMs) with few-shot prompting. We\nparticipated in Task 1: Subjectivity of the CheckThat! 2025 evaluation\ncampaign. We show that LLMs, when paired with carefully designed prompts, can\nmatch or outperform fine-tuned smaller language models (SLMs), particularly in\nnoisy or low-quality data settings. Despite experimenting with advanced prompt\nengineering techniques, such as debating LLMs and various example selection\nstrategies, we found limited benefit beyond well-crafted standard few-shot\nprompts. Our system achieved top rankings across multiple languages in the\nCheckThat! 2025 subjectivity detection task, including first place in Arabic\nand Polish, and top-four finishes in Italian, English, German, and multilingual\ntracks. Notably, our method proved especially robust on the Arabic dataset,\nlikely due to its resilience to annotation inconsistencies. These findings\nhighlight the effectiveness and adaptability of LLM-based few-shot learning for\nmultilingual sentiment tasks, offering a strong alternative to traditional\nfine-tuning, particularly when labeled data is scarce or inconsistent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5c11\u6837\u672c\u63d0\u793a\u7684\u591a\u8bed\u8a00\u4e3b\u89c2\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728CheckThat! 2025\u8bc4\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u566a\u58f0\u6216\u4f4e\u8d28\u91cf\u6570\u636e\u73af\u5883\u4e2d\uff0c\u662f\u5426\u80fd\u591f\u8d85\u8d8a\u4f20\u7edf\u5fae\u8c03\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u3002", "method": "\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5c11\u6837\u672c\u63d0\u793a\uff0c\u7ed3\u5408LLMs\uff0c\u5e76\u5c1d\u8bd5\u4e86\u8fa9\u8bba\u5f0f\u63d0\u793a\u548c\u591a\u79cd\u793a\u4f8b\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728CheckThat! 2025\u8bc4\u6d4b\u4e2d\uff0c\u7cfb\u7edf\u5728\u591a\u4e2a\u8bed\u8a00\uff08\u5982\u963f\u62c9\u4f2f\u8bed\u548c\u6ce2\u5170\u8bed\uff09\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5176\u4ed6\u8bed\u8a00\u4e5f\u8fdb\u5165\u524d\u56db\u3002\u5c24\u5176\u5728\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u4e0a\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "LLM-based\u5c11\u6837\u672c\u5b66\u4e60\u5728\u591a\u8bed\u8a00\u60c5\u611f\u4efb\u52a1\u4e2d\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\uff0c\u4e3a\u4f20\u7edf\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u529b\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u6216\u4e0d\u4e00\u81f4\u65f6\u3002"}}
{"id": "2507.07381", "pdf": "https://arxiv.org/pdf/2507.07381", "abs": "https://arxiv.org/abs/2507.07381", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Wells", "Mohamed Reda Bouadjenek", "Richard Dazeley", "Sunil Aryal"], "title": "Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting in Videos", "categories": ["cs.CV"], "comment": null, "summary": "Precise Event Spotting (PES) in sports videos requires frame-level\nrecognition of fine-grained actions from single-camera footage. Existing PES\nmodels typically incorporate lightweight temporal modules such as Gate Shift\nModule (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with\ntemporal context. However, these modules are limited in both temporal receptive\nfield and spatial adaptability. We propose a Multi-Scale Attention Gate Shift\nModule (MSAGSM) that enhances GSM with multi-scale temporal dilations and\nmulti-head spatial attention, enabling efficient modeling of both short- and\nlong-term dependencies while focusing on salient regions. MSAGSM is a\nlightweight plug-and-play module that can be easily integrated with various 2D\nbackbones. To further advance the field, we introduce the Table Tennis\nAustralia (TTA) dataset-the first PES benchmark for table tennis-containing\nover 4800 precisely annotated events. Extensive experiments across five PES\nbenchmarks demonstrate that MSAGSM consistently improves performance with\nminimal overhead, setting new state-of-the-art results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u95e8\u79fb\u4f4d\u6a21\u5757\uff08MSAGSM\uff09\uff0c\u7528\u4e8e\u589e\u5f3a\u4f53\u80b2\u89c6\u9891\u4e2d\u7684\u7cbe\u786e\u4e8b\u4ef6\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6TTA\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709PES\u6a21\u578b\u7684\u65f6\u7a7a\u6a21\u5757\u5728\u65f6\u95f4\u611f\u53d7\u91ce\u548c\u7a7a\u95f4\u9002\u5e94\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faMSAGSM\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u65f6\u95f4\u6269\u5f20\u548c\u591a\u5934\u7a7a\u95f4\u6ce8\u610f\u529b\uff0c\u589e\u5f3a\u65f6\u7a7a\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u4e94\u4e2aPES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMSAGSM\u5747\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "MSAGSM\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u80fd\u6709\u6548\u63d0\u5347\u4f53\u80b2\u89c6\u9891\u4e8b\u4ef6\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.07543", "pdf": "https://arxiv.org/pdf/2507.07543", "abs": "https://arxiv.org/abs/2507.07543", "authors": ["Chen Amiraz", "Yaroslav Fyodorov", "Elad Haramaty", "Zohar Karnin", "Liane Lewin-Eytan"], "title": "The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Cross-lingual retrieval-augmented generation (RAG) is a critical capability\nfor retrieving and generating answers across languages. Prior work in this\ncontext has mostly focused on generation and relied on benchmarks derived from\nopen-domain sources, most notably Wikipedia. In such settings, retrieval\nchallenges often remain hidden due to language imbalances, overlap with\npretraining data, and memorized content. To address this gap, we study\nArabic-English RAG in a domain-specific setting using benchmarks derived from\nreal-world corporate datasets. Our benchmarks include all combinations of\nlanguages for the user query and the supporting document, drawn independently\nand uniformly at random. This enables a systematic study of multilingual\nretrieval behavior.\n  Our findings reveal that retrieval is a critical bottleneck in cross-lingual\ndomain-specific scenarios, with significant performance drops occurring when\nthe user query and supporting document languages differ. A key insight is that\nthese failures stem primarily from the retriever's difficulty in ranking\ndocuments across languages. Finally, we propose a simple retrieval strategy\nthat addresses this source of failure by enforcing equal retrieval from both\nlanguages, resulting in substantial improvements in cross-lingual and overall\nperformance. These results highlight meaningful opportunities for improving\nmultilingual retrieval, particularly in practical, real-world RAG applications.", "AI": {"tldr": "\u7814\u7a76\u8de8\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u7279\u5b9a\u9886\u57df\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u68c0\u7d22\u662f\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u8de8\u8bed\u8a00\u68c0\u7d22\u5728\u7279\u5b9a\u9886\u57df\u4e2d\u7684\u9690\u85cf\u95ee\u9898\uff0c\u5982\u8bed\u8a00\u4e0d\u5e73\u8861\u548c\u9884\u8bad\u7ec3\u6570\u636e\u91cd\u53e0\u3002", "method": "\u4f7f\u7528\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u7684\u771f\u5b9e\u4f01\u4e1a\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e0d\u540c\u8bed\u8a00\u7ec4\u5408\u7684\u68c0\u7d22\u884c\u4e3a\u3002", "result": "\u68c0\u7d22\u662f\u8de8\u8bed\u8a00\u573a\u666f\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u63d0\u51fa\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u6539\u8fdb\u8de8\u8bed\u8a00\u68c0\u7d22\u5728\u73b0\u5b9eRAG\u5e94\u7528\u4e2d\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.07393", "pdf": "https://arxiv.org/pdf/2507.07393", "abs": "https://arxiv.org/abs/2507.07393", "authors": ["Jinseong Kim", "Junghoon Song", "Gyeongseon Baek", "Byeongjoon Noh"], "title": "KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware Representation in Videos", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 2 figures,", "summary": "We propose \\textbf{KeyRe-ID}, a keypoint-guided video-based person\nre-identification framework consisting of global and local branches that\nleverage human keypoints for enhanced spatiotemporal representation learning.\nThe global branch captures holistic identity semantics through\nTransformer-based temporal aggregation, while the local branch dynamically\nsegments body regions based on keypoints to generate fine-grained, part-aware\nfeatures. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate\nstate-of-the-art performance, achieving 91.73\\% mAP and 97.32\\% Rank-1 accuracy\non MARS, and 96.00\\% Rank-1 and 100.0\\% Rank-5 accuracy on iLIDS-VID. The code\nfor this work will be publicly available on GitHub upon publication.", "AI": {"tldr": "KeyRe-ID\u662f\u4e00\u4e2a\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u89c6\u9891\u884c\u4eba\u91cd\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u5206\u652f\u589e\u5f3a\u65f6\u7a7a\u8868\u793a\u5b66\u4e60\uff0c\u5728MARS\u548ciLIDS-VID\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u4eba\u4f53\u5173\u952e\u70b9\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u65f6\u7a7a\u8868\u793a\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u5168\u5c40\u5206\u652f\uff08Transformer\u65f6\u5e8f\u805a\u5408\uff09\u548c\u5c40\u90e8\u5206\u652f\uff08\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u52a8\u6001\u8eab\u4f53\u533a\u57df\u5206\u5272\uff09\uff0c\u751f\u6210\u7ec6\u7c92\u5ea6\u7279\u5f81\u3002", "result": "\u5728MARS\u4e0a\u8fbe\u523091.73% mAP\u548c97.32% Rank-1\uff0c\u5728iLIDS-VID\u4e0a\u8fbe\u523096.00% Rank-1\u548c100.0% Rank-5\u3002", "conclusion": "KeyRe-ID\u901a\u8fc7\u5173\u952e\u70b9\u5f15\u5bfc\u7684\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2507.07562", "pdf": "https://arxiv.org/pdf/2507.07562", "abs": "https://arxiv.org/abs/2507.07562", "authors": ["Jierun Chen", "Tiezheng Yu", "Haoli Bai", "Lewei Yao", "Jiannan Wu", "Kaican Li", "Fei Mi", "Chaofan Tao", "Lei Zhu", "Manyi Zhang", "Xiaohui Li", "Lu Hou", "Lifeng Shang", "Qun Liu"], "title": "The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large vision-language models (VLMs) increasingly adopt post-training\ntechniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and\nreinforcement learning (RL) to elicit sophisticated reasoning. While these\nmethods exhibit synergy in language-only models, their joint effectiveness in\nVLMs remains uncertain. We present a systematic investigation into the distinct\nroles and interplay of long-CoT SFT and RL across multiple multimodal reasoning\nbenchmarks. We find that SFT improves performance on difficult questions by\nin-depth, structured reasoning, but introduces verbosity and degrades\nperformance on simpler ones. In contrast, RL promotes generalization and\nbrevity, yielding consistent improvements across all difficulty levels, though\nthe improvements on the hardest questions are less prominent compared to SFT.\nSurprisingly, combining them through two-staged, interleaved, or progressive\ntraining strategies, as well as data mixing and model merging, all fails to\nproduce additive benefits, instead leading to trade-offs in accuracy, reasoning\nstyle, and response length. This ``synergy dilemma'' highlights the need for\nmore seamless and adaptive approaches to unlock the full potential of combined\npost-training techniques for reasoning VLMs.", "AI": {"tldr": "\u7814\u7a76\u4e86\u957f\u94fe\u601d\u7ef4\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u534f\u540c\u6548\u679c\uff0c\u53d1\u73b0\u4e24\u8005\u7ed3\u5408\u672a\u80fd\u4ea7\u751f\u53e0\u52a0\u4f18\u52bf\uff0c\u53cd\u800c\u5b58\u5728\u6743\u8861\u3002", "motivation": "\u63a2\u7d22\u957f\u94fe\u601d\u7ef4SFT\u548cRL\u5728\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8054\u5408\u6709\u6548\u6027\u53ca\u5176\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790SFT\u548cRL\u7684\u72ec\u7acb\u53ca\u8054\u5408\u6548\u679c\u3002", "result": "SFT\u63d0\u5347\u590d\u6742\u95ee\u9898\u6027\u80fd\u4f46\u964d\u4f4e\u7b80\u5355\u95ee\u9898\u8868\u73b0\uff1bRL\u63d0\u9ad8\u6cdb\u5316\u6027\u4f46\u590d\u6742\u95ee\u9898\u6539\u8fdb\u6709\u9650\uff1b\u7ed3\u5408\u65b9\u6cd5\u672a\u80fd\u5b9e\u73b0\u53e0\u52a0\u4f18\u52bf\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\u4ee5\u5145\u5206\u53d1\u6325\u8054\u5408\u540e\u8bad\u7ec3\u6280\u672f\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.07394", "pdf": "https://arxiv.org/pdf/2507.07394", "abs": "https://arxiv.org/abs/2507.07394", "authors": ["Zhimin Zhang", "Bi'an Du", "Caoyuan Ma", "Zheng Wang", "Wei Hu"], "title": "Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Animal motion embodies species-specific behavioral habits, making the\ntransfer of motion across categories a critical yet complex task for\napplications in animation and virtual reality. Existing motion transfer\nmethods, primarily focused on human motion, emphasize skeletal alignment\n(motion retargeting) or stylistic consistency (motion style transfer), often\nneglecting the preservation of distinct habitual behaviors in animals. To\nbridge this gap, we propose a novel habit-preserved motion transfer framework\nfor cross-category animal motion. Built upon a generative framework, our model\nintroduces a habit-preservation module with category-specific habit encoder,\nallowing it to learn motion priors that capture distinctive habitual\ncharacteristics. Furthermore, we integrate a large language model (LLM) to\nfacilitate the motion transfer to previously unobserved species. To evaluate\nthe effectiveness of our approach, we introduce the DeformingThings4D-skl\ndataset, a quadruped dataset with skeletal bindings, and conduct extensive\nexperiments and quantitative analyses, which validate the superiority of our\nproposed model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u7c7b\u522b\u52a8\u7269\u8fd0\u52a8\u8fc1\u79fb\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u4fdd\u7559\u7269\u79cd\u7279\u6709\u7684\u4e60\u60ef\u884c\u4e3a\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5904\u7406\u672a\u89c2\u5bdf\u5230\u7684\u7269\u79cd\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u8fc1\u79fb\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u4eba\u7c7b\u8fd0\u52a8\uff0c\u5ffd\u89c6\u4e86\u52a8\u7269\u7279\u6709\u7684\u4e60\u60ef\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u7559\u8fd9\u4e9b\u884c\u4e3a\u7684\u8de8\u7c7b\u522b\u8fc1\u79fb\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u751f\u6210\u6846\u67b6\uff0c\u5f15\u5165\u4e60\u60ef\u4fdd\u7559\u6a21\u5757\u548c\u7c7b\u522b\u7279\u5b9a\u4e60\u60ef\u7f16\u7801\u5668\uff0c\u7ed3\u5408LLM\u5904\u7406\u65b0\u7269\u79cd\u3002", "result": "\u5728DeformingThings4D-skl\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5b9e\u9a8c\u548c\u5b9a\u91cf\u5206\u6790\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u7c7b\u522b\u52a8\u7269\u8fd0\u52a8\u8fc1\u79fb\uff0c\u5e76\u4fdd\u7559\u4e86\u4e60\u60ef\u884c\u4e3a\uff0c\u4e3a\u52a8\u753b\u548c\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
