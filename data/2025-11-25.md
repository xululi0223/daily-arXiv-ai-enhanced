<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.CV](#cs.CV) [Total: 320]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 7]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.AI](#cs.AI) [Total: 7]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 43]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.NE](#cs.NE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.RO](#cs.RO) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering](https://arxiv.org/abs/2511.17559)
*Gyubok Lee,Woosog Chay,Edward Choi*

Main category: cs.CL

TL;DR: SCARE是一个用于评估电子健康记录问答系统中后验安全机制的基准测试，专注于问题可回答性分类和SQL查询验证/修正的联合任务。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的临床环境中部署文本到SQL模型存在挑战，错误的SQL查询可能危及患者护理。现有工作缺乏统一的基准来评估独立的后验验证机制。

Method: 构建包含4,200个问题-SQL查询-期望输出三元组的基准数据集，基于MIMIC-III、MIMIC-IV和eICU数据库，涵盖7种不同文本到SQL模型生成的多样化查询。

Result: 实验揭示了问题分类和SQL错误修正之间的关键权衡，突出了主要挑战并为未来研究指明了方向。

Conclusion: SCARE基准填补了电子健康记录问答系统后验安全层评估的空白，为安全部署提供了重要工具。

Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of text-to-SQL models that allow clinicians to query structured data stored in Electronic Health Records (EHRs) using natural language. However, deploying these models for EHR question answering (QA) systems in safety-critical clinical environments remains challenging: incorrect SQL queries-whether caused by model errors or problematic user inputs-can undermine clinical decision-making and jeopardize patient care. While prior work has mainly focused on improving SQL generation accuracy or filtering questions before execution, there is a lack of a unified benchmark for evaluating independent post-hoc verification mechanisms (i.e., a component that inspects and validates the generated SQL before execution), which is crucial for safe deployment. To fill this gap, we introduce SCARE, a benchmark for evaluating methods that function as a post-hoc safety layer in EHR QA systems. SCARE evaluates the joint task of (1) classifying question answerability (i.e., determining whether a question is answerable, ambiguous, or unanswerable) and (2) verifying or correcting candidate SQL queries. The benchmark comprises 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in the MIMIC-III, MIMIC-IV, and eICU databases. It covers a diverse set of questions and corresponding candidate SQL queries generated by seven different text-to-SQL models, ensuring a realistic and challenging evaluation. Using SCARE, we benchmark a range of approaches-from two-stage methods to agentic frameworks. Our experiments reveal a critical trade-off between question classification and SQL error correction, highlighting key challenges and outlining directions for future research.

</details>


### [2] [$A^3$: Attention-Aware Accurate KV Cache Fusion for Fast Large Language Model Serving](https://arxiv.org/abs/2511.17560)
*Yuechi Zhou,Yi Su,Jianxin Zhang,Juntao Li,Qingrong Xia,Zhefeng Wang,Xinyu Duan,Baoxing Huai*

Main category: cs.CL

TL;DR: 提出了A³算法，通过基于问题相关性预计算和选择性融合文本块的KV缓存，在减少解码延迟的同时保持最佳任务性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能够处理长上下文，但解码延迟和内存开销仍然很大。现有的KV缓存重用方法存在性能下降问题，重新计算的token往往与问题最相关的上下文段不匹配。

Method: 提出注意力感知的精确KV缓存融合算法(A³)，基于文本块与问题的相关性预计算并选择性融合KV缓存，实现准确集成且计算开销最小。

Result: 在各种基准测试和LLM上的实验表明，A³相比四个基线方法实现了最佳任务性能，同时将首token时间(TTFT)减少了2倍。

Conclusion: A³算法有效解决了长上下文处理中的延迟和性能问题，通过精确的KV缓存融合实现了高效的长文本处理。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in processing long contexts, enabling them to tackle tasks involving long textual inputs such as multi-turn conversations, legal documents, or retrieved documents in Retrieval-Augmented Generation (RAG) systems. However, despite their ability to handle long sequences, the resulting decoding latency and memory overhead remain substantial, posing challenges for real-world deployment. Recent advances in KV Cache reuse have shown potential to mitigate these costs, but still suffer from notable performance degradation. To address this issue, we conduct an in-depth investigation of recomputation-based reuse methods and observe that the recomputed tokens often fail to align with the context segments most relevant to the question. This misalignment hinders proper updates to the critical contextual representations. Therefore, we propose the $\textbf{A}$ttention-$\textbf{A}$ware $\textbf{A}$ccurate KV Cache Fusion algorithm ($A^3$), which precomputes and selectively fuses the KV Cache of text chunks based on their relevance to the question, achieving accurate integration with minimal computational overhead. Extensive experiments on various benchmarks and LLMs demonstrate that $A^3$ achieves the best task performance compared to four baselines while reducing the time-to-first-token (TTFT) by 2$\times$.

</details>


### [3] [LexInstructEval: Lexical Instruction Following Evaluation for Large Language Models](https://arxiv.org/abs/2511.17561)
*Huimin Ren,Yan Liang,Baiqiao Su,Chaobo Sun,Hengtong Lu,Kaike Zhang,Chen Wei*

Main category: cs.CL

TL;DR: LexInstructEval是一个新的基准测试和评估框架，用于评估大语言模型在细粒度词汇指令遵循方面的能力，通过基于规则的语法将复杂指令解构为<过程、关系、值>三元组，实现客观验证。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM遵循复杂词汇指令能力的方法存在局限性，要么依赖主观且昂贵的人工评估，要么使用有偏见和不可靠的自动LLM评判系统，而现有的程序化基准测试缺乏表达复杂组合约束的能力。

Method: 构建基于规则的形式化语法，将复杂指令解构为<过程、关系、值>三元组；通过多阶段、人在回路的数据生成流程创建多样化数据集；使用透明、程序化的引擎进行客观验证。

Result: 开发了LexInstructEval基准测试框架，包含数据集和开源评估工具，支持对LLM可控性和可靠性的进一步研究。

Conclusion: LexInstructEval为解决LLM细粒度词汇指令遵循评估的挑战提供了一个系统化、客观的解决方案，有助于提升LLM的可控性和可靠性研究。

Abstract: The ability of Large Language Models (LLMs) to precisely follow complex and fine-grained lexical instructions is a cornerstone of their utility and controllability. However, evaluating this capability remains a significant challenge. Current methods either rely on subjective and costly human evaluation or on automated LLM-as-a-judge systems, which suffer from inherent biases and unreliability. Existing programmatic benchmarks, while objective, often lack the expressiveness to test intricate, compositional constraints at a granular level. To address these limitations, we introduce LexInstructEval, a new benchmark and evaluation framework for fine-grained lexical instruction following. Our framework is built upon a formal, rule-based grammar that deconstructs complex instructions into a canonical <Procedure, Relation, Value> triplet. This grammar enables the systematic generation of a diverse dataset through a multi-stage, human-in-the-loop pipeline and facilitates objective verification via a transparent, programmatic engine. We release our dataset and open-source evaluation tools to facilitate further research into the controllability and reliability of LLMs.

</details>


### [4] [ChineseErrorCorrector3-4B: State-of-the-Art Chinese Spelling and Grammar Corrector](https://arxiv.org/abs/2511.17562)
*Wei Tian,YuhaoZhou*

Main category: cs.CL

TL;DR: 基于Qwen3-4B开发的中文拼写和语法纠错统一模型ChineseErrorCorrector3-4B，在多个权威基准测试中取得最优性能


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的中文拼写和语法纠错模型，以提升中文文本纠错的整体性能

Method: 基于Qwen3-4B构建统一的中文拼写和语法纠错模型

Result: 在SIGHAN-2015、EC-LAW、MCSC和NaCGEC等基准测试中，F1和F0.5分数显著超越现有公开模型，在拼写和语法纠错任务中均排名第一

Conclusion: ChineseErrorCorrector3-4B在中文文本纠错任务中表现出色，达到了最先进的性能水平

Abstract: This paper introduces ChineseErrorCorrector3-4B, a unified model for Chinese spelling and grammatical error correction based on Qwen3-4B. The model demonstrates outstanding performance in general text correction tasks and achieves state-of-the-art results in both spelling correction (CSC) and grammatical correction (CGC). On several authoritative benchmark datasets -- including SIGHAN-2015, EC-LAW, MCSC, and NaCGEC -- the model's F1 and F0.5 scores significantly surpass existing publicly available models, ranking first in both spelling and grammatical error correction tasks.

</details>


### [5] [Generative Caching for Structurally Similar Prompts and Responses](https://arxiv.org/abs/2511.17565)
*Sarthak Chakraborty,Suman Nath,Xuchao Zhang,Chetan Bansal,Indranil Gupta*

Main category: cs.CL

TL;DR: 提出了一种生成式缓存方法，能够为结构相似的提示生成变体感知的响应，提高缓存命中率并减少执行延迟


<details>
  <summary>Details</summary>
Motivation: 在重复性工作流和智能体场景中，提示往往具有相似结构但存在微小变化，精确匹配无法处理这种情况，而语义缓存可能忽略关键差异导致错误响应

Method: 识别相似提示结构中的可重用响应模式，为新请求合成定制化输出

Result: 在无提示重复的数据集上达到83%缓存命中率且错误命中率最低；在智能体工作流中，相比标准提示匹配提高约20%缓存命中率，减少约34%端到端执行延迟

Conclusion: 该方法有效解决了结构相似提示的缓存问题，显著提升了LLM在重复性任务中的性能表现

Abstract: Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce \ourmethod{}, a generative cache that produces variation-aware responses for structurally similar prompts. \ourmethod{} identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that \ourmethod{} achieves 83\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\sim$20\% and reduces end-to-end execution latency by $\sim$34\% compared to standard prompt matching.

</details>


### [6] [Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs](https://arxiv.org/abs/2511.17572)
*Patrick Gerard,Aiden Chang,Svitlana Volkova*

Main category: cs.CL

TL;DR: 本文提出了一个测试认知立场转移的框架，发现在即使删除事件知识后，对齐的大语言模型仍能保持社区特定的行为模式，证明了对齐编码了超越表面模仿的结构化、可泛化行为。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在对齐到特定在线社区时，是表现出可泛化的行为模式来反映该社区的态度和对新不确定性的反应，还是仅仅从训练数据中回忆模式。

Method: 引入认知立场转移测试框架：通过多重探针验证的事件知识定向删除，然后评估模型在无知状态下是否仍能重现社区的有机反应模式。使用俄乌军事讨论和美国党派推特数据进行实验。

Result: 即使经过激进的事实删除，对齐的LLMs仍保持稳定的、社区特定的处理不确定性的行为模式。

Conclusion: 对齐编码了超越表面模仿的结构化、可泛化行为，该框架为检测在无知状态下持续存在的行为偏见提供了系统方法，有助于实现更安全、更透明的LLM部署。

Abstract: When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.

</details>


### [7] [Random Text, Zipf's Law, Critical Length,and Implications for Large Language Models](https://arxiv.org/abs/2511.17575)
*Vladimir Berman*

Main category: cs.CL

TL;DR: 该论文提出了一个完全非语言学的文本模型，通过独立抽取有限字母表加空格符号来研究文本结构。在仅假设组合学和分割的情况下，推导出几何分布的字长、临界字长以及Zipf型秩频分布。


<details>
  <summary>Details</summary>
Motivation: 为自然语言和大语言模型中的词汇统计提供结构基础的空模型，证明Zipf类模式可以纯粹从组合学和分割中产生，无需优化原则或语言组织。

Method: 使用有限字母表加空格符号的独立抽取模型，定义词为最大非空格符号块。通过组合数学和优惠券收集者论证推导结构结果。

Result: 推导出字长服从几何分布，得到临界字长k*，获得Zipf型秩频律p(r)∝r^{-α}，其中指数由字母表大小和空格概率显式确定。

Conclusion: 该模型为语言统计提供了结构基础的空模型，表明Zipf类模式可以纯粹从组合学和分割中产生，有助于澄清哪些现象需要超出随机文本结构的深层解释。

Abstract: We study a deliberately simple, fully non-linguistic model of text: a sequence of independent draws from a finite alphabet of letters plus a single space symbol. A word is defined as a maximal block of non-space symbols. Within this symbol-level framework, which assumes no morphology, syntax, or semantics, we derive several structural results. First, word lengths follow a geometric distribution governed solely by the probability of the space symbol. Second, the expected number of words of a given length, and the expected number of distinct words of that length, admit closed-form expressions based on a coupon-collector argument. This yields a critical word length k* at which word types transition from appearing many times on average to appearing at most once. Third, combining the exponential growth of the number of possible strings of length k with the exponential decay of the probability of each string, we obtain a Zipf-type rank-frequency law p(r) proportional to r^{-alpha}, with an exponent determined explicitly by the alphabet size and the space probability.
  Our contribution is twofold. Mathematically, we give a unified derivation linking word lengths, vocabulary growth, critical length, and rank-frequency structure in a single explicit model. Conceptually, we argue that this provides a structurally grounded null model for both natural-language word statistics and token statistics in large language models. The results show that Zipf-like patterns can arise purely from combinatorics and segmentation, without optimization principles or linguistic organization, and help clarify which phenomena require deeper explanation beyond random-text structure.

</details>


### [8] [Computational frame analysis revisited: On LLMs for studying news coverage](https://arxiv.org/abs/2511.17746)
*Sharaj Kunjar,Alyssa Hasegawa Smith,Tyler R Mckenzie,Rushali Mohbe,Samuel V Scarpino,Brooke Foucault Welles*

Main category: cs.CL

TL;DR: 评估生成式LLM在媒体框架分析中的有效性，发现它们在某些情况下被小型语言模型超越，且始终需要人工验证来确定合适的模型选择。


<details>
  <summary>Details</summary>
Motivation: 研究生成式LLM（如GPT和Claude）作为内容分析工具在媒体框架识别中的效果，并与传统计算方法（词袋模型、编码器转换器）和人工编码进行比较。

Method: 使用新颖的金标准数据集，系统地评估生成式LLM、词袋模型、编码器转换器和人工编码在分析2022年美国Mpox疫情新闻报道框架时的表现。

Result: 生成式LLM在某些应用中表现出潜力，但总体上被人工编码员超越，在某些情况下甚至被小型语言模型超越。不同方法的适用性取决于具体任务性质。

Conclusion: 支持方法论多元化方法，提出了计算框架分析的路线图，建议研究人员利用这些方法的互补性来协同使用。

Abstract: Computational approaches have previously shown various promises and pitfalls when it comes to the reliable identification of media frames. Generative LLMs like GPT and Claude are increasingly being used as content analytical tools, but how effective are they for frame analysis? We address this question by systematically evaluating them against their computational predecessors: bag-of-words models and encoder-only transformers; and traditional manual coding procedures. Our analysis rests on a novel gold standard dataset that we inductively and iteratively developed through the study, investigating six months of news coverage of the US Mpox epidemic of 2022. While we discover some potential applications for generative LLMs, we demonstrate that they were consistently outperformed by manual coders, and in some instances, by smaller language models. Some form of human validation was always necessary to determine appropriate model choice. Additionally, by examining how the suitability of various approaches depended on the nature of different tasks that were part of our frame analytical workflow, we provide insights as to how researchers may leverage the complementarity of these approaches to use them in tandem. We conclude by endorsing a methodologically pluralistic approach and put forth a roadmap for computational frame analysis for researchers going forward.

</details>


### [9] [PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese](https://arxiv.org/abs/2511.17808)
*Thales Sales Almeida,Rodrigo Nogueira,Hélio Pedrini*

Main category: cs.CL

TL;DR: PoETa v2是迄今为止对葡萄牙语LLMs最广泛的评估，包含40多个任务的基准测试，评估了20多个模型，揭示了计算投入和语言特定适应对葡萄牙语性能的影响。


<details>
  <summary>Details</summary>
Motivation: LLMs在不同语言和文化背景下的性能存在显著差异，需要系统评估多种语言。葡萄牙语作为重要语言，缺乏全面的评估基准。

Method: 引入PoETa v2基准套件，包含40多个葡萄牙语任务，评估了20多个涵盖不同训练规模和计算资源的模型，并与英语任务进行对比分析。

Result: 研究揭示了计算投资和语言特定适应对葡萄牙语性能的影响，同时分析了与英语任务相比的性能差距。

Conclusion: PoETa v2为葡萄牙语语言建模和评估的未来研究奠定了基础，基准测试已在GitHub上开源。

Abstract: Large Language Models (LLMs) exhibit significant variations in performance across linguistic and cultural contexts, underscoring the need for systematic evaluation in diverse languages. In this work, we present the most extensive evaluation of LLMs for the Portuguese language to date. Leveraging our newly introduced PoETa v2 benchmark -- a comprehensive suite of over 40 tasks in Portuguese -- we assess more than 20 models covering a broad spectrum of training scales and computational resources. Our study reveals how computational investment and language-specific adaptation impact performance in Portuguese, while also analyzing performance gaps in comparison to equivalent tasks in English. Through this benchmark and analysis, PoETa v2 lays the groundwork for future research on Portuguese language modeling and evaluation. The benchmark is available at https://github.com/PoETaV2/PoETaV2.

</details>


### [10] [Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation](https://arxiv.org/abs/2511.17813)
*Scott Merrill,Shashank Srivastava*

Main category: cs.CL

TL;DR: 本文提出了一个可复现的流程，将公开的Zoom录音转换为带有说话者身份、人物档案和语用行为标签的转录本，并发布了三个地方政府审议数据集。使用这种"行为感知"数据微调LLMs，在困惑度上降低了67%，说话者保真度和真实性的分类器性能指标几乎翻倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型为模拟多方审议提供了机会，但由于缺乏说话者归因数据，现实建模仍然受限。自动语音识别产生的转录本使用匿名说话者标签，无法捕捉一致的人类行为。

Method: 开发了一个可复现的流程，将公开的Zoom录音转换为带有说话者身份、人物档案和语用行为标签（如[propose_motion]）的转录本，并发布了三个地方政府审议数据集。使用这种"行为感知"数据微调LLMs来建模特定参与者。

Result: 使用行为感知数据微调LLMs产生了67%的困惑度降低，说话者保真度和真实性的分类器性能指标几乎翻倍。图灵式人类评估显示，模拟结果通常与真实审议无法区分。

Conclusion: 该方法为复杂现实公民模拟提供了一种实用且可扩展的方法，能够产生高度逼真的多方审议模拟。

Abstract: Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this "action-aware" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.

</details>


### [11] [A superpersuasive autonomous policy debating system](https://arxiv.org/abs/2511.17854)
*Allen Roush,Devin Gonier,John Hines,Judah Goldfeder,Philippe Martin Wyder,Sanjay Basu,Ravid Shwartz Ziv*

Main category: cs.CL

TL;DR: DeepDebater是一个能够参与并赢得完整政策辩论的自主AI系统，采用分层多智能体架构，结合大规模辩论证据库，生成完整辩论内容并通过AI语音和动画呈现。


<details>
  <summary>Details</summary>
Motivation: 解决AI在复杂、基于证据且具有战略适应性的说服能力方面的重大挑战，超越之前简化的辩论系统，实现完整的政策辩论参与。

Method: 采用分层多智能体工作流架构，LLM驱动的智能体团队协作执行辩论任务，使用大规模辩论证据库进行迭代检索、综合和自校正，生成完整辩论内容。

Result: 在初步评估中，DeepDebater产生质量更高的论证组件，在模拟辩论中持续获胜，辩论专家也更偏好其构建的论点、证据和案例。

Conclusion: DeepDebater展示了AI在复杂说服任务中的显著进步，支持全自主和混合人机操作模式，为AI辩论能力设定了新标准。

Abstract: The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main

</details>


### [12] [Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction](https://arxiv.org/abs/2511.17908)
*Debashish Chakraborty,Eugene Yang,Daniel Khashabi,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 本文提出了一种基于共形预测的覆盖控制过滤框架，用于RAG系统中的上下文工程，能在保证相关证据召回率的同时有效减少上下文长度。


<details>
  <summary>Details</summary>
Motivation: RAG系统在长文本或噪声文本中准确率下降，现有预生成过滤器缺乏统计控制机制，无法保证相关证据的保留。

Method: 使用共形预测框架，结合嵌入和LLM评分函数，在NeuCLIR和RAGTIME数据集上进行测试，实现覆盖控制的上下文过滤。

Result: 共形过滤始终达到目标覆盖率，将保留上下文减少2-3倍，在严格过滤下ARGUE F1分数提高，表明丢弃内容多为冗余或无关。

Conclusion: 共形预测为RAG提供了可靠、覆盖控制的上下文缩减方法，是一种模型无关且原则性的上下文工程方法。

Abstract: Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.

</details>


### [13] [L2V-CoT: Cross-Modal Transfer of Chain-of-Thought Reasoning via Latent Intervention](https://arxiv.org/abs/2511.17910)
*Yuliang Zhan,Xinyu Tang,Han Wan,Jian Li,Ji-Rong Wen,Hao Sun*

Main category: cs.CL

TL;DR: 本文提出L2V-CoT方法，通过频率域的低频潜在表示干预，无需训练即可将LLMs的CoT推理能力迁移到VLMs。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多步推理任务上表现不佳，现有方法需要高训练成本或架构对齐。研究发现LLMs和VLMs在CoT推理的低频潜在表示上具有相似性。

Method: 使用线性人工层析技术发现模型间相似表示，提出L2V-CoT方法：在频率域提取和重采样LLMs的低频CoT表示，通过维度匹配和潜在注入增强VLMs推理能力。

Result: 实验表明该方法在无需训练的情况下持续优于基线方法，甚至超过有监督方法。

Conclusion: 基于频率域潜在表示干预的L2V-CoT方法能有效将CoT推理从LLMs迁移到VLMs，无需额外训练成本。

Abstract: Recently, Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs), but Vision-Language Models (VLMs) still struggle with multi-step reasoning tasks due to limited multimodal reasoning data. To bridge this gap, researchers have explored methods to transfer CoT reasoning from LLMs to VLMs. However, existing approaches either need high training costs or require architectural alignment. In this paper, we use Linear Artificial Tomography (LAT) to empirically show that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning despite architectural differences. Based on this insight, we propose L2V-CoT, a novel training-free latent intervention approach that transfers CoT reasoning from LLMs to VLMs. L2V-CoT extracts and resamples low-frequency CoT representations from LLMs in the frequency domain, enabling dimension matching and latent injection into VLMs during inference to enhance reasoning capabilities. Extensive experiments demonstrate that our approach consistently outperforms training-free baselines and even surpasses supervised methods.

</details>


### [14] [Towards Efficient LLM-aware Heterogeneous Graph Learning](https://arxiv.org/abs/2511.17923)
*Wenda Li,Tongya Zheng,Shunyu Liu,Yu Wang,Kaixuan Chen,Hanyang Yuan,Bingde Hu,Zujie Ren,Mingli Song,Gang Chen*

Main category: cs.CL

TL;DR: 提出了一个高效的LLM感知框架ELLA，用于解决异质图中复杂关系语义建模问题，通过LLM感知关系分词器、跳级关系图转换器和任务感知CoT提示，在性能和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异质图中节点和关系类型的多样性导致复杂语义，现有方法受限于预定义的语义依赖和监督信号稀缺，LLM虽然能解决语义问题但计算复杂度高。

Method: 使用LLM感知关系分词器编码多跳多类型关系，采用跳级关系图转换器将LLM感知关系推理复杂度从指数级降至线性级，引入细粒度任务感知CoT提示桥接预训练和微调任务间的语义鸿沟。

Result: 在四个异质图上的实验表明，ELLA在性能和效率上优于最先进方法，可扩展到130亿参数LLM，相比现有基于LLM的方法实现高达4倍加速。

Conclusion: ELLA框架有效解决了异质图中复杂关系语义建模的挑战，在保持高性能的同时显著提升了计算效率。

Abstract: Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at https://github.com/l-wd/ELLA.

</details>


### [15] [SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization](https://arxiv.org/abs/2511.17938)
*Jianghao Wu,Yasmeen George,Jin Ye,Yicheng Wu,Daniel F. Schmidt,Jianfei Cai*

Main category: cs.CL

TL;DR: SPINE是一个基于令牌选择的测试时强化学习框架，通过只更新高熵分支点令牌来避免传统TTRL方法中的响应长度崩溃问题，在多个推理基准测试中稳定提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时强化学习方法在自一致性投票中容易出现多数投票奖励主导、响应长度缩短和Pass@1下降的问题，需要更稳定的测试时适应机制。

Method: SPINE框架：(i)只更新分叉令牌（从前向传播统计中识别的高熵分支点）；(ii)在这些令牌上应用熵带正则化器，在熵过低时维持探索，在熵过高时抑制噪声监督。

Result: 在10个基准测试中，SPINE始终比TTRL方法提高Pass@1，避免响应长度崩溃，并在LLM和MLLM骨干网络上产生更稳定的训练动态。

Conclusion: 将更新与思维链分支点对齐是推理模型中实现稳定有效测试时适应的简单且无标签机制。

Abstract: Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.

</details>


### [16] [Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models](https://arxiv.org/abs/2511.17946)
*Shuo Zhang,Fabrizio Gotti,Fengran Mo,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 该论文研究了利用预训练数据的词汇覆盖度作为大语言模型幻觉检测的补充信号，发现虽然单独的词汇覆盖特征预测能力较弱，但与对数概率结合使用时能带来适度提升。


<details>
  <summary>Details</summary>
Motivation: 探索预训练数据暴露与幻觉检测之间的关系，现有研究主要关注模型内部信号，而数据覆盖度作为检测信号的作用被忽视。

Method: 在RedPajama的1.3万亿token预训练语料上构建可扩展后缀数组，检索提示词和模型生成的n-gram统计信息，评估其在三个QA基准上的幻觉检测效果。

Result: 词汇覆盖特征单独使用时预测能力较弱，但与对数概率结合时能带来适度提升，特别是在内在模型不确定性较高的数据集上。

Conclusion: 词汇覆盖特征为幻觉检测提供了补充信号，在模型不确定性较高时尤其有效。

Abstract: Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at https://github.com/WWWonderer/ostd.

</details>


### [17] [MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content Moderation on TikTok](https://arxiv.org/abs/2511.17955)
*Dat Thanh Nguyen,Nguyen Hung Lam,Anh Hoang-Thi Nguyen,Trong-Hop Do*

Main category: cs.CL

TL;DR: MTikGuard是一个针对TikTok的多模态有害内容实时检测系统，通过扩展数据集、多模态融合和流式架构实现了89.37%的准确率。


<details>
  <summary>Details</summary>
Motivation: TikTok作为儿童和青少年广泛使用的平台，存在大量难以被传统方法检测的有害内容，需要实时有效的检测系统。

Method: 扩展TikHarm数据集至4,723个标注视频，构建结合视觉、音频和文本特征的多模态分类框架，并基于Apache Kafka和Spark构建可扩展的流式架构。

Result: 系统达到89.37%的准确率和89.45%的F1分数，在有害内容检测方面表现优异。

Conclusion: 结合数据集扩展、先进的多模态融合和稳健部署，能够有效实现大规模社交媒体内容审核的实用化。

Abstract: With the rapid rise of short-form videos, TikTok has become one of the most influential platforms among children and teenagers, but also a source of harmful content that can affect their perception and behavior. Such content, often subtle or deceptive, challenges traditional moderation methods due to the massive volume and real-time nature of uploads. This paper presents MTikGuard, a real-time multimodal harmful content detection system for TikTok, with three key contributions: (1) an extended TikHarm dataset expanded to 4,723 labeled videos by adding diverse real-world samples, (2) a multimodal classification framework integrating visual, audio, and textual features to achieve state-of-the-art performance with 89.37% accuracy and 89.45% F1-score, and (3) a scalable streaming architecture built on Apache Kafka and Apache Spark for real-time deployment. The results demonstrate the effectiveness of combining dataset expansion, advanced multimodal fusion, and robust deployment for practical large-scale social media content moderation. The dataset is available at https://github.com/ntdat-8324/MTikGuard-System.git.

</details>


### [18] [Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets](https://arxiv.org/abs/2511.18054)
*Gowtham,Sai Rupesh,Sanjay Kumar,Saravanan,Venkata Chaithanya*

Main category: cs.CL

TL;DR: Blu-WERP是一个新颖的数据预处理管道，专门优化Common Crawl WARC文件的质量，用于LLM训练。相比现有基线方法，它在多个模型规模和评估基准上都显著提升了性能，同时实现了质量-每令牌效率增益。


<details>
  <summary>Details</summary>
Motivation: 现有预处理管道在处理网络规模语料库时难以有效去除噪声和非结构化内容，而高质量训练数据对LLM性能至关重要。

Method: Blu-WERP处理CC WARC转储文件，实施先进的过滤和质量评估机制，包括对150M、400M、530M、750M和1B参数模型的全面评估。

Result: 在1B参数规模下，Blu-WERP相比DCLM和Fineweb分别实现了4.0%和9.5%的总体改进。分类分析显示：世界知识与推理提升2.4%，语言理解提升6.2%，常识推理提升4.2%。

Conclusion: Blu-WERP代表了数据质量优化的实际进展，为研究人员和从业者提供了改进LLM训练效率和模型性能的有效解决方案。

Abstract: High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.

</details>


### [19] [GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set](https://arxiv.org/abs/2511.18146)
*Yomal De Mel,Nisansa de Silva*

Main category: cs.CL

TL;DR: 提出了GeeSanBhava数据集，这是一个高质量僧伽罗语歌曲评论数据集，通过三位独立标注者使用Russell的效价-唤醒模型手动标注，在零样本设置下优化后的多层感知机模型取得了0.887的ROC-AUC分数。


<details>
  <summary>Details</summary>
Motivation: 解决僧伽罗语自然语言处理中情感分析数据稀缺的问题，探索基于评论的音乐情感识别，并解决用户生成内容中的偏见问题。

Method: 从YouTube提取僧伽罗语歌曲评论，三位独立标注者使用Russell效价-唤醒模型手动标注；使用在僧伽罗语新闻评论数据集上预训练的机器学习模型进行零样本测试；通过超参数调优优化多层感知机模型。

Result: 标注者间一致性高（Fleiss kappa = 84.96%）；不同歌曲呈现明显不同的情感特征；优化后的三层MLP模型（256-128-64神经元配置）在ROC-AUC上达到0.887。

Conclusion: 该研究为僧伽罗语NLP和音乐情感识别提供了有价值的标注数据集，证明了基于评论的情感分析在音乐领域的可行性。

Abstract: This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.

</details>


### [20] [Vector Arithmetic in Concept and Token Subspaces](https://arxiv.org/abs/2511.18162)
*Sheridan Feucht,Byron Wallace,David Bau*

Main category: cs.CL

TL;DR: 该论文展示了如何利用概念归纳头和标记归纳头来识别LLaMA-2-7b模型激活中的语义和表面级信息子空间，通过注意力权重变换隐藏状态，显著提高了平行四边形算术和词形变换的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了预测下一个标记，LLMs需要表示当前词的语义和表面级信息。先前研究发现概念归纳头和标记归纳头能够解耦这两类信息，本研究旨在探索如何利用这些头来识别模型激活中的结构化子空间。

Method: 使用概念归纳头的注意力权重变换隐藏状态，创建语义结构子空间；使用标记归纳头的注意力权重变换隐藏状态，揭示表面级词信息。通过平行四边形算术和词形变换操作验证效果。

Result: 经过概念头变换的隐藏状态在平行四边形算术任务中达到80%的最近邻准确率，显著高于原始隐藏状态的47%。标记头变换也能有效支持词形变换操作，如"coding" - "code" + "dance" = "dancing"。

Conclusion: 概念归纳头和标记归纳头能够识别模型激活中的语义和表面级信息子空间，通过注意力权重变换可以显著提高语义操作和词形变换任务的性能，为理解LLMs内部表示提供了新视角。

Abstract: In order to predict the next token, LLMs must represent semantic and surface-level information about the current word. Previous work identified two types of attention heads that disentangle this information: (i) Concept induction heads, which copy word meanings, and (ii) Token induction heads, which copy literal token representations (Feucht et al., 2025). We show that these heads can be used to identify subspaces of model activations that exhibit coherent semantic structure in Llama-2-7b. Specifically, when we transform hidden states using the attention weights of concept heads, we are able to more accurately perform parallelogram arithmetic (Mikolov et al., 2013) on the resulting hidden states, e.g., showing that "Athens" - "Greece" + "China" = "Beijing". This transformation allows for much higher nearest-neighbor accuracy (80%) than direct use of raw hidden states (47%). Analogously, we show that token heads allow for transformations that reveal surface-level word information in hidden states, allowing for operations like "coding" - "code" + "dance" = "dancing".

</details>


### [21] [Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models](https://arxiv.org/abs/2511.18177)
*Elias Lumer,Matt Melich,Olivia Zino,Elena Kim,Sara Dieter,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah,James A. Burke,Roberto Hernandez*

Main category: cs.CL

TL;DR: 本文对金融文档检索增强生成(RAG)系统进行了首次系统性评估，比较了基于向量的代理RAG与分层节点系统的性能，并测试了交叉编码器重排序和小到大块检索两种增强技术。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏对金融文档中基于向量和非向量RAG架构的系统性比较，且高级RAG技术对检索准确性、答案质量、延迟和成本的经验影响尚不明确。

Method: 评估了基于向量的代理RAG（使用混合搜索和元数据过滤）与分层节点系统（无需嵌入遍历文档结构），并测试了交叉编码器重排序和小到大块检索两种增强技术。

Result: 基于向量的代理RAG在1,200份SEC文件上的150个问题基准测试中，比分层节点系统获得68%的胜率，延迟相当（5.2秒 vs 5.98秒）。交叉编码器重排序在最优参数下MRR@5提升59%，小到大块检索比基线分块获得65%胜率，仅增加0.2秒延迟。

Conclusion: 将高级RAG技术应用于金融问答系统可提高检索准确性和答案质量，但在生产环境中需要考虑成本-性能权衡。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.

</details>


### [22] [Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems](https://arxiv.org/abs/2511.18194)
*Faheem Nizar,Elias Lumer,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 提出了Agent-as-a-Graph检索方法，通过知识图谱表示代理和工具，在LiveMCPBenchmark上相比现有方法在Recall@5和nDCG@5指标上分别提升14.9%和14.6%。


<details>
  <summary>Details</summary>
Motivation: 现有代理、MCP和检索方法通常只根据单个代理描述匹配查询，无法充分利用每个代理的细粒度工具能力，导致代理选择不理想。

Method: 使用知识图谱检索增强生成方法，将工具及其父代理表示为知识图谱中的节点和边。检索过程包括：向量搜索获取相关代理和工具节点，应用类型特定的加权互逆排序融合进行重排序，在知识图谱中遍历父代理获取最终代理集合。

Result: 在LiveMCPBenchmark评估中，相比现有最优检索器，Recall@5和nDCG@5分别提升14.9%和14.6%，wRRF优化提升2.4%。

Conclusion: Agent-as-a-Graph方法通过知识图谱表示有效提升了代理检索性能，能够更好地利用细粒度工具能力进行代理选择。

Abstract: Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.

</details>


### [23] [From Archives to Decisions: Multi-Agent Pharmaceutical Co-Scientist for Traceable Drug Discovery and Reverse Translation](https://arxiv.org/abs/2511.18259)
*Xiaochen Zheng,Alvaro Serra,Ilya Schneider Chernov,Maddalena Marchesi,Eunice Musvasva,Tatyana Y. Doktorova*

Main category: cs.CL

TL;DR: DiscoVerse是一个多智能体协同科学家系统，用于支持药物研发中的逆向转化研究，通过语义检索、跨文档链接和可审计合成来处理大型历史数据。


<details>
  <summary>Details</summary>
Motivation: 药物研发积累了大量的异构数据，其中许多来自已终止的项目。重新利用这些档案对逆向转化研究非常有价值，但在实践中往往不可行。

Method: 开发了DiscoVerse多智能体系统，实现语义检索、跨文档链接和可审计合成，在罗氏公司的大型历史语料库上进行验证，涵盖180个分子和超过8.7亿BPE标记。

Result: 在7个基准查询中，DiscoVerse实现了接近完美的召回率(≥0.99)和中等精确度(0.71-0.91)，对终止原因和器官特异性毒性的定性评估显示能够忠实地进行源链接合成。

Conclusion: 这是首个在真实药物数据上系统评估的智能体框架，展示了有前景的答案准确性和决策洞察力，为药物研发的逆向转化提供了有效支持。

Abstract: Pharmaceutical research and development has accumulated vast, heterogeneous archives of data. Much of this knowledge stems from discontinued programs, and reusing these archives is invaluable for reverse translation. However, in practice, such reuse is often infeasible. In this work, we introduce DiscoVerse, a multi-agent co-scientist designed to support pharmaceutical research and development. The system implements semantic retrieval, cross-document linking, and auditable synthesis on a large historical corpus from Roche. To validate our approach at real-world scale, we selected a subset of 180 molecules from the Roche research repositories, covering over 0.87 billion BPE tokens and more than four decades of research. Given that automated evaluation metrics are poorly aligned with scientific utility, we evaluate the performance of DiscoVerse using blinded expert evaluation of source-linked outputs. To our knowledge, this is the first agentic framework systematically assessed on real pharmaceutical data for reverse translation, enabled by authorized access to confidential, end-to-end drug-development archives. Our contributions include role-specialized agent designs aligned with scientist workflows; human-in-the-loop support for reverse translation; expert evaluation; and a large-scale demonstration showing promising answer accuracy and decision-making insights. In brief, across seven benchmark queries covering 180 molecules, DiscoVerse achieved near-perfect recall ($\geq 0.99$) with moderate precision ($0.71-0.91$), while qualitative assessments of discontinuation rationale and organ-specific toxicity showed faithful, source-linked synthesis across preclinical and clinical evidence.

</details>


### [24] ["AGI" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination Detection using XLM-RoBERTa](https://arxiv.org/abs/2511.18301)
*Harsh Rathva,Pruthwik Mishra,Shrikant Malviya*

Main category: cs.CL

TL;DR: 本文提出了一种数据中心的策略来解决多语言科学文本幻觉检测中的训练数据稀缺和不平衡问题，通过整合和平衡五个现有数据集创建了包含124,821个样本的综合训练语料库，在SHROOM-CAP 2025共享任务中取得了竞争性表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的多语言科学文本中的幻觉检测对可靠AI系统至关重要，但面临训练数据稀缺和类别不平衡的挑战。

Method: 采用数据中心策略，统一并平衡五个现有数据集创建综合训练语料库（50%正确，50%幻觉），使用包含5.6亿参数的XLM-RoBERTa-Large模型进行微调。

Result: 在9种语言上均取得竞争性表现，在零样本语言古吉拉特语中排名第2（事实性F1为0.5107），其余8种语言排名4-6位。

Conclusion: 系统化的数据整理策略能够显著超越仅依赖架构创新的方法，特别是在零样本设置下的低资源语言中表现突出。

Abstract: The detection of hallucinations in multilingual scientific text generated by Large Language Models (LLMs) presents significant challenges for reliable AI systems. This paper describes our submission to the SHROOM-CAP 2025 shared task on scientific hallucination detection across 9 languages. Unlike most approaches that focus primarily on model architecture, we adopted a data-centric strategy that addressed the critical issue of training data scarcity and imbalance. We unify and balance five existing datasets to create a comprehensive training corpus of 124,821 samples (50% correct, 50% hallucinated), representing a 172x increase over the original SHROOM training data. Our approach fine-tuned XLM-RoBERTa-Large with 560 million parameters on this enhanced dataset, achieves competitive performance across all languages, including \textbf{2nd place in Gujarati} (zero-shot language) with Factuality F1 of 0.5107, and rankings between 4th-6th place across the remaining 8 languages. Our results demonstrate that systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.

</details>


### [25] [Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning](https://arxiv.org/abs/2511.18306)
*Mohammad Aqib,Mohd Hamza,Ying Hei Chui,Qipei Mei*

Main category: cs.CL

TL;DR: 本文比较了两种从建筑规范表格数据中提取信息的方法：直接输入法和间接输入法，并通过LoRA微调显著提升了视觉语言模型在专业领域的表现。


<details>
  <summary>Details</summary>
Motivation: 建筑规范包含确保安全和合规性的关键信息，但表格数据因复杂布局和语义关系而难以提取，需要开发有效的自动问答系统。

Method: 比较两种方法：1) 直接输入法：将页面图像直接输入VLMs回答问题；2) 间接输入法：将表格图像转换为LaTeX代码后回答问题。使用LoRA对VLMs进行参数高效微调。

Result: 直接输入法通常比间接输入法准确率更高。经过LoRA微调后，Qwen2.5-VL-3B-Instruct模型准确率相对提升超过100%。

Conclusion: 参数高效微调方法能够有效提升VLMs在专业领域（如建筑规范解释）中理解复杂结构化数据的潜力。

Abstract: Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.

</details>


### [26] [Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search](https://arxiv.org/abs/2511.18313)
*Joseph Oladokun*

Main category: cs.CL

TL;DR: 提出了路径约束检索（PCR）方法，通过结合图结构约束和语义搜索，确保检索的信息在知识图中保持逻辑关系，从而提高LLM代理推理的可靠性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理从知识库中检索上下文时，由于缺乏与当前推理状态的结构一致性，导致推理链不连贯。

Method: 引入路径约束检索（PCR），将结构图约束与语义搜索相结合，限制搜索空间为从锚节点可达的节点，防止检索结构上断开的信息。

Result: 在PathRAG-6基准测试中，PCR实现100%结构一致性（基线方法为24-32%），同时保持强相关性得分。在技术领域，PCR在排名10时获得完全相关性和完全结构一致性，显著优于向量搜索和混合检索。PCR将检索上下文的平均图距离减少了78%。

Conclusion: 路径约束检索是提高LLM代理推理系统可靠性和连贯性的有效方法。

Abstract: Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.

</details>


### [27] [Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali using Ensemble-Based Adversarial Training for Hate Speech Detection](https://arxiv.org/abs/2511.18324)
*Syed Mohaiminul Hoque,Naimur Rahman,Md Sakhawat Hossain*

Main category: cs.CL

TL;DR: 本文提出了'Gradient Masters'方法用于BLP-2025任务1的孟加拉语多任务仇恨言论识别，采用集成微调策略处理YouTube评论的仇恨类型分类和目标群体分类子任务，在两个子任务中分别获得第6名和第3名的成绩。


<details>
  <summary>Details</summary>
Motivation: 解决低资源孟加拉语仇恨言论检测的挑战，特别是在YouTube评论中的仇恨类型和目标群体分类问题。

Method: 基于孟加拉语语言模型的混合集成微调方法，进行了广泛的实验评估模型鲁棒性。

Result: 在子任务1A中获得73.23%的微平均F1分数（第6名），在子任务1B中获得73.28%（第3名），超越了基线模型。

Conclusion: 该方法在低资源孟加拉语仇恨言论检测场景中表现出良好的泛化能力，并对误分类模式进行了详细分析。

Abstract: This paper introduces the approach of "Gradient Masters" for BLP-2025 Task 1: "Bangla Multitask Hate Speech Identification Shared Task". We present an ensemble-based fine-tuning strategy for addressing subtasks 1A (hate-type classification) and 1B (target group classification) in YouTube comments. We propose a hybrid approach on a Bangla Language Model, which outperformed the baseline models and secured the 6th position in subtask 1A with a micro F1 score of 73.23% and the third position in subtask 1B with 73.28%. We conducted extensive experiments that evaluated the robustness of the model throughout the development and evaluation phases, including comparisons with other Language Model variants, to measure generalization in low-resource Bangla hate speech scenarios and data set coverage. In addition, we provide a detailed analysis of our findings, exploring misclassification patterns in the detection of hate speech.

</details>


### [28] [OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas](https://arxiv.org/abs/2511.18335)
*James Y. Huang,Wenxuan Zhou,Nan Xu,Fei Wang,Qin Liu,Sheng Zhang,Hoifung Poon,Muhao Chen*

Main category: cs.CL

TL;DR: 该论文提出了OmniStruct基准，用于评估大语言模型在文本到结构化任务上的能力，并通过合成数据训练小模型达到与GPT-4o相当的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然现代大语言模型在生成非结构化自然语言响应方面表现出色，但它们在文本到结构化任务（如信息提取、表格生成和函数调用）上的性能仍不明确，需要建立统一的评估基准。

Method: 通过识别适合结构化答案格式的现有数据集，在统一的文本到结构化问题设置下构建OmniStruct基准，并通过合成任务生成收集高质量训练数据来训练小模型。

Result: 实验表明，不使用OmniStruct任务的监督数据，仅通过合成数据微调的小模型就能达到与GPT-4o相当的性能，成为通用的结构化生成模型。

Conclusion: 该研究证明了通过合成数据训练小模型实现高效文本到结构化生成的可能性，为开发更高效的结构化生成模型提供了可行路径。

Abstract: The ability of Large Language Models (LLMs) to generate structured outputs that follow arbitrary schemas is crucial to a wide range of downstream tasks that require diverse structured representations of results such as information extraction, table generation, and function calling. While modern LLMs excel in generating unstructured responses in natural language, whether this advancement translates to a strong performance on text-to-structure tasks remains unclear. To bridge this gap, we first introduce OmniStruct, a comprehensive benchmark for assessing LLMs' capabilities on diverse text-to-structure tasks such as information extraction, table generation, and function calling. We build OmniStruct by identifying existing datasets across a wide range of tasks that are suitable for a structured answer format, and adapting them under a unified text-to-structure problem setting. To facilitate the development of efficient text-to-structure models, we collect high-quality training data via synthetic task generation. Without using any supervised data for OmniStruct tasks, our experiments demonstrate the possibility of fine-tuning much smaller models on synthetic data into universal structured generation models that can rival the performance of GPT-4o.

</details>


### [29] [Tu crois que c'est vrai ? Diversite des regimes d'enonciation face aux fake news et mecanismes d'autoregulation conversationnelle](https://arxiv.org/abs/2511.18369)
*Manon Berriche*

Main category: cs.CL

TL;DR: 研究发现假新闻分享集中在少数高度政治化的用户群体中，这些用户并非教育程度低或认知能力差，而是对制度持批评态度。用户对假新闻的反应取决于其社会地位和互动情境，但很少产生真正的审议性辩论。


<details>
  <summary>Details</summary>
Motivation: 解决两个悖论：为什么社交媒体上假新闻只占信息消费的一小部分，以及为什么政治极化加剧而用户对假新闻并不特别敏感。

Method: 在Twitter和Facebook上进行混合方法研究，结合数字痕迹定量分析、在线观察和访谈，考察不同互动情境下的多样化实践。

Result: 1. 假新闻分享集中在少数高度活跃的政治化用户中；2. 用户根据社会地位和情境采取不同批判距离；3. 这些互动很少产生真正的审议辩论，而是形成'聋子对话'。

Conclusion: 假新闻的影响主要来自少数高度活跃用户的议程设置能力，而非广泛传播。用户反应受社会地位和互动情境影响，但难以促成真正的公共辩论。

Abstract: This thesis addresses two paradoxes: (1) why empirical studies find that fake news represent only a small share of the information consulted and shared on social media despite the absence of editorial control or journalistic norms, and (2) how political polarization has intensified even though users do not appear especially receptive to fake news. To investigate these issues, two complementary studies were carried out on Twitter and Facebook, combining quantitative analyses of digital traces with online observation and interviews. This mixed-methods design avoids reducing users to single reactions to identified fake items and instead examines the variety of practices across different interactional situations, online and offline, while recording socio-demographic traits. The first study mapped users who shared at least one item labeled fake by fact-checkers in the French Twittersphere. The second used a corpus of items flagged by Facebook users to study reactions to statements whose epistemic status is uncertain. Three main findings emerge. First, sharing fake news is concentrated among a limited group of users who are not less educated or cognitively disadvantaged but are more politicized and critical of institutions; owing to their high activity and prolific sharing, they can help set the agenda for their political camp. Second, exposed users can deploy varying forms of critical distance depending on their social position and the interactional norms of the situations they inhabit: either discursive caution (prudence énonciative) or interventions ('points d'arrêt') that express disagreement or corrections. Third, these forms of critical distance seldom yield genuine deliberative debates or agonistic pluralism; rather, they often produce dialogues of the deaf among a small, particularly active minority.

</details>


### [30] [Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models](https://arxiv.org/abs/2511.18393)
*Heejoon Koo*

Main category: cs.CL

TL;DR: 本文系统研究了文本退化对LLM在临床诊断预测中鲁棒性和公平性的影响，提出了临床标签缩减和分层思维链策略来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 临床文本常因人为错误或自动化流程故障而质量下降，这引发了AI辅助决策的可靠性和公平性担忧，但此类退化影响尚未得到充分研究。

Method: 引入临床标签缩减方案处理大诊断标签空间，采用分层思维链策略模拟临床医生推理过程，在不同文本退化场景下测试最先进LLM。

Result: 该方法提高了模型在退化输入下的鲁棒性，减少了子群不稳定性，推进了LLM在临床决策支持系统中的可靠使用。

Conclusion: 提出的方法有效提升了LLM在临床文本退化情况下的鲁棒性和公平性，为CDSS中可靠使用LLM提供了解决方案。

Abstract: A decade of rapid advances in artificial intelligence (AI) has opened new opportunities for clinical decision support systems (CDSS), with large language models (LLMs) demonstrating strong reasoning abilities on timely medical tasks. However, clinical texts are often degraded by human errors or failures in automated pipelines, raising concerns about the reliability and fairness of AI-assisted decision-making. Yet the impact of such degradations remains under-investigated, particularly regarding how noise-induced shifts can heighten predictive uncertainty and unevenly affect demographic subgroups. We present a systematic study of state-of-the-art LLMs under diverse text corruption scenarios, focusing on robustness and equity in next-visit diagnosis prediction. To address the challenge posed by the large diagnostic label space, we introduce a clinically grounded label-reduction scheme and a hierarchical chain-of-thought (CoT) strategy that emulates clinicians' reasoning. Our approach improves robustness and reduces subgroup instability under degraded inputs, advancing the reliable use of LLMs in CDSS. We release code at https://github.com/heejkoo9/NECHOv3.

</details>


### [31] [Findings of the BlackboxNLP 2025 Shared Task: Localizing Circuits and Causal Variables in Language Models](https://arxiv.org/abs/2511.18409)
*Dana Arad,Yonatan Belinkov,Hanjie Chen,Najoung Kim,Hosein Mohebbi,Aaron Mueller,Gabriele Sarti,Martin Tutek*

Main category: cs.CL

TL;DR: 基于Mechanistic Interpretability Benchmark (MIB)构建的BlackboxNLP 2025共享任务，通过电路定位和因果变量定位两个赛道，评估了机制可解释性方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决机制可解释性研究中进展衡量困难的问题，提供标准化评估框架来比较不同MI技术。

Method: 使用MIB基准框架，分为电路定位（识别因果影响组件和交互）和因果变量定位（将激活映射为可解释特征）两个赛道。

Result: 在电路定位中，三个团队的八种方法通过集成和正则化策略取得显著进展；在因果变量定位中，一个团队的两种方法使用低维和非线性投影获得重要提升。

Conclusion: MIB排行榜保持开放，鼓励继续使用这一标准评估框架来衡量MI研究的进展。

Abstract: Mechanistic interpretability (MI) seeks to uncover how language models (LMs) implement specific behaviors, yet measuring progress in MI remains challenging. The recently released Mechanistic Interpretability Benchmark (MIB; Mueller et al., 2025) provides a standardized framework for evaluating circuit and causal variable localization. Building on this foundation, the BlackboxNLP 2025 Shared Task extends MIB into a community-wide reproducible comparison of MI techniques. The shared task features two tracks: circuit localization, which assesses methods that identify causally influential components and interactions driving model behavior, and causal variable localization, which evaluates approaches that map activations into interpretable features. With three teams spanning eight different methods, participants achieved notable gains in circuit localization using ensemble and regularization strategies for circuit discovery. With one team spanning two methods, participants achieved significant gains in causal variable localization using low-dimensional and non-linear projections to featurize activation vectors. The MIB leaderboard remains open; we encourage continued work in this standard evaluation framework to measure progress in MI research going forward.

</details>


### [32] [SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality Arabic Post-Training Data](https://arxiv.org/abs/2511.18411)
*Sultan Alrashed,Chadi Helwe,Francesco Orabona*

Main category: cs.CL

TL;DR: SmolKalam是一个阿拉伯语多轮对话数据集，通过多模型集成翻译管道从Smoltalk2翻译而来，包含推理和工具调用能力，并应用了质量过滤。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大规模、高质量的阿拉伯语多轮对话数据集，特别是包含推理和工具调用的数据。直接翻译在预训练阶段可行，但后训练需要更高质量的数据集。

Method: 采用多模型集成翻译管道对Smoltalk2进行翻译，应用质量过滤，并通过消融实验研究传统仅解码器模型的有效翻译技术。

Result: 成功创建了SmolKalam阿拉伯语数据集，提供了高质量的多轮对话数据。

Conclusion: 通过严格的翻译流程和质量控制，可以创建高质量的阿拉伯语后训练数据集，填补了该领域的空白。

Abstract: Although the community has tackled the acquisition of high-quality Arabic pretraining data, we still lack large-scale, multi-turn Arabic datasets that include reasoning and tool calling. Naive translation can work at the pretraining scale, but post-training demands much higher quality, which requires a stricter approach to dataset curation. In this work, we introduce SmolKalam, a translation of Smoltalk2 that uses a multi-model ensemble translation pipeline, applies quality filtering, and examines effective translation techniques for traditional decoder-only models through ablations.

</details>


### [33] [Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations](https://arxiv.org/abs/2511.18413)
*Yu Xia,Sungchul Kim,Tong Yu,Ryan A. Rossi,Julian McAuely*

Main category: cs.CL

TL;DR: 提出了多智能体协同过滤（MACF）框架，将传统协同过滤算法与基于LLM的多智能体协作相类比，通过动态智能体招募和个性化协作指令来改进推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体推荐系统大多关注通用单智能体或多智能体任务分解流程，缺乏推荐导向设计，未能充分利用用户-物品交互历史中的协同信号，导致推荐效果不佳。

Method: MACF框架将相似用户和相关物品实例化为具有独特配置文件的LLM智能体，每个智能体能够调用检索工具、推荐候选物品并与其他智能体交互。中央协调器智能体通过动态智能体招募和个性化协作指令来管理用户和物品智能体之间的协作。

Result: 在三个不同领域的数据集上的实验结果表明，MACF框架相比强大的智能体推荐基线具有优势。

Conclusion: MACF框架通过将传统协同过滤与LLM多智能体协作相结合，有效提升了智能体推荐系统的性能。

Abstract: Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.

</details>


### [34] [General Agentic Memory Via Deep Research](https://arxiv.org/abs/2511.18423)
*B. Y. Yan,Chaofan Li,Hongjin Qian,Shuqi Lu,Zheng Liu*

Main category: cs.CL

TL;DR: 提出了一种名为GAM的新型智能体记忆框架，采用"即时编译"原则，在运行时为客户端创建优化上下文，同时在离线阶段仅保留简单但有用的记忆。


<details>
  <summary>Details</summary>
Motivation: 现有广泛采用的静态记忆系统在预先创建可用记忆时不可避免地遭受严重信息损失，需要解决这一局限性。

Method: GAM采用双组件设计：1) Memorizer使用轻量级记忆突出关键历史信息，同时在通用页面存储中维护完整历史信息；2) Researcher根据预构建的记忆从页面存储中检索和整合有用信息。该框架利用前沿大语言模型的智能体能力和测试时扩展性，并通过强化学习实现端到端性能优化。

Result: 实验研究表明，GAM在各种基于记忆的任务完成场景中相比现有记忆系统实现了显著改进。

Conclusion: GAM框架通过即时编译原则和双组件设计，有效解决了静态记忆系统的信息损失问题，在记忆相关任务中表现出优越性能。

Abstract: Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \textbf{general agentic memory (GAM)}. GAM follows the principle of "\textbf{just-in time (JIT) compilation}" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.

</details>


### [35] [MindEval: Benchmarking Language Models on Multi-turn Mental Health Support](https://arxiv.org/abs/2511.18491)
*José Pombal,Maya D'Eon,Nuno M. Guerreiro,Pedro Henrique Martins,António Farinhas,Ricardo Rei*

Main category: cs.CL

TL;DR: 提出了MindEval框架，用于自动评估语言模型在真实多轮心理健康对话中的表现，发现现有模型普遍表现不佳，存在沟通模式问题，且推理能力和模型规模不能保证更好性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI心理健康聊天机器人存在诸多限制，如奉承、过度验证和强化不良信念等，而现有基准测试无法捕捉真实治疗互动的复杂性。

Method: 与临床心理学家合作开发MindEval框架，通过患者模拟和LLM自动评估，在真实多轮对话中测试语言模型，验证模拟患者的真实性和自动评估与专家判断的相关性。

Result: 评估12个最先进LLM，所有模型平均得分低于4/6，存在AI特有的沟通模式问题，推理能力和模型规模不能保证更好性能，且在长对话或重症患者支持时表现更差。

Conclusion: 现有语言模型在心理健康对话中表现不足，需要专门改进，MindEval为评估和改进此类系统提供了重要基准。

Abstract: Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.

</details>


### [36] [For Those Who May Find Themselves on the Red Team](https://arxiv.org/abs/2511.18499)
*Tyler Shoemaker*

Main category: cs.CL

TL;DR: 文学学者需要参与大型语言模型可解释性研究，尽管这可能涉及意识形态斗争甚至妥协，但当前可解释性方法的工具性不能成为衡量LLM解释的唯一标准。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型可解释性研究过于注重工具性，文学学者需要参与其中以引入更丰富的解释标准和方法。

Method: 建议将红队（red team）作为文学学者参与LLM可解释性研究的一个具体场所。

Result: 提出了文学学者参与LLM可解释性研究的必要性，并指出了可能的参与途径。

Conclusion: 文学学者必须与LLM可解释性研究进行接触，尽管这涉及意识形态斗争，但这是必要的，红队可以作为一个具体的参与场所。

Abstract: This position paper argues that literary scholars must engage with large language model (LLM) interpretability research. While doing so will involve ideological struggle, if not out-right complicity, the necessity of this engagement is clear: the abiding instrumentality of current approaches to interpretability cannot be the only standard by which we measure interpretation with LLMs. One site at which this struggle could take place, I suggest, is the red team.

</details>


### [37] [Dealing with the Hard Facts of Low-Resource African NLP](https://arxiv.org/abs/2511.18557)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Panga Azazia Kamaté,Madani Amadou Tall,Emmanuel Élisé Koné,Aymane Dembélé,Michael Leventhal*

Main category: cs.CL

TL;DR: 本文报告了为低资源语言班巴拉语收集612小时自发语音数据、半自动转录标注、创建多个单语超紧凑和小型模型，并进行自动和人工评估的研究工作。


<details>
  <summary>Details</summary>
Motivation: 为低资源语言创建语音数据集、模型和评估框架具有挑战性，因为缺乏相关经验基础。

Method: 野外收集班巴拉语自发语音数据，半自动标注转录，创建单语超紧凑和小型模型，进行自动和人工评估。

Result: 收集了612小时班巴拉语语音数据，创建了多个模型，提供了数据收集协议、标注和模型设计的实用建议，并证明了人工评估的重要性。

Conclusion: 除了主要数据集外，还公开提供了多个评估数据集、模型和代码，为低资源语言研究提供了实用资源和方法指导。

Abstract: Creating speech datasets, models, and evaluation frameworks for low-resource languages remains challenging given the lack of a broad base of pertinent experience to draw from. This paper reports on the field collection of 612 hours of spontaneous speech in Bambara, a low-resource West African language; the semi-automated annotation of that dataset with transcriptions; the creation of several monolingual ultra-compact and small models using the dataset; and the automatic and human evaluation of their output. We offer practical suggestions for data collection protocols, annotation, and model design, as well as evidence for the importance of performing human evaluation. In addition to the main dataset, multiple evaluation datasets, models, and code are made publicly available.

</details>


### [38] [Toward Trustworthy Difficulty Assessments: Large Language Models as Judges in Programming and Synthetic Tasks](https://arxiv.org/abs/2511.18597)
*H. M. Shadman Tabib,Jaber Ahmed Deedar*

Main category: cs.CL

TL;DR: GPT-4o作为编程问题难度评估器表现不佳，准确率仅37.75%，远低于基于显式特征的LightGBM模型（86%）。GPT-4o倾向于低估问题难度，忽视数值约束等关键特征。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在结构化任务（如编程问题难度预测）中的表现，特别是在与可解释的传统机器学习模型对比下，评估LLMs作为自动评判器的可靠性。

Method: 在1,825个LeetCode问题上，系统比较GPT-4o（纯自然语言评估器）与基于显式数值和文本特征的LightGBM集成模型。使用混淆矩阵和SHAP可解释性分析，并通过合成Hard问题生成协议进一步测试GPT-4o。

Result: LightGBM达到86%准确率，GPT-4o仅37.75%。GPT-4o表现出强烈偏向简单类别的偏见，忽视输入大小限制和接受率等关键数值约束。在自生成Hard问题测试中，GPT-4o将几乎所有问题错误分类为Medium。

Conclusion: LLMs在编程难度评估等结构化任务中存在具体失败模式，在解决这些可靠性问题之前，不应在竞争性编程、教育平台或强化学习管道中信任LLM作为评判器。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language and code generation, and are increasingly deployed as automatic judges of model outputs and learning activities. Yet, their behavior on structured tasks such as predicting the difficulty of competitive programming problems remains under-explored. We conduct a systematic comparison of GPT-4o, used purely as a natural-language difficulty assessor, against an interpretable Light-GBM ensemble trained on explicit numeric and textual features. On a dataset of 1,825 LeetCode problems labeled Easy, Medium, or Hard, LightGBM attains 86% accuracy, whereas GPT-4o reaches only 37.75%. Detailed analyses, including confusion matrices and SHAP-based interpretability, show that numeric constraints -- such as input size limits and acceptance rates -- play a crucial role in separating Hard problems from easier ones. By contrast, GPT-4o often overlooks these cues and exhibits a strong bias toward simpler categories. We further probe GPT-4o through a synthetic Hard-problem generation protocol. Surprisingly, GPT-4o labels almost all of its own synthetic Hard problems as Medium, contradicting its tendency to downgrade real Hard problems to Easy. Our findings connect to recent work on LLMs-as-judges and automatic difficulty estimation in programming and education, and highlight concrete failure modes that must be addressed before LLM-based judges can be considered trustworthy in competitive programming, educational platforms, or reinforcement-learning pipelines.

</details>


### [39] [A Benchmark for Zero-Shot Belief Inference in Large Language Models](https://arxiv.org/abs/2511.18616)
*Joseph Malone,Rachith Aiyappa,Byunghwee Lee,Haewoon Kwak,Jisun An,Yong-Yeol Ahn*

Main category: cs.CL

TL;DR: 该研究提出了一个系统性基准测试，评估大语言模型在零样本设置下预测个体对各种话题立场的泛化能力，发现提供更多背景信息能提高预测准确性，但性能在不同信念领域差异显著。


<details>
  <summary>Details</summary>
Motivation: 信念是人类推理、沟通和社交的核心，但目前计算研究方法大多局限于特定社会政治背景且依赖微调。尽管大语言模型在各学科应用日益广泛，但其在不同信念领域的泛化能力尚不明确。

Method: 使用在线辩论平台数据，构建系统性可复现基准测试，在零样本设置下评估LLMs预测个体立场的能力，包含多个信息条件以分离人口统计背景和已知先验信念对预测成功的贡献。

Result: 在多个中小型模型上的实验表明，提供更多个体背景信息能提高预测准确性，但性能在不同信念领域间存在显著差异。

Conclusion: 研究揭示了当前LLMs模拟人类推理的能力和局限性，推动了机器行为研究，并为超越社会政治领域的信念系统建模提供了可扩展框架。

Abstract: Beliefs are central to how humans reason, communicate, and form social connections, yet most computational approaches to studying them remain confined to narrow sociopolitical contexts and rely on fine-tuning for optimal performance. Despite the growing use of large language models (LLMs) across disciplines, how well these systems generalize across diverse belief domains remains unclear. We introduce a systematic, reproducible benchmark that evaluates the ability of LLMs to predict individuals' stances on a wide range of topics in a zero-shot setting using data from an online debate platform. The benchmark includes multiple informational conditions that isolate the contribution of demographic context and known prior beliefs to predictive success. Across several small- to medium-sized models, we find that providing more background information about an individual improves predictive accuracy, but performance varies substantially across belief domains. These findings reveal both the capacity and limitations of current LLMs to emulate human reasoning, advancing the study of machine behavior and offering a scalable framework for modeling belief systems beyond the sociopolitical sphere.

</details>


### [40] [A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News](https://arxiv.org/abs/2511.18618)
*Mirza Raquib,Munazer Montasir Akash,Tawhid Ahmed,Saydul Akbar Murad,Farida Siddiqi Prity,Mohammad Amzad Hossain,Asif Pervez Polok,Nick Rahimi*

Main category: cs.CL

TL;DR: 本研究提出了一种结合孟加拉语新闻标题分类和情感分析的先进方法，使用BERT-CNN-BiLSTM混合迁移学习模型，在BAN-ABSA数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 报纸是重要的信息来源，但有效浏览大量新闻内容具有挑战性。新闻标题情感分析可以帮助快速理解新闻的情感基调。

Method: 使用BERT-CNN-BiLSTM混合迁移学习模型，在9014条孟加拉语新闻标题数据集上应用两种实验策略：技术1（分割前采样）和技术2（分割后采样）。

Result: 技术1中过采样在标题和情感分类上分别达到78.57%和73.43%的最佳性能；技术2中直接在原始不平衡数据集上训练分别达到81.37%和64.46%的最高结果。

Conclusion: BERT-CNN-BiLSTM模型显著优于所有基线模型，为孟加拉语文本分类在低资源环境下提供了强大的基准。

Abstract: In our daily lives, newspapers are an essential information source that impacts how the public talks about present-day issues. However, effectively navigating the vast amount of news content from different newspapers and online news portals can be challenging. Newspaper headlines with sentiment analysis tell us what the news is about (e.g., politics, sports) and how the news makes us feel (positive, negative, neutral). This helps us quickly understand the emotional tone of the news. This research presents a state-of-the-art approach to Bangla news headline classification combined with sentiment analysis applying Natural Language Processing (NLP) techniques, particularly the hybrid transfer learning model BERT-CNN-BiLSTM. We have explored a dataset called BAN-ABSA of 9014 news headlines, which is the first time that has been experimented with simultaneously in the headline and sentiment categorization in Bengali newspapers. Over this imbalanced dataset, we applied two experimental strategies: technique-1, where undersampling and oversampling are applied before splitting, and technique-2, where undersampling and oversampling are applied after splitting on the In technique-1 oversampling provided the strongest performance, both headline and sentiment, that is 78.57\% and 73.43\% respectively, while technique-2 delivered the highest result when trained directly on the original imbalanced dataset, both headline and sentiment, that is 81.37\% and 64.46\% respectively. The proposed model BERT-CNN-BiLSTM significantly outperforms all baseline models in classification tasks, and achieves new state-of-the-art results for Bangla news headline classification and sentiment analysis. These results demonstrate the importance of leveraging both the headline and sentiment datasets, and provide a strong baseline for Bangla text classification in low-resource.

</details>


### [41] [Prompt Optimization as a State-Space Search Problem](https://arxiv.org/abs/2511.18619)
*Maanas Taneja*

Main category: cs.CL

TL;DR: 将提示优化建模为状态空间搜索问题，使用束搜索和随机游走算法在提示图中探索优化路径，在多个NLP任务上取得开发集性能提升。


<details>
  <summary>Details</summary>
Motivation: 语言模型对输入提示字符串的微小变化极其敏感，现有方法如DSpy通过基于示例的提示优化避免此问题，本文提出将提示优化视为经典状态空间搜索问题的替代方法。

Method: 将提示空间建模为图结构，节点代表提示状态，边对应有意的转换操作（如缩短、添加示例、重新排序内容），使用束搜索和随机游走算法系统探索该空间，在开发集上评估候选提示并剪枝无希望的路径。

Result: 在五个NLP任务（情感分类、问答、摘要、推理和自然语言推理）上，即使浅层搜索配置（束宽=2，深度=2）也能在开发集上改进种子提示。例如，束搜索在推理任务上实现开发准确率从0.40提升到0.80，但测试集改进较为温和（0.20到0.50），表明对开发启发式存在过拟合。

Conclusion: 结果验证了将提示优化作为搜索问题的有效性，并表明通过更多计算资源和改进的评估指标，更深入的探索可以产生在开发集之外更稳健的提示。

Abstract: Language Models are extremely susceptible to performance collapse with even small changes to input prompt strings. Libraries such as DSpy (from Stanford NLP) avoid this problem through demonstration-based prompt optimisation. Inspired by this, I propose an alternative approach that treats prompt optimisation as a classical state-space search problem. I model the prompt space as a graph where nodes represent prompt states and edges correspond to deliberate transformations such as shortening, adding examples, or re- ordering content. Using beam search and random walk algorithms, I systematically explore this space, evaluating candidates on development sets and pruning unpromising branches. Across five NLP tasks (sentiment classification, question answering, summarisation, reason- ing, and natural language inference), I find that even shallow search configurations (beam width=2, depth=2) improve upon seed prompts on development sets. For instance, beam search achieves development accuracy gains from 0.40 to 0.80 on reasoning tasks, though test set improvements are more modest (0.20 to 0.50), indicating overfitting to the develop- ment heuristic. Analysis of successful optimisation paths reveals that transformations that make prompts concise appear most frequently, while verbosity operators are never selected. My results validate prompt optimization as a search problem and suggest that with greater computational resources and improved evaluation metrics, deeper exploration could yield more robust prompts that generalize beyond development sets. Code and implementation are available at [https://github.com/MaanasTaneja/PromptOptimiser].

</details>


### [42] [OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph](https://arxiv.org/abs/2511.18622)
*Michael J. Bommarito*

Main category: cs.CL

TL;DR: OpenGloss是一个合成的百科全书式词典和语义知识图谱，整合了词典定义、百科全书背景、词源历史和语义关系，包含53.7万个词义和150万个词条，成本低于1000美元且在一周内生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统词典资源创建成本高、周期长的问题，通过结构化生成技术快速创建综合性词汇资源，填补教学应用中的空白。

Method: 采用多智能体程序生成流水线，结合模式验证的LLM输出和自动化质量保证，实现高效的结构化生成。

Result: 生成了包含53.7万个词义、910万条语义边、100万个使用示例、300万个搭配和6000万词百科全书内容的资源，规模与WordNet相当但定义数量多4倍。

Conclusion: 结构化生成能够以传统人工整理无法实现的时间和成本规模创建综合性词汇资源，随着基础模型的改进可实现快速迭代，该资源已在Hugging Face公开。

Abstract: We present OpenGloss, a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates lexicographic definitions, encyclopedic context, etymological histories, and semantic relationships in a unified resource. OpenGloss contains 537K senses across 150K lexemes, on par with WordNet 3.1 and Open English WordNet, while providing more than four times as many sense definitions. These lexemes include 9.1M semantic edges, 1M usage examples, 3M collocations, and 60M words of encyclopedic content.
  Generated through a multi-agent procedural generation pipeline with schema-validated LLM outputs and automated quality assurance, the entire resource was produced in under one week for under $1,000. This demonstrates that structured generation can create comprehensive lexical resources at cost and time scales impractical for manual curation, enabling rapid iteration as foundation models improve. The resource addresses gaps in pedagogical applications by providing integrated content -- definitions, examples, collocations, encyclopedias, etymology -- that supports both vocabulary learning and natural language processing tasks.
  As a synthetically generated resource, OpenGloss reflects both the capabilities and limitations of current foundation models. The dataset is publicly available on Hugging Face under CC-BY 4.0, enabling researchers and educators to build upon and adapt this resource.

</details>


### [43] [No Free Lunch in Language Model Bias Mitigation? Targeted Bias Reduction Can Exacerbate Unmitigated LLM Biases](https://arxiv.org/abs/2511.18635)
*Shireen Chand,Faith Baca,Emilio Ferrara*

Main category: cs.CL

TL;DR: 研究探讨了针对性偏见缓解方法的跨类别后果，发现虽然能减少目标维度的偏见，但经常在其他维度产生负面后果，如增加模型偏见和降低整体连贯性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从训练数据中继承了社会偏见，可能导致有害或不公平的输出。现有偏见缓解技术通常只评估目标偏见维度，缺乏对跨类别影响的全面考察。

Method: 研究了四种偏见缓解技术，应用于来自七个模型家族的十个模型，探索了种族、宗教、职业和性别相关偏见，使用StereoSet基准评估偏见缓解对模型连贯性和刻板印象偏好的影响。

Result: 研究一致表明，针对性缓解有时能减少目标维度的偏见，但经常在其他维度导致意外负面后果，如增加模型偏见和降低一般连贯性。

Conclusion: 这些发现强调了在检查和开发偏见缓解策略时，需要强大的多维评估工具，以避免在未目标维度上无意中转移或加剧偏见。

Abstract: Large Language Models (LLMs) inherit societal biases from their training data, potentially leading to harmful or unfair outputs. While various techniques aim to mitigate these biases, their effects are often evaluated only along the dimension of the bias being targeted. This work investigates the cross-category consequences of targeted bias mitigation. We study four bias mitigation techniques applied across ten models from seven model families, and we explore racial, religious, profession- and gender-related biases. We measure the impact of debiasing on model coherence and stereotypical preference using the StereoSet benchmark. Our results consistently show that while targeted mitigation can sometimes reduce bias in the intended dimension, it frequently leads to unintended and often negative consequences in others, such as increasing model bias and decreasing general coherence. These findings underscore the critical need for robust, multi-dimensional evaluation tools when examining and developing bias mitigation strategies to avoid inadvertently shifting or worsening bias along untargeted axes.

</details>


### [44] [Evaluating Large Language Models on the 2026 Korean CSAT Mathematics Exam: Measuring Mathematical Ability in a Zero-Data-Leakage Setting](https://arxiv.org/abs/2511.18649)
*Goun Pyeon,Inbum Heo,Jeesu Jung,Taewook Hwang,Hyuk Namgoong,Hyein Seo,Yerim Han,Eunbin Kim,Hyeonseok Kang,Sangkeun Jung*

Main category: cs.CL

TL;DR: 本研究系统评估了大型语言模型在2026年韩国高考数学测试中的表现，建立了完全无数据污染的评估环境，发现GPT-5 Codex获得满分，几何是表现最差的领域，文本输入优于图像输入，增加推理强度能提升性能但大幅降低效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有基准测试中数据泄露问题，在考试公开后两小时内数字化所有题目，确保模型训练数据中不包含这些题目，实现完全无污染的评估环境。

Method: 使用2026年韩国高考数学全部46道题目，在24个最先进LLM上进行评估，涵盖不同输入模态（文本、图像、文本+图形）和提示语言（韩语、英语），并进行推理增强实验。

Result: GPT-5 Codex获得唯一满分，Grok 4、GPT-5和Deepseek R1得分超过95分；几何领域表现最差（77.7%平均分）；文本输入始终优于图像输入；增加推理强度从82.6分提升到100分但效率大幅降低。

Conclusion: 建立了完全无暴露的评估环境，提供了基于真实考试的LLM评估框架，并整合性能、成本和时间考虑的实际评估视角，表明最小推理模型可能更实用。

Abstract: This study systematically evaluated the mathematical reasoning capabilities of Large Language Models (LLMs) using the 2026 Korean College Scholastic Ability Test (CSAT) Mathematics section, ensuring a completely contamination-free evaluation environment. To address data leakage issues in existing benchmarks, we digitized all 46 questions (22 common and 24 elective) within two hours of the exam's public release, eliminating any possibility of inclusion in model training data. We conducted comprehensive evaluations of 24 state-of-the-art LLMs across varying input modalities (text, image, text+figure) and prompt languages (Korean, English).
  GPT-5 Codex achieved the only perfect score (100 points) with text input and Korean prompts, while Grok 4, GPT-5, and Deepseek R1 scored above 95 points. Notably, gpt-oss-20B achieved 95.7 points despite its relatively small size, demonstrating high cost-effectiveness. Problem-specific analysis revealed geometry as the weakest domain (77.7% average) with significant performance degradation on 4-point high-difficulty problems. Text input consistently outperformed image input, while prompt language effects varied by model scale.
  In reasoning enhancement experiments with GPT-5 series, increased reasoning intensity improved performance (from 82.6 to 100 points) but quadrupled token usage and drastically reduced efficiency, suggesting that models with minimal reasoning may be more practical. This research contributes: (1) implementation of a completely unexposed evaluation environment, (2) a real-exam-based LLM assessment framework, and (3) a practical evaluation perspective integrating performance, cost, and time considerations. Detailed results and model comparisons are available at the 2026 Korean CSAT LLM Evaluation Leaderboard (https://isoft.cnu.ac.kr/csat2026/).

</details>


### [45] [CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning](https://arxiv.org/abs/2511.18659)
*Jie He,Richard He Bai,Sinead Williamson,Jeff Z. Pan,Navdeep Jaitly,Yizhe Zhang*

Main category: cs.CL

TL;DR: CLaRa是一个统一的检索增强生成框架，通过共享连续空间中的嵌入压缩和联合优化，解决了RAG中长上下文和检索-生成分离优化的问题。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成虽然增强了LLMs的外部知识，但仍面临长上下文和检索-生成分离优化的问题，需要统一的框架来同时优化压缩、检索和生成。

Method: 提出CLaRa框架，包括SCP数据合成框架生成语义丰富的压缩向量，以及通过可微分top-k估计器实现重排序器和生成器的端到端训练，使用单一语言建模损失。

Result: 在多个QA基准测试中，CLaRa实现了最先进的压缩和重排序性能，通常超越基于文本的微调基线。

Conclusion: CLaRa通过统一的连续空间优化，理论上有助于对齐检索相关性与答案质量，实验证明其在压缩和重排序方面具有优越性能。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.

</details>


### [46] [Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models](https://arxiv.org/abs/2511.18696)
*Wangjiaxuan Xin*

Main category: cs.CL

TL;DR: ECN是一个多阶段提示框架，通过四个阶段提升大语言模型的共情和包容能力，在GPT模型上取得了最高的共情商数得分。


<details>
  <summary>Details</summary>
Motivation: 增强大型语言模型的共情和包容能力，使其能够生成更具情感共鸣和情境感知的响应。

Method: 采用四阶段提示方法：视角采纳、情感共鸣、反思理解和综合整合，引导模型生成情感丰富的响应。

Result: ECN在GPT-3.5-turbo和GPT-4上获得了最高的共情商数得分，同时在尊重度和困惑度指标上保持竞争力。

Conclusion: ECN框架在需要共情和包容性的对话AI应用中具有重要潜力。

Abstract: This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.

</details>


### [47] [RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context](https://arxiv.org/abs/2511.18743)
*Yu Lei,Shuzheng Si,Wei Wang,Yifei Wu,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: RhinoInsight是一个深度研究框架，通过添加可验证检查清单和证据审计两个控制机制，增强LLM代理的鲁棒性、可追溯性和研究质量，无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 现有系统采用线性管道（计划-搜索-写作-报告）存在错误累积和上下文腐化问题，缺乏对模型行为和上下文的显式控制。

Method: 1. 可验证检查清单模块：将用户需求转化为可追溯的子目标，通过人工或LLM批评家进行细化，生成分层大纲来锚定后续行动。2. 证据审计模块：结构化搜索内容，迭代更新大纲，修剪噪声上下文，通过批评家对证据进行排序和绑定以确保可验证性。

Result: 实验表明RhinoInsight在深度研究任务上达到最先进性能，同时在深度搜索任务上保持竞争力。

Conclusion: RhinoInsight通过添加控制机制有效解决了现有系统的局限性，提升了LLM代理在深度研究中的表现。

Abstract: Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.

</details>


### [48] [Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search](https://arxiv.org/abs/2511.18749)
*Matthew R. DeVerna,Kai-Cheng Yang,Harry Yaojun Yan,Filippo Menczer*

Main category: cs.CL

TL;DR: 评估15个主流LLM在6000多个事实核查任务上的表现，发现标准模型表现不佳，推理能力提升有限，网络搜索仅带来中等改进，而使用高质量上下文信息的RAG系统性能显著提升233%。


<details>
  <summary>Details</summary>
Motivation: 随着LLM具备推理能力和网络搜索工具，数百万用户依赖它们进行事实核查，急需对这些模型在事实核查任务上的表现进行严格评估。

Method: 评估15个来自OpenAI、Google、Meta和DeepSeek的LLM在6000多个PolitiFact事实核查声明上的表现，比较标准模型、推理变体和网络搜索变体，并与使用PolitiFact摘要的RAG系统对比。

Result: 标准模型表现差，推理能力提供有限改进，网络搜索仅带来中等增益，而RAG系统在模型变体上的平均宏F1得分提高了233%。

Conclusion: 为模型提供经过筛选的高质量上下文是自动化事实核查的有前景路径，而非单纯依赖模型的推理能力或网络搜索功能。

Abstract: Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.

</details>


### [49] [Robust Multimodal Sentiment Analysis with Distribution-Based Feature Recovery and Fusion](https://arxiv.org/abs/2511.18751)
*Daiqing Wu,Dongbao Yang,Can Ma,Yu Zhou*

Main category: cs.CL

TL;DR: 提出了一种基于分布的特征恢复与融合方法（DRF），用于处理图像-文本对中低质量和缺失模态的鲁棒多模态情感分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法在同时利用图像和文本信息方面取得了显著成就，但缺乏对可能出现的低质量和缺失模态的考虑。在现实应用中，这些问题频繁发生，迫切需要能够稳健预测情感的模型。

Method: 为每个模态维护特征队列以近似其特征分布，通过统一框架同时处理低质量和缺失模态。对于低质量模态，基于分布定量估计模态质量以降低其融合贡献；对于缺失模态，通过样本和分布监督建立模态间映射关系，从可用模态中恢复缺失模态。

Result: 在三个公开可用的图像-文本数据集上的综合实验表明，与最先进方法相比，DRF在两种破坏策略下均实现了普遍改进，验证了其在鲁棒多模态情感分析中的有效性。

Conclusion: DRF方法能够有效处理现实场景中的低质量和缺失模态问题，在多模态情感分析任务中展现出优越的鲁棒性。

Abstract: As posts on social media increase rapidly, analyzing the sentiments embedded in image-text pairs has become a popular research topic in recent years. Although existing works achieve impressive accomplishments in simultaneously harnessing image and text information, they lack the considerations of possible low-quality and missing modalities. In real-world applications, these issues might frequently occur, leading to urgent needs for models capable of predicting sentiment robustly. Therefore, we propose a Distribution-based feature Recovery and Fusion (DRF) method for robust multimodal sentiment analysis of image-text pairs. Specifically, we maintain a feature queue for each modality to approximate their feature distributions, through which we can simultaneously handle low-quality and missing modalities in a unified framework. For low-quality modalities, we reduce their contributions to the fusion by quantitatively estimating modality qualities based on the distributions. For missing modalities, we build inter-modal mapping relationships supervised by samples and distributions, thereby recovering the missing modalities from available ones. In experiments, two disruption strategies that corrupt and discard some modalities in samples are adopted to mimic the low-quality and missing modalities in various real-world scenarios. Through comprehensive experiments on three publicly available image-text datasets, we demonstrate the universal improvements of DRF compared to SOTA methods under both two strategies, validating its effectiveness in robust multimodal sentiment analysis.

</details>


### [50] [Context-Aware Whisper for Arabic ASR Under Linguistic Varieties](https://arxiv.org/abs/2511.18774)
*Bashar Talafha,Amin Abu Alhassan,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出基于上下文提示的方法，无需重新训练即可适配Whisper模型用于阿拉伯语语音识别，在多种阿拉伯语变体上显著降低词错误率


<details>
  <summary>Details</summary>
Motivation: 解决低资源阿拉伯语ASR的挑战，特别是处理阿拉伯语方言变体多样性和标注数据有限的问题

Method: 使用解码器提示（首遍转录或检索语句）和编码器前缀（目标说话人语音合成），结合提示重排序、说话人感知前缀合成和多模态检索技术

Result: 在9种阿拉伯语语言条件下，现代标准阿拉伯语WER降低22.3%，方言语音WER降低9.2%，显著减少幻觉和说话人不匹配问题

Conclusion: 上下文感知提示策略能有效提升Whisper在阿拉伯语ASR中的性能，特别是在零样本设置下

Abstract: Low-resource ASR remains a challenging problem, especially for languages like Arabic that exhibit wide dialectal variation and limited labeled data. We propose context-aware prompting strategies to adapt OpenAI's Whisper for Arabic speech recognition without retraining. Our methods include decoder prompting with first-pass transcriptions or retrieved utterances, and encoder prefixing using speech synthesized in the target speaker's voice. We introduce techniques such as prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval (lexical, semantic, acoustic) to improve transcription in real-world, zero-shot settings. Evaluated on nine Arabic linguistic conditions, our approach reduces WER by up to 22.3% on Modern Standard Arabic and 9.2% on dialectal speech, significantly mitigating hallucinations and speaker mismatch.

</details>


### [51] [HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations](https://arxiv.org/abs/2511.18808)
*Cao Linxiao,Wang Ruitao,Li Jindong,Zhou Zhipeng,Yang Menglin*

Main category: cs.CL

TL;DR: HyperbolicRAG是一个基于双曲几何的检索增强生成框架，通过在Poincare流形中嵌入节点来同时捕获语义相似性和层次结构关系，显著提升了图结构RAG的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图结构RAG方法依赖欧几里得嵌入，虽然能捕获语义相似性，但缺乏对层次深度关系的几何表示能力，限制了在复杂知识图中表示抽象关系的能力。

Method: 提出三个关键设计：1）深度感知表示学习器在Poincare流形中嵌入节点；2）无监督对比正则化确保跨抽象层次的几何一致性；3）互排名融合机制联合利用欧几里得和双曲空间的检索信号。

Result: 在多个问答基准测试上的广泛实验表明，HyperbolicRAG优于包括标准RAG和图增强基线在内的竞争方法。

Conclusion: 双曲几何能够有效增强图结构RAG的表示能力，通过同时捕获细粒度语义和全局层次结构，显著提升了检索增强生成的性能。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.

</details>


### [52] [Concept than Document: Context Compression via AMR-based Conceptual Entropy](https://arxiv.org/abs/2511.18832)
*Kaize Shi,Xueyao Sun,Xiaohui Tao,Lin Li,Qika Lin,Guandong Xu*

Main category: cs.CL

TL;DR: 提出基于抽象意义表示（AMR）图的无监督上下文压缩框架，通过节点级熵量化概念重要性，过滤冗余信息，在保持核心语义的同时显著减少上下文长度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文时面临信息过载问题，特别是在检索增强生成中，大量支持文档常包含冗余内容，这会降低推理准确性并增加计算开销。

Method: 构建AMR图，计算节点级概念熵来估计每个节点的概念重要性，筛选重要信息节点形成压缩的语义聚焦上下文。

Result: 在PopQA和EntityQuestions数据集上的实验表明，该方法优于基准方法，在显著减少上下文长度的同时获得更高准确率。

Conclusion: 这是首个将基于AMR的概念熵用于上下文压缩的工作，展示了稳定语言特征在上下文工程中的潜力。

Abstract: Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.

</details>


### [53] [A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis](https://arxiv.org/abs/2511.18843)
*Heger Arfaoui,Mohammed Iheb Hergli,Beya Benzina,Slimane BenMiled*

Main category: cs.CL

TL;DR: 提出了一个用于焦点小组讨论文本的神经网络主题建模计算框架，解决了超参数敏感性、模型稳定性和可解释性验证等挑战，在突尼斯HPV疫苗认知研究中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统焦点小组讨论分析依赖劳动密集型手动编码，限制了可扩展性和可重复性，需要开发可重复的计算分析框架。

Method: 使用BERTopic对10个焦点小组的1,076条话语进行分析，系统评估27种超参数配置，通过30次自举重采样评估稳定性，并由3名领域专家进行正式人类评估验证可解释性。

Result: 分析显示对超参数选择高度敏感，分层合并策略（先提取细粒度主题评估稳定性，再合并提高可解释性）有效平衡稳定性与连贯性，人类验证显示主题质量良好（ICC=0.79，加权Cohen's kappa=0.578）。

Conclusion: 该框架为研究人员提供了实用的指导方针，所有代码、数据处理脚本和评估协议都公开可用，支持工作的复制和扩展。

Abstract: Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a rigorous, reproducible computational framework for applying neural topic modeling to focus group transcripts, addressing fundamental methodological challenges: hyperparameter sensitivity, model stability, and validation of interpretability. Using BERTopic applied to ten focus groups exploring HPV vaccine perceptions in Tunisia (1,076 utterances), we conducted systematic evaluation across 27 hyperparameter configurations, assessed stability through bootstrap resampling with 30 replicates per configuration, and validated interpretability through formal human evaluation by three domain experts. Our analysis demonstrates substantial sensitivity to hyperparameter choices and reveals that metric selection for stability assessment must align with analytical goals. A hierarchical merging strategy (extracting fine-grained topics for stability then consolidating for interpretability) effectively navigates the stability-coherence tradeoff, achieving coherence of 0.558 compared to 0.539 for direct extraction. Human validation confirmed topic quality with very good inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578). Our framework provides practical guidelines that researchers can adapt to their own qualitative research contexts. All code, data processing scripts, and evaluation protocols are publicly available to support reproduction and extension of this work.

</details>


### [54] [Large Language Models for the Summarization of Czech Documents: From History to the Present](https://arxiv.org/abs/2511.18848)
*Václav Tran,Jakub Šmíd,Ladislav Lenc,Jean-Pierre Salmon,Pavel Král*

Main category: cs.CL

TL;DR: 该研究探索了捷克语文本摘要任务，针对现代和历史捷克文本，利用大语言模型（Mistral和mT5）和翻译方法取得了最先进的性能，并创建了新的历史捷克文本摘要数据集。


<details>
  <summary>Details</summary>
Motivation: 捷克语摘要任务，特别是历史文档摘要，由于语言复杂性和缺乏高质量标注数据集而研究不足。本研究旨在填补这一空白。

Method: 使用大语言模型（Mistral和mT5）进行直接摘要，并提出翻译方法：先将捷克文本翻译成英语，用英语模型摘要，再翻译回捷克语。

Result: 在SumeCzech数据集上取得了新的最先进结果，证明了多语言LLM对形态丰富的捷克语的有效性，并创建了Posel od Čerchova历史文本摘要数据集。

Conclusion: 该研究为捷克语摘要奠定了基础，为捷克历史文档处理和低资源语言摘要提供了宝贵资源。

Abstract: Text summarization is the task of automatically condensing longer texts into shorter, coherent summaries while preserving the original meaning and key information. Although this task has been extensively studied in English and other high-resource languages, Czech summarization, particularly in the context of historical documents, remains underexplored. This is largely due to the inherent linguistic complexity of Czech and the lack of high-quality annotated datasets.
  In this work, we address this gap by leveraging the capabilities of Large Language Models (LLMs), specifically Mistral and mT5, which have demonstrated strong performance across a wide range of natural language processing tasks and multilingual settings. In addition, we also propose a translation-based approach that first translates Czech texts into English, summarizes them using an English-language model, and then translates the summaries back into Czech. Our study makes the following main contributions: We demonstrate that LLMs achieve new state-of-the-art results on the SumeCzech dataset, a benchmark for modern Czech text summarization, showing the effectiveness of multilingual LLMs even for morphologically rich, medium-resource languages like Czech. We introduce a new dataset, Posel od Čerchova, designed for the summarization of historical Czech texts. This dataset is derived from digitized 19th-century publications and annotated for abstractive summarization. We provide initial baselines using modern LLMs to facilitate further research in this underrepresented area.
  By combining cutting-edge models with both modern and historical Czech datasets, our work lays the foundation for further progress in Czech summarization and contributes valuable resources for future research in Czech historical document processing and low-resource summarization more broadly.

</details>


### [55] [Cognitive Alpha Mining via LLM-Driven Code-Based Evolution](https://arxiv.org/abs/2511.18850)
*Fengyuan Liu,Huang Yi,Sichun Luo,Yuqi Wang,Yazheng Yang,Xinye Li,Zefa Hu,Junlan Feng,Qi Liu*

Main category: cs.CL

TL;DR: 提出了CogAlpha框架，结合代码级alpha表示、LLM驱动推理和进化搜索，用于从高维金融数据中发现有效的预测信号。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索广阔的alpha搜索空间方面存在局限，神经网络模型产生不透明模式，符号方法产生冗余表达式，缺乏平衡逻辑一致性和创造性的人类式探索。

Method: 将LLM作为自适应认知代理，通过多阶段提示和金融反馈迭代优化、变异和重组alpha候选，结合代码级alpha表示与进化搜索。

Result: 在A股测试中，CogAlpha发现的alpha在预测准确性、鲁棒性和泛化能力上优于现有方法。

Conclusion: 将进化优化与基于LLM的推理相结合，有望实现自动化和可解释的alpha发现。

Abstract: Discovering effective predictive signals, or ``alphas,'' from financial data with high dimensionality and extremely low signal-to-noise ratio remains a difficult open problem. Despite progress in deep learning, genetic programming, and, more recently, large language model (LLM)--based factor generation, existing approaches still explore only a narrow region of the vast alpha search space. Neural models tend to produce opaque and fragile patterns, while symbolic or formula-based methods often yield redundant or economically ungrounded expressions that generalize poorly. Although different in form, these paradigms share a key limitation: none can conduct broad, structured, and human-like exploration that balances logical consistency with creative leaps. To address this gap, we introduce the Cognitive Alpha Mining Framework (CogAlpha), which combines code-level alpha representation with LLM-driven reasoning and evolutionary search. Treating LLMs as adaptive cognitive agents, our framework iteratively refines, mutates, and recombines alpha candidates through multi-stage prompts and financial feedback. This synergistic design enables deeper thinking, richer structural diversity, and economically interpretable alpha discovery, while greatly expanding the effective search space. Experiments on A-share equities demonstrate that CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods. Our results highlight the promise of aligning evolutionary optimization with LLM-based reasoning for automated and explainable alpha discovery. All source code will be released.

</details>


### [56] [FanarGuard: A Culturally-Aware Moderation Filter for Arabic Language Models](https://arxiv.org/abs/2511.18852)
*Masoomali Fatehkia,Enes Altinisik,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: FanarGuard是一个双语内容审核过滤器，专注于阿拉伯语和英语的安全性和文化对齐评估，在文化敏感性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有内容审核过滤器主要关注一般安全性而忽视文化背景，需要开发能够同时评估安全性和文化对齐的解决方案。

Method: 构建包含46.8万对提示-响应的数据集，使用LLM评委评估无害性和文化意识，训练两个过滤器变体，并开发首个针对阿拉伯文化背景的基准测试。

Result: FanarGuard在人类标注一致性方面优于标注者间可靠性，同时在安全基准测试中达到最先进过滤器的性能水平。

Conclusion: 将文化意识整合到内容审核中至关重要，FanarGuard是实现更上下文敏感保护的实际步骤。

Abstract: Content moderation filters are a critical safeguard against alignment failures in language models. Yet most existing filters focus narrowly on general safety and overlook cultural context. In this work, we introduce FanarGuard, a bilingual moderation filter that evaluates both safety and cultural alignment in Arabic and English. We construct a dataset of over 468K prompt and response pairs, drawn from synthetic and public datasets, scored by a panel of LLM judges on harmlessness and cultural awareness, and use it to train two filter variants. To rigorously evaluate cultural alignment, we further develop the first benchmark targeting Arabic cultural contexts, comprising over 1k norm-sensitive prompts with LLM-generated responses annotated by human raters. Results show that FanarGuard achieves stronger agreement with human annotations than inter-annotator reliability, while matching the performance of state-of-the-art filters on safety benchmarks. These findings highlight the importance of integrating cultural awareness into moderation and establish FanarGuard as a practical step toward more context-sensitive safeguards.

</details>


### [57] [Generating Reading Comprehension Exercises with Large Language Models for Educational Applications](https://arxiv.org/abs/2511.18860)
*Xingyu Huang,Fei Jiang,Jianli Xiao*

Main category: cs.CL

TL;DR: 提出了一个名为RCEG的LLM框架，用于自动生成高质量的个性化英语阅读理解练习，通过微调LLM生成候选内容并利用判别器选择最佳候选，显著提升了生成内容的质量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，其在教育领域特别是自动文本生成方面展现出巨大潜力，能够创建智能和自适应的学习内容。

Method: RCEG框架首先使用微调的大语言模型生成内容候选，然后通过判别器选择最佳候选，最终大幅提升生成内容的质量。

Result: 实验结果表明，RCEG显著提高了生成练习的相关性和认知适切性，在内容多样性、事实准确性、语言毒性和教学对齐等综合评估指标上表现优异。

Conclusion: RCEG框架能够有效生成高质量的个性化英语阅读理解练习，在教育领域具有重要应用价值。

Abstract: With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.

</details>


### [58] [Think Before You Prune: Selective Self-Generated Calibration for Pruning Large Reasoning Models](https://arxiv.org/abs/2511.18864)
*Yang Xiang,Yixin Ji,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文首次研究大型推理模型（LRMs）的剪枝问题，发现直接应用现有剪枝技术效果不佳，提出使用自生成推理数据进行校准可显著提升剪枝性能，并开发了选择性自生成推理（SSGR）数据构建策略。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现出色，但其长链推理过程带来高昂推理成本。剪枝是减少计算成本的有效方法，但现有研究主要关注大语言模型（LLMs），LRMs的剪枝尚未被探索。

Method: 提出选择性自生成推理（SSGR）数据构建策略，通过分析推理数据的难度和长度对剪枝效果的影响，发现具有挑战性且长度适中的自生成推理数据是最佳校准数据。

Result: 在DeepSeek-R1-Distill模型系列上的实验表明，相比通用剪枝方法，该策略将剪枝后LRMs的推理能力提升了10%-13%。

Conclusion: 自生成推理数据是LRMs剪枝的有效校准数据，SSGR策略能够显著提升剪枝后模型的推理性能，为LRMs的高效部署提供了新思路。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning benchmarks. However, their long chain-of-thought reasoning processes incur significant inference overhead. Pruning has emerged as a promising approach to reducing computational costs. However, existing efforts have primarily focused on large language models (LLMs), while pruning LRMs remains unexplored. In this work, we conduct the first empirical study on pruning LRMs and show that directly applying existing pruning techniques fails to yield satisfactory results. Our findings indicate that using self-generated reasoning data for calibration can substantially improve pruning performance. We further investigate how the difficulty and length of reasoning data affect pruning outcomes. Our analysis reveals that challenging and moderately long self-generated reasoning data serve as ideal calibration data. Based on these insights, we propose a Selective Self-Generated Reasoning (SSGR) data construction strategy to provide effective calibration data for pruning LRMs. Experimental results on the DeepSeek-R1-Distill model series validate that our strategy improves the reasoning ability of pruned LRMs by 10%-13% compared to general pruning methods.

</details>


### [59] [CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation](https://arxiv.org/abs/2511.18889)
*Jingqian Zhao,Bingbing Wang,Geng Tu,Yice Zhang,Qianlong Wang,Bin Liang,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: CoreEval是一种抗数据污染的评估策略，通过从GDELT数据库获取最新知识来更新数据集，解决LLM评估中的数据污染问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法完全消除模型中的预存知识或保持原始数据集的语义复杂性，导致抗污染评估不足。数据污染会不公平地影响LLM评估结果。

Method: 从原始数据提取实体关系，使用GDELT数据库检索最新相关知识，重新语境化并整合知识，通过数据反射机制迭代验证和优化标签。

Result: 在更新数据集上的广泛实验验证了CoreEval的鲁棒性，证明其能有效缓解数据污染导致的性能高估问题。

Conclusion: CoreEval提供了一种有效的抗数据污染评估方法，通过动态更新数据和确保语义一致性来提升LLM评估的公平性。

Abstract: Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \textbf{CoreEval}, a \textbf{Co}ntamination-\textbf{re}silient \textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.

</details>


### [60] [Reproducibility Study of Large Language Model Bayesian Optimization](https://arxiv.org/abs/2511.18891)
*Adam Rychert,Gasper Spagnolo,Evgenii Posashkov*

Main category: cs.CL

TL;DR: 本研究复现了LLAMBO框架，使用开源的Llama 3.1 70B模型替代GPT-3.5，验证了该框架在贝叶斯优化中的有效性。结果表明文本上下文显著改善早期优化性能，语言模型的跨任务语义先验提升了预测能力，但较小模型容量不足导致不稳定预测。


<details>
  <summary>Details</summary>
Motivation: 验证LLAMBO框架在不同语言模型骨干下的鲁棒性，特别是使用开源模型替代GPT-3.5时的表现，以确认该方法的可复现性和通用性。

Method: 在原始评估协议下复现核心实验，使用Llama 3.1 70B模型替换GPT-3.5，进行Bayesmark和HPOBench实验，并对比不同规模模型（Gemma 27B、Llama 3.1 8B）的表现。

Result: LLAMBO架构在Llama 3.1 70B上表现稳健，文本上下文显著改善早期遗憾行为和运行方差，候选采样器生成更高质量和多样化的提案，但较小模型产生不稳定预测。

Conclusion: LLAMBO框架对语言模型骨干的变化具有鲁棒性，使用Llama 3.1 70B仍能保持有效性，但需要足够容量的语言模型来确保可靠的代理行为。

Abstract: In this reproducibility study, we revisit the LLAMBO framework of Daxberger et al. (2024), a prompting-based Bayesian optimization (BO) method that uses large language models as discriminative surrogates and acquisition optimizers via text-only interactions. We replicate the core Bayesmark and HPOBench experiments under the original evaluation protocol, but replace GPT-3.5 with the open-weight Llama 3.1 70B model used for all text encoding components.
  Our results broadly confirm the main claims of LLAMBO. Contextual warm starting via textual problem and hyperparameter descriptions substantially improves early regret behaviour and reduces variance across runs. LLAMBO's discriminative surrogate is weaker than GP or SMAC as a pure single task regressor, yet benefits from cross task semantic priors induced by the language model. Ablations that remove textual context markedly degrade predictive accuracy and calibration, while the LLAMBO candidate sampler consistently generates higher quality and more diverse proposals than TPE or random sampling. Experiments with smaller backbones (Gemma 27B, Llama 3.1 8B) yield unstable or invalid predictions, suggesting insufficient capacity for reliable surrogate behaviour.
  Overall, our study shows that the LLAMBO architecture is robust to changing the language model backbone and remains effective when instantiated with Llama 3.1 70B.

</details>


### [61] [Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs](https://arxiv.org/abs/2511.18931)
*Sahil Kale*

Main category: cs.CL

TL;DR: 评估大语言模型是否需要以及如何有效使用网络搜索的基准测试，发现网络搜索能提高准确性但存在过度自信和查询质量差的问题


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型集成了网络搜索来提供实时答案，但尚不清楚它们是否能在真正需要时有效使用搜索功能

Method: 创建包含静态和动态问题的基准数据集，静态部分测试模型基于低内部置信度调用搜索的能力，动态部分测试模型识别何时需要搜索并检索更新信息的能力

Result: 网络访问显著提高了GPT-5-mini和Claude Haiku 4.5的静态准确性，但置信度校准变差；动态查询中模型频繁调用搜索但准确率仍低于70%，主要由于查询质量差

Conclusion: 内置网络搜索能显著提高事实准确性且可选择性调用，但模型仍存在过度自信、在必需时跳过检索、以及初始搜索查询表现不佳时效果下降的问题

Abstract: Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.

</details>


### [62] [Skeletons Matter: Dynamic Data Augmentation for Text-to-Query](https://arxiv.org/abs/2511.18934)
*Yuchen Ji,Bo Xu,Jie Shi,Jiaqing Liang,Deqing Yang,Yu Mao,Hai Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了Text-to-Query任务范式，统一了不同查询语言的语义解析任务，通过识别查询骨架作为共享优化目标，并提出了动态数据增强框架来诊断模型弱点并合成针对性训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常只关注单一查询语言，导致方法在不同语言间的泛化能力有限。为了克服这一局限性，需要统一的Text-to-Query任务范式。

Method: 提出动态数据增强框架，明确诊断模型在处理查询骨架时的特定弱点，并合成针对性训练数据。识别查询骨架作为Text-to-Query任务的共享优化目标。

Result: 在四个Text-to-Query基准测试上的实验表明，仅使用少量合成数据即可达到最先进的性能，证明了方法的效率和通用性。

Conclusion: 该方法为Text-to-Query任务的统一研究奠定了坚实基础，展示了在少量合成数据下实现高性能的潜力。

Abstract: The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.

</details>


### [63] [Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials](https://arxiv.org/abs/2511.18937)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: 提出了一种基于图形化知识的方法来审查临床试验中的治疗相关不良事件，通过为MedDRA添加隐藏的医学知识层（Safeterm）来改进不良事件分析。


<details>
  <summary>Details</summary>
Motivation: 改进临床试验中不良事件审查的清晰度、效率和准确性，通过增强MedDRA术语系统的语义理解能力。

Method: 使用Safeterm知识层捕获术语间的语义关系，在2D地图中自动将不良事件术语重新分组为相似性簇，并计算与试验疾病的关联度。采用收缩发生率比计算治疗特异性不成比例指标。

Result: 应用于三个历史试验，自动化方法清晰地恢复了所有预期的安全信号。提供了语义地图和期望度-不成比例度图两种可视化输出。

Conclusion: 通过为MedDRA添加医学知识层，显著提高了临床试验中不良事件解释的清晰度、效率和准确性。

Abstract: We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.

</details>


### [64] [Logic of Montage](https://arxiv.org/abs/2511.19063)
*Hayami Takahashi,Kensuke Takahashi*

Main category: cs.CL

TL;DR: 提出了一种与自然语言分离的情感表达形式，通过矛盾结构效应和蒙太奇操作来补充自然语言，作为情感状态的代理窗口。


<details>
  <summary>Details</summary>
Motivation: 为情感表达提供一种不同于自然语言的替代形式，作为情感状态的代理或窗口来补充自然语言表达。

Method: 建立矛盾结构效应模型，通过蒙太奇操作重叠这些效应，引入强度概念作为模型元素，使用系统间词语导入的理论框架。

Result: 构建了一个理论框架，能够通过矛盾结构效应和蒙太奇操作来表达情感状态，并以教育升级为例展示了结构效应的处理过程。

Conclusion: 成功建立了一种补充自然语言的情感表达理论模型，通过矛盾结构效应和蒙太奇操作有效表达了情感状态。

Abstract: In expressing emotions, as an expression form separate from natural language, we propose an alternative form that complements natural language, acting as a proxy or window for emotional states. First, we set up an expression form "Effect of Contradictory Structure." "Effect of Contradictory Structure" is not static but dynamic. Effect in "Effect of Contradictory Structure" is unpleasant or pleasant, and the orientation to avoid that unpleasantness is considered pseudo-expression of will. Second, "Effect of Contradictory Structure" can be overlapped with each other. This overlapping operation is called "montage." A broader "Structure" that includes related "Effect of Contradictory Structure" and "Effect of Structure" are set up. Montage produces "Effect of Structure". In montage, it is necessary to set something like "strength," so we adopted Deleuze and Deleuze/Guattari's word "intensity" and set it as an element of our model. We set up a general theoretical framework - Word Import Between Systems (Models) and justified the import of "intensity" through Austin's use of the word "force." "Effect of Structure" process is demonstrated using the example of proceeding to the next level of education.

</details>


### [65] [GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning](https://arxiv.org/abs/2511.19078)
*Yutong Li,Yitian Zhou,Xudong Wang,GuoChen,Caiyan Qin*

Main category: cs.CL

TL;DR: GraphMind是一个将图神经网络与LLMs结合的动态图框架，用于多步推理中的定理选择和中间结论生成，在多个QA数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在多步推理中缺乏显式和动态的机制来表示和演化中间推理状态，限制了上下文感知的定理选择和迭代结论生成能力。

Method: 将推理过程建模为异构演化图，节点表示条件、定理和结论，边捕捉逻辑依赖关系，通过GNN编码当前推理状态并利用语义匹配进行定理选择。

Result: 在多个QA数据集上的实验表明，GraphMind方法实现了持续的性能提升，在多步推理中显著优于现有基线。

Conclusion: GraphMind框架通过动态图表示和GNN编码，实现了上下文感知、可解释和结构化的推理，验证了该方法的有效性和泛化能力。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.

</details>


### [66] [A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis](https://arxiv.org/abs/2511.19083)
*Wenxuan Mu,Jinzhong Ning,Di Zhao,Yijia Zhang*

Main category: cs.CL

TL;DR: KDR-Agent是一个多智能体框架，通过知识检索、消歧和反思分析来解决低资源命名实体识别问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习的NER方法存在三个关键局限：依赖动态检索标注数据、对未见领域泛化能力有限、无法整合外部知识或解决实体歧义。

Method: 提出KDR-Agent多智能体框架，使用自然语言类型定义和静态实体级对比演示，通过中央规划器协调专门智能体进行知识检索、消歧和反思分析。

Result: 在5个领域的10个数据集上实验表明，KDR-Agent在多个LLM骨干网络上显著优于现有的零样本和少样本ICL基线方法。

Conclusion: KDR-Agent通过集成知识检索、消歧和反思分析，有效解决了低资源多领域NER的关键挑战，为ICL-based NER提供了新的解决方案。

Abstract: In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.

</details>


### [67] [DeCoRL: Decoupling Reasoning Chains via Parallel Sub-Step Generation and Cascaded Reinforcement for Interpretable and Scalable RLHF](https://arxiv.org/abs/2511.19097)
*Ziyuan Gao,Di Liang,Xianjie Wu,Philippe Morel,Minlong Peng*

Main category: cs.CL

TL;DR: DeCoRL是一个新颖的强化学习框架，通过将推理过程从顺序处理转变为协作式模块化编排，解决了现有方法的两个关键限制：缺乏细粒度奖励信号和顺序解码的时间复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在思维链推理中存在两个关键限制：1) 作为黑盒提供无差别奖励信号，难以识别单个步骤贡献和错误诊断；2) 顺序解码具有O(n)时间复杂度，使得复杂推理任务的实时部署不切实际。

Method: DeCoRL训练轻量级专用模型并行生成推理子步骤，消除顺序瓶颈。设计模块化奖励函数独立评分每个子步骤，并通过级联DRPO优化协调这些奖励同时保持步骤间依赖关系。

Result: 在RM-Bench、RMB和RewardBench上的综合评估显示达到最先进结果，优于包括大规模模型在内的现有方法。推理速度提升3.8倍，可解释性提高22.7%，能耗降低72.4%，吞吐量增加68%。

Conclusion: DeCoRL通过并行处理、模块化奖励和级联优化，实现了复杂推理系统的实时部署，在保持解决方案质量的同时显著提升了效率和可解释性。

Abstract: Existing reinforcement learning methods for Chain-of-Thought reasoning suffer from two critical limitations. First, they operate as monolithic black boxes that provide undifferentiated reward signals, obscuring individual step contributions and hindering error diagnosis. Second, sequential decoding has O(n) time complexity. This makes real-time deployment impractical for complex reasoning tasks. We present DeCoRL (Decoupled Reasoning Chains via Coordinated Reinforcement Learning), a novel framework that transforms reasoning from sequential processing into collaborative modular orchestration. DeCoRL trains lightweight specialized models to generate reasoning sub-steps concurrently, eliminating sequential bottlenecks through parallel processing. To enable precise error attribution, the framework designs modular reward functions that score each sub-step independently. Cascaded DRPO optimization then coordinates these rewards while preserving inter-step dependencies. Comprehensive evaluation demonstrates state-of-the-art results across RM-Bench, RMB, and RewardBench, outperforming existing methods including large-scale models. DeCoRL delivers 3.8 times faster inference while maintaining superior solution quality and offers a 22.7\% improvement in interpretability through explicit reward attribution. These advancements, combined with a 72.4\% reduction in energy consumption and a 68\% increase in throughput, make real-time deployment of complex reasoning systems a reality.

</details>


### [68] [A symbolic Perl algorithm for the unification of Nahuatl word spellings](https://arxiv.org/abs/2511.19118)
*Juan-José Guzmán-Landa,Jesús Vázquez-Osorio,Juan-Manuel Torres-Moreno,Ligia Quintana Torres,Miguel Figueroa-Saavedra,Martha-Lorena Avendaño-Garrido,Graham Ranger,Patricia Velázquez-Morales,Gerardo Eugenio Sierra Martínez*

Main category: cs.CL

TL;DR: 提出了一种用于纳瓦特尔语文本自动正字法统一的符号模型，基于先前分析纳瓦特尔语句子的算法和π-yalli语料库，使用符号正则表达式实现语言规则，并通过人工评估协议测试统一句子的语义质量。


<details>
  <summary>Details</summary>
Motivation: 解决纳瓦特尔语文本在不同正字法系统中的统一问题，便于语言处理和分析。

Method: 基于π-yalli语料库和符号正则表达式，开发自动统一算法实现语言规则，并设计人工评估协议进行语义任务测试。

Result: 评估者对于大多数期望特征给出了令人鼓舞的结果。

Conclusion: 该方法在纳瓦特尔语文本自动正字法统一方面取得了积极成效，为语言处理提供了有效工具。

Abstract: In this paper, we describe a symbolic model for the automatic orthographic unification of Nawatl text documents. Our model is based on algorithms that we have previously used to analyze sentences in Nawatl, and on the corpus called $π$-yalli, consisting of texts in several Nawatl orthographies. Our automatic unification algorithm implements linguistic rules in symbolic regular expressions. We also present a manual evaluation protocol that we have proposed and implemented to assess the quality of the unified sentences generated by our algorithm, by testing in a sentence semantic task. We have obtained encouraging results from the evaluators for most of the desired features of our artificially unified sentences

</details>


### [69] [On the Optimality of Discrete Object Naming: a Kinship Case Study](https://arxiv.org/abs/2511.19120)
*Phong Le,Mees Lindeman,Raquel G. Alhama*

Main category: cs.CL

TL;DR: 该研究提出了一个信息理论框架来分析命名系统，证明当听者的解码器等同于说话者的贝叶斯解码器时，才能实现信息量与复杂度的最优权衡。


<details>
  <summary>Details</summary>
Motivation: 解决先前研究的两个局限性：(i) 假设最优听者，(ii) 假设跨语言普遍沟通需求。

Method: 采用涌现通信中的指称游戏设置，聚焦亲属关系语义域，分析离散对象命名系统。

Result: 理论上的最优性在学习的通信系统中也能经验性地涌现。

Conclusion: 命名系统的最优权衡在特定条件下是可实现的，且能在实际学习系统中观察到。

Abstract: The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.

</details>


### [70] [Emotion-Enhanced Multi-Task Learning with LLMs for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2511.19122)
*Yaping Chai,Haoran Xie,Joe S. Qin*

Main category: cs.CL

TL;DR: 提出了一种情感增强的多任务方面类别情感分析框架，联合学习情感极性和基于Ekman六种基本情感的分类特定情感，通过VAD维度框架进行情感精炼，显著提升了ACSA性能。


<details>
  <summary>Details</summary>
Motivation: 现有ACSA方法主要关注情感极性，但忽略了塑造情感表达的情感维度，这限制了模型捕捉特定方面类别的细粒度情感信号的能力。

Method: 利用LLM的生成能力，为每个方面类别生成情感描述；基于VAD维度框架引入情感精炼机制，将LLM预测的情感投影到VAD空间，对不一致的情感进行重新标注。

Result: 在所有基准数据集上显著优于强基线方法。

Conclusion: 将情感维度整合到ACSA中是有效的，情感增强方法能够丰富情感表示并提升分析性能。

Abstract: Aspect category sentiment analysis (ACSA) has achieved remarkable progress with large language models (LLMs), yet existing approaches primarily emphasize sentiment polarity while overlooking the underlying emotional dimensions that shape sentiment expressions. This limitation hinders the model's ability to capture fine-grained affective signals toward specific aspect categories. To address this limitation, we introduce a novel emotion-enhanced multi-task ACSA framework that jointly learns sentiment polarity and category-specific emotions grounded in Ekman's six basic emotions. Leveraging the generative capabilities of LLMs, our approach enables the model to produce emotional descriptions for each aspect category, thereby enriching sentiment representations with affective expressions. Furthermore, to ensure the accuracy and consistency of the generated emotions, we introduce an emotion refinement mechanism based on the Valence-Arousal-Dominance (VAD) dimensional framework. Specifically, emotions predicted by the LLM are projected onto a VAD space, and those inconsistent with their corresponding VAD coordinates are re-annotated using a structured LLM-based refinement strategy. Experimental results demonstrate that our approach significantly outperforms strong baselines on all benchmark datasets. This underlines the effectiveness of integrating affective dimensions into ACSA.

</details>


### [71] [Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization](https://arxiv.org/abs/2511.19131)
*Zijian Wang,Yanxiang Ma,Chang Xu*

Main category: cs.CL

TL;DR: 提出了一种基于概率条件生成的新方法，通过优化隐藏状态来激发基础大语言模型的思维链推理能力，在保持语言连贯性的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 基础大语言模型在预训练后往往缺乏专门的推理训练，现有隐藏状态操纵方法存在刚性约束、分布偏移和文本质量下降等问题，需要更有效的推理能力激发方法。

Method: 将挑战重新表述为优化问题，采用平衡似然和先验正则化的框架，在概率条件生成的基础上引导隐藏状态朝向推理导向的轨迹发展。

Result: 在数学、常识和逻辑推理基准测试中的广泛评估表明，该方法一致优于现有的引导方法。

Conclusion: 该方法为增强基础大语言模型的推理能力提供了一个理论上合理且有效的解决方案。

Abstract: Chain-of-Thought (CoT) reasoning is a critical capability for large language models (LLMs), enabling them to tackle com- plex multi-step tasks. While base LLMs, pre-trained on general text corpora, often struggle with reasoning due to a lack of specialized training, recent studies reveal their latent reason- ing potential tied to hidden states. However, existing hidden state manipulation methods, such as linear activation steering, suffer from limitations due to their rigid and unconstrained nature, often leading to distribution shifts and degraded text quality. In this work, we propose a novel approach for elic- iting CoT reasoning from base LLMs through hidden state manipulation grounded in probabilistic conditional generation. By reformulating the challenge as an optimization problem with a balanced likelihood and prior regularization framework, our method guides hidden states toward reasoning-oriented trajectories while preserving linguistic coherence. Extensive evaluations across mathematical, commonsense, and logical reasoning benchmarks demonstrate that our approach con- sistently outperforms existing steering methods, offering a theoretically principled and effective solution for enhancing reasoning capabilities in base LLMs.

</details>


### [72] [Representational Stability of Truth in Large Language Models](https://arxiv.org/abs/2511.19166)
*Samantha Dies,Courtney Maynard,Germans Savcisens,Tina Eliassi-Rad*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在表示真假内容时的稳定性，提出了表示稳定性的概念，并通过线性探针实验发现模型对不熟悉的中性陈述最为敏感，而对熟悉虚构内容更稳定。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在其内部概率表示中如何稳定地区分真实、虚假和中性内容，以及这种表示对真值定义扰动的鲁棒性。

Method: 通过在LLM激活上训练线性探针来分离真假陈述，并测量在受控标签变化下学习决策边界的变化，比较了两种中性陈述：不熟悉的中性陈述和熟悉虚构陈述。

Result: 不熟悉的中性陈述导致最大的边界偏移，在脆弱领域（如词汇定义）产生高达40%的真值判断翻转，而熟悉虚构陈述保持更一致的聚类，变化较小（≤8.2%）。

Conclusion: 表示稳定性更多源于认知熟悉度而非语言形式，该方法为审计和训练LLMs提供了诊断工具，以在语义不确定性下保持连贯的真值分配。

Abstract: Large language models (LLMs) are widely used for factual tasks such as "What treats asthma?" or "What is the capital of Latvia?". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\leq 8.2\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.

</details>


### [73] [In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations](https://arxiv.org/abs/2511.19232)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 本文研究了transformer模型如何检测语义异常，发现模型在中间层开始能有效区分合理与不合理句子结尾，且异常检测呈现先扩展后压缩的维度变化模式。


<details>
  <summary>Details</summary>
Motivation: 探索transformer模型在何处以及如何检测到句子语义异常，并与人类语言处理的心理学发现进行对比。

Method: 使用phi-2因果语言模型，通过线性探测器和维度分析，研究模型各层对合理与不合理句子结尾的编码方式。

Result: 线性探测器在模型底层三分之一难以区分语义异常，但在中间层准确率急剧上升；异常编码维度呈现先扩展后压缩的模式。

Conclusion: transformer模型的语义异常检测过程与人类阅读中的语义异常检测模式相似，都发生在句法解析之后，支持了模型与人类语言处理机制的对应关系。

Abstract: How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.

</details>


### [74] [MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset](https://arxiv.org/abs/2511.19317)
*Md. Tanzim Ferdous,Naeem Ahsan Chowdhury,Prithwiraj Bhattacharjee*

Main category: cs.CL

TL;DR: 开发了一个包含54,000多篇孟加拉语文章和摘要的新数据集，涵盖多个领域和写作风格，为孟加拉语抽象摘要研究提供了基准。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注新闻文章，但现实世界中的孟加拉语文本具有多样性，需要能够适应不同写作风格的摘要系统来解决信息过载问题。

Method: 从多个来源收集孟加拉语文章和摘要，包括博客和报纸，使用LSTM、BanglaT5-small和MTS-small等深度学习模型进行训练和评估。

Result: 建立了强大的基线模型，展示了该数据集作为孟加拉语自然语言处理未来研究基准的潜力。

Conclusion: 该数据集为构建鲁棒的摘要系统提供了坚实基础，有助于扩展低资源语言的NLP资源。

Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.

</details>


### [75] [Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces](https://arxiv.org/abs/2511.19333)
*Shaltiel Shmidman,Asher Fredman,Oleg Sudakov,Meriem Bendris*

Main category: cs.CL

TL;DR: 比较DeepSeek-R1和gpt-oss两种大型语言模型生成的推理轨迹对中等规模LLM数学问题解决能力的训练效果差异


<details>
  <summary>Details</summary>
Motivation: 利用大型推理模型生成的推理轨迹作为监督数据，训练中小型语言模型获得推理能力，避免昂贵的人工标注

Method: 对中等规模LLM进行后训练，使用DeepSeek-R1和gpt-oss生成的推理轨迹作为训练数据，比较其在数学问题上的表现

Result: 比较了两种推理轨迹在准确性和推理效率方面的影响

Conclusion: 评估不同来源的推理轨迹对中小型模型推理能力训练的效果差异

Abstract: Test-time scaling, which leverages additional computation during inference to improve model accuracy, has enabled a new class of Large Language Models (LLMs) that are able to reason through complex problems by understanding the goal, turning this goal into a plan, working through intermediate steps, and checking their own work before answering . Frontier large language models with reasoning capabilities, such as DeepSeek-R1 and OpenAI's gpt-oss, follow the same procedure when solving complex problems by generating intermediate reasoning traces before giving the final answer. Today, these models are being increasingly used to generate reasoning traces that serve as high-quality supervised data for post-training of small and medium-sized language models to teach reasoning capabilities without requiring expensive human curation. In this work, we compare the performance of medium-sized LLMs on Math problems after post-training on two kinds of reasoning traces. We compare the impact of reasoning traces generated by DeepSeek-R1 and gpt-oss LLMs in terms of accuracy and inference efficiency.

</details>


### [76] [DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research](https://arxiv.org/abs/2511.19399)
*Rulin Shao,Akari Asai,Shannon Zejiang Shen,Hamish Ivison,Varsha Kishore,Jingming Zhuo,Xinran Zhao,Molly Park,Samuel G. Finlayson,David Sontag,Tyler Murray,Sewon Min,Pradeep Dasigi,Luca Soldaini,Faeze Brahman,Wen-tau Yih,Tongshuang Wu,Luke Zettlemoyer,Yoon Kim,Hannaneh Hajishirzi,Pang Wei Koh*

Main category: cs.CL

TL;DR: 提出了RLER（强化学习与演进式评分标准）方法，开发了Deep Research Tulu (DR Tulu-8B)模型，这是首个直接针对开放式长格式深度研究训练的开源模型，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的开源深度研究模型主要通过在可验证的短格式QA任务上进行强化学习训练，这无法扩展到现实的长格式任务。需要一种能够处理开放式长格式深度研究的方法。

Method: 采用RLER方法，构建和维护与策略模型共同演进的评分标准，使评分标准能够整合模型新探索的信息并提供区分性的在线反馈。

Result: 在科学、医疗和通用领域的四个长格式深度研究基准测试中，DR Tulu显著优于现有开源深度研究模型，匹配或超过专有深度研究系统，同时模型更小、查询成本更低。

Conclusion: RLER方法有效解决了长格式深度研究的训练挑战，DR Tulu模型在性能上达到或超过专有系统，为深度研究领域提供了开源解决方案，并发布了所有数据、模型和代码。

Abstract: Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.

</details>


### [77] [Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration](https://arxiv.org/abs/2511.19417)
*James Y. Huang,Sheng Zhang,Qianchu Liu,Guanghui Qin,Tinghui Zhu,Tristan Naumann,Muhao Chen,Hoifung Poon*

Main category: cs.CL

TL;DR: BeMyEyes是一个模块化多智能体框架，通过让高效的可视语言模型作为感知器与强大的LLM作为推理器进行对话协作，扩展LLM的多模态推理能力，无需训练大规模多模态模型。


<details>
  <summary>Details</summary>
Motivation: 扩展LLM到新模态（如视觉）通常需要开发成本高昂的大规模视觉语言模型，而较小的VLM缺乏前沿LLM的广泛知识和推理能力。

Method: 提出模块化多智能体框架，通过数据合成和监督微调管道训练感知器代理与推理器代理有效协作。

Result: 实验表明该框架解锁了LLM的多模态推理能力，轻量级开源解决方案（DeepSeek-R1 + Qwen2.5-VL-7B）在知识密集型多模态任务上超越GPT-4o等专有大模型。

Conclusion: 该方法展示了构建未来多模态推理系统的有效性、模块化和可扩展性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks](https://arxiv.org/abs/2511.17576)
*Rayan Aldajani*

Main category: cs.CV

TL;DR: AI模型使用正面身体图像和基本人体测量数据作为低成本替代DEXA扫描来估计体脂百分比，图像模型达到4.44%的RMSE和0.807的R²。


<details>
  <summary>Details</summary>
Motivation: 标准的DEXA扫描方法昂贵且难以普及，需要开发低成本、易获取的体脂估计方法。

Method: 使用535个样本数据集，包括253个人体测量数据和282张网络爬取的正面身体图像，开发了基于ResNet的图像模型和使用人体测量数据的回归模型。

Result: 图像模型实现了4.44%的均方根误差和0.807的决定系数，表明AI模型能够提供可接受的体脂估计精度。

Conclusion: AI辅助模型可以提供可访问且低成本的体脂估计，支持未来健康和健身领域的消费应用。

Abstract: Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.

</details>


### [79] [Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding](https://arxiv.org/abs/2511.17596)
*Yassir Benhammou,Suman Kalyan,Sujay Kumar*

Main category: cs.CV

TL;DR: 提出了一个多模态自编码器（MMAE），通过学习文本、音频和视觉数据的统一表示，实现广播内容元数据提取和语义聚类的端到端自动化。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统通常只处理单一模态，限制了其对广播材料中复杂跨模态关系的理解。广播和媒体组织需要自动化内容索引、标记和元数据生成。

Method: 使用多模态自编码器（MMAE）在LUMA数据集上训练，通过最小化跨模态的联合重构损失来学习模态不变的语义结构，无需依赖大型配对或对比数据集。

Result: 在聚类和对齐指标（Silhouette、ARI、NMI）上相比线性基线有显著改进，表明基于重构的多模态嵌入可作为广播档案中可扩展元数据生成和跨模态检索的基础。

Conclusion: 重构驱动的多模态学习有潜力提升现代广播工作流中的自动化、可搜索性和内容管理效率。

Abstract: Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.

</details>


### [80] [BCWildfire: A Long-term Multi-factor Dataset and Deep Learning Benchmark for Boreal Wildfire Risk Prediction](https://arxiv.org/abs/2511.17597)
*Zhengsen Xu,Sibo Cheng,Hongjie He,Lanying Wang,Wentao Sun,Jonathan Li,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出了一个25年、每日分辨率的野火数据集，覆盖2.4亿公顷区域，包含38个协变量，评估了多种时间序列预测模型。


<details>
  <summary>Details</summary>
Motivation: 野火风险预测因燃料条件、气象、地形和人类活动等复杂相互作用而具有挑战性，且缺乏支持长期时间建模、大规模空间覆盖和多模态驱动因素的公开基准数据集。

Method: 构建了一个25年、每日分辨率的野火数据集，涵盖不列颠哥伦比亚省及周边地区，包含38个协变量（活跃火点探测、天气变量、燃料条件、地形特征和人为因素），并评估了CNN、线性、Transformer和Mamba等多种时间序列预测模型。

Result: 创建了一个包含240万公顷区域、25年每日数据的野火基准数据集，并比较了不同模型的性能，同时研究了位置嵌入的有效性和不同火驱动因素的相对重要性。

Conclusion: 该数据集填补了野火预测领域基准数据的空白，为长期时间建模、大规模空间分析和多模态驱动因素研究提供了重要资源，相关代码和数据已开源。

Abstract: Wildfire risk prediction remains a critical yet challenging task due to the complex interactions among fuel conditions, meteorology, topography, and human activity. Despite growing interest in data-driven approaches, publicly available benchmark datasets that support long-term temporal modeling, large-scale spatial coverage, and multimodal drivers remain scarce. To address this gap, we present a 25-year, daily-resolution wildfire dataset covering 240 million hectares across British Columbia and surrounding regions. The dataset includes 38 covariates, encompassing active fire detections, weather variables, fuel conditions, terrain features, and anthropogenic factors. Using this benchmark, we evaluate a diverse set of time-series forecasting models, including CNN-based, linear-based, Transformer-based, and Mamba-based architectures. We also investigate effectiveness of position embedding and the relative importance of different fire-driving factors. The dataset and the corresponding code can be found at https://github.com/SynUW/mmFire

</details>


### [81] [Robustness of Structured Data Extraction from Perspectively Distorted Documents](https://arxiv.org/abs/2511.17607)
*Hyakka Nakada,Yoshiyasu Tanaka*

Main category: cs.CV

TL;DR: 本文研究了透视畸变对多模态大语言模型OCR数据提取准确性的影响，发现结构识别准确率显著下降，但简单的旋转校正可以改善性能。


<details>
  <summary>Details</summary>
Motivation: 真实世界文档图像通常存在透视畸变而不仅仅是平面旋转，这会影响多模态LLMs的数据提取准确性，但相关研究较少。

Method: 通过观察典型文档畸变模式，将其近似为等腰梯形变换，将参数从8个减少到2个（旋转角度和畸变比例），在合成文档上评估字符识别和结构识别准确率。

Result: 文档畸变显著降低了结构识别准确率，但简单的旋转校正可以改善性能。

Conclusion: 透视畸变对多模态LLMs的OCR性能有显著影响，特别是结构识别方面，但可以通过简单的预处理技术来缓解。

Abstract: Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.

</details>


### [82] [3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF](https://arxiv.org/abs/2511.17609)
*Linh Van Ma,Unse Fatima,Tepy Sokun Chriv,Haroon Imran,Moongu Jeon*

Main category: cs.CV

TL;DR: 提出一种基于无迹卡尔曼滤波的多相机单目标跟踪算法，将2D边界框或关键点标注融合为精确的3D地面真值，能处理遮挡问题并输出完整3D形状


<details>
  <summary>Details</summary>
Motivation: 精确的3D地面真值对于自动驾驶、监控和机器人等应用至关重要，现有方法通常只能提供地面平面信息

Method: 使用无迹卡尔曼滤波器融合来自多个标定相机的2D标注，通过单应性投影和UKF融合将2D图像坐标转换为鲁棒的3D世界坐标

Result: 在CMC、Wildtrack和Panoptic数据集上评估，相比现有3D地面真值显示出高精度的3D定位能力

Conclusion: 该方法为多相机系统提供了一种可扩展且完全自动化的解决方案，仅需2D图像标注即可输出完整3D形状

Abstract: Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.

</details>


### [83] [Unified Low-Light Traffic Image Enhancement via Multi-Stage Illumination Recovery and Adaptive Noise Suppression](https://arxiv.org/abs/2511.17612)
*Siddiqua Namrah*

Main category: cs.CV

TL;DR: 提出了一种完全无监督的多阶段深度学习框架，用于增强低光照交通图像，通过分解图像为光照和反射率分量，并使用三个专门模块进行渐进式优化。


<details>
  <summary>Details</summary>
Motivation: 低光照交通图像在自动驾驶、智能交通和城市监控系统中存在能见度差、噪声、运动模糊、光照不均和眩光等问题，影响物体检测和场景理解任务的可靠性。

Method: 采用多阶段深度学习框架，将图像分解为光照和反射率分量，通过三个专门模块进行优化：光照适应模块进行全局和局部亮度校正；反射率恢复模块使用空间通道注意力进行噪声抑制和结构细节恢复；过曝光补偿模块重建饱和区域并平衡场景亮度。

Result: 在通用和交通专用数据集上的实验表明，该方法在定量指标（PSNR、SSIM、LPIPS、NIQE）和定性视觉质量方面均优于现有最先进方法。

Conclusion: 该方法能有效增强低光照交通场景的能见度，保持结构完整性，并提高下游感知任务的可靠性。

Abstract: Enhancing low-light traffic images is crucial for reliable perception in autonomous driving, intelligent transportation, and urban surveillance systems. Nighttime and dimly lit traffic scenes often suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare from vehicle headlights or street lamps, which hinder tasks such as object detection and scene understanding. To address these challenges, we propose a fully unsupervised multi-stage deep learning framework for low-light traffic image enhancement. The model decomposes images into illumination and reflectance components, progressively refined by three specialized modules: (1) Illumination Adaptation, for global and local brightness correction; (2) Reflectance Restoration, for noise suppression and structural detail recovery using spatial-channel attention; and (3) Over-Exposure Compensation, for reconstructing saturated regions and balancing scene luminance. The network is trained using self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses, eliminating the need for paired ground-truth images. Experiments on general and traffic-specific datasets demonstrate superior performance over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality. Our approach enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios.

</details>


### [84] [HSMix: Hard and Soft Mixing Data Augmentation for Medical Image Segmentation](https://arxiv.org/abs/2511.17614)
*Danyang Sun,Fadi Dornaika,Nagore Barrena*

Main category: cs.CV

TL;DR: HSMix是一种用于医学图像分割的局部图像编辑数据增强方法，通过硬混合和软混合结合超像素区域和亮度调整来增强数据多样性


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临数据稀缺和过拟合问题，现有自监督和半监督学习方法复杂，而数据增强方法在分割任务中的有效性尚未充分探索

Method: 提出HSMix方法：硬混合将两个源图像的同质区域（超像素）组合，软混合通过基于局部显著性系数的亮度混合调整组合区域亮度，同时对分割掩码进行相同混合操作

Result: 实验证明该方法在各种医学分割任务中有效，能够充分利用轮廓和显著性先验信息，保持局部语义信息的同时丰富增强多样性

Conclusion: HSMix是一种即插即用的模型无关解决方案，适用于多种医学成像模态，能有效解决医学图像分割中的数据稀缺问题

Abstract: Due to the high cost of annotation or the rarity of some diseases, medical image segmentation is often limited by data scarcity and the resulting overfitting problem. Self-supervised learning and semi-supervised learning can mitigate the data scarcity challenge to some extent. However, both of these paradigms are complex and require either hand-crafted pretexts or well-defined pseudo-labels. In contrast, data augmentation represents a relatively simple and straightforward approach to addressing data scarcity issues. It has led to significant improvements in image recognition tasks. However, the effectiveness of local image editing augmentation techniques in the context of segmentation has been less explored. We propose HSMix, a novel approach to local image editing data augmentation involving hard and soft mixing for medical semantic segmentation. In our approach, a hard-augmented image is created by combining homogeneous regions (superpixels) from two source images. A soft mixing method further adjusts the brightness of these composed regions with brightness mixing based on locally aggregated pixel-wise saliency coefficients. The ground-truth segmentation masks of the two source images undergo the same mixing operations to generate the associated masks for the augmented images. Our method fully exploits both the prior contour and saliency information, thus preserving local semantic information in the augmented images while enriching the augmentation space with more diversity. Our method is a plug-and-play solution that is model agnostic and applicable to a range of medical imaging modalities. Extensive experimental evidence has demonstrated its effectiveness in a variety of medical segmentation tasks. The source code is available in https://github.com/DanielaPlusPlus/HSMix.

</details>


### [85] [Plug-and-Play Multi-Concept Adaptive Blending for High-Fidelity Text-to-Image Synthesis](https://arxiv.org/abs/2511.17615)
*Young-Beom Woo*

Main category: cs.CV

TL;DR: PnP-MIX是一种无需调优的即插即用多概念自适应融合方法，用于高保真文本到图像合成，能够无缝嵌入多个个性化概念到单张生成图像中。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂多对象场景时表现不佳，会导致个性化区域和非个性化区域的意外改变，破坏提示结构并造成语义不一致。

Method: 使用引导外观注意力忠实反映每个个性化概念的外观；采用掩码引导噪声混合策略保护非个性化区域完整性；提出背景稀释++策略减少概念泄漏。

Result: 广泛的实验结果表明，PnP-MIX在单概念和多概念个性化场景中均优于现有方法，展现了其鲁棒性和优越性能。

Conclusion: PnP-MIX无需额外模型调优即可实现多概念的高质量融合，解决了现有方法在复杂场景中的语义不一致和概念泄漏问题。

Abstract: Integrating multiple personalized concepts into a single image has recently become a significant area of focus within Text-to-Image (T2I) generation. However, existing methods often underperform on complex multi-object scenes due to unintended alterations in both personalized and non-personalized regions. This not only fails to preserve the intended prompt structure but also disrupts interactions among regions, leading to semantic inconsistencies. To address this limitation, we introduce plug-and-play multi-concept adaptive blending for high-fidelity text-to-image synthesis (PnP-MIX), an innovative, tuning-free approach designed to seamlessly embed multiple personalized concepts into a single generated image. Our method leverages guided appearance attention to faithfully reflect the intended appearance of each personalized concept. To further enhance compositional fidelity, we present a mask-guided noise mixing strategy that preserves the integrity of non-personalized regions such as the background or unrelated objects while enabling the precise integration of personalized objects. Finally, to mitigate concept leakage, i.e., the inadvertent leakage of personalized concept features into other regions, we propose background dilution++, a novel strategy that effectively reduces such leakage and promotes accurate localization of features within personalized regions. Extensive experimental results demonstrate that PnP-MIX consistently surpasses existing methodologies in both single- and multi-concept personalization scenarios, underscoring its robustness and superior performance without additional model tuning.

</details>


### [86] [Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach](https://arxiv.org/abs/2511.17618)
*Ju-Young Oh*

Main category: cs.CV

TL;DR: FIQ框架通过生成描述性Q&A对来增强VQA模型的基础理解能力，结合VQ-CAlign模块对齐视觉特征和问题嵌入，在SUTD-TrafficQA数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有VQA方法主要依赖事件中心的Q&A对，缺乏对场景基础信息（如物体类别、空间配置、视觉属性）的理解，限制了模型的泛化和推理能力。

Method: 提出FIQ框架：1）从视频中提取描述性信息生成Q&A对，丰富数据集；2）设计VQ-CAlign模块对齐任务特定问题嵌入与视觉特征。

Result: 在SUTD-TrafficQA数据集上的实验结果表明，FIQ超越了现有基线方法，达到了最先进的性能。

Conclusion: 通过增强对视频内容的基础理解，FIQ框架有效提升了VQA模型的推理能力和泛化性能。

Abstract: Conventional VQA approaches primarily rely on question-answer (Q&A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.

</details>


### [87] [Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds](https://arxiv.org/abs/2511.17619)
*Qinghao Meng,Junbo Yin,Jianbing Shen,Yunde Jia*

Main category: cs.CV

TL;DR: 提出角点对齐回归方法，将3D目标检测的预测目标从易受稀疏点云影响的不稳定中心点转移到几何信息更丰富的角点，并支持弱监督学习。


<details>
  <summary>Details</summary>
Motivation: 解决基于LiDAR的3D目标检测中中心对齐回归的根本不稳定性问题，因为物体中心常落在BEV视图的稀疏或空区域，导致边界框预测不准确。

Method: 设计角点感知检测头，将预测目标从中心转移到几何信息丰富的角点，利用角点和图像2D框之间的几何约束实现弱监督学习。

Result: 在KITTI数据集上，相比基于中心的方法AP提升3.5%，仅使用BEV角点标注即可达到全监督83%的准确率。

Conclusion: 角点对齐回归策略有效解决了中心对齐回归的不稳定性问题，并支持弱监督学习，显著提升了3D目标检测性能。

Abstract: Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.

</details>


### [88] [BD-Net: Has Depth-Wise Convolution Ever Been Applied in Binary Neural Networks?](https://arxiv.org/abs/2511.17633)
*DoYoung Kim,Jin-Seop Lee,Noo-ri Kim,SungJoon Lee,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 提出1.58位卷积增强表达能力，使用预BN残差连接稳定优化，首次成功二值化深度卷积，在ImageNet上达到33M OPs，在多个数据集上优于现有方法最高9.3个百分点。


<details>
  <summary>Details</summary>
Motivation: 解决二值神经网络中极端量化限制表示能力和训练不稳定的问题，特别是在轻量级架构的深度卷积中。

Method: 使用1.58位卷积增强表达能力，引入预BN残差连接来改善Hessian条件数从而稳定优化，首次实现深度卷积的二值化。

Result: 在ImageNet上使用MobileNet V1达到33M OPs，在CIFAR-10、CIFAR-100、STL-10、Tiny ImageNet和Oxford Flowers 102等多个数据集上优于现有方法，准确率提升最高达9.3个百分点。

Conclusion: 该方法在二值神经网络中实现了新的最先进性能，成功解决了深度卷积二值化的挑战，为轻量级模型提供了高效的解决方案。

Abstract: Recent advances in model compression have highlighted the potential of low-bit precision techniques, with Binary Neural Networks (BNNs) attracting attention for their extreme efficiency. However, extreme quantization in BNNs limits representational capacity and destabilizes training, posing significant challenges for lightweight architectures with depth-wise convolutions. To address this, we propose a 1.58-bit convolution to enhance expressiveness and a pre-BN residual connection to stabilize optimization by improving the Hessian condition number. These innovations enable, to the best of our knowledge, the first successful binarization of depth-wise convolutions in BNNs. Our method achieves 33M OPs on ImageNet with MobileNet V1, establishing a new state-of-the-art in BNNs by outperforming prior methods with comparable OPs. Moreover, it consistently outperforms existing methods across various datasets, including CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and Oxford Flowers 102, with accuracy improvements of up to 9.3 percentage points.

</details>


### [89] [Efficient Score Pre-computation for Diffusion Models via Cross-Matrix Krylov Projection](https://arxiv.org/abs/2511.17634)
*Kaikwan Lau,Andrew S. Na,Justin W. L. Wan*

Main category: cs.CV

TL;DR: 提出一种加速基于分数的扩散模型的新框架，通过将稳定扩散模型转换为Fokker-Planck公式，并采用跨矩阵Krylov投影方法来减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 标准稳定扩散模型在训练多张图像时需要求解大型线性系统，导致计算成本高昂，需要更高效的求解方法。

Method: 将稳定扩散模型转换为Fokker-Planck公式，提出跨矩阵Krylov投影方法，利用矩阵间的数学相似性，通过从"种子"矩阵构建共享子空间来快速求解后续"目标"矩阵。

Result: 相比标准稀疏求解器，时间减少15.8%到43.7%；在去噪任务中比DDPM基线快达115倍；在固定计算预算下能生成高质量图像，而DDPM无法生成可识别内容。

Conclusion: 该方法是在资源受限环境下进行高效生成的实际可行方法。

Abstract: This paper presents a novel framework to accelerate score-based diffusion models. It first converts the standard stable diffusion model into the Fokker-Planck formulation which results in solving large linear systems for each image. For training involving many images, it can lead to a high computational cost. The core innovation is a cross-matrix Krylov projection method that exploits mathematical similarities between matrices, using a shared subspace built from ``seed" matrices to rapidly solve for subsequent ``target" matrices. Our experiments show that this technique achieves a 15.8\% to 43.7\% time reduction over standard sparse solvers. Additionally, we compare our method against DDPM baselines in denoising tasks, showing a speedup of up to 115$\times$. Furthermore, under a fixed computational budget, our model is able to produce high-quality images while DDPM fails to generate recognizable content, illustrating our approach is a practical method for efficient generation in resource-limited settings.

</details>


### [90] [Upstream Probabilistic Meta-Imputation for Multimodal Pediatric Pancreatitis Classification](https://arxiv.org/abs/2511.17635)
*Max A. Nelson,Elif Keles,Eminenur Sen Tasci,Merve Yazol,Halil Ertugrul Aktas,Ziliang Hong,Andrea Mia Bejar,Gorkem Durak,Oznur Leman Boyunaga,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出UPMI方法，在元特征空间进行数据增强，提升小儿胰腺炎诊断性能


<details>
  <summary>Details</summary>
Motivation: 小儿胰腺炎诊断面临样本有限和多模态影像复杂性的挑战，传统机器学习方法效果受限

Method: UPMI方法：在低维元特征空间使用模态特定逻辑回归生成概率输出，通过GMM拟合和采样合成元特征，训练随机森林元分类器

Result: 在67名儿科患者数据上，UPMI获得平均AUC 0.908±0.072，相比仅使用真实数据的基线(AUC 0.864±0.061)提升约5%

Conclusion: UPMI是一种有效的轻量级数据增强策略，能显著提升小儿胰腺炎多模态MRI诊断性能

Abstract: Pediatric pancreatitis is a progressive and debilitating inflammatory condition, including acute pancreatitis and chronic pancreatitis, that presents significant clinical diagnostic challenges. Machine learning-based methods also face diagnostic challenges due to limited sample availability and multimodal imaging complexity. To address these challenges, this paper introduces Upstream Probabilistic Meta-Imputation (UPMI), a light-weight augmentation strategy that operates upstream of a meta-learner in a low-dimensional meta-feature space rather than in image space. Modality-specific logistic regressions (T1W and T2W MRI radiomics) produce probability outputs that are transformed into a 7-dimensional meta-feature vector. Class-conditional Gaussian mixture models (GMMs) are then fit within each cross-validation fold to sample synthetic meta-features that, combined with real meta-features, train a Random Forest (RF) meta-classifier. On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieves a mean AUC of 0.908 $\pm$ 0.072, a $\sim$5% relative gain over a real-only baseline (AUC 0.864 $\pm$ 0.061).

</details>


### [91] [TSRE: Channel-Aware Typical Set Refinement for Out-of-Distribution Detection](https://arxiv.org/abs/2511.17636)
*Weijun Gao,Rundong He,Jinyang Dong,Yongshun Gong*

Main category: cs.CV

TL;DR: 提出了一种基于可区分性和活动性的典型集精炼方法，通过通道感知的典型集和偏度精炼来改进OOD检测，在ImageNet-1K和CIFAR-100基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有激活修正方法往往忽略通道的内在特性和分布偏斜，导致典型集估计不准确，可能不适当地包含异常激活。

Method: 1. 基于可区分性和活动性的典型集精炼方法，将激活修正为通道感知的典型集；2. 引入偏度精炼来减轻典型集估计中的分布偏差；3. 使用修正后的激活计算能量得分进行OOD检测。

Result: 在ImageNet-1K和CIFAR-100基准测试中实现了最先进的性能，并在不同骨干网络和得分函数上具有良好泛化能力。

Conclusion: 提出的通道感知典型集精炼方法有效改进了OOD检测性能，通过考虑通道特性和分布偏斜提高了典型集估计的准确性。

Abstract: Out-of-Distribution (OOD) detection is a critical capability for ensuring the safe deployment of machine learning models in open-world environments, where unexpected or anomalous inputs can compromise model reliability and performance. Activation-based methods play a fundamental role in OOD detection by mitigating anomalous activations and enhancing the separation between in-distribution (ID) and OOD data. However, existing methods apply activation rectification while often overlooking channel's intrinsic characteristics and distributional skewness, which results in inaccurate typical set estimation. This discrepancy can lead to the improper inclusion of anomalous activations across channels. To address this limitation, we propose a typical set refinement method based on discriminability and activity, which rectifies activations into a channel-aware typical set. Furthermore, we introduce a skewness-based refinement to mitigate distributional bias in typical set estimation. Finally, we leverage the rectified activations to compute the energy score for OOD detection. Experiments on the ImageNet-1K and CIFAR-100 benchmarks demonstrate that our method achieves state-of-the-art performance and generalizes effectively across backbones and score functions.

</details>


### [92] [SWITCH: Benchmarking Modeling and Handling of Tangible Interfaces in Long-horizon Embodied Scenarios](https://arxiv.org/abs/2511.17649)
*Jieru Lin,Zhiwei Yu,Börje F. Karlsson*

Main category: cs.CV

TL;DR: SWITCH是一个具身智能基准测试，评估AI在现实环境中与物理控制界面交互的能力，包括视觉问答、语义UI定位、动作生成、状态转换预测和结果验证。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少测试AI在真实环境中的基础能力、部分可观察性和事后验证，而现实世界中的控制界面交互需要常识推理、物理推理以及时空因果预测。

Method: 通过迭代发布创建SWITCH基准测试，第一版SWITCH-Basic包含351个任务，覆盖98种真实设备和家电，使用第一人称RGB视频输入评估五个互补能力。

Result: 商业和开源的多模态大模型在单步交互任务上表现不稳定，过度依赖文本线索而未能充分利用视觉或视频证据，高分可能掩盖实际失败。

Conclusion: SWITCH提供了数据、代码和保留集，支持可复现评估和社区贡献，旨在推动更具挑战性的基准测试迭代和训练数据集的创建。

Abstract: Autonomous intelligence requires not only perception and reasoning, but critically, effective interaction with the existing world and its infrastructure. Everyday environments are rich in tangible control interfaces (TCIs), e.g., light switches, appliance panels, and embedded GUIs, that demand commonsense and physics reasoning, but also causal prediction and outcome verification in time and space (e.g., delayed heating, remote lights). Moreover, failures here have potential safety implications, yet current benchmarks rarely test grounding, partial observability (video), or post-hoc verification in situated settings. We introduce SWITCH (Semantic World Interface Tasks for Control and Handling), an embodied, task-driven benchmark created through iterative releases to probe these gaps. Its first iteration, SWITCH-Basic, evaluates five complementary abilities:task-aware VQA, semantic UI grounding, action generation, state-transition prediction, and result verification, under egocentric RGB video input and device diversity. Across 351 tasks spanning 98 real devices and appliances, commercial and open LMMMs exhibit inconsistent performance even on single-step interactions, often over-relying on textual cues and under-using visual or video evidence (and high aggregate scores can mask such failures). SWITCH provides data, code, and held-out splits to enable reproducible evaluation and community contributions toward more challenging future iterations of the benchmark and the creation of training datasets. Benchmark resources are available at: https://github.com/BAAI-Agents/SWITCH.

</details>


### [93] [Explainable Deep Learning for Brain Tumor Classification: Comprehensive Benchmarking with Dual Interpretability and Lightweight Deployment](https://arxiv.org/abs/2511.17655)
*Md. Mohaiminul Islam,Md. Mofazzal Hossen,Maher Ali Rusho,Nahiyan Nazah Ridita,Zarin Tasnia Shanta,Md. Simanto Haider,Ahmed Faizul Haque Dhrubo,Md. Khurshid Jahan,Mohammad Abdul Qayum*

Main category: cs.CV

TL;DR: 开发了一个完整的深度学习系统用于脑肿瘤MRI图像自动分类，包括6种架构对比，其中轻量级CNN模型在保持高准确率的同时实现了边缘设备实时推理。


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤分类中模型可解释性、部署性和标准化评估的问题，为资源受限的医疗环境提供可信赖的AI解决方案。

Method: 使用5种ImageNet预训练模型和1个自定义紧凑CNN，采用标准化预处理和训练协议，结合Grad-CAM和GradientShap进行可解释性分析。

Result: Inception-ResNet V2达到99.53%测试准确率，轻量CNN达到96.49%准确率且模型大小仅为1.31M参数，比Inception-ResNet V2小100倍。

Conclusion: 提供了一个端到端的可信AI框架，在准确性、可解释性和部署性方面表现优异，适合在先进和资源受限的医疗系统中临床筛查使用。

Abstract: Our study provides a full deep learning system for automated classification of brain tumors from MRI images, includes six benchmarked architectures (five ImageNet-pre-trained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom built, compact CNN (1.31M params)). The study moves the needle forward in a number of ways, including (1) full standardization of assessment with respect to preprocessing, training sets/protocols (optimizing networks with the AdamW optimizer, CosineAnnealingLR, patiene for early stopping = 7), and metrics to assess performance were identical along all models; (2) a high level of confidence in the localizations based on prior studies as both Grad-CAM and GradientShap explanation were used to establish anatomically important and meaningful attention regions and address the black-box issue; (3) a compact 1.31 million parameter CNN was developed that achieved 96.49% testing accuracy and was 100 times smaller than Inception-ResNet V2 while permitting real-time inference (375ms) on edge devices; (4) full evaluation beyond accuracy reporting based on measures of intersection over union, Hausdorff distance, and precision-recall curves, and confusion matrices across all splits. Inception-ResNet V2 reached state-of-the-art performance, achieving a 99.53% accuracy on testing and obtaining a precision, recall, and F1-score of at least 99.50% dominant performance based on metrics of recent studies. We demonstrated a lightweight model that is suitable to deploy on devices that do not have multi-GPU infrastructure in under-resourced settings. This end-to-end solution considers accuracy, interpretability, and deployability of trustworthy AI to create the framework necessary for performance assessment and deployment within advance and low-resource healthcare systems to an extent that enabled participation at the clinical screening and triage level.

</details>


### [94] [MedPEFT-CL: Dual-Phase Parameter-Efficient Continual Learning with Medical Semantic Adapter and Bidirectional Memory Consolidation](https://arxiv.org/abs/2511.17668)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: 提出了MedPEFT-CL框架，通过双阶段架构解决医学视觉语言分割模型在新任务学习中的灾难性遗忘问题，使用语义驱动的适配器分配和双向Fisher记忆协调来实现参数高效的持续学习。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言分割模型在适应新解剖结构时会出现灾难性遗忘，需要完全重新训练，限制了临床部署。针对医学视觉语言任务的持续学习方法研究不足。

Method: 基于CLIPSeg的双阶段架构：自适应学习阶段使用语义相似度适配器分配和参数高效微调；知识巩固阶段采用双向Fisher记忆协调。包括语义驱动适配器分配机制、双模态LoRA适配和双向Fisher记忆协调。

Result: 在多个医学数据集上的实验表明，该框架在最小参数开销下实现了优越的遗忘缓解和性能保持。

Conclusion: MedPEFT-CL框架在医学视觉语言场景中实现了有效的持续学习，显著减少了可训练参数同时保持了跨模态学习能力。

Abstract: Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits their clinical deployment. Although continual learning approaches have been studied for various applications, targeted research on continual learning approaches specifically designed for medical vision-language tasks remains underexplored. We propose MedPEFT-CL, a parameter-efficient continual learning framework that addresses both efficient learning of new tasks and preservation of previous knowledge through a dual-phase architecture based on CLIPSeg. Our dual-phase architecture features an adaptive learning phase that employs semantic similarity-based adapter allocation and parameter-efficient fine-tuning for medical tasks through prompt similarity analysis, and a knowledge consolidation phase employing bi-directional Fisher-memory coordination. This creates a reinforcing cycle: consolidation directs replay priorities while new tasks provide challenging samples that improve retention strategies. Our key contributions are: (1) a semantic-driven adapter allocation mechanism that enables efficient learning of new medical tasks, (2) a bi-modal LoRA adaptation that significantly reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination that prevents catastrophic forgetting from previous medical tasks. Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead, making the framework effective for continual learning in medical vision-language scenarios.

</details>


### [95] [Person Recognition in Aerial Surveillance: A Decade Survey](https://arxiv.org/abs/2511.17674)
*Kien Nguyen,Feng Liu,Clinton Fookes,Sridha Sridharan,Xiaoming Liu,Arun Ross*

Main category: cs.CV

TL;DR: 本文对过去10年150多篇关于以人为中心的空中监视任务的论文进行了系统性综述，重点分析了无人机等空中平台在人类检测、识别和重识别任务中的计算机视觉和机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 随着空中平台和成像传感器的快速发展，空中监视因其在规模、机动性、部署和隐蔽观察能力方面的独特优势而受到广泛关注。本文旨在系统梳理和分析空中监视任务的技术现状。

Method: 首先识别空中环境相比地面环境的独特挑战，然后整理分析可用的空中数据集，深入探讨现有方法如何应对空中挑战并提出改进技术。

Result: 提供了对150多篇论文的全面技术分析，涵盖了人类检测、识别和重识别等关键任务，并识别了当前方法的优势和局限。

Conclusion: 讨论了现有研究中的差距和开放性问题，为未来研究方向提供了指导。

Abstract: The rapid emergence of airborne platforms and imaging sensors is enabling new forms of aerial surveillance due to their unprecedented advantages in scale, mobility, deployment, and covert observation capabilities. This paper provides a comprehensive overview of 150+ papers over the last 10 years of human-centric aerial surveillance tasks from a computer vision and machine learning perspective. It aims to provide readers with an in-depth systematic review and technical analysis of the current state of aerial surveillance tasks using drones, UAVs, and other airborne platforms. The object of interest is humans, where human subjects are to be detected, identified, and re-identified. More specifically, for each of these tasks, we first identify unique challenges in performing these tasks in an aerial setting compared to the popular ground-based setting and subsequently compile and analyze aerial datasets publicly available for each task. Most importantly, we delve deep into the approaches in the aerial surveillance literature with a focus on investigating how they presently address aerial challenges and techniques for improvement. We conclude the paper by discussing the gaps and open research questions to inform future research avenues.

</details>


### [96] [Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models](https://arxiv.org/abs/2511.17681)
*Weiyi Lv,Ning Zhang,Hanyang Sun,Haoran Jiang,Kai Zhao,Jing Xiao,Dan Zeng*

Main category: cs.CV

TL;DR: 提出了VMRMOT框架，通过引入运动模态来增强视觉模态与语言参考之间的对齐，解决RMOT中静态参考无法捕捉对象动态运动变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RMOT基准只描述对象的外观、相对位置和初始运动状态，这种静态调节无法捕捉对象运动的动态变化（如速度变化和运动方向变化），导致静态参考与动态视觉模态之间存在时间差异，限制了多模态跟踪性能。

Method: 提出VMRMOT框架：1）从对象动态行为中提取运动感知描述，利用MLLMs的时间推理能力提取运动特征作为运动模态；2）设计视觉-运动-参考对齐模块（VMRA）分层对齐视觉查询与运动和参考线索；3）开发运动引导预测头（MGPH）利用运动模态增强预测头性能。

Result: 在多个RMOT基准上的广泛实验表明，VMRMOT优于现有的最先进方法。

Conclusion: VMRMOT是首个在RMOT任务中使用MLLMs进行视觉-参考对齐的方法，通过引入运动模态有效解决了静态参考与动态视觉模态之间的对齐问题。

Abstract: Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.

</details>


### [97] [Understanding Counting Mechanisms in Large Language and Vision-Language Models](https://arxiv.org/abs/2511.17699)
*Hosein Hasani,Amirmohammad Izadi,Fatemeh Askari,Mobin Bagherian,Sadegh Mohammadian,Mohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 本文研究LLMs和LVLMs在计数任务中如何表示和处理数字信息，通过因果干预和激活修补分析发现模型内部存在逐层递增的数字表示机制和计数器机制。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型和大视觉语言模型在计数任务中如何内部表示和计算数字信息，理解其工作机制。

Method: 使用重复文本和视觉项目的受控实验，通过因果中介分析和激活修补技术，开发专门的CountScope工具进行机制可解释性分析。

Result: 发现单个token或视觉特征编码潜在位置计数信息，数字表示逐层递增，存在内部计数器机制，视觉模型中数字信息出现在视觉嵌入中并随空间构图变化。

Conclusion: 计数在LLMs中表现为结构化的逐层过程，在LVLMs中遵循相同模式但受视觉编码器特性影响，模型依赖分隔符等结构线索作为计数捷径。

Abstract: This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count information that can be extracted and transferred across contexts. Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones. We identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts. In LVLMs, numerical information also appears in visual embeddings, shifting between background and foreground regions depending on spatial composition. Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts and influence the accuracy of numerical predictions. Overall, counting emerges as a structured, layerwise process in LLMs and follows the same general pattern in LVLMs, shaped by the properties of the vision encoder.

</details>


### [98] [Can Vision-Language Models Count? A Synthetic Benchmark and Analysis of Attention-Based Interventions](https://arxiv.org/abs/2511.17722)
*Saurav Sengupta,Nazanin Moradinasab,Jiebei Liu,Donald E. Brown*

Main category: cs.CV

TL;DR: 开发了一个合成基准数据集和评估框架，分析视觉语言模型在计数任务中的表现如何随图像和提示属性变化，并研究注意力干预对计数性能的影响。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在回答关于图像视觉属性的查询时，往往依赖训练中学到的固有偏见，特别是在需要关注图像特定区域的计数任务中，这些偏见会更加明显。

Method: 构建合成基准数据集和评估框架，使用开源视觉语言模型分析注意力分配如何随输入参数变化，并实施基于注意力的干预来调节不同层的视觉标记焦点。

Result: 实验表明，虽然视觉语言模型的计数性能仍然具有挑战性，特别是在高视觉或语言复杂度下，但某些注意力干预可以在计数性能上带来适度提升。

Conclusion: 视觉语言模型在计数任务中面临挑战，但通过注意力干预可以在一定程度上改善性能，特别是在复杂的视觉和语言条件下。

Abstract: Recent research suggests that Vision Language Models (VLMs) often rely on inherent biases learned during training when responding to queries about visual properties of images. These biases are exacerbated when VLMs are asked highly specific questions that require them to focus on particular areas of the image in tasks such as counting. We build upon this research by developing a synthetic benchmark dataset and evaluation framework to systematically determine how counting performance varies as image and prompt properties change. Using open-source VLMs, we then analyze how attention allocation fluctuates with varying input parameters (e.g. number of objects in the image, objects color, background color, objects texture, background texture, and prompt specificity). We further implement attention-based interventions to modulate focus on visual tokens at different layers and evaluate their impact on counting performance across a range of visual conditions. Our experiments reveal that while VLM counting performance remains challenging, especially under high visual or linguistic complexity, certain attention interventions can lead to modest gains in counting performance.

</details>


### [99] [AngioDG: Interpretable Channel-informed Feature-modulated Single-source Domain Generalization for Coronary Vessel Segmentation in X-ray Angiography](https://arxiv.org/abs/2511.17724)
*Mohammad Atwany,Mojtaba Lashgari,Robin P. Choudhury,Vicente Grau,Abhirup Banerjee*

Main category: cs.CV

TL;DR: 提出AngioDG方法，通过通道正则化策略解决X射线冠状动脉造影血管分割中的单源域泛化问题，提高模型在未见域上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，XCA是实时心脏介入的金标准。由于成像协议和患者群体的差异导致域偏移，且标注数据稀缺，需要单源域泛化方法来解决泛化问题。现有方法主要基于数据增强，但可能无法有效缓解对增强或合成域的过拟合。

Method: 提出AngioDG方法，采用通道正则化策略：识别早期特征通道对任务特定指标的贡献以促进可解释性，然后重新加权通道以校准和放大域不变特征，同时衰减域特定特征。

Result: 在6个X射线血管造影数据集上评估，相比其他方法获得了最佳的外分布性能，同时保持了稳定的域内测试性能。

Conclusion: AngioDG通过通道正则化策略有效提升了冠状动脉血管分割模型的泛化能力，在未见域上表现优异，为解决域偏移问题提供了有效解决方案。

Abstract: Cardiovascular diseases are the leading cause of death globally, with X-ray Coronary Angiography (XCA) as the gold standard during real-time cardiac interventions. Segmentation of coronary vessels from XCA can facilitate downstream quantitative assessments, such as measurement of the stenosis severity and enhancing clinical decision-making. However, developing generalizable vessel segmentation models for XCA is challenging due to variations in imaging protocols and patient demographics that cause domain shifts. These limitations are exacerbated by the lack of annotated datasets, making Single-source Domain Generalization (SDG) a necessary solution for achieving generalization. Existing SDG methods are largely augmentation-based, which may not guarantee the mitigation of overfitting to augmented or synthetic domains. We propose a novel approach, ``AngioDG", to bridge this gap by channel regularization strategy to promote generalization. Our method identifies the contributions of early feature channels to task-specific metrics for DG, facilitating interpretability, and then reweights channels to calibrate and amplify domain-invariant features while attenuating domain-specific ones. We evaluate AngioDG on 6 x-ray angiography datasets for coronary vessels segmentation, achieving the best out-of-distribution performance among the compared methods, while maintaining consistent in-domain test performance.

</details>


### [100] [The Potential and Limitations of Vision-Language Models for Human Motion Understanding: A Case Study in Data-Driven Stroke Rehabilitation](https://arxiv.org/abs/2511.17727)
*Victor Li,Naveenraj Kamalakannan,Avinash Parnandi,Heidi Schambra,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 本研究评估了视觉语言模型在卒中康复视频分析中的应用，发现现有VLM在精细运动理解方面存在局限，但通过优化提示和后期处理，在高层活动分类和运动检测方面展现出潜力。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在数字健康领域的应用潜力，特别是解决卒中康复中的两个关键挑战：从视频中自动量化康复剂量和功能障碍。

Method: 将康复剂量和功能障碍量化问题构建为运动识别任务，使用VLM处理，在29名健康对照和51名卒中幸存者队列上进行评估，采用优化提示和后期处理策略。

Result: 当前VLM缺乏精细运动理解能力：剂量估计与排除视觉信息的基线相当，功能障碍评分无法可靠预测。但通过优化，VLM能够从少量帧中分类高层活动，以中等准确度检测运动和抓握，对轻度障碍和健康参与者的剂量计数误差在25%以内。

Conclusion: 研究揭示了VLM在数据驱动卒中康复中的当前局限性和新兴机遇，为更广泛的临床视频分析提供了参考。

Abstract: Vision-language models (VLMs) have demonstrated remarkable performance across a wide range of computer-vision tasks, sparking interest in their potential for digital health applications. Here, we apply VLMs to two fundamental challenges in data-driven stroke rehabilitation: automatic quantification of rehabilitation dose and impairment from videos. We formulate these problems as motion-identification tasks, which can be addressed using VLMs. We evaluate our proposed framework on a cohort of 29 healthy controls and 51 stroke survivors. Our results show that current VLMs lack the fine-grained motion understanding required for precise quantification: dose estimates are comparable to a baseline that excludes visual information, and impairment scores cannot be reliably predicted. Nevertheless, several findings suggest future promise. With optimized prompting and post-processing, VLMs can classify high-level activities from a few frames, detect motion and grasp with moderate accuracy, and approximate dose counts within 25% of ground truth for mildly impaired and healthy participants, all without task-specific training or finetuning. These results highlight both the current limitations and emerging opportunities of VLMs for data-driven stroke rehabilitation and broader clinical video analysis.

</details>


### [101] [VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.17731)
*Lingxiao Li,Yifan Wang,Xinyan Gao,Chen Tang,Xiangyu Yue,Chenyu You*

Main category: cs.CV

TL;DR: 提出了VisReason大规模数据集，用于提升多模态大语言模型的视觉思维链推理能力，包含489K标注样本和165K专家级子集VisReason-Pro，显著改善了模型的逐步推理准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉思维链资源通常规模小、领域特定或缺乏类似人类的逐步推理结构，限制了多模态大语言模型在视觉理解中的复杂推理能力开发。

Method: 构建VisReason数据集，包含四个多样化领域的489K标注样本，每个样本都有人类化的多轮推理依据；进一步创建VisReason-Pro子集，使用更强的GPT专家标注器生成详细推理轨迹和3D空间定位。

Result: 在Qwen2.5-VL模型上微调VisReason和VisReason-Pro后，在逐步视觉推理准确性、可解释性和跨基准泛化方面取得了显著提升。

Conclusion: VisReason为多模态大语言模型提供了更系统和可泛化的推理能力，是培养人类化视觉推理的重要基础，推动下一代多模态智能发展。

Abstract: Chain-of-Thought (CoT) prompting has proven remarkably effective for eliciting complex reasoning in large language models (LLMs). Yet, its potential in multimodal large language models (MLLMs) remains largely untapped, hindered by the absence of large-scale datasets that capture the rich, spatially grounded reasoning intrinsic to visual understanding. Existing visual-CoT resources are typically small, domain-specific, or lack the human-like stepwise structure necessary for compositional visual reasoning. In this paper, we introduce VisReason, a large-scale dataset designed to advance visual Chain-of-Thought reasoning. VisReason comprises 489K annotated examples spanning four diverse domains, each featuring multi-round, human-like rationales that guide MLLMs through interpretable visual reasoning steps. Building upon this, we curate VisReason-Pro, a 165K subset produced with a stronger expert-level GPT annotator, enriched with detailed reasoning traces and 3D spatial grounding via depth-informed annotations. Fine-tuning the state-of-the-art Qwen2.5-VL model on VisReason and VisReason-Pro yields substantial improvements in step-by-step visual reasoning accuracy, interpretability, and cross-benchmark generalization. These results demonstrate that VisReason equips MLLMs with more systematic and generalizable reasoning capabilities. We envision VisReason as a cornerstone for cultivating human-like visual reasoning, paving the way toward the next generation of multimodal intelligence.

</details>


### [102] [Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders](https://arxiv.org/abs/2511.17735)
*Samuel Stevens,Jacob Beattie,Tanya Berger-Wolf,Yu Su*

Main category: cs.CV

TL;DR: 稀疏自编码器(SAEs)可用于从基础模型表示中进行开放式特征发现，支持科学数据的无监督模式探索


<details>
  <summary>Details</summary>
Motivation: 科学档案包含海量数据，但现有方法只能针对预设目标提取结构，不支持开放式发现未知模式

Method: 使用稀疏自编码器分解基础模型的内部表示，在受控的再发现研究中评估特征与语义概念的对齐度

Result: 在生态图像中成功发现了细粒度解剖结构，无需分割或部件标签，在概念对齐指标上优于无标签替代方法

Conclusion: 稀疏分解为探索科学基础模型所学内容提供了实用工具，是从确认转向真正发现的重要前提

Abstract: Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.

</details>


### [103] [AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations](https://arxiv.org/abs/2511.17747)
*Dawid Wolkiewicz,Anastasiya Pechko,Przemysław Spurek,Piotr Syga*

Main category: cs.CV

TL;DR: AEGIS是首个针对3D高斯化身的隐私保护身份掩蔽框架，通过对抗性扰动隐藏身份相关面部特征，同时保持感知真实性和功能完整性。


<details>
  <summary>Details</summary>
Motivation: 随着逼真3D面部化身的广泛采用，特别是使用高效3D高斯溅射表示的系统，带来了在线身份盗窃的新风险，而现有方法主要针对2D图像，缺乏对动态3D化身的鲁棒、视角一致的身份保护。

Method: AEGIS对高斯颜色系数应用对抗性扰动，通过预训练的人脸验证网络进行指导，确保跨多个视角的一致保护，无需重新训练或修改化身的几何结构。

Result: AEGIS实现了完全去识别化，将人脸检索和验证准确率降至0%，同时保持高感知质量（SSIM = 0.9555，PSNR = 35.52 dB），并保留了年龄、种族、性别和情感等关键面部属性。

Conclusion: 该方法展示了强大的隐私保护能力，同时产生最小的视觉失真，为3D高斯化身提供了有效的身份保护解决方案。

Abstract: The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.

</details>


### [104] [SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration](https://arxiv.org/abs/2511.17750)
*Zhimin Shao,Abhay Yadav,Rama Chellappa,Cheng Peng*

Main category: cs.CV

TL;DR: SPIDER是一个通用的特征匹配框架，通过结合2D和3D对应关系，在无约束场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统2D特征匹配在跨域场景（如航拍、室内外）中面临外观、尺度和视角变化的挑战，而现有3D基础模型虽然提供空间特征匹配，但对精细几何细节不敏感。

Method: 提出SPIDER框架，使用共享特征提取主干网络，配备两个专门网络头分别估计2D和3D对应关系，从粗到细进行匹配。

Result: SPIDER在包含大基线的无约束场景图像匹配基准测试中显著优于现有最先进方法。

Conclusion: SPIDER作为一个通用的图像匹配方法，通过整合2D和3D对应关系，在挑战性场景下表现出强大的匹配能力。

Abstract: Reliable image correspondences form the foundation of vision-based spatial perception, enabling recovery of 3D structure and camera poses. However, unconstrained feature matching across domains such as aerial, indoor, and outdoor scenes remains challenging due to large variations in appearance, scale and viewpoint. Feature matching has been conventionally formulated as a 2D-to-2D problem; however, recent 3D foundation models provides spatial feature matching properties based on two-view geometry. While powerful, we observe that these spatially coherent matches often concentrate on dominant planar regions, e.g., walls or ground surfaces, while being less sensitive to fine-grained geometric details, particularly under large viewpoint changes. To better understand these trade-offs, we first perform linear probe experiments to evaluate the performance of various vision foundation models for image matching. Building on these insights, we introduce SPIDER, a universal feature matching framework that integrates a shared feature extraction backbone with two specialized network heads for estimating both 2D-based and 3D-based correspondences from coarse to fine. Finally, we introduce an image-matching evaluation benchmark that focuses on unconstrained scenarios with large baselines. SPIDER significantly outperforms SoTA methods, demonstrating its strong ability as a universal image-matching method.

</details>


### [105] [CORA: Consistency-Guided Semi-Supervised Framework for Reasoning Segmentation](https://arxiv.org/abs/2511.17755)
*Prantik Howlader,Hoang Nguyen-Canh,Srijan Das,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: CORA是一个半监督推理分割框架，通过结合有限标注数据和大量未标注图像，使用条件视觉指令、噪声伪标签过滤和token级对比对齐三个组件，在最小监督下实现鲁棒的推理分割。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法受限于高质量像素标注和丰富语言监督的高成本，导致在分布偏移下性能脆弱，需要开发能够从有限标注数据中学习的框架。

Method: 1) 条件视觉指令编码对象间的空间和上下文关系；2) 基于多模态LLM在语义等价查询中输出一致性的噪声伪标签过滤器；3) 标注和伪标注样本间的token级对比对齐以增强特征一致性。

Result: 在Cityscapes数据集上仅需100张标注图像就达到SOTA，比基线提升+2.3%；在PanNuke数据集上仅需180张标注图像提升+2.4%。

Conclusion: CORA框架能够在最小监督下实现鲁棒的推理分割，在受限标注设置下优于现有基线方法。

Abstract: Reasoning segmentation seeks pixel-accurate masks for targets referenced by complex, often implicit instructions, requiring context-dependent reasoning over the scene. Recent multimodal language models have advanced instruction following segmentation, yet generalization remains limited. The key bottleneck is the high cost of curating diverse, high-quality pixel annotations paired with rich linguistic supervision leading to brittle performance under distribution shift. Therefore, we present CORA, a semi-supervised reasoning segmentation framework that jointly learns from limited labeled data and a large corpus of unlabeled images. CORA introduces three main components: 1) conditional visual instructions that encode spatial and contextual relationships between objects; 2) a noisy pseudo-label filter based on the consistency of Multimodal LLM's outputs across semantically equivalent queries; and 3) a token-level contrastive alignment between labeled and pseudo-labeled samples to enhance feature consistency. These components enable CORA to perform robust reasoning segmentation with minimal supervision, outperforming existing baselines under constrained annotation settings. CORA achieves state-of-the-art results, requiring as few as 100 labeled images on Cityscapes, a benchmark dataset for urban scene understanding, surpassing the baseline by $+2.3\%$. Similarly, CORA improves performance by $+2.4\%$ with only 180 labeled images on PanNuke, a histopathology dataset.

</details>


### [106] [Latent Dirichlet Transformer VAE for Hyperspectral Unmixing with Bundled Endmembers](https://arxiv.org/abs/2511.17757)
*Giancarlo Giannetti,Faisal Z. Qureshi*

Main category: cs.CV

TL;DR: 提出了LDVAE-T模型用于高光谱解混，结合Transformer的全局上下文建模能力和狄利克雷先验的物理约束，将材料视为捆绑端元而非固定光谱，在三个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像中的光谱混合会掩盖纯材料特征，需要开发能够处理材料内在变异性的解混方法，同时保持物理可解释性。

Method: 使用Transformer编码器提取狄利克雷分布的丰度，解码器为每个端元和每个补丁预测均值光谱和结构化协方差，通过混合学习到的捆绑端元形成重建。

Result: 在Samson、Jasper Ridge和HYDICE Urban三个数据集上，LDVAE-T在丰度估计和端元提取方面均优于现有最先进模型，分别通过均方根误差和光谱角距离衡量。

Conclusion: LDVAE-T通过结合Transformer架构和狄利克雷先验，成功处理了高光谱解混问题，能够表示材料内在变异性同时保持物理约束，在多个基准数据集上表现出色。

Abstract: Hyperspectral images capture rich spectral information that enables per-pixel material identification; however, spectral mixing often obscures pure material signatures. To address this challenge, we propose the Latent Dirichlet Transformer Variational Autoencoder (LDVAE-T) for hyperspectral unmixing. Our model combines the global context modeling capabilities of transformer architectures with physically meaningful constraints imposed by a Dirichlet prior in the latent space. This prior naturally enforces the sum-to-one and non-negativity conditions essential for abundance estimation, thereby improving the quality of predicted mixing ratios. A key contribution of LDVAE-T is its treatment of materials as bundled endmembers, rather than relying on fixed ground truth spectra. In the proposed method our decoder predicts, for each endmember and each patch, a mean spectrum together with a structured (segmentwise) covariance that captures correlated spectral variability. Reconstructions are formed by mixing these learned bundles with Dirichlet-distributed abundances garnered from a transformer encoder, allowing the model to represent intrinsic material variability while preserving physical interpretability. We evaluate our approach on three benchmark datasets, Samson, Jasper Ridge, and HYDICE Urban and show that LDVAE-T consistently outperforms state-of-the-art models in abundance estimation and endmember extraction, as measured by root mean squared error and spectral angle distance, respectively.

</details>


### [107] [Deepfake Geography: Detecting AI-Generated Satellite Images](https://arxiv.org/abs/2511.17766)
*Mansur Yerzhanuly*

Main category: cs.CV

TL;DR: 本研究比较了CNN和ViT在检测AI生成卫星图像方面的性能，发现ViT在准确性和鲁棒性方面显著优于CNN，准确率分别为95.11%和87.02%。


<details>
  <summary>Details</summary>
Motivation: 随着StyleGAN2和Stable Diffusion等生成模型的快速发展，卫星图像的真实性面临威胁，这对科学和安全领域的可靠分析和决策至关重要。尽管人脸深度伪造检测已得到广泛研究，但卫星图像检测面临地形级不一致性和结构伪影等独特挑战。

Method: 使用包含超过13万张标记RGB图像的DM-AER和FSI数据集，对CNN和ViT进行综合比较。采用Grad-CAM和Chefer注意力归因等架构特定可解释性方法来增强模型透明度。

Result: ViT在准确率（95.11%）和整体鲁棒性方面显著优于CNN（87.02%），这归因于ViT能够建模长距离依赖关系和全局语义结构。可解释性分析揭示了不同的检测行为，验证了模型可信度。

Conclusion: ViT在检测合成图像特有的结构不一致性和重复纹理模式方面表现出色。未来工作将扩展到多光谱和SAR模态，并集成频域分析以进一步加强检测能力，保护卫星图像完整性。

Abstract: The rapid advancement of generative models such as StyleGAN2 and Stable Diffusion poses a growing threat to the authenticity of satellite imagery, which is increasingly vital for reliable analysis and decision-making across scientific and security domains. While deepfake detection has been extensively studied in facial contexts, satellite imagery presents distinct challenges, including terrain-level inconsistencies and structural artifacts. In this study, we conduct a comprehensive comparison between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for detecting AI-generated satellite images. Using a curated dataset of over 130,000 labeled RGB images from the DM-AER and FSI datasets, we show that ViTs significantly outperform CNNs in both accuracy (95.11 percent vs. 87.02 percent) and overall robustness, owing to their ability to model long-range dependencies and global semantic structures. We further enhance model transparency using architecture-specific interpretability methods, including Grad-CAM for CNNs and Chefer's attention attribution for ViTs, revealing distinct detection behaviors and validating model trustworthiness. Our results highlight the ViT's superior performance in detecting structural inconsistencies and repetitive textural patterns characteristic of synthetic imagery. Future work will extend this research to multispectral and SAR modalities and integrate frequency-domain analysis to further strengthen detection capabilities and safeguard satellite imagery integrity in high-stakes applications.

</details>


### [108] [Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?](https://arxiv.org/abs/2511.17792)
*Dingrui Wang,Hongyuan Ye,Zhihao Liang,Zhexiao Sun,Zhaowei Lu,Yuchen Zhang,Yuyu Zhao,Yuan Gao,Marvin Seegert,Finn Schäfer,Haotong Qin,Wei Li,Luigi Palmieri,Felix Jahncke,Mattia Piccinini,Johannes Betz*

Main category: cs.CV

TL;DR: Target-Bench是首个专门评估世界模型在无地图路径规划中性能的基准测试，包含450个机器人收集的视频序列和SLAM真实轨迹。评估显示当前最先进的世界模型在机器人规划任务中存在显著局限性，但通过微调可以大幅提升性能。


<details>
  <summary>Details</summary>
Motivation: 虽然当前世界模型能生成高度逼真的视频，但它们在机器人路径规划方面的能力仍不明确且缺乏量化评估。需要专门的基准测试来评估世界模型在真实环境中的语义目标路径规划能力。

Method: 开发了Target-Bench基准测试，包含450个机器人收集的视频序列，涵盖45个语义类别，并基于SLAM提供真实轨迹。评估流程从生成的视频中恢复相机运动，并使用五个互补指标来衡量目标到达能力、轨迹精度和方向一致性。

Result: 评估了Sora 2、Veo 3.1和Wan系列等最先进模型。最佳现成模型(Wan2.2-Flash)仅获得0.299总分，显示当前世界模型在机器人规划任务中存在显著局限性。通过在数据集上微调开源5B参数模型，仅使用325个场景就达到了0.345总分，比基础版本提升400%以上，比最佳现成模型高15%。

Conclusion: 当前世界模型在机器人路径规划方面仍有很大改进空间，但通过针对性的微调可以显著提升性能。Target-Bench为评估和改进世界模型的规划能力提供了重要工具，代码和数据集将开源。

Abstract: While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.

</details>


### [109] [Attention Guided Alignment in Efficient Vision-Language Models](https://arxiv.org/abs/2511.17793)
*Shweta Mahajan,Hoang Le,Hyojin Park,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文分析了高效视觉语言模型中的注意力模式，发现基于拼接的架构难以区分语义匹配/不匹配的图像-文本对，这是导致物体幻觉的关键因素。为此提出了AGE-VLM框架，通过交叉注意力层和SAM的空间知识增强视觉基础能力，显著减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有高效视觉语言模型在视觉-文本对齐方面存在不足，特别是基于拼接的架构无法有效区分语义匹配关系，导致严重的物体幻觉问题。

Method: 提出AGE-VLM框架，在预训练小语言模型中引入交错的交叉注意力层，并利用Segment Anything Model的空间知识来增强视觉基础能力。

Result: 在多个视觉中心基准测试中，该方法在减少幻觉方面表现优于或与现有高效视觉语言模型相当。

Conclusion: 该研究为未来实现增强视觉和语言理解的视觉语言模型提供了有价值的见解，证明了通过改进注意力机制和空间知识整合可以有效缓解物体幻觉问题。

Abstract: Large Vision-Language Models (VLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to integrate visual and textual information. This paper presents a comprehensive analysis of attention patterns in efficient VLMs, revealing that concatenation-based architectures frequently fail to distinguish between semantically matching and non-matching image-text pairs. This is a key factor for object hallucination in these models. To address this, we introduce Attention-Guided Efficient Vision-Language Models (AGE-VLM), a novel framework that enhances visual grounding through interleaved cross-attention layers to instill vision capabilities in pretrained small language models. This enforces in VLM the ability "look" at the correct image regions by leveraging spatial knowledge distilled from the Segment Anything Model (SAM), significantly reducing hallucination. We validate our approach across different vision-centric benchmarks where our method is better or comparable to prior work on efficient VLMs. Our findings provide valuable insights for future research aimed at achieving enhanced visual and linguistic understanding in VLMs.

</details>


### [110] [Pillar-0: A New Frontier for Radiology Foundation Models](https://arxiv.org/abs/2511.17803)
*Kumar Krishna Agrawal,Longchao Liu,Long Lian,Michael Nercessian,Natalia Harguindeguy,Yufu Wu,Peter Mikhael,Gigin Lin,Lecia V. Sequist,Florian Fintelmann,Trevor Darrell,Yutong Bai,Maggie Chung,Adam Yala*

Main category: cs.CV

TL;DR: Pillar-0是一个放射学基础模型，在大量CT和MRI数据上预训练，结合RATE框架自动提取放射学发现标签，在多个任务上显著超越现有模型，为高性能放射学系统提供了开放基础。


<details>
  <summary>Details</summary>
Motivation: 放射学在现代医学中至关重要，但成像量的增长远超放射科医生数量。现有医学模型处理3D体积数据为低质量2D切片，丢弃关键灰度对比信息，且缺乏反映真实临床实践的评价框架。

Method: 在42,990个腹部盆腔CT、86,411个胸部CT、14,348个头部CT和11,543个乳腺MRI上预训练Pillar-0模型，结合RATE框架使用LLM以接近完美的准确率提取366个放射学发现的结构化标签。

Result: 在内部测试集上，Pillar-0平均AUROC达到86.4-90.1，比MedGemma、MedImageInsight等模型高7.8-15.8 AUROC点，在87.2%的任务中排名第一。在外部验证和肺癌风险预测等任务上也显著优于现有方法。

Conclusion: Pillar-0和RATE共同为构建高性能放射学系统提供了开放、临床严谨的基础，使之前因计算、数据和评估限制而不可行的应用成为可能。

Abstract: Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.

</details>


### [111] [A Stitch in Time: Learning Procedural Workflow via Self-Supervised Plackett-Luce Ranking](https://arxiv.org/abs/2511.17805)
*Chengan Che,Chao Wang,Xinyue Chen,Sophia Tsoka,Luis C. Garcia-Peraza-Herrera*

Main category: cs.CV

TL;DR: PL-Stitch是一个自监督学习框架，利用视频帧的固有时间顺序作为监督信号，通过Plackett-Luce模型的两个概率目标来学习程序性活动的时序结构。


<details>
  <summary>Details</summary>
Motivation: 当前自监督学习方法在处理程序性活动时忽略了其固有的时序结构，模型无法区分正向和反向时间序列的特征，表明缺乏程序意识。

Method: 提出PL-Stitch框架，包含基于Plackett-Luce模型的两个目标：主要目标训练模型按时间顺序对采样帧进行排序，次要目标通过时空拼图损失捕捉细粒度的跨帧对象相关性。

Result: 在五个手术和烹饪基准测试中表现优异，特别是在手术阶段识别（Cholec80上k-NN准确率提升11.4个百分点）和烹饪动作分割（Breakfast上线性探测准确率提升5.7个百分点）方面取得显著提升。

Conclusion: PL-Stitch通过利用程序性活动的时间顺序作为监督信号，有效提升了视频表示学习的效果，特别适用于程序性活动的理解。

Abstract: Procedural activities, ranging from routine cooking to complex surgical operations, are highly structured as a set of actions conducted in a specific temporal order. Despite their success on static images and short clips, current self-supervised learning methods often overlook the procedural nature that underpins such activities. We expose the lack of procedural awareness in current SSL methods with a motivating experiment: models pretrained on forward and time-reversed sequences produce highly similar features, confirming that their representations are blind to the underlying procedural order. To address this shortcoming, we propose PL-Stitch, a self-supervised framework that harnesses the inherent temporal order of video frames as a powerful supervisory signal. Our approach integrates two novel probabilistic objectives based on the Plackett-Luce (PL) model. The primary PL objective trains the model to sort sampled frames chronologically, compelling it to learn the global workflow progression. The secondary objective, a spatio-temporal jigsaw loss, complements the learning by capturing fine-grained, cross-frame object correlations. Our approach consistently achieves superior performance across five surgical and cooking benchmarks. Specifically, PL-Stitch yields significant gains in surgical phase recognition (e.g., +11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (e.g., +5.7 pp linear probing accuracy on Breakfast), demonstrating its effectiveness for procedural video representation learning.

</details>


### [112] [REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion](https://arxiv.org/abs/2511.17806)
*Ryoma Yataka,Pu Perry Wang,Petros Boufounos,Ryuhei Takahashi*

Main category: cs.CV

TL;DR: REXO提出了一种基于3D边界框扩散的多视角雷达目标检测方法，通过显式的跨视角雷达特征关联来提升复杂室内场景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖隐式的跨视角雷达特征关联，在复杂室内场景中容易导致特征匹配模糊和检测性能下降。

Method: 将DiffusionDet的2D边界框扩散过程提升到3D雷达空间，利用噪声3D边界框指导显式的跨视角雷达特征关联，并结合先验知识减少扩散参数。

Result: 在两个公开室内雷达数据集上，HIBER数据集AP提升4.22，MMVR数据集AP提升11.02，超越现有最优方法。

Conclusion: REXO通过3D边界框扩散和显式特征关联有效解决了多视角雷达感知中的特征匹配问题，显著提升了检测性能。

Abstract: Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.

</details>


### [113] [Importance-Weighted Non-IID Sampling for Flow Matching Models](https://arxiv.org/abs/2511.17812)
*Xinshuang Liu,Runfa Blark Li,Shaoxiu Wei,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出了一个重要性加权的非独立同分布采样框架，通过联合抽取多个样本来覆盖流匹配模型分布的关键区域，同时通过估计的重要性权重保持无偏估计。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型能有效表示复杂分布，但在有限采样预算下估计其输出函数的期望仍然具有挑战性。独立采样通常会产生高方差估计，特别是当罕见但高影响的结果主导期望时。

Method: 引入基于分数的正则化多样性机制，使用分数函数确保样本在数据流形的高密度区域内分散，减轻离流形漂移。开发了首个非独立同分布流样本重要性加权方法，通过学习残差速度场来重现非独立同分布样本的边际分布。

Result: 实验表明，该方法能产生多样化、高质量的样本，并能准确估计重要性权重和期望值。

Conclusion: 该方法推进了流匹配模型输出的可靠表征，代码将在GitHub上公开。

Abstract: Flow-matching models effectively represent complex distributions, yet estimating expectations of functions of their outputs remains challenging under limited sampling budgets. Independent sampling often yields high-variance estimates, especially when rare but with high-impact outcomes dominate the expectation. We propose an importance-weighted non-IID sampling framework that jointly draws multiple samples to cover diverse, salient regions of a flow's distribution while maintaining unbiased estimation via estimated importance weights. To balance diversity and quality, we introduce a score-based regularization for the diversity mechanism, which uses the score function, i.e., the gradient of the log probability, to ensure samples are pushed apart within high-density regions of the data manifold, mitigating off-manifold drift. We further develop the first approach for importance weighting of non-IID flow samples by learning a residual velocity field that reproduces the marginal distribution of the non-IID samples. Empirically, our method produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing the reliable characterization of flow-matching model outputs. Our code will be publicly available on GitHub.

</details>


### [114] [QAL: A Loss for Recall Precision Balance in 3D Reconstruction](https://arxiv.org/abs/2511.17824)
*Pranay Meshram,Yash Turkar,Kartikeya Singh,Praveen Raj Masilamani,Charuvahan Adhivarahan,Karthik Dantu*

Main category: cs.CV

TL;DR: 提出Quality-Aware Loss (QAL)作为Chamfer Distance和Earth Mover's Distance的替代方案，通过解耦召回率和精确度来改善3D体积学习的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有的3D体积学习训练目标（CD和EMD）无法平衡召回率和精确度，导致薄结构和代表性不足区域被忽视。

Method: QAL结合了覆盖加权的最近邻项和未覆盖真实数据吸引项，将召回率和精确度明确解耦为可调组件。

Result: 在多种管道中，QAL平均比CD提高4.3个百分点，比最佳替代方案提高2.8个百分点，能可靠恢复薄结构和代表性不足区域。

Conclusion: QAL为稳健的3D视觉和安全关键机器人管道提供了有原则、可解释且实用的训练目标。

Abstract: Volumetric learning underpins many 3D vision tasks such as completion, reconstruction, and mesh generation, yet training objectives still rely on Chamfer Distance (CD) or Earth Mover's Distance (EMD), which fail to balance recall and precision. We propose Quality-Aware Loss (QAL), a drop-in replacement for CD/EMD that combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into tunable components.
  Across diverse pipelines, QAL achieves consistent coverage gains, improving by an average of +4.3 pts over CD and +2.8 pts over the best alternatives. Though modest in percentage, these improvements reliably recover thin structures and under-represented regions that CD/EMD overlook. Extensive ablations confirm stable performance across hyperparameters and across output resolutions, while full retraining on PCN and ShapeNet demonstrates generalization across datasets and backbones. Moreover, QAL-trained completions yield higher grasp scores under GraspNet evaluation, showing that improved coverage translates directly into more reliable robotic manipulation.
  QAL thus offers a principled, interpretable, and practical objective for robust 3D vision and safety-critical robotics pipelines

</details>


### [115] [Toward explainable AI approaches for breast imaging: adapting foundation models to diverse populations](https://arxiv.org/abs/2511.17828)
*Guilherme J. Cavalcante,José Gabriel A. Moreira,Gabriel A. B. do Nascimento,Vincent Dong,Alex Nguyen,Thaís G. do Rêgo,Yuri Malheiros,Telmo M. Silva Filho,Carla R. Zeballos Torrez,James C. Gee,Anne Marie McCarthy,Andrew D. A. Maidment,Bruno Barufaldi*

Main category: cs.CV

TL;DR: 本研究利用BiomedCLIP基础模型进行乳腺密度BI-RADS分类，通过多模态训练和加权对比学习处理类别不平衡，在多种乳腺成像模态上表现出良好的泛化能力和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在专业医学影像任务中具有潜力，但在乳腺成像领域的有效性尚未充分探索。本研究旨在解决模型泛化挑战，探索基础模型在乳腺密度分类中的应用。

Method: 使用BiomedCLIP基础模型，采用多模态乳腺成像数据（合成2D图像、数字乳腺X线摄影、数字乳腺断层合成），通过加权对比学习处理类别不平衡，比较单模态和多模态训练方法。

Result: 多模态和单模态方法准确率相近（0.74 vs 0.73），但多模态模型在不同成像模态上具有更广泛适用性，AUC值在BI-RADS各类别中均高于0.84。在RSNA和EMBED数据集的外部验证显示强泛化能力（AUC范围：0.80-0.93）。

Conclusion: 研究证实了基础模型在乳腺成像应用中的潜力，为未来扩展到诊断任务奠定了基础，模型具有良好的可解释性和鲁棒性。

Abstract: Foundation models hold promise for specialized medical imaging tasks, though their effectiveness in breast imaging remains underexplored. This study leverages BiomedCLIP as a foundation model to address challenges in model generalization. BiomedCLIP was adapted for automated BI-RADS breast density classification using multi-modality mammographic data (synthesized 2D images, digital mammography, and digital breast tomosynthesis). Using 96,995 images, we compared single-modality (s2D only) and multi-modality training approaches, addressing class imbalance through weighted contrastive learning. Both approaches achieved similar accuracy (multi-modality: 0.74, single-modality: 0.73), with the multi-modality model offering broader applicability across different imaging modalities and higher AUC values consistently above 0.84 across BI-RADS categories. External validation on the RSNA and EMBED datasets showed strong generalization capabilities (AUC range: 0.80-0.93). GradCAM visualizations confirmed consistent and clinically relevant attention patterns, highlighting the models interpretability and robustness. This research underscores the potential of foundation models for breast imaging applications, paving the way for future extensions for diagnostic tasks.

</details>


### [116] [Show Me: Unifying Instructional Image and Video Generation with Diffusion Models](https://arxiv.org/abs/2511.17839)
*Yujiang Pu,Zhanbo Huang,Vishnu Boddeti,Yu Kong*

Main category: cs.CV

TL;DR: ShowMe是一个统一的视觉指令生成框架，通过选择性激活视频扩散模型的空间和时间组件，同时支持文本引导的图像编辑和视频预测任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法将文本引导图像编辑和视频预测视为独立任务，导致图像编辑方法忽略动作的时间演化，而视频预测模型忽视预期结果。需要统一框架来克服这些局限性。

Method: 提出ShowMe框架，选择性激活视频扩散模型的空间和时间组件，引入结构和运动一致性奖励来提升结构保真度和时间连贯性。

Result: 在多个基准测试中，该方法在指令图像和视频生成方面均优于专家模型，展示了视频扩散模型作为统一动作-状态转换器的优势。

Conclusion: 视频扩散模型可以作为统一的动作-状态转换器，通过统一框架在图像编辑和视频预测任务中实现相互增强的效果。

Abstract: Generating visual instructions in a given context is essential for developing interactive world simulators. While prior works address this problem through either text-guided image manipulation or video prediction, these tasks are typically treated in isolation. This separation reveals a fundamental issue: image manipulation methods overlook how actions unfold over time, while video prediction models often ignore the intended outcomes. To this end, we propose ShowMe, a unified framework that enables both tasks by selectively activating the spatial and temporal components of video diffusion models. In addition, we introduce structure and motion consistency rewards to improve structural fidelity and temporal coherence. Notably, this unification brings dual benefits: the spatial knowledge gained through video pretraining enhances contextual consistency and realism in non-rigid image edits, while the instruction-guided manipulation stage equips the model with stronger goal-oriented reasoning for video prediction. Experiments on diverse benchmarks demonstrate that our method outperforms expert models in both instructional image and video generation, highlighting the strength of video diffusion models as a unified action-object state transformer.

</details>


### [117] [JigsawComm: Joint Semantic Feature Encoding and Transmission for Communication-Efficient Cooperative Perception](https://arxiv.org/abs/2511.17843)
*Chenyi Wang,Zhaowei Li,Ming F. Li,Wujie Wen*

Main category: cs.CV

TL;DR: JigsawComm是一个端到端训练的语义感知多智能体协同感知框架，通过提取语义相关特征和特征效用估计，在有限带宽下最大化感知精度，实现500倍数据压缩。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体协同感知中通信带宽有限的问题，现有方法未考虑语义相关性和跨智能体冗余，需要最大化每个传输比特对最终感知任务的贡献。

Method: 使用正则化编码器提取语义相关稀疏特征，轻量级特征效用估计器预测特征贡献度，交换元效用图并计算最优传输策略，选择每个位置效用最高的特征。

Result: 在OPV2V和DAIR-V2X基准测试中，JigsawComm将总数据量减少500倍以上，同时达到或优于最先进方法的精度。

Conclusion: JigsawComm通过语义感知的特征选择和传输策略，实现了通信高效的协同感知，通信成本随智能体数量增加保持O(1)复杂度。

Abstract: Multi-agent cooperative perception (CP) promises to overcome the inherent occlusion and sensing-range limitations of single-agent systems (e.g., autonomous driving). However, its practicality is severely constrained by the limited communication bandwidth. Existing approaches attempt to improve bandwidth efficiency via compression or heuristic message selection, without considering the semantic relevance or cross-agent redundancy of sensory data. We argue that a practical CP system must maximize the contribution of every transmitted bit to the final perception task, by extracting and transmitting semantically essential and non-redundant data. In this paper, we formulate a joint semantic feature encoding and transmission problem, which aims to maximize CP accuracy under limited bandwidth. To solve this problem, we introduce JigsawComm, an end-to-end trained, semantic-aware, and communication-efficient CP framework that learns to ``assemble the puzzle'' of multi-agent feature transmission. It uses a regularized encoder to extract semantically-relevant and sparse features, and a lightweight Feature Utility Estimator to predict the contribution of each agent's features to the final perception task. The resulting meta utility maps are exchanged among agents and leveraged to compute a provably optimal transmission policy, which selects features from agents with the highest utility score for each location. This policy inherently eliminates redundancy and achieves a scalable $\mathcal{O}(1)$ communication cost as the number of agents increases. On the benchmarks OPV2V and DAIR-V2X, JigsawComm reduces the total data volume by up to $>$500$\times$ while achieving matching or superior accuracy compared to state-of-the-art methods.

</details>


### [118] [Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation](https://arxiv.org/abs/2511.17844)
*Shihan Cheng,Nilesh Kulkarni,David Hyde,Dmitriy Smirnov*

Main category: cs.CV

TL;DR: 提出了一种数据高效的精调策略，使用稀疏、低质量的合成数据学习文本到视频扩散模型的物理相机参数控制，效果优于使用真实数据精调的模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量高质量数据集来精调文本到视频扩散模型以添加新的生成控制（如物理相机参数），但这些数据集难以获取。

Method: 提出数据高效的精调策略，从稀疏、低质量的合成数据中学习物理相机参数控制。

Result: 使用简单合成数据精调不仅实现了期望的控制，而且比使用真实数据精调的模型效果更好。

Conclusion: 提供了一个框架来直观和定量地解释这一现象，证明了使用低质量合成数据精调的有效性。

Abstract: Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.

</details>


### [119] [MGA-VQA: Secure and Interpretable Graph-Augmented Visual Question Answering with Memory-Guided Protection Against Unauthorized Knowledge Use](https://arxiv.org/abs/2511.17881)
*Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.CV

TL;DR: 提出了MGA-VQA框架，通过多模态集成解决文档视觉问答中的空间关系建模、高分辨率处理、多跳推理和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在显式空间关系建模、高分辨率文档处理效率、多跳推理和可解释性方面存在不足，需要更有效的解决方案。

Method: 集成token级编码、空间图推理、记忆增强推理和问题引导压缩的多模态框架，引入可解释的基于图的决策路径和结构化内存访问。

Result: 在六个基准测试（FUNSD、CORD、SROIE、DocVQA、STE-VQA和RICO）上表现出卓越的准确性和效率，在答案预测和空间定位方面均有持续改进。

Conclusion: MGA-VQA框架通过可解释的图推理和记忆增强机制，显著提升了文档视觉问答的性能和透明度。

Abstract: Document Visual Question Answering (DocVQA) requires models to jointly understand textual semantics, spatial layout, and visual features. Current methods struggle with explicit spatial relationship modeling, inefficiency with high-resolution documents, multi-hop reasoning, and limited interpretability. We propose MGA-VQA, a multi-modal framework that integrates token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways and structured memory access for enhanced reasoning transparency. Evaluation across six benchmarks (FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO) demonstrates superior accuracy and efficiency, with consistent improvements in both answer prediction and spatial localization.

</details>


### [120] [ArticFlow: Generative Simulation of Articulated Mechanisms](https://arxiv.org/abs/2511.17883)
*Jiong Lin,Jinchen Ruan,Hod Lipson*

Main category: cs.CV

TL;DR: ArticFlow是一个两阶段流匹配框架，用于生成可控的关节3D形状，通过动作控制从噪声生成目标点集，可作为生成模型和神经模拟器。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在静态3D形状生成方面表现良好，但关节3D生成由于动作依赖变形和数据集有限而面临挑战。

Method: 采用两阶段流匹配框架：(i)潜在流将噪声传输到形状先验代码，(ii)点流在动作和形状先验条件下传输点，使单个模型能表示多样关节类别并在动作间泛化。

Result: 在MuJoCo Menagerie上，ArticFlow作为生成模型和神经模拟器，从紧凑先验预测动作条件运动学并通过潜在插值合成新形态，相比特定对象模拟器和静态点云生成器的动作条件变体，获得更高运动学精度和更好形状质量。

Conclusion: 动作条件流匹配是实现可控高质量关节机制生成的实用途径。

Abstract: Recent advances in generative models have produced strong results for static 3D shapes, whereas articulated 3D generation remains challenging due to action-dependent deformations and limited datasets. We introduce ArticFlow, a two-stage flow matching framework that learns a controllable velocity field from noise to target point sets under explicit action control. ArticFlow couples (i) a latent flow that transports noise to a shape-prior code and (ii) a point flow that transports points conditioned on the action and the shape prior, enabling a single model to represent diverse articulated categories and generalize across actions. On MuJoCo Menagerie, ArticFlow functions both as a generative model and as a neural simulator: it predicts action-conditioned kinematics from a compact prior and synthesizes novel morphologies via latent interpolation. Compared with object-specific simulators and an action-conditioned variant of static point-cloud generators, ArticFlow achieves higher kinematic accuracy and better shape quality. Results show that action-conditioned flow matching is a practical route to controllable and high-quality articulated mechanism generation.

</details>


### [121] [FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning](https://arxiv.org/abs/2511.17885)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Wei Chen,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CV

TL;DR: FastMMoE是一个无需训练的加速框架，专门针对基于专家混合(MoE)的多模态大语言模型，通过减少专家激活和基于路由概率的token剪枝，在保持95.5%性能的同时减少55%的计算量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率视觉输入导致视觉token序列过长和推理延迟增加，需要减少冗余视觉token以降低计算/内存负担，同时保持性能，使MLLM能够在资源受限或延迟敏感的场景中部署。

Method: 提出FastMMoE框架，包含两个互补策略：(1)减少视觉token的专家激活以最小化不必要的专家计算；(2)基于路由概率分布相似性的路由感知token剪枝，识别并移除高度冗余的视觉token。

Result: 在DeepSeek-VL2和InternVL3.5等大规模MoE-MLLMs上的实验表明，FastMMoE可减少高达55.0%的FLOPs，同时保留约95.5%的原始性能，在多个保留率下持续优于FastV和SparseVLM等稠密模型剪枝基线。

Conclusion: FastMMoE为MoE-based MLLMs提供了一种有效的训练免费加速解决方案，通过路由分析视角显著减少了计算开销，同时保持了模型性能，优于现有的稠密模型剪枝方法。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance, enabling MLLM deployment in resource-constrained or latency-sensitive scenarios. Current visual token pruning methods mainly rely on attention-based redundancy analysis and are tailored to dense architectures. We propose Fast Multimodal Mixture-of-Experts (FastMMoE), a training-free acceleration framework for mixture-of-experts (MoE) based MLLMs, developed from a routing analysis perspective. FastMMoE combines two complementary strategies: (i) expert activation reduction for visual tokens to minimize unnecessary expert computation; and (ii) routing-aware token pruning that leverages similarity in routing probability distributions to identify and remove highly redundant visual tokens. Experiments on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 demonstrate that FastMMoE can reduce FLOPs by up to 55.0% while retaining approximately 95.5% of the original performance, consistently outperforming dense-model pruning baselines including FastV and SparseVLM across multiple retention rates.

</details>


### [122] [When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA](https://arxiv.org/abs/2511.17886)
*Pume Tuchinda,Parinthapat Pengpun,Romrawin Chumpu,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CV

TL;DR: 对CLIP风格视觉语言模型进行知识蒸馏的系统研究，发现更强的教师模型并不总能产生更好的学生模型，现有蒸馏框架在多模态任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型计算需求大，知识蒸馏是构建轻量级模型的有效方法，但在CLIP风格模型中的应用有限，主要局限于小规模教师模型和狭窄评估任务。

Method: 对一系列CLIP风格教师模型进行系统蒸馏研究，涵盖从标准基线到大规模最先进模型的范围。

Result: 与NLP和视觉领域的趋势相反，更强的教师模型并不总能产生更好的学生模型；现有蒸馏框架在多模态任务（如视觉问答）中表现不佳。

Conclusion: 研究结果挑战了知识蒸馏中的普遍假设，为设计参数高效的多模态模型指出了新方向。

Abstract: Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.

</details>


### [123] [MINDiff: Mask-Integrated Negative Attention for Controlling Overfitting in Text-to-Image Personalization](https://arxiv.org/abs/2511.17888)
*Seulgi Jeong,Jaeil Kim*

Main category: cs.CV

TL;DR: MINDiff是一种在推理阶段解决文本到图像模型过拟合问题的新方法，通过引入负注意力机制在掩码无关区域抑制主题影响，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法如DreamBooth使用类别特定的先验保留损失来解决过拟合，但这增加了训练计算成本并限制了推理时的用户控制。

Method: 提出负注意力概念，在推理时修改交叉注意力机制来抑制主题在掩码无关区域的影响，用户可通过调整参数λ平衡主题保真度和文本对齐。

Result: 定性和定量实验表明，MINDiff比类别特定的先验保留损失更有效地缓解过拟合，且可直接应用于现有DreamBooth模型。

Conclusion: MINDiff在推理阶段有效解决过拟合问题，提供更好的语义控制和文本对齐，无需修改模型架构或重新训练。

Abstract: In the personalization process of large-scale text-to-image models, overfitting often occurs when learning specific subject from a limited number of images. Existing methods, such as DreamBooth, mitigate this issue through a class-specific prior-preservation loss, which requires increased computational cost during training and limits user control during inference time. To address these limitations, we propose Mask-Integrated Negative Attention Diffusion (MINDiff). MINDiff introduces a novel concept, negative attention, which suppresses the subject's influence in masked irrelevant regions. We achieve this by modifying the cross-attention mechanism during inference. This enables semantic control and improves text alignment by reducing subject dominance in irrelevant regions. Additionally, during the inference time, users can adjust a scale parameter lambda to balance subject fidelity and text alignment. Our qualitative and quantitative experiments on DreamBooth models demonstrate that MINDiff mitigates overfitting more effectively than class-specific prior-preservation loss. As our method operates entirely at inference time and does not alter the model architecture, it can be directly applied to existing DreamBooth models without re-training. Our code is available at https://github.com/seuleepy/MINDiff.

</details>


### [124] [Decoupled Audio-Visual Dataset Distillation](https://arxiv.org/abs/2511.17890)
*Wenyuan Li,Guang Li,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出了DAVDD框架，通过预训练和表示解耦解决音频-视觉数据集蒸馏中的模态对齐问题，在多个基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统分布匹配方法难以捕捉跨模态对齐，现有方法存在模态映射空间不一致和模态特定信息受损的问题。

Method: 使用预训练编码器获取稳定模态特征，通过轻量化解耦器将特征分解为公共和私有表示，采用公共跨模态匹配和样本-分布联合对齐策略。

Result: 在多个基准测试的所有IPC设置下均达到最优结果，证明了表示解耦学习对高质量音频-视觉数据集蒸馏的有效性。

Conclusion: DAVDD框架通过解耦表示学习成功解决了音频-视觉数据集蒸馏中的关键挑战，实现了高质量的跨模态数据压缩。

Abstract: Audio-Visual Dataset Distillation aims to compress large-scale datasets into compact subsets while preserving the performance of the original data. However, conventional Distribution Matching (DM) methods struggle to capture intrinsic cross-modal alignment. Subsequent studies have attempted to introduce cross-modal matching, but two major challenges remain: (i) independently and randomly initialized encoders lead to inconsistent modality mapping spaces, increasing training difficulty; and (ii) direct interactions between modalities tend to damage modality-specific (private) information, thereby degrading the quality of the distilled data. To address these challenges, we propose DAVDD, a pretraining-based decoupled audio-visual distillation framework. DAVDD leverages a diverse pretrained bank to obtain stable modality features and uses a lightweight decoupler bank to disentangle them into common and private representations. To effectively preserve cross-modal structure, we further introduce Common Intermodal Matching together with a Sample-Distribution Joint Alignment strategy, ensuring that shared representations are aligned both at the sample level and the global distribution level. Meanwhile, private representations are entirely isolated from cross-modal interaction, safeguarding modality-specific cues throughout distillation. Extensive experiments across multiple benchmarks show that DAVDD achieves state-of-the-art results under all IPC settings, demonstrating the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation. Code will be released.

</details>


### [125] [CUS-GS: A Compact Unified Structured Gaussian Splatting Framework for Multimodal Scene Representation](https://arxiv.org/abs/2511.17904)
*Yuhang Ming,Chenxin Fang,Xingyuan Yu,Fan Zhang,Weichen Dai,Wanzeng Kong,Guofeng Zhang*

Main category: cs.CV

TL;DR: CUS-GS是一种紧凑的统一结构化高斯泼溅表示，通过体素化锚点结构连接多模态语义特征与结构化3D几何，在保持语义完整性的同时显著减少模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法存在语义导向方法缺乏显式3D几何建模，而结构导向方法语义抽象能力有限的问题，需要一种能同时处理语义和几何的统一表示。

Method: 设计体素化锚点结构构建空间支架，从基础模型提取多模态语义特征；引入多模态潜在特征分配机制统一外观、几何和语义；提出特征感知重要性评估策略动态指导锚点生长和修剪。

Result: 仅使用600万参数即可达到与最先进方法相竞争的性能，比最接近的竞争对手（3500万参数）少一个数量级。

Conclusion: CUS-GS框架在性能和模型效率之间取得了优异的平衡，为3D场景表示提供了一种紧凑而统一的解决方案。

Abstract: Recent advances in Gaussian Splatting based 3D scene representation have shown two major trends: semantics-oriented approaches that focus on high-level understanding but lack explicit 3D geometry modeling, and structure-oriented approaches that capture spatial structures yet provide limited semantic abstraction. To bridge this gap, we present CUS-GS, a compact unified structured Gaussian Splatting representation, which connects multimodal semantic features with structured 3D geometry. Specifically, we design a voxelized anchor structure that constructs a spatial scaffold, while extracting multimodal semantic features from a set of foundation models (e.g., CLIP, DINOv2, SEEM). Moreover, we introduce a multimodal latent feature allocation mechanism to unify appearance, geometry, and semantics across heterogeneous feature spaces, ensuring a consistent representation across multiple foundation models. Finally, we propose a feature-aware significance evaluation strategy to dynamically guide anchor growing and pruning, effectively removing redundant or invalid anchors while maintaining semantic integrity. Extensive experiments show that CUS-GS achieves competitive performance compared to state-of-the-art methods using as few as 6M parameters - an order of magnitude smaller than the closest rival at 35M - highlighting the excellent trade off between performance and model efficiency of the proposed framework.

</details>


### [126] [Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation](https://arxiv.org/abs/2511.17914)
*Chenyang Jiang,Hang Zhao,Xinyu Zhang,Zhengcen Li,Qiben Shan,Shaocong Wu,Jingyong Su*

Main category: cs.CV

TL;DR: 提出了ADSA（自适应软标签对齐）模块来解决长尾数据集蒸馏中的软标签偏差问题，在ImageNet-1k-LT上显著提升了尾部类别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法主要针对平衡数据集，在现实世界长尾分布下性能下降严重，需要解决软标签偏差问题。

Method: 通过系统扰动数据不平衡水平识别软标签偏差来源，提出自适应软标签对齐模块来校准这些偏差，该模块可无缝集成到现有蒸馏流程中。

Result: 在ImageNet-1k-LT上，ADSA将尾部类别准确率提升高达11.8%，整体准确率达到41.4%（EDC, IPC=50）。

Conclusion: ADSA为有限标签预算下的长尾数据集蒸馏提供了稳健且可泛化的解决方案。

Abstract: Dataset distillation compresses large-scale datasets into compact, highly informative synthetic data, significantly reducing storage and training costs. However, existing research primarily focuses on balanced datasets and struggles to perform under real-world long-tailed distributions. In this work, we emphasize the critical role of soft labels in long-tailed dataset distillation and uncover the underlying mechanisms contributing to performance degradation. Specifically, we derive an imbalance-aware generalization bound for model trained on distilled dataset. We then identify two primary sources of soft-label bias, which originate from the distillation model and the distilled images, through systematic perturbation of the data imbalance levels. To address this, we propose ADSA, an Adaptive Soft-label Alignment module that calibrates the entangled biases. This lightweight module integrates seamlessly into existing distillation pipelines and consistently improves performance. On ImageNet-1k-LT with EDC and IPC=50, ADSA improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4%. Extensive experiments demonstrate that ADSA provides a robust and generalizable solution under limited label budgets and across a range of distillation techniques. Code is available at: https://github.com/j-cyoung/ADSA_DD.git.

</details>


### [127] [Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization](https://arxiv.org/abs/2511.17918)
*Youngsik Yun,Dongjun Gu,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出频率自适应锐度正则化(FASR)方法，通过考虑图像局部频率来改进3D高斯泼溅(3DGS)在少样本场景下的泛化能力，防止过拟合稀疏观测并减少伪影。


<details>
  <summary>Details</summary>
Motivation: 3DGS在少样本场景下容易过拟合稀疏观测，缺乏对新视角的泛化能力，需要从机器学习泛化角度重新审视其优化问题。

Method: 将3DGS训练目标重新表述，引入频率自适应锐度正则化，根据图像局部频率设置正则化权重和邻域半径，平衡高频细节重建与锐度惩罚。

Result: 在多种配置的数据集上，该方法持续改进各种基线模型，能防止新视角中的漂浮伪影并重建SAM倾向于过度平滑的精细细节。

Conclusion: FASR通过频率自适应方法有效提升了3DGS的泛化性能，在少样本场景下实现了更好的新视角合成效果。

Abstract: Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.

</details>


### [128] [PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning](https://arxiv.org/abs/2511.17927)
*Yingjie Ma,Xun Lin,Yong Xu,Weicheng Xie,Zitong Yu*

Main category: cs.CV

TL;DR: PA-FAS提出了一种增强多模态人脸反欺诈推理路径的方法，通过构建高质量扩展推理序列和答案混洗机制，解决了传统监督微调+强化学习在多模态推理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统SFT+RL方法在多模态人脸反欺诈中存在两个关键问题：多模态推理路径受限导致探索空间缩小，以及单任务监督与多样化推理路径不匹配导致推理混淆和捷径学习。

Method: 1. 从有限标注中构建高质量扩展推理序列，丰富推理路径并放松探索约束；2. 在SFT阶段引入答案混洗机制，强制模型进行全面多模态分析而非依赖表面线索。

Result: PA-FAS显著提高了多模态推理准确性和跨域泛化能力，更好地统一了多模态融合、泛化性和可解释性，为可信赖的人脸反欺诈系统提供了支持。

Conclusion: 通过增强推理路径和防止捷径学习，PA-FAS有效解决了多模态人脸反欺诈中的关键挑战，为策略性训练在多模态任务中的应用提供了新思路。

Abstract: Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.

</details>


### [129] [MambaTAD: When State-Space Models Meet Long-Range Temporal Action Detection](https://arxiv.org/abs/2511.17929)
*Hui Lu,Yi Yu,Shijian Lu,Deepu Rajan,Boon Poh Ng,Alex C. Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: MambaTAD是一种基于状态空间模型的时序动作检测方法，通过双向状态空间模块和全局特征融合头解决长跨度动作检测问题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统时序动作检测方法在处理长跨度动作实例时存在全局感知能力不足和检测头效率低下的问题，而现有的结构化状态空间模型在TAD中面临时间上下文衰减和全局视觉上下文建模中的自元素冲突等挑战。

Method: 提出MambaTAD模型，包含两个核心设计：1) 对角掩码双向状态空间模块(DMBSS)促进全局特征融合；2) 全局特征融合头通过多粒度特征和全局感知逐步优化检测；3) 使用状态空间时序适配器(SSTA)实现端到端单阶段检测。

Result: 在多个公共基准测试上的广泛实验表明，MambaTAD始终实现了优越的时序动作检测性能。

Conclusion: MambaTAD通过引入长距离建模和全局特征检测能力，有效解决了时序动作检测中的关键挑战，特别是在处理长跨度动作实例方面表现出色。

Abstract: Temporal Action Detection (TAD) aims to identify and localize actions by determining their starting and ending frames within untrimmed videos. Recent Structured State-Space Models such as Mamba have demonstrated potential in TAD due to their long-range modeling capability and linear computational complexity. On the other hand, structured state-space models often face two key challenges in TAD, namely, decay of temporal context due to recursive processing and self-element conflict during global visual context modeling, which become more severe while handling long-span action instances. Additionally, traditional methods for TAD struggle with detecting long-span action instances due to a lack of global awareness and inefficient detection heads. This paper presents MambaTAD, a new state-space TAD model that introduces long-range modeling and global feature detection capabilities for accurate temporal action detection. MambaTAD comprises two novel designs that complement each other with superior TAD performance. First, it introduces a Diagonal-Masked Bidirectional State-Space (DMBSS) module which effectively facilitates global feature fusion and temporal action detection. Second, it introduces a global feature fusion head that refines the detection progressively with multi-granularity features and global awareness. In addition, MambaTAD tackles TAD in an end-to-end one-stage manner using a new state-space temporal adapter(SSTA) which reduces network parameters and computation cost with linear complexity. Extensive experiments show that MambaTAD achieves superior TAD performance consistently across multiple public benchmarks.

</details>


### [130] [UniRSCD: A Unified Novel Architectural Paradigm for Remote Sensing Change Detection](https://arxiv.org/abs/2511.17930)
*Yuan Qu,Zhipeng Zhang,Chaojun Xu,Qiao Wan,Mengying Xie,Yuzeng Chen,Zhenqi Liu,Yanfei Zhong*

Main category: cs.CV

TL;DR: 提出UniRSCD统一遥感变化检测框架，基于状态空间模型，通过频率变化提示生成器和统一解码器，将BCD、SCD、BDA等不同输出粒度的变化检测任务整合到统一架构中。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量专家知识设计专用解码器来补偿编码过程中的信息损失，这不仅在选择最优模型时引入不确定性，也限制了架构的通用性。

Method: 使用状态空间模型作为骨干，引入频率变化提示生成器作为统一编码器，动态扫描双时相全局上下文信息，集成高频细节和低频整体信息；通过分层特征交互和任务自适应输出映射建立共享表示空间。

Result: 在五个数据集上实现领先性能，包括二进制变化数据集LEVIR-CD、语义变化数据集SECOND和建筑损伤评估数据集xBD。

Conclusion: UniRSCD框架能够适应多种变化检测任务，为不同输出粒度的变化检测任务提供了统一的解决方案。

Abstract: In recent years, remote sensing change detection has garnered significant attention due to its critical role in resource monitoring and disaster assessment. Change detection tasks exist with different output granularities such as BCD, SCD, and BDA. However, existing methods require substantial expert knowledge to design specialized decoders that compensate for information loss during encoding across different tasks. This not only introduces uncertainty into the process of selecting optimal models for abrupt change scenarios (such as disaster outbreaks) but also limits the universality of these architectures. To address these challenges, this paper proposes a unified, general change detection framework named UniRSCD. Building upon a state space model backbone, we introduce a frequency change prompt generator as a unified encoder. The encoder dynamically scans bitemporal global context information while integrating high-frequency details with low-frequency holistic information, thereby eliminating the need for specialized decoders for feature compensation. Subsequently, the unified decoder and prediction head establish a shared representation space through hierarchical feature interaction and task-adaptive output mapping. This integrating various tasks such as binary change detection and semantic change detection into a unified architecture, thereby accommodating the differing output granularity requirements of distinct change detection tasks. Experimental results demonstrate that the proposed architecture can adapt to multiple change detection tasks and achieves leading performance on five datasets, including the binary change dataset LEVIR-CD, the semantic change dataset SECOND, and the building damage assessment dataset xBD.

</details>


### [131] [Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion](https://arxiv.org/abs/2511.17932)
*Yan Xu,Yixing Wang,Stella X. Yu*

Main category: cs.CV

TL;DR: 提出一种零样本、生成引导的稀疏输入新视角合成方法，利用预训练视频扩散模型完成自然视频，通过不确定性感知机制生成伪视角，结合3D高斯泼溅进行场景重建


<details>
  <summary>Details</summary>
Motivation: 解决稀疏输入新视角合成问题，不仅要填补稀疏视角之间的空间间隙，还要完成在空间中展开的自然视频

Method: 使用预训练视频扩散模型进行测试时自然视频补全，通过不确定性感知机制生成空间连贯的伪视角，用3D高斯泼溅进行场景重建，并建立迭代反馈循环

Result: 在LLFF、DTU、DL3DV和MipNeRF-360数据集上，在极端稀疏条件下显著优于强3D-GS基线

Conclusion: 无需场景特定训练或微调，即可从稀疏输入生成连贯、高保真的渲染结果

Abstract: Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \emph{completing a natural video} unfolding through space.
  We recast the task as \emph{test-time natural video completion}, using powerful priors from \emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.
  The result is coherent, high-fidelity renderings from sparse inputs \emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.

</details>


### [132] [V2X-RECT: An Efficient V2X Trajectory Prediction Framework via Redundant Interaction Filtering and Tracking Error Correction](https://arxiv.org/abs/2511.17941)
*Xiangyan Kong,Xuecheng Wu,Xiongwei Zhao,Xiaodong Li,Yunyun Shi,Gang Wang,Dingkang Yang,Yang Liu,Hong Chen,Yulong Gao*

Main category: cs.CV

TL;DR: V2X-RECT是一个针对高密度交通场景的轨迹预测框架，通过多源身份匹配校正、交通信号引导交互和局部时空坐标编码，解决了身份切换、冗余交互和重复编码问题，提升了预测精度和推理效率。


<details>
  <summary>Details</summary>
Motivation: 在密集交通场景中，频繁的目标身份切换阻碍了跨视角关联和融合，多源信息在编码阶段产生冗余交互，传统车辆中心编码导致大量重复历史轨迹特征编码，影响实时推理性能。

Method: 1. 多源身份匹配校正模块：利用多视角时空关系实现稳定一致的目标关联；2. 交通信号引导交互模块：编码交通灯变化趋势，过滤关键交互车辆；3. 局部时空坐标编码：实现历史轨迹和地图特征的可重用性，支持并行解码。

Result: 在V2X-Seq和V2X-Traj数据集上的实验表明，V2X-RECT相比SOTA方法取得显著改进，同时在不同交通密度下提升了鲁棒性和推理效率。

Conclusion: V2X-RECT通过增强数据关联一致性、减少冗余交互和重用历史信息，实现了更高效准确的轨迹预测，特别适用于高密度交通环境。

Abstract: V2X prediction can alleviate perception incompleteness caused by limited line of sight through fusing trajectory data from infrastructure and vehicles, which is crucial to traffic safety and efficiency. However, in dense traffic scenarios, frequent identity switching of targets hinders cross-view association and fusion. Meanwhile, multi-source information tends to generate redundant interactions during the encoding stage, and traditional vehicle-centric encoding leads to large amounts of repetitive historical trajectory feature encoding, degrading real-time inference performance. To address these challenges, we propose V2X-RECT, a trajectory prediction framework designed for high-density environments. It enhances data association consistency, reduces redundant interactions, and reuses historical information to enable more efficient and accurate prediction. Specifically, we design a multi-source identity matching and correction module that leverages multi-view spatiotemporal relationships to achieve stable and consistent target association, mitigating the adverse effects of mismatches on trajectory encoding and cross-view feature fusion. Then we introduce traffic signal-guided interaction module, encoding trend of traffic light changes as features and exploiting their role in constraining spatiotemporal passage rights to accurately filter key interacting vehicles, while capturing the dynamic impact of signal changes on interaction patterns. Furthermore, a local spatiotemporal coordinate encoding enables reusable features of historical trajectories and map, supporting parallel decoding and significantly improving inference efficiency. Extensive experimental results across V2X-Seq and V2X-Traj datasets demonstrate that our V2X-RECT achieves significant improvements compared to SOTA methods, while also enhancing robustness and inference efficiency across diverse traffic densities.

</details>


### [133] [SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System](https://arxiv.org/abs/2511.17943)
*Zhiyu Xu,Weilong Yan,Yufei Shi,Xin Meng,Tao He,Huiping Zhuang,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: SciEducator是一个基于戴明循环的自进化多智能体系统，专门用于科学视频理解和教育，在科学视频问答基准上显著优于现有MLLMs和视频智能体。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型和视频智能体在需要外部专业知识和严格逐步推理的科学视频理解与教育领域表现不佳，需要专门解决方案。

Method: 基于戴明循环（计划-执行-研究-行动）设计自进化推理和反馈机制，构建多智能体系统，能够生成包含文本指导、视觉引导、音频叙述和交互参考的多模态教育内容。

Result: 在包含500个专家验证科学问答对的SciVBench基准测试中，SciEducator显著优于领先的闭源MLLMs（如Gemini、GPT-4o）和最先进的视频智能体。

Conclusion: SciEducator为科学视频理解和教育社区建立了新的范式，证明了自进化多智能体系统在该领域的有效性。

Abstract: Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.

</details>


### [134] [Test-Time Temporal Sampling for Efficient MLLM Video Understanding](https://arxiv.org/abs/2511.17945)
*Kaibin Wang,Mingbao Lin*

Main category: cs.CV

TL;DR: 提出T3S方法，一种无需训练、即插即用的推理包装器，通过生成多个短而多样的视频子序列来高效处理长视频，降低计算成本同时提升准确性。


<details>
  <summary>Details</summary>
Motivation: 处理长视频时，多模态大语言模型的自注意力机制计算复杂度呈二次方增长，导致计算需求高、推理速度慢。现有方法在准确性、额外训练需求或推理速度方面存在权衡。

Method: T3S利用时空冗余性，在推理时生成多个短而多样的视频子序列，将它们打包在单个前向传递中，并聚合它们的预测结果。

Result: 在长视频理解基准测试中，T3S将准确性提升高达3.1%，并将首个token延迟降低2.04倍，且集成工作量最小。

Conclusion: T3S将视频冗余转化为计算优势，为长视频理解提供了可扩展的解决方案，完全在推理时运行，无需模型修改或微调，与多种预训练MLLM兼容。

Abstract: Processing long videos with multimodal large language models (MLLMs) poses a significant computational challenge, as the model's self-attention mechanism scales quadratically with the number of video tokens, resulting in high computational demand and slow inference speed. Current solutions, such as rule-based sub-sampling, learned frame selector, or memory-based summarization, often introduce their own trade-offs: they compromise accuracy, necessitate additional training, or decrease inference speed. In this paper, we propose Test-Time Temporal Sampling (T3S), a training-free, plug-and-play inference wrapper that enables MLLMs to process long videos both efficiently and effectively. T3S exploits spatiotemporal redundancy by generating multiple short and diverse subsequences of video tokens at inference time, packing them within a single forward pass, and aggregating their predictions. This multi-subsequence formulation broadens visual coverage while reducing the computational cost of self-attention from $O(L^2)$ to $O(\sum_{i=1}^m α_i^2L^2)$, where $\sum_{i=1}^m α_i^2 < 1$. Extensive experiments on long video understanding benchmarks demonstrate that T3S improves accuracy by up to 3.1% and reduces first token delay by $2.04\times$, all with minimal integration effort. Our approach operates entirely at inference time, requires no model modifications or fine-tuning, and is compatible with a wide range of pretrained MLLMs. T3S turns video redundancy into a computational advantage, offering a scalable solution for long-video understanding. The code is available at https://github.com/kaibinwang3/T3S.

</details>


### [135] [Multi-speaker Attention Alignment for Multimodal Social Interaction](https://arxiv.org/abs/2511.17952)
*Liangyang Ouyang,Yifei Huang,Mingfang Zhang,Caixin Kang,Ryosuke Furuta,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文提出了一种多模态多说话者注意力对齐方法，通过动态跨模态头选择和自适应社交感知注意力偏置，解决了MLLMs在多说话者场景中视觉和文本标记缺乏说话者一致对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在社交任务中表现不一致，主要问题是在多说话者场景中，视觉和文本标记缺乏说话者一致的对齐，跨模态注意力较弱。

Method: 提出动态跨模态头选择来识别负责基础化的注意力头，然后注入基于现有注意力模式和说话者位置的自适应社交感知注意力偏置，无需引入可训练参数或架构更改。

Result: 在三个不同的MLLMs和三个基准测试上评估，在四个社交任务中显著提升了模型能力并达到了最先进的结果，注意力可视化证实方法成功聚焦于说话者相关区域。

Conclusion: 该方法有效解决了MLLMs在多说话者社交推理中的对齐问题，实现了更稳健的多方社交推理能力。

Abstract: Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.

</details>


### [136] [HEAL: Learning-Free Source Free Unsupervised Domain Adaptation for Cross-Modality Medical Image Segmentation](https://arxiv.org/abs/2511.17958)
*Yulong Shi,Jiapeng Li,Lin Qi*

Main category: cs.CV

TL;DR: 提出了HEAL框架，一种新颖的源自由无监督域自适应方法，通过层次去噪、边缘引导选择、尺寸感知融合和无学习特性来解决域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 临床数据隐私和存储限制的需求增长推动了源自由无监督域自适应的发展，该方法在无法访问源数据和目标域标签的情况下解决域偏移问题。

Method: HEAL框架整合了层次去噪、边缘引导选择、尺寸感知融合和无学习特性，在源自由和无监督设置下进行域适应。

Result: 大规模跨模态实验表明，该方法优于现有的源自由无监督域自适应方法，达到了最先进的性能。

Conclusion: HEAL框架有效解决了源自由无监督域自适应中的挑战，在跨模态任务中实现了优异的性能。

Abstract: Growing demands for clinical data privacy and storage constraints have spurred advances in Source Free Unsupervised Domain Adaptation (SFUDA). SFUDA addresses the domain shift by adapting models from the source domain to the unseen target domain without accessing source data, even when target-domain labels are unavailable. However, SFUDA faces significant challenges: the absence of source domain data and label supervision in the target domain due to source free and unsupervised settings. To address these issues, we propose HEAL, a novel SFUDA framework that integrates Hierarchical denoising, Edge-guided selection, size-Aware fusion, and Learning-free characteristic. Large-scale cross-modality experiments demonstrate that our method outperforms existing SFUDA approaches, achieving state-of-the-art (SOTA) performance. The source code is publicly available at: https://github.com/derekshiii/HEAL.

</details>


### [137] [VITAL: Vision-Encoder-centered Pre-training for LMMs in Visual Quality Assessment](https://arxiv.org/abs/2511.17962)
*Ziheng Jia,Linhan Cao,Jinliang Han,Zicheng Zhang,Jiaying Qian,Jiarui Wang,Zijian Chen,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出了VITAL-Series LMMs，通过视觉编码器中心的生成预训练流程，构建了最大的视觉质量评估数据集，采用多任务训练增强模型评分精度和解释能力，实现了高效的模型扩展。


<details>
  <summary>Details</summary>
Motivation: 现有视觉质量评估大模型通常专注于单一任务并依赖全参数微调，容易在特定模态或任务类型上过拟合，限制了泛化能力和可迁移性。

Method: 采用机器执行的标注-审查范式构建超过450万视觉语言对的数据集；使用多任务训练工作流同时提升定量评分精度和质量解释能力；基于视觉编码器实现高效模型扩展。

Result: 模型在零样本设置下表现强劲，每个配对解码器仅需使用不到预训练数据1/1000进行快速预热即可达到与完全训练模型相当的性能。

Conclusion: 这项工作为推进视觉质量评估基础大模型的发展奠定了基石。

Abstract: Developing a robust visual quality assessment (VQualA) large multi-modal model (LMM) requires achieving versatility, powerfulness, and transferability.
  However, existing VQualA LMMs typically focus on a single task and rely on full-parameter fine-tuning, which makes them prone to overfitting on specific modalities or task types, thereby limiting their generalization capacity and transferability. To address this, we propose a vision-encoder-centered generative pre-training pipeline and develop the VITAL-Series LMMs. (1) We adopt a machine-executed annotation-scrutiny paradigm, constructing over 4.5M vision-language (VL) pairs-the largest VQualA training dataset to date. (2) We employ a multi-task training workflow that simultaneously enhances the model's quantitative scoring precision and strengthens its capability for quality interpretation across both image and video modalities. (3) Building upon the vision encoder, we realize an efficient model zoo extension: the model zoo exhibits strong zero-shot performance, and each paired decoder requires only a swift warm-up using less than 1/1000 of the pre-training data to achieve performance comparable to the fully trained counterpart. Overall, our work lays a cornerstone for advancing toward the foundation LMM for VQualA.

</details>


### [138] [X-ReID: Multi-granularity Information Interaction for Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.17964)
*Chenyang Yu,Xuehu Liu,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了X-ReID框架用于视频可见光-红外行人重识别，通过跨模态原型协作和多粒度信息交互来减少模态差异并增强时空建模


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型在检索任务中表现出色，但在视频可见光-红外行人重识别中的应用尚未充分探索，主要挑战是缩小模态差距和利用视频序列的时空信息

Method: 提出跨模态原型协作(CPC)来对齐和整合不同模态特征，设计多粒度信息交互(MII)包含短时相邻帧交互、长时跨帧信息融合和跨模态特征对齐，最终集成多粒度信息实现鲁棒的序列级表示

Result: 在两个大规模VVI-ReID基准数据集(HITSZ-VCM和BUPTCampus)上的广泛实验表明，该方法优于现有最先进方法

Conclusion: X-ReID框架通过有效的跨模态特征学习和多粒度时空建模，在视频可见光-红外行人重识别任务中取得了优越性能

Abstract: Large-scale vision-language models (e.g., CLIP) have recently achieved remarkable performance in retrieval tasks, yet their potential for Video-based Visible-Infrared Person Re-Identification (VVI-ReID) remains largely unexplored. The primary challenges are narrowing the modality gap and leveraging spatiotemporal information in video sequences. To address the above issues, in this paper, we propose a novel cross-modality feature learning framework named X-ReID for VVI-ReID. Specifically, we first propose a Cross-modality Prototype Collaboration (CPC) to align and integrate features from different modalities, guiding the network to reduce the modality discrepancy. Then, a Multi-granularity Information Interaction (MII) is designed, incorporating short-term interactions from adjacent frames, long-term cross-frame information fusion, and cross-modality feature alignment to enhance temporal modeling and further reduce modality gaps. Finally, by integrating multi-granularity information, a robust sequence-level representation is achieved. Extensive experiments on two large-scale VVI-ReID benchmarks (i.e., HITSZ-VCM and BUPTCampus) demonstrate the superiority of our method over state-of-the-art methods. The source code is released at https://github.com/AsuradaYuci/X-ReID.

</details>


### [139] [Signal: Selective Interaction and Global-local Alignment for Multi-Modal Object Re-Identification](https://arxiv.org/abs/2511.17965)
*Yangyang Liu,Yuhao Wang,Pingping Zhang*

Main category: cs.CV

TL;DR: 提出名为Signal的多模态物体重识别框架，通过选择性交互和全局-局部对齐来解决背景干扰和多模态一致性对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注多模态特征融合，但忽视了背景干扰问题，且当前融合方法在模态对对齐时存在多模态一致性对齐困难。

Method: 提出选择性交互模块(SIM)选择重要补丁标记进行交互，全局对齐模块(GAM)在Gramian空间中最小化3D多面体体积来对齐多模态特征，局部对齐模块(LAM)以移位感知方式对齐局部特征。

Result: 在三个多模态物体重识别基准数据集(RGBNT201、RGBNT100、MSVR310)上的广泛实验验证了方法的有效性。

Conclusion: 所提框架能够为物体重识别提取更具判别性的特征，解决了多模态融合中的背景干扰和一致性对齐问题。

Abstract: Multi-modal object Re-IDentification (ReID) is devoted to retrieving specific objects through the exploitation of complementary multi-modal image information. Existing methods mainly concentrate on the fusion of multi-modal features, yet neglecting the background interference. Besides, current multi-modal fusion methods often focus on aligning modality pairs but suffer from multi-modal consistency alignment. To address these issues, we propose a novel selective interaction and global-local alignment framework called Signal for multi-modal object ReID. Specifically, we first propose a Selective Interaction Module (SIM) to select important patch tokens with intra-modal and inter-modal information. These important patch tokens engage in the interaction with class tokens, thereby yielding more discriminative features. Then, we propose a Global Alignment Module (GAM) to simultaneously align multi-modal features by minimizing the volume of 3D polyhedra in the gramian space. Meanwhile, we propose a Local Alignment Module (LAM) to align local features in a shift-aware manner. With these modules, our proposed framework could extract more discriminative features for object ReID. Extensive experiments on three multi-modal object ReID benchmarks (i.e., RGBNT201, RGBNT100, MSVR310) validate the effectiveness of our method. The source code is available at https://github.com/010129/Signal.

</details>


### [140] [CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking](https://arxiv.org/abs/2511.17967)
*Hao Li,Yuhao Wang,Xiantao Hu,Wenning Hao,Pingping Zhang,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了CADTrack框架，通过Mamba特征交互、上下文聚合和可变形对齐模块解决RGB-热成像跟踪中的模态差异问题，实现鲁棒的全天候目标跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有的RGB-热成像跟踪器难以解决模态差异问题，这限制了有效的跨模态信息传播和融合，显著降低了跟踪精度。

Method: 1. Mamba特征交互模块：基于状态空间模型建立高效特征交互，具有线性复杂度；2. 上下文聚合模块：基于混合专家机制动态激活骨干网络层，编码跨层特征的互补上下文信息；3. 可变形对齐模块：结合可变形采样和时间传播，缓解空间不对齐和定位漂移。

Result: 在五个RGB-热成像跟踪基准测试上进行了广泛实验，验证了所提方法的有效性。

Conclusion: CADTrack框架在复杂场景下实现了鲁棒且准确的跟踪性能。

Abstract: RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.

</details>


### [141] [Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning](https://arxiv.org/abs/2511.17973)
*Hiroto Honda*

Main category: cs.CV

TL;DR: 本文提出了一种无需示例的类增量学习方法APR，通过对抗性攻击扰动新任务图像来合成伪重放样本，解决了存储限制和隐私问题下的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决无需示例的类增量学习中的可塑性-稳定性困境，避免存储先前任务图像带来的存储限制和隐私问题。

Method: 使用对抗性攻击在新任务图像上生成伪重放样本，以增强的旧类均值原型为目标进行知识蒸馏，并通过学习转移矩阵校准协方差矩阵来补偿语义漂移。

Result: 在标准EFCIL基准测试的冷启动设置中实现了最先进的性能，平衡了稳定性和可塑性。

Conclusion: APR方法有效解决了EFCIL中的灾难性遗忘问题，无需存储重放样本即可保持旧知识，同时学习新任务。

Abstract: Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.

</details>


### [142] [FeRA: Frequency-Energy Constrained Routing for Effective Diffusion Adaptation Fine-Tuning](https://arxiv.org/abs/2511.17979)
*Bo Yin,Xiaobin Hu,Xingyu Zhou,Peng-Tao Jiang,Yue Liao,Junwei Zhu,Jiangning Zhang,Ying Tai,Chengjie Wang,Shuicheng Yan*

Main category: cs.CV

TL;DR: FeRA是一个基于频率能量的扩散模型微调框架，通过分析扩散模型去噪过程中的频率能量机制，提出频率能量指示器、软频率路由器和频率能量一致性正则化三个组件，实现与扩散内在频率能量进展对齐的参数更新。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模方面取得了显著成功，但如何有效将大型预训练模型适应新任务仍然具有挑战性。作者通过重新审视扩散模型在去噪过程中的重建行为，揭示了控制这一过程的底层频率能量机制。

Method: FeRA框架包含三个协同组件：(1)紧凑的频率能量指示器，表征潜在频带能量分布；(2)软频率路由器，自适应融合多个频率特定适配器专家；(3)频率能量一致性正则化，稳定扩散优化并确保跨频带的一致性适应。路由在训练和推理时都运行，推理时路由由潜在频率能量动态确定。

Result: FeRA与基于适配器的调优方案无缝集成，并在不同扩散骨干网络和分辨率上具有良好的泛化能力。通过将适应与频率能量机制对齐，FeRA为有效和鲁棒的扩散模型适应提供了简单、稳定和兼容的范式。

Conclusion: FeRA通过揭示和利用扩散模型的内在频率能量机制，提供了一个简单而有效的微调框架，能够稳定地适应不同任务，同时保持与现有适配器方法的兼容性。

Abstract: Diffusion models have achieved remarkable success in generative modeling, yet how to effectively adapt large pretrained models to new tasks remains challenging. We revisit the reconstruction behavior of diffusion models during denoising to unveil the underlying frequency energy mechanism governing this process. Building upon this observation, we propose FeRA, a frequency driven fine tuning framework that aligns parameter updates with the intrinsic frequency energy progression of diffusion. FeRA establishes a comprehensive frequency energy framework for effective diffusion adaptation fine tuning, comprising three synergistic components: (i) a compact frequency energy indicator that characterizes the latent bandwise energy distribution, (ii) a soft frequency router that adaptively fuses multiple frequency specific adapter experts, and (iii) a frequency energy consistency regularization that stabilizes diffusion optimization and ensures coherent adaptation across bands. Routing operates in both training and inference, with inference time routing dynamically determined by the latent frequency energy. It integrates seamlessly with adapter based tuning schemes and generalizes well across diffusion backbones and resolutions. By aligning adaptation with the frequency energy mechanism, FeRA provides a simple, stable, and compatible paradigm for effective and robust diffusion model adaptation.

</details>


### [143] [Plan-X: Instruct Video Generation via Semantic Planning](https://arxiv.org/abs/2511.17986)
*Lun Huang,You Xie,Hongyi Xu,Tianpei Gu,Chenxu Zhang,Guoxian Song,Zenan Li,Xiaochen Zhao,Linjie Luo,Guillermo Sapiro*

Main category: cs.CV

TL;DR: Plan-X是一个通过显式语义规划来指导视频生成的框架，通过语义规划器生成时空语义标记来减少视觉幻觉，实现细粒度的指令对齐视频生成。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器在视觉合成中表现出色，但在高级语义推理和长时程规划方面存在困难，导致视觉幻觉和用户指令不对齐的问题，特别是在复杂场景理解、人-物交互、多阶段动作和上下文运动推理等场景中。

Method: 提出Plan-X框架，核心是语义规划器——一个可学习的多模态语言模型，从文本提示和视觉上下文中推理用户意图，并自回归生成基于文本的时空语义标记序列。这些语义标记作为视频扩散模型的结构化"语义草图"。

Result: 大量实验表明，该框架显著减少了视觉幻觉，实现了与多模态上下文一致的细粒度、指令对齐的视频生成。

Conclusion: Plan-X有效整合了语言模型在多模态上下文推理和规划方面的优势，以及扩散模型在逼真视频合成方面的优势，解决了现有方法在语义推理方面的局限性。

Abstract: Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured "semantic sketches" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.

</details>


### [144] [HyM-UNet: Synergizing Local Texture and Global Context via Hybrid CNN-Mamba Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.17988)
*Haodong Chen,Xianfei Han,Qwen*

Main category: cs.CV

TL;DR: 提出HyM-UNet混合架构，结合CNN的局部特征提取能力和Mamba的全局建模能力，用于医学图像分割任务


<details>
  <summary>Details</summary>
Motivation: CNN受限于局部感受野，难以捕捉复杂的全局解剖结构，需要结合全局建模能力来提升分割精度

Method: 设计分层编码器：浅层使用卷积模块保留高频纹理细节，深层引入Visual Mamba模块捕获长距离语义依赖；提出Mamba引导融合跳跃连接，利用深层语义特征作为门控信号动态抑制浅层特征中的背景噪声

Result: 在ISIC 2018数据集上显著优于现有方法，Dice系数和IoU指标表现优异，同时保持较低参数数量和推理延迟

Conclusion: HyM-UNet能有效处理形状复杂和尺度变化的医学分割任务，验证了所提方法的有效性和鲁棒性

Abstract: Accurate organ and lesion segmentation is a critical prerequisite for computer-aided diagnosis. Convolutional Neural Networks (CNNs), constrained by their local receptive fields, often struggle to capture complex global anatomical structures. To tackle this challenge, this paper proposes a novel hybrid architecture, HyM-UNet, designed to synergize the local feature extraction capabilities of CNNs with the efficient global modeling capabilities of Mamba. Specifically, we design a Hierarchical Encoder that utilizes convolutional modules in the shallow stages to preserve high-frequency texture details, while introducing Visual Mamba modules in the deep stages to capture long-range semantic dependencies with linear complexity. To bridge the semantic gap between the encoder and the decoder, we propose a Mamba-Guided Fusion Skip Connection (MGF-Skip). This module leverages deep semantic features as gating signals to dynamically suppress background noise within shallow features, thereby enhancing the perception of ambiguous boundaries. We conduct extensive experiments on public benchmark dataset ISIC 2018. The results demonstrate that HyM-UNet significantly outperforms existing state-of-the-art methods in terms of Dice coefficient and IoU, while maintaining lower parameter counts and inference latency. This validates the effectiveness and robustness of the proposed method in handling medical segmentation tasks characterized by complex shapes and scale variations.

</details>


### [145] [SD-PSFNet: Sequential and Dynamic Point Spread Function Network for Image Deraining](https://arxiv.org/abs/2511.17993)
*Jiayu Wang,Haoyu Bian,Haoran Sun,Shaoning Zeng*

Main category: cs.CV

TL;DR: 提出SD-PSFNet，一种基于点扩散函数机制的多阶段图像去雨方法，通过动态物理建模和序列特征融合实现高效雨纹去除。


<details>
  <summary>Details</summary>
Motivation: 图像去雨对视觉应用至关重要，但面临雨的多尺度物理特性及其与场景复杂耦合的挑战。

Method: 采用三阶段级联架构，利用学习PSF机制动态模拟雨纹光学特性，结合自适应门控融合实现跨阶段特征集成。

Result: 在Rain100H、RealRain-1k-L和RealRain-1k-H数据集上达到最先进的PSNR/SSIM指标。

Conclusion: SD-PSFNet在复杂场景和密集降雨条件下表现出色，为图像去雨提供了新的物理感知方法。

Abstract: Image deraining is crucial for vision applications but is challenged by the complex multi-scale physics of rain and its coupling with scenes. To address this challenge, a novel approach inspired by multi-stage image restoration is proposed, incorporating Point Spread Function (PSF) mechanisms to reveal the image degradation process while combining dynamic physical modeling with sequential feature fusion transfer, named SD-PSFNet. Specifically, SD-PSFNet employs a sequential restoration architecture with three cascaded stages, allowing multiple dynamic evaluations and refinements of the degradation process estimation. The network utilizes components with learned PSF mechanisms to dynamically simulate rain streak optics, enabling effective rain-background separation while progressively enhancing outputs through novel PSF components at each stage. Additionally, SD-PSFNet incorporates adaptive gated fusion for optimal cross-stage feature integration, enabling sequential refinement from coarse rain removal to fine detail restoration. Our model achieves state-of-the-art PSNR/SSIM metrics on Rain100H (33.12dB/0.9371), RealRain-1k-L (42.28dB/0.9872), and RealRain-1k-H (41.08dB/0.9838). In summary, SD-PSFNet demonstrates excellent capability in complex scenes and dense rainfall conditions, providing a new physics-aware approach to image deraining.

</details>


### [146] [RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale](https://arxiv.org/abs/2511.18005)
*Shengyuan Wang,Zhiheng Zheng,Yu Shang,Lixuan He,Yangcheng Yu,Fan Hangyu,Jie Feng,Qingmin Liao,Yong Li*

Main category: cs.CV

TL;DR: RAISECity是一个现实对齐的智能合成引擎，用于生成详细的城市级3D世界，通过代理框架利用多模态基础工具，在3D质量、现实对齐和可扩展性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在城市级3D生成中面临质量、保真度和可扩展性的挑战，需要开发能够创建详细、城市级3D世界的新方法。

Method: 采用代理框架，利用多样化多模态基础工具获取现实世界知识，维护鲁棒的中间表示，构建复杂3D场景，包括动态数据处理、迭代自反思和精炼，以及调用高级多模态工具。

Result: 在现实对齐、形状精度、纹理保真度和美学水平方面表现优异，在整体感知质量上对现有基线方法实现了超过90%的胜率。

Conclusion: RAISECity结合了3D质量、现实对齐、可扩展性和与计算机图形管道的无缝兼容性，为沉浸式媒体、具身智能和世界模型应用提供了有前景的基础。

Abstract: City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \textbf{R}eality-\textbf{A}ligned \textbf{I}ntelligent \textbf{S}ynthesis \textbf{E}ngine that creates detailed, \textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.

</details>


### [147] [Is Complete Labeling Necessary? Understanding Active Learning in Longitudinal Medical Imaging](https://arxiv.org/abs/2511.18007)
*Siteng Ma,Honghui Du,Prateek Mathur,Brendan S. Kelly,Ronan P. Killeen,Aonghus Lawlor,Ruihai Dong*

Main category: cs.CV

TL;DR: 提出了一种针对纵向医学影像的深度主动学习框架LMI-AL，通过配对和差分基线及随访3D图像的2D切片，迭代选择最具信息量的样本进行标注，仅需标注不到8%的数据即可达到全标注数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 纵向医学影像标注成本高、耗时长，现有深度主动学习方法主要针对静态任务，无法直接应用于需要识别多图像间细微变化的变化检测任务。

Method: LMI-AL框架将基线和随访3D图像的所有2D切片配对并差分，使用深度主动学习迭代选择最具信息量的图像对进行标注，训练深度学习模型。

Result: 实验结果表明，仅标注不到8%的数据，LMI-AL就能达到与全标注数据集训练模型相当的性能。

Conclusion: LMI-AL为纵向医学影像变化检测提供了一种高效的标注策略，显著降低了标注成本，为未来研究提供了指导。

Abstract: Detecting changes in longitudinal medical imaging using deep learning requires a substantial amount of accurately labeled data. However, labeling these images is notably more costly and time-consuming than labeling other image types, as it requires labeling across various time points, where new lesions can be minor, and subtle changes are easily missed. Deep Active Learning (DAL) has shown promise in minimizing labeling costs by selectively querying the most informative samples, but existing studies have primarily focused on static tasks like classification and segmentation. Consequently, the conventional DAL approach cannot be directly applied to change detection tasks, which involve identifying subtle differences across multiple images. In this study, we propose a novel DAL framework, named Longitudinal Medical Imaging Active Learning (LMI-AL), tailored specifically for longitudinal medical imaging. By pairing and differencing all 2D slices from baseline and follow-up 3D images, LMI-AL iteratively selects the most informative pairs for labeling using DAL, training a deep learning model with minimal manual annotation. Experimental results demonstrate that, with less than 8% of the data labeled, LMI-AL can achieve performance comparable to models trained on fully labeled datasets. We also provide a detailed analysis of the method's performance, as guidance for future research. The code is publicly available at https://github.com/HelenMa9998/Longitudinal_AL.

</details>


### [148] [RoadBench: Benchmarking MLLMs on Fine-Grained Spatial Understanding and Reasoning under Urban Road Scenarios](https://arxiv.org/abs/2511.18011)
*Jun Zhang,Jie Feng,Long Chen,Junhui Wang,Zhicheng Liu,Depeng Jin,Yong Li*

Main category: cs.CV

TL;DR: RoadBench是一个针对多模态大语言模型在复杂城市场景中细粒度空间理解能力的系统性基准测试，包含6个任务和9121个测试用例，评估模型在道路标记识别、联合理解和推理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂城市场景中的细粒度空间理解和推理能力尚未得到充分研究，特别是道路标记作为城市交通网络的重要组成部分，需要专门的评估基准。

Method: 提出RoadBench基准，使用BEV和FPV图像输入，构建包含6个任务的系统性评估框架，涵盖从局部空间范围理解到全局推理的能力测试。

Result: 评估14个主流MLLMs后发现，RoadBench对现有模型具有挑战性，某些任务中模型表现甚至低于简单的基于规则或随机选择的基线方法。

Conclusion: RoadBench揭示了现有MLLMs在城市场景细粒度空间理解方面的显著不足，该基准将有助于全面推动MLLMs空间理解能力的发展。

Abstract: Multimodal large language models (MLLMs) have demonstrated powerful capabilities in general spatial understanding and reasoning. However, their fine-grained spatial understanding and reasoning capabilities in complex urban scenarios have not received significant attention in the fields of both research and industry. To fill this gap, we focus primarily on road markings as a typical example of fine-grained spatial elements under urban scenarios, given the essential role of the integrated road traffic network they form within cities. Around road markings and urban traffic systems, we propose RoadBench, a systematic benchmark that comprehensively evaluates MLLMs' fine-grained spatial understanding and reasoning capabilities using BEV and FPV image inputs. This benchmark comprises six tasks consisting of 9,121 strictly manually verified test cases. These tasks form a systematic evaluation framework that bridges understanding at local spatial scopes to global reasoning. They not only test MLLMs' capabilities in recognition, joint understanding, and reasoning but also assess their ability to integrate image information with domain knowledge. After evaluating 14 mainstream MLLMs, we confirm that RoadBench is a challenging benchmark for MLLMs while revealing significant shortcomings in existing MLLMs' fine-grained spatial understanding and reasoning capabilities within urban scenarios. In certain tasks, their performance even falls short of simple rule-based or random selection baselines. These findings, along with RoadBench itself, will contribute to the comprehensive advancement of spatial understanding capabilities for MLLMs. The benchmark code, example datasets, and raw evaluation results are available in the supplementary material.

</details>


### [149] [State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection](https://arxiv.org/abs/2511.18012)
*Jiaying Zhou,Qingchao Chen*

Main category: cs.CV

TL;DR: 提出两种原型增强策略来解决弱监督开放词汇目标检测中的问题：状态增强语义原型(SESP)捕获类内视觉变化，场景增强伪原型(SAPP)解决语义不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有语义原型是静态的，无法捕捉由不同物体状态引起的丰富类内视觉变化；标准伪框生成导致视觉区域提案与物体中心文本嵌入之间的语义不匹配。

Method: 提出SESP生成状态感知的文本描述来捕获多样化物体外观；提出SAPP结合上下文语义并使用软对齐机制促进上下文一致的视觉-文本表示。

Result: 通过整合SESP和SAPP，有效增强了语义原型的丰富性和视觉-文本对齐，取得了显著改进。

Conclusion: 该方法通过状态增强和场景增强策略，成功解决了弱监督开放词汇目标检测中的关键挑战，提升了检测性能。

Abstract: Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., "cat lying on sofa") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.

</details>


### [150] [Modeling Retinal Ganglion Cells with Neural Differential Equations](https://arxiv.org/abs/2511.18014)
*Kacper Dobek,Daniel Jankowski,Krzysztof Krawiec*

Main category: cs.CV

TL;DR: LTC和CfC网络在模拟虎蝾螈视网膜神经节细胞活动方面优于卷积基线和LSTM，具有更低的MAE、更快的收敛速度、更小的模型尺寸和更优的查询时间，但皮尔逊相关性略低。


<details>
  <summary>Details</summary>
Motivation: 探索LTC和CfC网络在视网膜神经节细胞活动建模中的应用，特别是在数据有限和需要频繁重新训练的场景下，如视觉假体边缘部署。

Method: 使用Liquid Time-Constant Networks (LTCs)和Closed-form Continuous-time Networks (CfCs)对三个数据集中的虎蝾螈视网膜神经节细胞活动进行建模，并与卷积基线和LSTM进行比较。

Result: LTC和CfC架构在MAE、收敛速度、模型大小和查询时间方面均优于基线模型，但皮尔逊相关性略低。

Conclusion: LTC和CfC网络的高效性和适应性使其特别适合数据有限和需要频繁重新训练的场景，如视觉假体边缘部署。

Abstract: This work explores Liquid Time-Constant Networks (LTCs) and Closed-form Continuous-time Networks (CfCs) for modeling retinal ganglion cell activity in tiger salamanders across three datasets. Compared to a convolutional baseline and an LSTM, both architectures achieved lower MAE, faster convergence, smaller model sizes, and favorable query times, though with slightly lower Pearson correlation. Their efficiency and adaptability make them well suited for scenarios with limited data and frequent retraining, such as edge deployments in vision prosthetics.

</details>


### [151] [MambaX: Image Super-Resolution with State Predictive Control](https://arxiv.org/abs/2511.18028)
*Chenyu Li,Danfeng Hong,Bing Zhang,Zhaojie Pan,Naoto Yokoya,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 提出了MambaX模型，通过非线性状态预测控制来改进图像超分辨率任务，解决了现有方法在中间阶段误差传播控制不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率方法主要关注最终分辨率提升，忽视了中间阶段的误差传播和累积控制。Mamba虽然能将重建过程表示为状态序列，但其固定线性映射器感受野有限且灵活性不足。

Method: 创建非线性状态预测控制模型MambaX，将连续光谱带映射到潜在状态空间，通过动态学习控制方程的非线性状态参数来泛化超分辨率任务。具体包括动态状态预测控制学习、状态交叉控制范式和渐进过渡学习。

Result: 评估表明动态频谱状态表示模型在单图像超分辨率和多模态融合超分辨率任务中表现优异。

Conclusion: MambaX在任意维度和模态的光谱泛化建模方面具有显著潜力。

Abstract: Image super-resolution (SR) is a critical technology for overcoming the inherent hardware limitations of sensors. However, existing approaches mainly focus on directly enhancing the final resolution, often neglecting effective control over error propagation and accumulation during intermediate stages. Recently, Mamba has emerged as a promising approach that can represent the entire reconstruction process as a state sequence with multiple nodes, allowing for intermediate intervention. Nonetheless, its fixed linear mapper is limited by a narrow receptive field and restricted flexibility, which hampers its effectiveness in fine-grained images. To address this, we created a nonlinear state predictive control model \textbf{MambaX} that maps consecutive spectral bands into a latent state space and generalizes the SR task by dynamically learning the nonlinear state parameters of control equations. Compared to existing sequence models, MambaX 1) employs dynamic state predictive control learning to approximate the nonlinear differential coefficients of state-space models; 2) introduces a novel state cross-control paradigm for multimodal SR fusion; and 3) utilizes progressive transitional learning to mitigate heterogeneity caused by domain and modality shifts. Our evaluation demonstrates the superior performance of the dynamic spectrum-state representation model in both single-image SR and multimodal fusion-based SR tasks, highlighting its substantial potential to advance spectrally generalized modeling across arbitrary dimensions and modalities.

</details>


### [152] [Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation](https://arxiv.org/abs/2511.18037)
*Yunfan Lu,Nico Messikommer,Xiaogang Xu,Liming Chen,Yuhan Chen,Nikola Zubic,Davide Scaramuzza,Hui Xiong*

Main category: cs.CV

TL;DR: 提出了首个统一的事件帧混合传感器噪声模型，联合描述APS和EVS像素的噪声行为，并开发了校准流程和统计模拟器HESIM。


<details>
  <summary>Details</summary>
Motivation: 事件帧混合传感器虽然结合了APS和EVS的优势，但其复杂电路架构引入了难以理解的噪声模式，目前缺乏统一的噪声模型。

Method: 建立基于统计的成像噪声模型，包含光子散粒噪声、暗电流噪声、固定模式噪声和量化噪声，开发校准流程估计噪声参数，并创建统计模拟器HESIM。

Result: 在两个混合传感器上的实验验证了模型在多个成像任务（视频帧插值和去模糊）中的有效性，展示了从模拟到真实数据的强迁移能力。

Conclusion: 该工作为事件帧混合传感器提供了首个统一的噪声建模框架，显著提升了模拟到真实数据的性能迁移。

Abstract: Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.

</details>


### [153] [UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios](https://arxiv.org/abs/2511.18050)
*Tian Ye,Song Fei,Lei Zhu*

Main category: cs.CV

TL;DR: UltraFlux是一个基于Flux的扩散变换器，通过数据-模型协同设计原生支持4K分辨率生成，解决了位置编码、VAE压缩和优化等耦合问题，在4K多宽高比生成中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有扩散变换器在1K分辨率表现良好，但扩展到原生4K分辨率时暴露出位置编码、VAE压缩和优化之间的耦合问题，单独解决任一因素都无法达到理想质量。

Method: 采用数据-模型协同设计：数据方面构建MultiAspect-4K-1M数据集；模型方面结合Resonance 2D RoPE与YaRN位置编码、非对抗性VAE后训练、SNR-Aware Huber小波目标函数和分阶段美学课程学习策略。

Result: 在4096分辨率的Aesthetic-Eval基准测试和多宽高比4K设置中，UltraFlux在保真度、美学质量和文本对齐方面持续优于开源基线，配合LLM提示优化器可匹敌或超越专有模型Seedream 4.0。

Conclusion: 通过系统性解决4K生成中的耦合问题，UltraFlux实现了稳定、细节保留的4K扩散变换器，能够泛化到宽屏、方形和竖屏等多种宽高比。

Abstract: Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.

</details>


### [154] [IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment](https://arxiv.org/abs/2511.18055)
*Bowen Qu,Shangkun Sun,Xiaoyu Liang,Wei Gao*

Main category: cs.CV

TL;DR: 提出了IE-Bench基准套件和IE-Critic-R1评估方法，用于改进文本驱动图像编辑的质量评估，通过强化学习从可验证奖励中训练，能更好地与人类感知对齐。


<details>
  <summary>Details</summary>
Motivation: 文本驱动图像编辑的评估具有挑战性，因为编辑后的图像既依赖文本又依赖源图像，现有方法要么只关注文本-图像对齐，要么未能很好地对齐人类感知。

Method: 构建了IE-Bench基准套件，包含多样化源图像、编辑提示和编辑结果，以及近4000个样本的人类评分；提出了IE-Critic-R1，利用强化学习从可验证奖励中训练，提供更全面和可解释的质量评估。

Result: 大量实验表明，IE-Critic-R1在文本驱动图像编辑任务上相比先前指标具有更优越的主观对齐性能。

Conclusion: IE-Bench和IE-Critic-R1为文本驱动图像编辑提供了更好的评估基准和方法，相关数据和代码已公开。

Abstract: Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.

</details>


### [155] [Hierarchical Semi-Supervised Active Learning for Remote Sensing](https://arxiv.org/abs/2511.18058)
*Wei Huang,Zhitong Xiong,Chenying Liu,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出了一种分层半监督主动学习框架HSSAL，通过结合半监督学习和分层主动学习，在遥感场景分类中显著提高了标签效率，仅需少量标注数据即可达到接近全监督的性能。


<details>
  <summary>Details</summary>
Motivation: 遥感领域深度学习模型性能依赖于高质量标注数据，但大规模标注成本高且耗时，大量未标注图像未被充分利用。

Method: HSSAL框架将半监督学习与分层主动学习结合在闭环迭代中：SSL通过监督学习和弱到强自训练优化模型；HAL使用渐进聚类策略选择同时满足可扩展性、多样性和不确定性的信息量最大样本。

Result: 在UCM、AID和NWPU-RESISC45三个基准数据集上的实验表明，HSSAL始终优于仅使用SSL或AL的基线方法。仅需8%、4%和2%标注数据即可达到超过95%的全监督准确率。

Conclusion: HSSAL通过有效利用未标注数据的信息量，在遥感场景分类中实现了卓越的标签效率，为解决标注数据稀缺问题提供了有效方案。

Abstract: The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be released at https://github.com/zhu-xlab/RS-SSAL.

</details>


### [156] [A Lightweight, Interpretable Deep Learning System for Automated Detection of Cervical Adenocarcinoma In Situ (AIS)](https://arxiv.org/abs/2511.18063)
*Gabriela Fernandes*

Main category: cs.CV

TL;DR: 开发基于深度学习的虚拟病理助手，使用EfficientNet-B3网络区分宫颈腺癌原位病变与正常组织，在CAISHI数据集上达到73.23%准确率。


<details>
  <summary>Details</summary>
Motivation: 宫颈腺癌原位病变的准确病理诊断具有挑战性，早期检测对预防进展为浸润性腺癌至关重要。

Method: 使用2240张专家标注的H&E图像，经过Macenko染色归一化和基于补丁的预处理，采用EfficientNet-B3网络训练，使用类别平衡采样和焦点损失函数处理数据不平衡问题。

Result: 模型总体准确率0.7323，异常类别F1分数0.75，正常类别F1分数0.71。Grad-CAM热图显示与AIS形态一致的核异型性和腺体拥挤激活模式。

Conclusion: 证明了轻量级、可解释AI系统在宫颈腺体病理学中的可行性，在筛查工作流程、教育和资源匮乏环境中具有应用潜力。

Abstract: Cervical adenocarcinoma in situ (AIS) is a critical premalignant lesion whose accurate histopathological diagnosis is challenging. Early detection is essential to prevent progression to invasive cervical adenocarcinoma. In this study, we developed a deep learning-based virtual pathology assistant capable of distinguishing AIS from normal cervical gland histology using the CAISHI dataset, which contains 2240 expert-labeled H&E images (1010 normal and 1230 AIS). All images underwent Macenko stain normalization and patch-based preprocessing to enhance morphological feature representation. An EfficientNet-B3 convolutional neural network was trained using class-balanced sampling and focal loss to address dataset imbalance and emphasize difficult examples. The final model achieved an overall accuracy of 0.7323, with an F1-score of 0.75 for the Abnormal class and 0.71 for the Normal class. Grad-CAM heatmaps demonstrated biologically interpretable activation patterns, highlighting nuclear atypia and glandular crowding consistent with AIS morphology. The trained model was deployed in a Gradio-based virtual diagnostic assistant. These findings demonstrate the feasibility of lightweight, interpretable AI systems for cervical gland pathology, with potential applications in screening workflows, education, and low-resource settings.

</details>


### [157] [VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection](https://arxiv.org/abs/2511.18075)
*Jianhang Yao,Yongbin Zheng,Siqi Lu,Wanying Xu,Peng Sun*

Main category: cs.CV

TL;DR: VK-Det是一个无需额外监督的视觉知识引导开放词汇目标检测框架，通过利用视觉编码器的固有区域感知能力和原型感知伪标签策略，在航空图像上实现了最先进的开放词汇检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇航空目标检测方法依赖文本监督，导致语义偏差，限制了向文本未指定概念的扩展。需要摆脱对文本的依赖，实现真正的开放词汇检测。

Method: 1. 利用视觉编码器的固有信息区域感知能力实现细粒度定位和自适应蒸馏；2. 提出原型感知伪标签策略，通过特征聚类建模类间决策边界，通过原型匹配将检测区域映射到潜在类别。

Result: 在DIOR数据集上达到30.1 mAP^N，在DOTA数据集上达到23.3 mAP^N，超越了需要额外监督的方法。

Conclusion: VK-Det证明了无需额外监督即可实现高性能开放词汇目标检测的可行性，通过视觉知识引导和原型匹配有效解决了文本依赖带来的语义偏差问题。

Abstract: To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\textbf{VK-Det}$, a $\textbf{V}$isual $\textbf{K}$nowledge-guided open-vocabulary object $\textbf{Det}$ection framework $\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\mathrm{mAP}^{N}$ on DIOR and 23.3 $\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.

</details>


### [158] [ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models](https://arxiv.org/abs/2511.18082)
*Wencheng Ye,Tianshi Wang,Lei Zhu,Fengling Li,Guoli Yang*

Main category: cs.CV

TL;DR: ActDistill是一个动作引导的自蒸馏框架，将现有VLA模型的动作预测能力转移到轻量级模型，通过动作先验指导知识转移和模型压缩，实现超过50%的计算减少和1.67倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人操作中面临计算开销大和推理延迟高的问题，限制了实际部署。

Method: 使用图结构封装策略建模动作预测的层次演化，教师模型指导学生模型，配备动态路由器自适应选择计算路径，通过层次图监督确保平滑高效演化。

Result: 在具身基准测试中，ActDistill达到与全规模VLA模型相当或更优的性能，计算减少超过50%，加速达1.67倍。

Conclusion: ActDistill为高效具身智能建立了一个通用范式，实现了动作导向的VLA模型效率优化。

Abstract: Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.

</details>


### [159] [Less Is More: An Explainable AI Framework for Lightweight Malaria Classification](https://arxiv.org/abs/2511.18083)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CV

TL;DR: 该研究提出EMFE管道，通过提取细胞形态特征，使用轻量级机器学习模型实现疟疾细胞分类，性能媲美深度学习但计算需求极低。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医学图像分类中计算需求高、可解释性差的问题，探索在简单二元分类任务中是否必须使用复杂神经网络。

Method: 从NIH疟疾细胞图像数据集中提取两个形态特征（非背景像素数和细胞内部孔洞数），使用逻辑回归和随机森林模型，并与多种深度学习模型对比，还构建了两阶段集成模型。

Result: 单变量逻辑回归模型测试准确率达94.80%，文件大小仅1.2kB，推理延迟2.3ms；集成模型准确率提升至97.15%；而深度学习模型需要13.6-44.7MB存储，推理时间68ms。

Conclusion: 紧凑的特征工程方法能够提供临床意义的分类性能，同时在透明度、可重复性、速度和部署可行性方面具有优势，适用于计算资源有限的环境。

Abstract: Background and Objective: Deep learning models have high computational needs and lack interpretability but are often the first choice for medical image classification tasks. This study addresses whether complex neural networks are essential for the simple binary classification task of malaria. We introduce the Extracted Morphological Feature Engineered (EMFE) pipeline, a transparent, reproducible, and low compute machine learning approach tailored explicitly for simple cell morphology, designed to achieve deep learning performance levels on a simple CPU only setup with the practical aim of real world deployment.
  Methods: The study used the NIH Malaria Cell Images dataset, with two features extracted from each cell image: the number of non background pixels and the number of holes within the cell. Logistic Regression and Random Forest were compared against ResNet18, DenseNet121, MobileNetV2, and EfficientNet across accuracy, model size, and CPU inference time. An ensemble model was created by combining Logistic Regression and Random Forests to achieve higher accuracy while retaining efficiency.
  Results: The single variable Logistic Regression model achieved a test accuracy of 94.80 percent with a file size of 1.2 kB and negligible inference latency (2.3 ms). The two stage ensemble improved accuracy to 97.15 percent. In contrast, the deep learning methods require 13.6 MB to 44.7 MB of storage and show significantly higher inference times (68 ms).
  Conclusion: This study shows that a compact feature engineering approach can produce clinically meaningful classification performance while offering gains in transparency, reproducibility, speed, and deployment feasibility. The proposed pipeline demonstrates that simple interpretable features paired with lightweight models can serve as a practical diagnostic solution for environments with limited computational resources.

</details>


### [160] [Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective](https://arxiv.org/abs/2511.18089)
*Wenjing Liu,Qin Ren,Wen Zhang,Yuewei Lin,Chenyu You*

Main category: cs.CV

TL;DR: 提出了Together-Then-Apart (TTA)框架，通过双重优化同时建模共享和模态特定表示，在多模态生存分析中平衡对齐和独特性。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度强调跨模态对齐，导致表示崩溃和多样性减少，需要同时保持模态特定特征和语义一致性。

Method: TTA框架包含Together阶段（最小化语义差异，通过共享原型对齐嵌入）和Apart阶段（最大化表示多样性，通过模态锚点和对比正则化器）。

Result: 在五个TCGA基准测试中，TTA始终优于最先进方法。

Conclusion: 该框架为多模态生存分析提供了新的理论视角，展示了如何同时实现对齐和独特性，以获得稳健、可解释且具有生物学意义的结果。

Abstract: Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.

</details>


### [161] [Versatile Recompression-Aware Perceptual Image Super-Resolution](https://arxiv.org/abs/2511.18090)
*Mingwei He,Tongda Xu,Xingtong Ge,Ming Sun,Chao Zhou,Yan Wang*

Main category: cs.CV

TL;DR: VRPSR是一种感知图像超分辨率方法，通过将压缩建模为条件文本到图像生成，利用预训练扩散模型构建通用编解码器模拟器，使现有感知SR方法能够适应各种压缩设置。


<details>
  <summary>Details</summary>
Motivation: 现有的感知图像超分辨率方法在输出后通常会被重新压缩存储和传输，忽略重新压缩会导致编解码器在恢复图像上添加额外伪影，但联合优化SR和重新压缩具有挑战性。

Method: 将压缩建模为条件文本到图像生成，利用预训练扩散模型构建通用编解码器模拟器；提出针对感知SR的训练技术，包括使用感知目标优化模拟器，以及采用轻微压缩图像作为训练目标。

Result: 基于Real-ESRGAN和S3Diff，在H.264/H.265/H.266压缩下节省超过10%的比特率；促进SR与重新压缩后处理模型的联合优化。

Conclusion: VRPSR成功使现有感知SR方法能够感知多种压缩设置，显著节省比特率并支持联合优化。

Abstract: Perceptual image super-resolution (SR) methods restore degraded images and produce sharp outputs. In practice, those outputs are usually recompressed for storage and transmission. Ignoring recompression is suboptimal as the downstream codec might add additional artifacts to restored images. However, jointly optimizing SR and recompression is challenging, as the codecs are not differentiable and vary in configuration. In this paper, we present Versatile Recompression-Aware Perceptual Super-Resolution (VRPSR), which makes existing perceptual SR aware of versatile compression. First, we formulate compression as conditional text-to-image generation and utilize a pre-trained diffusion model to build a generalizable codec simulator. Next, we propose a set of training techniques tailored for perceptual SR, including optimizing the simulator using perceptual targets and adopting slightly compressed images as the training target. Empirically, our VRPSR saves more than 10\% bitrate based on Real-ESRGAN and S3Diff under H.264/H.265/H.266 compression. Besides, our VRPSR facilitates joint optimization of the SR and post-processing model after recompression.

</details>


### [162] [Spotlight: Identifying and Localizing Video Generation Errors Using VLMs](https://arxiv.org/abs/2511.18102)
*Aditya Chinchure,Sahithya Ravi,Pushkar Shukla,Vered Shwartz,Leonid Sigal*

Main category: cs.CV

TL;DR: 提出了Spotlight任务，用于定位和解释视频生成中的错误，发现当前VLMs在视频错误识别和定位方面显著落后于人类


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频模型虽然能生成高质量视频，但仍存在细微错误，而现有评估方法无法定位具体错误发生的时间和描述错误性质

Method: 使用200个多样化文本提示和3个先进视频生成器生成600个视频，标注了1600多个细粒度错误，涵盖6种错误类型

Result: 发现提示遵循和物理错误占主导且持续时间长，外观消失和身体姿态错误持续时间短；VLMs在错误识别和定位方面表现远不如人类

Conclusion: Spotlight任务为构建细粒度评估工具和更复杂的视频生成器奖励模型铺平了道路

Abstract: Current text-to-video models (T2V) can generate high-quality, temporally coherent, and visually realistic videos. Nonetheless, errors still often occur, and are more nuanced and local compared to the previous generation of T2V models. While current evaluation paradigms assess video models across diverse dimensions, they typically evaluate videos holistically without identifying when specific errors occur or describing their nature. We address this gap by introducing Spotlight, a novel task aimed at localizing and explaining video-generation errors. We generate 600 videos using 200 diverse textual prompts and three state-of-the-art video generators (Veo 3, Seedance, and LTX-2), and annotate over 1600 fine-grained errors across six types, including motion, physics, and prompt adherence. We observe that adherence and physics errors are predominant and persist across longer segments, whereas appearance-disappearance and body pose errors manifest in shorter segments. We then evaluate current VLMs on Spotlight and find that VLMs lag significantly behind humans in error identification and localization in videos. We propose inference-time strategies to probe the limits of current VLMs on our task, improving performance by nearly 2x. Our task paves a way forward to building fine-grained evaluation tools and more sophisticated reward models for video generators.

</details>


### [163] [Consolidating Diffusion-Generated Video Detection with Unified Multimodal Forgery Learning](https://arxiv.org/abs/2511.18104)
*Xiaohong Liu,Xiufeng Song,Huayu Zheng,Lei Bai,Xiaoming Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了MM-Det++算法，用于检测扩散模型生成的视频伪造内容，通过时空分支和多模态分支结合统一多模态学习模块，在大型DVF数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的视频日益增多引发信息安全担忧，现有方法主要关注图像级伪造检测，视频级通用伪造检测研究不足。

Method: 采用双分支结构：时空分支使用帧中心视觉变换器聚合时空信息；多模态分支利用多模态大语言模型获取伪造表示；通过统一多模态学习模块整合多模态表示。

Result: 在大型DVF数据集上的广泛实验证明了MM-Det++的优越性，展示了统一多模态伪造学习在检测扩散生成视频方面的有效性。

Conclusion: MM-Det++通过创新的多模态方法成功解决了扩散生成视频的检测问题，为视频取证领域提供了有效的解决方案。

Abstract: The proliferation of videos generated by diffusion models has raised increasing concerns about information security, highlighting the urgent need for reliable detection of synthetic media. Existing methods primarily focus on image-level forgery detection, leaving generic video-level forgery detection largely underexplored. To advance video forensics, we propose a consolidated multimodal detection algorithm, named MM-Det++, specifically designed for detecting diffusion-generated videos. Our approach consists of two innovative branches and a Unified Multimodal Learning (UML) module. Specifically, the Spatio-Temporal (ST) branch employs a novel Frame-Centric Vision Transformer (FC-ViT) to aggregate spatio-temporal information for detecting diffusion-generated videos, where the FC-tokens enable the capture of holistic forgery traces from each video frame. In parallel, the Multimodal (MM) branch adopts a learnable reasoning paradigm to acquire Multimodal Forgery Representation (MFR) by harnessing the powerful comprehension and reasoning capabilities of Multimodal Large Language Models (MLLMs), which discerns the forgery traces from a flexible semantic perspective. To integrate multimodal representations into a coherent space, a UML module is introduced to consolidate the generalization ability of MM-Det++. In addition, we also establish a large-scale and comprehensive Diffusion Video Forensics (DVF) dataset to advance research in video forgery detection. Extensive experiments demonstrate the superiority of MM-Det++ and highlight the effectiveness of unified multimodal forgery learning in detecting diffusion-generated videos.

</details>


### [164] [AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens](https://arxiv.org/abs/2511.18105)
*Purvish Jajal,Nick John Eliopoulos,Benjamin Shiue-Hal Chou,George K. Thiruvathukal,Yung-Hsiang Lu,James C. Davis*

Main category: cs.CV

TL;DR: AdaPerceiver是首个在单个模型中统一支持深度、宽度和token维度自适应计算的transformer架构，能够在不同硬件和延迟约束下动态调整计算量。


<details>
  <summary>Details</summary>
Motivation: 现有transformer模型在推理时计算分配方式固定，无法适应多样化的硬件和延迟需求。大多数动态计算方法只关注单一维度（如减少token数量），缺乏统一的自适应能力。

Method: 提出AdaPerceiver架构，支持在深度、宽度和token三个维度进行自适应计算，并设计了高效的联合训练机制来确保模型在各种配置下都能保持性能。

Result: 在图像分类任务上，AdaPerceiver扩展了准确率-吞吐量的Pareto前沿，达到85.4%准确率，吞吐量比FlexiViT-L高36%。在密集预测任务上，匹配ViT-H/14性能的同时，语义分割和深度估计的编码器FLOPs减少约26倍。配备策略后，能在保持ImageNet1K准确率（±0.1%）的同时减少24-33%的FLOPs。

Conclusion: AdaPerceiver通过统一的多维度自适应计算，为transformer模型提供了灵活适应不同部署环境的有效解决方案，显著提升了计算效率。

Abstract: Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.

</details>


### [165] [Muskie: Multi-view Masked Image Modeling for 3D Vision Pre-training](https://arxiv.org/abs/2511.18115)
*Wenyu Li,Sidun Liu,Peng Qiao,Yong Dou,Tongrui Hu*

Main category: cs.CV

TL;DR: Muskie是一个原生多视角视觉骨干网络，专为3D视觉任务设计，通过同时处理多个视角并在预训练阶段引入多视角一致性，无需3D监督即可学习视角不变特征和几何理解。


<details>
  <summary>Details</summary>
Motivation: 现有模型是逐帧处理的，多视角一致性有限，需要设计能同时处理多个视角并确保多视角一致性的骨干网络。

Method: 通过重建一个视角中被严重遮挡的内容，从其他视角寻找和利用几何对应关系，结合激进的遮挡策略，在预训练阶段学习视角不变特征。

Result: 相比DINO等最先进的逐帧骨干网络，Muskie实现了更高的多视角对应精度，在下游3D任务（相机姿态估计和点云重建）中持续提升性能。

Conclusion: Muskie通过多视角一致性预训练，无需3D监督即可学习强大的几何理解能力，在3D视觉任务中表现优异。

Abstract: We present Muskie, a native multi-view vision backbone designed for 3D vision tasks. Unlike existing models, which are frame-wise and exhibit limited multi-view consistency, Muskie is designed to process multiple views simultaneously and introduce multi-view consistency in pre-training stage. Muskie is trained to reconstruct heavily masked content in one view by finding and utilizing geometric correspondences from other views. Through this pretext task and our proposed aggressive masking strategy, the model implicitly to learn view-invariant features and develop strong geometric understanding without any 3D supervision. Compared with state-of-the-art frame-wise backbones such as DINO, Muskie achieves higher multi-view correspondence accuracy. Furthermore, we demonstrate that using Muskie as a backbone consistently enhances performance on downstream 3D tasks, including camera pose estimation and pointmap reconstruction. Codes are publicly available at https://leo-frank.github.io/Muskie/

</details>


### [166] [PromptMoE: Generalizable Zero-Shot Anomaly Detection via Visually-Guided Prompt Mixtures](https://arxiv.org/abs/2511.18116)
*Yuheng Shao,Lizhang Wang,Changhao Li,Peixian Chen,Qinyuan Liu*

Main category: cs.CV

TL;DR: 提出了PromptMoE方法，通过混合专家机制动态组合专家提示来解决零样本异常检测中的表示瓶颈和过拟合问题


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的零样本异常检测方法受限于提示工程策略，存在表示瓶颈和过拟合问题，难以泛化到复杂多样的未见异常

Method: 学习专家提示池作为可组合的语义基元，通过视觉引导的混合专家机制为每个实例动态组合提示，生成语义丰富的文本表示

Result: 在工业和医疗领域的15个数据集上进行了广泛实验，证明了方法的有效性和最先进性能

Conclusion: PromptMoE通过组合式提示学习方法解决了零样本异常检测中的关键挑战，实现了更好的泛化能力

Abstract: Zero-Shot Anomaly Detection (ZSAD) aims to identify and localize anomalous regions in images of unseen object classes. While recent methods based on vision-language models like CLIP show promise, their performance is constrained by existing prompt engineering strategies. Current approaches, whether relying on single fixed, learnable, or dense dynamic prompts, suffer from a representational bottleneck and are prone to overfitting on auxiliary data, failing to generalize to the complexity and diversity of unseen anomalies. To overcome these limitations, we propose $\mathtt{PromptMoE}$. Our core insight is that robust ZSAD requires a compositional approach to prompt learning. Instead of learning monolithic prompts, $\mathtt{PromptMoE}$ learns a pool of expert prompts, which serve as a basis set of composable semantic primitives, and a visually-guided Mixture-of-Experts (MoE) mechanism to dynamically combine them for each instance. Our framework materializes this concept through a Visually-Guided Mixture of Prompt (VGMoP) that employs an image-gated sparse MoE to aggregate diverse normal and abnormal expert state prompts, generating semantically rich textual representations with strong generalization. Extensive experiments across 15 datasets in industrial and medical domains demonstrate the effectiveness and state-of-the-art performance of $\mathtt{PromptMoE}$.

</details>


### [167] [MVS-TTA: Test-Time Adaptation for Multi-View Stereo via Meta-Auxiliary Learning](https://arxiv.org/abs/2511.18120)
*Hannuo Zhang,Zhixiang Chi,Yang Wang,Xinxin Zuo*

Main category: cs.CV

TL;DR: MVS-TTA是一个高效的测试时自适应框架，通过元辅助学习策略将优化方法的场景自适应能力引入学习型多视图立体方法，提升其泛化性能。


<details>
  <summary>Details</summary>
Motivation: 学习型MVS方法受限于训练数据分布，泛化能力不足；而优化型方法虽然能场景自适应但缺乏可扩展性且计算成本高。需要结合两者优势。

Method: 使用自监督的跨视图一致性损失作为辅助任务指导推理时自适应，采用元辅助学习策略训练模型从辅助任务更新中获益，框架与模型无关。

Result: 在标准数据集和跨数据集泛化场景下的广泛实验表明，MVS-TTA能持续提升性能，即使应用于最先进的MVS模型。

Conclusion: 这是首次通过元学习将优化型测试时自适应集成到学习型MVS中的尝试，显著提升了模型的适应性和泛化能力。

Abstract: Recent learning-based multi-view stereo (MVS) methods are data-driven and have achieved remarkable progress due to large-scale training data and advanced architectures. However, their generalization remains sub-optimal due to fixed model parameters trained on limited training data distributions. In contrast, optimization-based methods enable scene-specific adaptation but lack scalability and require costly per-scene optimization. In this paper, we propose MVS-TTA, an efficient test-time adaptation (TTA) framework that enhances the adaptability of learning-based MVS methods by bridging these two paradigms. Specifically, MVS-TTA employs a self-supervised, cross-view consistency loss as an auxiliary task to guide inference-time adaptation. We introduce a meta-auxiliary learning strategy to train the model to benefit from auxiliary-task-based updates explicitly. Our framework is model-agnostic and can be applied to a wide range of MVS methods with minimal architectural changes. Extensive experiments on standard datasets (DTU, BlendedMVS) and a challenging cross-dataset generalization setting demonstrate that MVS-TTA consistently improves performance, even when applied to state-of-the-art MVS models. To our knowledge, this is the first attempt to integrate optimization-based test-time adaptation into learning-based MVS using meta-learning. The code will be available at https://github.com/mart87987-svg/MVS-TTA.

</details>


### [168] [VCU-Bridge: Hierarchical Visual Connotation Understanding via Semantic Bridging](https://arxiv.org/abs/2511.18121)
*Ming Zhong,Yuanlei Wang,Liuzhou Zhang,Arctanx An,Renrui Zhang,Hao Liang,Ming Lu,Ying Shen,Wentao Zhang*

Main category: cs.CV

TL;DR: VCU-Bridge框架模拟人类视觉内涵理解的层次结构，从基础感知到语义桥接再到抽象内涵，构建了HVCU-Bench基准测试，发现模型性能随推理层级升高而下降，通过MCTS指导的数据生成增强低层能力可提升高层性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估协议往往将低层感知与高层推理解耦，忽略了它们的语义和因果依赖关系，导致非诊断性结果和性能瓶颈模糊。

Method: 提出VCU-Bridge框架，构建HVCU-Bench基准测试，采用蒙特卡洛树搜索指导的数据生成管道进行指令调优。

Result: 实验显示模型性能随推理层级升高而下降，增强低层能力可提升高层性能，在HVCU-Bench和通用基准上均有改善（平均+2.53%，MMStar +7.26%）。

Conclusion: 层次化思维模式对增强MLLM能力具有重要意义，通过加强低层能力可有效提升高层推理性能。

Abstract: While Multimodal Large Language Models (MLLMs) excel on benchmarks, their processing paradigm differs from the human ability to integrate visual information. Unlike humans who naturally bridge details and high-level concepts, models tend to treat these elements in isolation. Prevailing evaluation protocols often decouple low-level perception from high-level reasoning, overlooking their semantic and causal dependencies, which yields non-diagnostic results and obscures performance bottlenecks. We present VCU-Bridge, a framework that operationalizes a human-like hierarchy of visual connotation understanding: multi-level reasoning that advances from foundational perception through semantic bridging to abstract connotation, with an explicit evidence-to-inference trace from concrete cues to abstract conclusions. Building on this framework, we construct HVCU-Bench, a benchmark for hierarchical visual connotation understanding with explicit, level-wise diagnostics. Comprehensive experiments demonstrate a consistent decline in performance as reasoning progresses to higher levels. We further develop a data generation pipeline for instruction tuning guided by Monte Carlo Tree Search (MCTS) and show that strengthening low-level capabilities yields measurable gains at higher levels. Interestingly, it not only improves on HVCU-Bench but also brings benefits on general benchmarks (average +2.53%), especially with substantial gains on MMStar (+7.26%), demonstrating the significance of the hierarchical thinking pattern and its effectiveness in enhancing MLLM capabilities. The project page is at https://vcu-bridge.github.io .

</details>


### [169] [Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models](https://arxiv.org/abs/2511.18123)
*Dachuan Zhao,Weiyue Li,Zhenda Shen,Yushu Qiu,Bowen Xu,Haoyu Chen,Yongchao Chen*

Main category: cs.CV

TL;DR: 本文提出SPD方法，通过识别和移除线性可解码偏见的整个子空间来解决视觉语言模型中的偏见问题，相比现有方法在公平性指标上平均提升18.5%。


<details>
  <summary>Details</summary>
Motivation: 现有的事后去偏见方法仅替换最相关的嵌入坐标，但存在特征纠缠、跨数据集泛化差和不完全去偏见三个关键问题。研究发现偏见不是局部化在少数坐标上，而是分布在几个线性子空间中。

Method: SPD（子空间投影去偏见）是一个几何原理框架，识别并移除线性可解码偏见的整个子空间，同时重新插入中性均值分量以保持语义保真度。

Result: 在零样本分类、文本到图像检索和图像生成任务上的广泛实验验证了SPD的有效性：在四个公平性指标上平均提升18.5%，同时与最佳去偏见基线相比任务性能损失最小。

Conclusion: SPD方法能够实现更稳健的去偏见效果，同时保持语义保真度，为解决视觉语言模型中的偏见问题提供了有效的解决方案。

Abstract: Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\textbf{S}$ubspace $\textbf{P}$rojection $\textbf{D}$ebiasing ($\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.

</details>


### [170] [SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation](https://arxiv.org/abs/2511.18127)
*Ruicong Liu,Yifei Huang,Liangyang Ouyang,Caixin Kang,Yoichi Sato*

Main category: cs.CV

TL;DR: SFHand是首个用于语言引导3D手部预测的流式框架，能够从连续视频流和语言指令中预测未来3D手部状态，在AR和机器人应用中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法不适合实时人机交互场景，因为它们需要离线处理累积视频序列且无法整合传达任务意图的语言指导。

Method: 结合流式自回归架构和ROI增强记忆层，从连续视频流和语言指令中预测手部类型、2D边界框、3D姿态和轨迹等完整手部状态。

Result: 在3D手部预测方面达到新的最先进水平，比先前工作性能提升高达35.8%，在下游操作任务中任务成功率提升达13.4%。

Conclusion: SFHand框架成功解决了实时3D手部预测的挑战，并通过EgoHaFL数据集和下游任务验证了其实际效用。

Abstract: Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.

</details>


### [171] [Video4Edit: Viewing Image Editing as a Degenerate Temporal Process](https://arxiv.org/abs/2511.18131)
*Xiaofan Li,Yanpeng Sun,Chenming Wu,Fan Duan,YuAn Wang,Weihao Bo,Yumeng Zhang,Dingkang Liang*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频预训练的时间建模方法，用于高效的多模态图像编辑，仅需1%的监督数据就能达到主流编辑模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图像编辑方法需要大量高质量的{指令、源图像、编辑图像}三元组数据，成本高昂，且编辑保真度依赖于指令对目标语义的精确描述。

Method: 将图像编辑视为简化的时间过程，从视频预训练中迁移单帧演化先验，实现数据高效的精调机制。

Result: 实验表明，该方法仅使用主流编辑模型约1%的监督数据，就能匹配领先开源基线的性能。

Conclusion: 通过时间建模视角和视频预训练先验转移，实现了高效的多模态图像编辑，大幅降低了数据需求。

Abstract: We observe that recent advances in multimodal foundation models have propelled instruction-driven image generation and editing into a genuinely cross-modal, cooperative regime. Nevertheless, state-of-the-art editing pipelines remain costly: beyond training large diffusion/flow models, they require curating massive high-quality triplets of \{instruction, source image, edited image\} to cover diverse user intents. Moreover, the fidelity of visual replacements hinges on how precisely the instruction references the target semantics. We revisit this challenge through the lens of temporal modeling: if video can be regarded as a full temporal process, then image editing can be seen as a degenerate temporal process. This perspective allows us to transfer single-frame evolution priors from video pre-training, enabling a highly data-efficient fine-tuning regime. Empirically, our approach matches the performance of leading open-source baselines while using only about one percent of the supervision demanded by mainstream editing models.

</details>


### [172] [SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation](https://arxiv.org/abs/2511.18136)
*Chunming He,Rihan Zhang,Longxiang Tang,Ziyun Yang,Kai Li,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: SCALER是一个用于标签不足隐藏目标分割的统一协作框架，通过联合优化均值教师分割器和可学习的SAM，在交替的两个阶段中实现相互监督和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LDCOS方法依赖一致性约束或SAM伪标签，但由于目标内在隐藏性和标注稀缺性，性能有限。本研究探索能否将两种方法结合互补，并实现分割器与SAM的相互指导。

Method: SCALER框架包含两个交替阶段：阶段I在固定SAM监督下优化分割器，使用基于熵的图像级和基于不确定性的像素级权重选择可靠伪标签区域；阶段II通过增强不变性和噪声抵抗损失更新SAM，利用其对扰动的固有鲁棒性。

Result: 实验表明SCALER在八个半监督和弱监督COS任务中均获得一致性能提升，可作为标签稀缺条件下增强轻量分割器和大型基础模型的通用训练范式。

Conclusion: SCALER成功验证了将一致性约束与SAM监督联合集成，并实现分割器与SAM相互指导的可行性，为标签不足场景下的隐藏目标分割提供了有效解决方案。

Abstract: Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \textbf{Phase \uppercase\expandafter{\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \textbf{Phase \uppercase\expandafter{\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.

</details>


### [173] [Compact neural networks for astronomy with optimal transport bias correction](https://arxiv.org/abs/2511.18139)
*Shuhuan Wang,Yuzhen Xie,Jiayi Li*

Main category: cs.CV

TL;DR: WaveletMamba框架通过小波分解与状态空间建模，在64x64分辨率下实现81.72%分类准确率，仅需3.54M参数，在低分辨率输入下获得高分辨率性能，计算效率提升9.7倍。


<details>
  <summary>Details</summary>
Motivation: 解决天文成像中效率与分辨率之间的权衡问题，限制大规模形态分类和红移预测。

Method: 集成小波分解、状态空间建模、数学正则化和多级偏差校正的理论驱动框架。

Result: 在64x64分辨率下达到81.72%分类准确率，244x244分辨率下80.93%准确率；多级偏差校正实现22.96% Log-MSE改进和26.10%异常值减少。

Conclusion: 数学严谨性在科学AI中实现了前所未有的效率和全面偏差校正，连接计算机视觉和天体物理学以革新跨学科科学发现。

Abstract: Astronomical imaging confronts an efficiency-resolution tradeoff that limits large-scale morphological classification and redshift prediction. We introduce WaveletMamba, a theory-driven framework integrating wavelet decomposition with state-space modeling, mathematical regularization, and multi-level bias correction. WaveletMamba achieves 81.72% +/- 0.53% classification accuracy at 64x64 resolution with only 3.54M parameters, delivering high-resolution performance (80.93% +/- 0.27% at 244x244) at low-resolution inputs with 9.7x computational efficiency gains. The framework exhibits Resolution Multistability, where models trained on low-resolution data achieve consistent accuracy across different input scales despite divergent internal representations. The framework's multi-level bias correction synergizes HK distance (distribution-level optimal transport) with Color-Aware Weighting (sample-level fine-tuning), achieving 22.96% Log-MSE improvement and 26.10% outlier reduction without explicit selection function modeling. Here, we show that mathematical rigor enables unprecedented efficiency and comprehensive bias correction in scientific AI, bridging computer vision and astrophysics to revolutionize interdisciplinary scientific discovery.

</details>


### [174] [UnfoldLDM: Deep Unfolding-based Blind Image Restoration with Latent Diffusion Priors](https://arxiv.org/abs/2511.18152)
*Chunming He,Rihan Zhang,Zheng Chen,Bowen Yang,CHengyu Fang,Yunlong Lin,Fengyang Xiao,Sina Farsiu*

Main category: cs.CV

TL;DR: 提出UnfoldLDM方法，将深度展开网络与潜在扩散模型结合，解决盲图像恢复中的退化特定依赖和过平滑偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度展开网络存在两个主要问题：(1) 退化特定依赖，其优化框架依赖于已知退化模型，不适合盲图像恢复任务；(2) 过平滑偏差，梯度下降输出直接馈入近端项会抑制精细纹理。

Method: 提出UnfoldLDM框架：使用多粒度退化感知模块作为梯度下降步骤来估计未知退化；设计退化抵抗潜在扩散模型提取紧凑的退化不变先验；用过平滑校正变换器显式恢复高频分量。

Result: 实验表明UnfoldLDM在各种盲图像恢复任务中达到领先水平，并能提升下游任务性能。该设计兼容现有的基于深度展开网络的方法，可作为即插即用框架。

Conclusion: UnfoldLDM成功解决了深度展开网络在盲图像恢复中的局限性，通过结合潜在扩散模型实现了退化无关且视觉丰富的恢复效果。

Abstract: Deep unfolding networks (DUNs) combine the interpretability of model-based methods with the learning ability of deep networks, yet remain limited for blind image restoration (BIR). Existing DUNs suffer from: (1) \textbf{Degradation-specific dependency}, as their optimization frameworks are tied to a known degradation model, making them unsuitable for BIR tasks; and (2) \textbf{Over-smoothing bias}, resulting from the direct feeding of gradient descent outputs, dominated by low-frequency content, into the proximal term, suppressing fine textures. To overcome these issues, we propose UnfoldLDM to integrate DUNs with latent diffusion model (LDM) for BIR. In each stage, UnfoldLDM employs a multi-granularity degradation-aware (MGDA) module as the gradient descent step. MGDA models BIR as an unknown degradation estimation problem and estimates both the holistic degradation matrix and its decomposed forms, enabling robust degradation removal. For the proximal step, we design a degradation-resistant LDM (DR-LDM) to extract compact degradation-invariant priors from the MGDA output. Guided by this prior, an over-smoothing correction transformer (OCFormer) explicitly recovers high-frequency components and enhances texture details. This unique combination ensures the final result is degradation-free and visually rich. Experiments show that our UnfoldLDM achieves a leading place on various BIR tasks and benefits downstream tasks. Moreover, our design is compatible with existing DUN-based methods, serving as a plug-and-play framework. Code will be released.

</details>


### [175] [Matching-Based Few-Shot Semantic Segmentation Models Are Interpretable by Design](https://arxiv.org/abs/2511.18163)
*Pasquale De Marinis,Uzay Kaymak,Rogier Brussee,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: 本文提出了首个专门用于解释基于匹配的少样本语义分割模型的方法——Affinity Explainer，通过利用模型固有结构特性生成归因图，揭示支持图像中哪些像素对查询分割预测贡献最大。


<details>
  <summary>Details</summary>
Motivation: 少样本语义分割模型在分割新类别方面表现优异，但其决策过程不透明。尽管可解释AI在标准计算机视觉任务中已有显著进展，但在FSS领域的可解释性研究仍几乎空白，这对于理解模型行为和在数据稀缺场景中指导支持集选择至关重要。

Method: Affinity Explainer方法提取归因图，通过计算支持图像和查询图像特征在多个特征层级上的匹配分数，突出显示支持图像中对查询分割预测贡献最大的像素。

Result: 在FSS基准数据集上的综合实验表明，Affinity Explainer显著优于适应的标准归因方法。定性分析显示，该方法提供的解释具有结构化、连贯的注意力模式，与模型架构一致，并能实现有效的模型诊断。

Conclusion: 这项工作为可解释的FSS研究奠定了基础，使能更好的模型理解和诊断，从而构建更可靠的少样本分割系统。

Abstract: Few-Shot Semantic Segmentation (FSS) models achieve strong performance in segmenting novel classes with minimal labeled examples, yet their decision-making processes remain largely opaque. While explainable AI has advanced significantly in standard computer vision tasks, interpretability in FSS remains virtually unexplored despite its critical importance for understanding model behavior and guiding support set selection in data-scarce scenarios. This paper introduces the first dedicated method for interpreting matching-based FSS models by leveraging their inherent structural properties. Our Affinity Explainer approach extracts attribution maps that highlight which pixels in support images contribute most to query segmentation predictions, using matching scores computed between support and query features at multiple feature levels. We extend standard interpretability evaluation metrics to the FSS domain and propose additional metrics to better capture the practical utility of explanations in few-shot scenarios. Comprehensive experiments on FSS benchmark datasets, using different models, demonstrate that our Affinity Explainer significantly outperforms adapted standard attribution methods. Qualitative analysis reveals that our explanations provide structured, coherent attention patterns that align with model architectures and and enable effective model diagnosis. This work establishes the foundation for interpretable FSS research, enabling better model understanding and diagnostic for more reliable few-shot segmentation systems. The source code is publicly available at https://github.com/pasqualedem/AffinityExplainer.

</details>


### [176] [Nested Unfolding Network for Real-World Concealed Object Segmentation](https://arxiv.org/abs/2511.18164)
*Chunming He,Rihan Zhang,Dingming Zhang,Fengyang Xiao,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: 提出了嵌套展开网络(NUN)，通过DUN-in-DUN设计将图像恢复与分割解耦，解决了现有方法中背景估计与图像恢复目标冲突的问题，在真实场景隐蔽目标分割中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度展开网络的方法将背景估计与图像恢复耦合，导致目标冲突，且需要预定义退化类型，这在真实场景中不现实。

Method: 采用DUN-in-DUN设计，在分割导向展开网络(SODUN)的每个阶段嵌入抗退化展开网络(DeRUN)，通过视觉语言模型动态推断退化语义，使用图像质量评估选择最佳输出。

Result: 在干净和退化基准测试中都取得了领先地位，展示了在真实场景中的优越性能。

Conclusion: NUN框架成功解决了真实世界隐蔽目标分割中的退化问题，通过解耦恢复与分割实现了鲁棒的分割性能。

Abstract: Deep unfolding networks (DUNs) have recently advanced concealed object segmentation (COS) by modeling segmentation as iterative foreground-background separation. However, existing DUN-based methods (RUN) inherently couple background estimation with image restoration, leading to conflicting objectives and requiring pre-defined degradation types, which are unrealistic in real-world scenarios. To address this, we propose the nested unfolding network (NUN), a unified framework for real-world COS. NUN adopts a DUN-in-DUN design, embedding a degradation-resistant unfolding network (DeRUN) within each stage of a segmentation-oriented unfolding network (SODUN). This design decouples restoration from segmentation while allowing mutual refinement. Guided by a vision-language model (VLM), DeRUN dynamically infers degradation semantics and restores high-quality images without explicit priors, whereas SODUN performs reversible estimation to refine foreground and background. Leveraging the multi-stage nature of unfolding, NUN employs image-quality assessment to select the best DeRUN outputs for subsequent stages, naturally introducing a self-consistency loss that enhances robustness. Extensive experiments show that NUN achieves a leading place on both clean and degraded benchmarks. Code will be released.

</details>


### [177] [EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses](https://arxiv.org/abs/2511.18173)
*Enrico Pallotta,Sina Mokhtarzadeh Azar,Lars Doorenbos,Serdar Ozsoy,Umar Iqbal,Juergen Gall*

Main category: cs.CV

TL;DR: EgoControl是一个基于姿态控制的视频扩散模型，用于生成以自我为中心视角的视频，通过3D身体姿态序列精确控制未来帧的生成。


<details>
  <summary>Details</summary>
Motivation: 实现通过身体姿态进行细粒度控制的自我中心视频生成，是构建能够模拟、预测和规划动作的具身AI代理的关键需求。

Method: 提出了一种新颖的姿态表示方法，捕捉全局相机动态和关节身体运动，并在扩散过程中通过专用控制机制集成该表示。模型基于短序列观察帧和目标姿态序列生成未来帧。

Result: 实验结果表明，EgoControl能够生成高质量、姿态一致的自我中心视频，时间相干性和视觉真实感良好。

Conclusion: EgoControl为实现可控的具身视频模拟和理解铺平了道路。

Abstract: Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.

</details>


### [178] [Assessing the alignment between infants' visual and linguistic experience using multimodal language models](https://arxiv.org/abs/2511.18824)
*Alvin Wei Ming Tan,Jane Yang,Tarun Sepuri,Khai Loong Aw,Robert Z. Sparks,Zi Yin,Virginia A. Marchman,Michael C. Frank,Bria Long*

Main category: cs.CV

TL;DR: 使用CLIP模型自动分析婴儿视角视频中的视觉-语言对齐情况，发现理想化的学习对齐时刻在儿童日常经验中相对罕见


<details>
  <summary>Details</summary>
Motivation: 理解儿童语言学习过程中视觉和语言经验的时间对齐程度，传统方法需要人工标注，限制了研究规模

Method: 使用对比语言-图像预训练(CLIP)模型自动评估婴儿视角视频中的视觉-语言对齐，并通过人工判断验证CLIP对齐分数的有效性

Result: 理想化的学习对齐时刻(如"看球"时球出现在儿童视野中)在儿童日常经验中相对罕见，且在不同儿童之间和同一儿童内部存在变异性

Conclusion: 不频繁的对齐是早期词汇学习模型的约束条件，该方法为研究儿童多模态环境提供了新工具

Abstract: Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., "look at the ball" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.

</details>


### [179] [Unified Spherical Frontend: Learning Rotation-Equivariant Representations of Spherical Images from Any Camera](https://arxiv.org/abs/2511.18174)
*Mukai Yu,Mosam Dabhi,Liuyue Xie,Sebastian Scherer,László A. Jeni*

Main category: cs.CV

TL;DR: USF是一个统一球形前端框架，可将任何校准相机的图像转换为单位球面表示，直接在空间域执行球形重采样、卷积和池化，避免了昂贵的球谐变换，提供可配置的旋转等变性。


<details>
  <summary>Details</summary>
Motivation: 现代感知系统越来越多地使用鱼眼、全景等广角相机，但大多数管道仍使用为针孔图像设计的平面CNN，其中图像空间邻域无法正确表示物理邻接，且模型对全局旋转敏感。

Method: 通过射线方向对应关系将图像转换为单位球面表示，在空间域直接进行球形重采样、卷积和池化，使用仅距离的球形核提供旋转等变性，完全避免球谐变换。

Result: 在合成和真实数据集上的分类、检测和分割任务中，USF能高效处理高分辨率球形图像，在随机测试时旋转下性能下降小于1%，即使没有旋转增强，还能实现从一种镜头类型到未见广角镜头的零样本泛化。

Conclusion: USF提供了一个模块化、镜头无关的框架，有效解决了广角相机图像处理中的旋转敏感性和邻接表示问题，在保持高效率的同时显著提升了模型的鲁棒性和泛化能力。

Abstract: Modern perception increasingly relies on fisheye, panoramic, and other wide field-of-view (FoV) cameras, yet most pipelines still apply planar CNNs designed for pinhole imagery on 2D grids, where image-space neighborhoods misrepresent physical adjacency and models are sensitive to global rotations. Frequency-domain spherical CNNs partially address this mismatch but require costly spherical harmonic transforms that constrain resolution and efficiency. We introduce the Unified Spherical Frontend (USF), a lens-agnostic framework that transforms images from any calibrated camera into a unit-sphere representation via ray-direction correspondences, and performs spherical resampling, convolution, and pooling directly in the spatial domain. USF is modular: projection, location sampling, interpolation, and resolution control are fully decoupled. Its distance-only spherical kernels offer configurable rotation-equivariance (mirroring translation-equivariance in planar CNNs) while avoiding harmonic transforms entirely. We compare standard planar backbones with their spherical counterparts across classification, detection, and segmentation tasks on synthetic (Spherical MNIST) and real-world datasets (PANDORA, Stanford 2D-3D-S), and stress-test robustness to extreme lens distortions, varying FoV, and arbitrary rotations. USF processes high-resolution spherical imagery efficiently and maintains less than 1% performance drop under random test-time rotations, even without rotational augmentation, and even enables zero-shot generalization from one lens type to unseen wide-FoV lenses with minimal performance degradation.

</details>


### [180] [Early Lung Cancer Diagnosis from Virtual Follow-up LDCT Generation via Correlational Autoencoder and Latent Flow Matching](https://arxiv.org/abs/2511.18185)
*Yutong Wu,Yifan Wang,Qining Zhang,Chuan Zhou,Lei Ying*

Main category: cs.CV

TL;DR: 本文提出CorrFlowNet方法，通过生成虚拟一年随访CT扫描来早期检测肺结节恶性/良性，减少等待临床随访的需求。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期诊断困难，现有AI方法主要关注单次早期CT扫描的影像特征提取，而临床实践中需要多次随访才能确诊，可能错过最佳治疗时机。

Method: 使用相关性自编码器将早期基线和随访CT图像编码到潜在空间，捕捉结节进展动态和相关性，然后结合流匹配算法和神经常微分方程，并利用辅助分类器提升诊断准确性。

Result: 在真实临床数据集上的评估显示，该方法显著优于现有基线模型，其诊断准确性与真实临床CT随访相当。

Conclusion: CorrFlowNet方法有潜力改善癌症诊断，通过生成虚拟随访扫描实现早期恶性/良性结节检测。

Abstract: Lung cancer is one of the most commonly diagnosed cancers, and early diagnosis is critical because the survival rate declines sharply once the disease progresses to advanced stages. However, achieving an early diagnosis remains challenging, particularly in distinguishing subtle early signals of malignancy from those of benign conditions. In clinical practice, a patient with a high risk may need to undergo an initial baseline and several annual follow-up examinations (e.g., CT scans) before receiving a definitive diagnosis, which can result in missing the optimal treatment. Recently, Artificial Intelligence (AI) methods have been increasingly used for early diagnosis of lung cancer, but most existing algorithms focus on radiomic features extraction from single early-stage CT scans. Inspired by recent advances in diffusion models for image generation, this paper proposes a generative method, named CorrFlowNet, which creates a virtual, one-year follow-up CT scan after the initial baseline scan. This virtual follow-up would allow for an early detection of malignant/benign nodules, reducing the need to wait for clinical follow-ups. During training, our approach employs a correlational autoencoder to encode both early baseline and follow-up CT images into a latent space that captures the dynamics of nodule progression as well as the correlations between them, followed by a flow matching algorithm on the latent space with a neural ordinary differential equation. An auxiliary classifier is used to further enhance the diagnostic accuracy. Evaluations on a real clinical dataset show our method can significantly improve downstream lung nodule risk assessment compared with existing baseline models. Moreover, its diagnostic accuracy is comparable with real clinical CT follow-ups, highlighting its potential to improve cancer diagnosis.

</details>


### [181] [ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization](https://arxiv.org/abs/2511.18192)
*Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.CV

TL;DR: ARIAL是一个基于LLM规划代理的模块化框架，通过协调专用工具实现文档视觉问答的精确答案提取和可靠空间定位，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有文档VQA系统要么在文本准确性上表现良好但空间定位不可靠，要么为了可解释性而牺牲性能，需要同时实现精确答案提取和可靠空间定位。

Method: 使用LLM规划代理协调专用工具：TrOCR进行OCR文本提取、语义搜索进行检索增强上下文选择、微调Gemma 3-27B模型生成答案、文本到区域对齐进行显式边界框定位。

Result: 在四个基准测试中达到SOTA：DocVQA(88.7 ANLS/50.1 mAP)、FUNSD(90.0 ANLS/50.3 mAP)、CORD(85.5 ANLS/60.2 mAP)、SROIE(93.1 ANLS)，比之前最佳方法DLaVA在DocVQA上提升+2.8 ANLS和+3.9 mAP。

Conclusion: 代理化协调专用工具可以同时提高性能和可解释性，为可信赖、可解释的文档AI系统提供了一条路径。

Abstract: Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.

</details>


### [182] [InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity](https://arxiv.org/abs/2511.18200)
*Haoming Wang,Qiyao Xue,Wei Gao*

Main category: cs.CV

TL;DR: InfiniBench是一个完全自动化的可定制基准测试生成器，能够合成理论上无限多样的3D场景，通过参数化控制场景复杂度，用于评估视觉语言模型的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在场景复杂度定制方面有限，无法在特定空间条件下隔离和分析VLM的失败模式，需要更灵活、可扩展的评估工具。

Method: 使用基于LLM的代理框架迭代优化程序化场景约束；灵活的基于聚类的布局优化器生成密集杂乱场景；任务感知的相机轨迹优化方法渲染视频作为VLM输入。

Result: InfiniBench在提示保真度和物理合理性方面优于最先进的程序化和基于LLM的3D生成方法，特别是在高复杂度场景中。

Conclusion: InfiniBench能够为测量、视角转换和时空跟踪等代表性空间推理任务生成有效的基准测试，填补了VLM空间推理能力评估的空白。

Abstract: Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.

</details>


### [183] [Generating Synthetic Human Blastocyst Images for In-Vitro Fertilization Blastocyst Grading](https://arxiv.org/abs/2511.18204)
*Pavan Narahari,Suraj Rajendran,Lorena Bori,Jonas E. Malmsten,Qiansheng Zhan,Zev Rosenwaks,Nikica Zaninovic,Iman Hajirasouliha*

Main category: cs.CV

TL;DR: 提出了DIA框架，使用潜在扩散模型生成高质量的第5天囊胚图像，通过数据增强显著提升胚胎分类准确率，解决了胚胎数据集稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 体外受精(IVF)中第5天囊胚的形态学评估存在主观性和不一致性，AI模型需要大量多样化数据集，但面临数据稀缺、类别不平衡和隐私限制等挑战。

Method: 开发DIA框架，训练潜在扩散模型生成高保真囊胚图像，可基于Gardner形态分类和z轴焦距进行细粒度控制，并通过多种评估指标验证模型性能。

Result: DIA生成的图像在胚胎学家图灵测试中难以与真实图像区分；合成图像增强不平衡数据集显著提升分类准确率(p<0.05)；在大型平衡数据集中添加合成图像也能获得统计显著的性能提升；合成数据最多可替代40%真实数据而不损失准确率。

Conclusion: DIA为胚胎数据集中的数据稀缺和类别不平衡问题提供了稳健解决方案，通过生成新颖、高保真且可控的合成图像，能够提升AI胚胎评估工具的性能、公平性和标准化程度。

Abstract: The success of in vitro fertilization (IVF) at many clinics relies on the accurate morphological assessment of day 5 blastocysts, a process that is often subjective and inconsistent. While artificial intelligence can help standardize this evaluation, models require large, diverse, and balanced datasets, which are often unavailable due to data scarcity, natural class imbalance, and privacy constraints. Existing generative embryo models can mitigate these issues but face several limitations, such as poor image quality, small training datasets, non-robust evaluation, and lack of clinically relevant image generation for effective data augmentation. Here, we present the Diffusion Based Imaging Model for Artificial Blastocysts (DIA) framework, a set of latent diffusion models trained to generate high-fidelity, novel day 5 blastocyst images. Our models provide granular control by conditioning on Gardner-based morphological categories and z-axis focal depth. We rigorously evaluated the models using FID, a memorization metric, an embryologist Turing test, and three downstream classification tasks. Our results show that DIA models generate realistic images that embryologists could not reliably distinguish from real images. Most importantly, we demonstrated clear clinical value. Augmenting an imbalanced dataset with synthetic images significantly improved classification accuracy (p < 0.05). Also, adding synthetic images to an already large, balanced dataset yielded statistically significant performance gains, and synthetic data could replace up to 40% of real data in some cases without a statistically significant loss in accuracy. DIA provides a robust solution for mitigating data scarcity and class imbalance in embryo datasets. By generating novel, high-fidelity, and controllable synthetic images, our models can improve the performance, fairness, and standardization of AI embryo assessment tools.

</details>


### [184] [From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation](https://arxiv.org/abs/2511.19149)
*Moazzam Umer Gondal,Hamad Ul Qudous,Daniya Siddiqui,Asma Ahmad Farhan*

Main category: cs.CV

TL;DR: 提出基于检索增强的时尚图片描述和标签生成框架，结合多服装检测、属性推理和LLM提示，生成视觉基础扎实、描述性强且风格有趣的文本。


<details>
  <summary>Details</summary>
Motivation: 解决端到端图像描述模型在属性保真度和领域泛化方面的局限性，为时尚图像生成更具视觉基础、描述性和风格化的文本内容。

Method: 使用YOLO检测器进行多服装定位，k-means聚类提取主色调，CLIP-FAISS检索模块基于结构化产品索引推断面料和性别属性，结合检索到的风格示例构建事实证据包来指导LLM生成描述和标签。

Result: YOLO检测器在9个服装类别上获得0.71的mAP@0.5；RAG-LLM管道生成表达力强的属性对齐描述，在标签生成中达到0.80的平均属性覆盖率，50%阈值下实现完全覆盖；相比BLIP模型具有更好的事实基础和更少的幻觉。

Conclusion: 检索增强生成是自动化和视觉基础时尚内容生成的有效且可解释的范式，在多种服装领域具有可扩展部署的巨大潜力。

Abstract: This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.

</details>


### [185] [Large-Scale Pre-training Enables Multimodal AI Differentiation of Radiation Necrosis from Brain Metastasis Progression on Routine MRI](https://arxiv.org/abs/2511.18208)
*Ahmed Gomaa,Annette Schwarz,Ludwig Singer,Arnd Dörfler,Matthias Stefan May,Pluvio Stephan,Ishita Sheth,Juliane Szkitsak,Katharina Breininger,Yixing Huang,Benjamin Frey,Oliver Schnell,Daniel Delev,Roland Coras,Daniel Höfler,Philipp Schubert,Jenny Stritzelberger,Sabine Semrau,Andreas Maier,Dieter H Heiland,Udo S. Gaipl,Andrea Wittig,Rainer Fietkau,Christoph Bert,Stefanie Corradini,Florian Putz*

Main category: cs.CV

TL;DR: 该论文提出了一种基于自监督学习的双阶段深度学习策略，用于区分脑转移瘤放疗后的放射性坏死与肿瘤进展。该方法在未标记的大规模MRI数据集上进行预训练，然后在活检确认的数据集上进行微调，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 脑转移瘤立体定向放疗后区分放射性坏死与肿瘤进展是临床关键挑战。传统有监督深度学习方法受限于活检确认训练数据的稀缺性，而自监督学习可以利用大规模未标记脑转移瘤影像数据集克服这一限制。

Method: 采用双阶段深度学习策略：首先在10,167个未标记多源T1CE MRI子体积上通过自监督学习预训练Vision Transformer，然后在MOLAB数据集（n=109）上使用双通道输入（T1CE MRI和分割掩码）进行微调，并进行外部验证。

Result: 自监督模型在相同中心测试集上AUC为0.916，在第二中心测试集上AUC为0.764，显著优于全监督ViT（AUC 0.624/0.496）和影像组学方法（AUC 0.807/0.691）。多模态集成进一步提升了性能（AUC 0.947/0.821）。

Conclusion: 大规模未标记脑转移瘤数据集的预训练显著提升了AI模型性能。双阶段多模态深度学习策略仅使用常规T1CE MRI和标准临床数据即可高精度区分放射性坏死与肿瘤进展，提供了可解释、临床可及的解决方案。

Abstract: Background: Differentiating radiation necrosis (RN) from tumor progression after stereotactic radiosurgery (SRS) remains a critical challenge in brain metastases. While histopathology represents the gold standard, its invasiveness limits feasibility. Conventional supervised deep learning approaches are constrained by scarce biopsy-confirmed training data. Self-supervised learning (SSL) overcomes this by leveraging the growing availability of large-scale unlabeled brain metastases imaging datasets. Methods: In a two-phase deep learning strategy inspired by the foundation model paradigm, a Vision Transformer (ViT) was pre-trained via SSL on 10,167 unlabeled multi-source T1CE MRI sub-volumes. The pre-trained ViT was then fine-tuned for RN classification using a two-channel input (T1CE MRI and segmentation masks) on the public MOLAB dataset (n=109) using 20% of datasets as same-center held-out test set. External validation was performed on a second-center test cohort (n=28). Results: The self-supervised model achieved an AUC of 0.916 on the same-center test set and 0.764 on the second center test set, surpassing the fully supervised ViT (AUC 0.624/0.496; p=0.001/0.008) and radiomics (AUC 0.807/0.691; p=0.005/0.014). Multimodal integration further improved performance (AUC 0.947/0.821; p=0.073/0.001). Attention map visualizations enabled interpretability showing the model focused on clinically relevant lesion subregions. Conclusion: Large-scale pre-training on increasingly available unlabeled brain metastases datasets substantially improves AI model performance. A two-phase multimodal deep learning strategy achieved high accuracy in differentiating radiation necrosis from tumor progression using only routine T1CE MRI and standard clinical data, providing an interpretable, clinically accessible solution that warrants further validation.

</details>


### [186] [Using MLIR Transform to Design Sliced Convolution Algorithm](https://arxiv.org/abs/2511.18222)
*Victor Ferrari,Marcio Pereira,Lucas Alvarenga,Gustavo Leite,Guido Araujo*

Main category: cs.CV

TL;DR: SConvTransform是MLIR中的Transform方言扩展，通过声明式转换管道将Linalg卷积优化为分块和打包的通用操作，利用卷积切片分析确定分块大小和数据布局策略，在ARM SME和Intel AVX512上分别达到峰值性能的60%和67%。


<details>
  <summary>Details</summary>
Motivation: 在MLIR中提供专门的操作来优化2D卷积，通过结合静态形状分析与结构化分块和打包策略，实现可重用和可分析的卷积变换。

Method: 使用SConvOp操作，通过卷积切片分析基于输入和滤波器形状以及目标架构参数确定分块大小和数据布局策略，使用参数化仿射方程推导所有打包和分块操作，处理边缘情况时分割不规则区域并调整仿射映射。

Result: 在标准卷积配置下，生成的代码在ARM SME上达到峰值性能的60%，在Intel AVX512上达到67%，验证了静态形状分析与结构化分块打包策略结合的有效性。

Conclusion: SConvTransform的模块化设计便于与未来扩展集成，通过MLIR的可扩展编译基础设施持续优化卷积工作负载，未来工作将专注于性能优化和移植到其他目标设备。

Abstract: This paper proposes SConvTransform, a Transform dialect extension that provides operations for optimizing 2D convolutions in MLIR. Its main operation, SConvOp, lowers Linalg convolutions into tiled and packed generic operations through a fully declarative transformation pipeline. The process is guided by a Convolution Slicing Analysis that determines tile sizes and data layout strategies based on input and filter shapes, as well as target architecture parameters. SConvOp handles edge cases by splitting irregular regions and adjusting affine maps where needed. All packing and tiling operations are derived from a parametric set of affine equations, enabling reusable and analyzable transformations. Although functional correctness was the primary goal of this work, the experimental evaluation demonstrates the effectiveness of SConvTransform, achieving good enough performance across different target architectures. Future work will focus on optimizing performance and porting to other target devices. When applied to standard convolution configurations, the generated code achieves up to 60% of peak performance on ARM SME and 67% on Intel AVX512. These results validate the benefit of combining static shape analysis with structured tiling and packing strategies within the MLIR Transform dialect. Furthermore, the modular design of SConvTransform facilitates integration with future extensions, enabling continued optimization of convolution workloads through MLIR's extensible compilation infrastructure.

</details>


### [187] [Parallel qMRI Reconstruction from 4x Accelerated Acquisitions](https://arxiv.org/abs/2511.18232)
*Mingi Kang*

Main category: cs.CV

TL;DR: 提出端到端深度学习框架，仅使用4倍加速的欠采样k空间数据联合估计线圈灵敏度图并重建MRI图像，相比传统SENSE方法获得更平滑的重建结果。


<details>
  <summary>Details</summary>
Motivation: MRI采集时间长，限制患者吞吐量并增加运动伪影风险。传统并行MRI技术需要预计算线圈灵敏度图，限制了临床应用。

Method: 两模块架构：线圈灵敏度图估计模块和基于U-Net的MRI重建模块，仅使用欠采样k空间测量值进行端到端训练。

Result: 在10名受试者的多线圈脑MRI数据上评估，相比传统SENSE输出获得视觉上更平滑的重建，尽管PSNR/SSIM指标较低但视觉质量相当。

Conclusion: 该方法展示了仅从欠采样数据重建MRI图像的可行性，但存在不同加速因子间空间错位等挑战，需要进一步改进重建质量。

Abstract: Magnetic Resonance Imaging (MRI) acquisitions require extensive scan times, limiting patient throughput and increasing susceptibility to motion artifacts. Accelerated parallel MRI techniques reduce acquisition time by undersampling k-space data, but require robust reconstruction methods to recover high-quality images. Traditional approaches like SENSE require both undersampled k-space data and pre-computed coil sensitivity maps. We propose an end-to-end deep learning framework that jointly estimates coil sensitivity maps and reconstructs images from only undersampled k-space measurements at 4x acceleration. Our two-module architecture consists of a Coil Sensitivity Map (CSM) estimation module and a U-Net-based MRI reconstruction module. We evaluate our method on multi-coil brain MRI data from 10 subjects with 8 echoes each, using 2x SENSE reconstructions as ground truth. Our approach produces visually smoother reconstructions compared to conventional SENSE output, achieving comparable visual quality despite lower PSNR/SSIM metrics. We identify key challenges including spatial misalignment between different acceleration factors and propose future directions for improved reconstruction quality.

</details>


### [188] [EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning](https://arxiv.org/abs/2511.18242)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: EgoVITA是一个强化学习框架，通过第一人称规划与第三人称验证的交替阶段，提升多模态大语言模型在自我中心视角下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在第一人称视角下推理的挑战，包括部分可观测性、有限视野和自参考运动等问题。

Method: 基于GRPO强化学习框架，交替进行自我中心规划阶段（预测未来动作步骤）和外部中心验证阶段（检查计划的一致性和逻辑性）。

Result: 在自我中心推理任务上显著提升，EgoBlind任务上比Qwen2.5-VL-7B基线提升+7.7，EgoOrient任务上提升+4.4，同时在外部中心视频任务上保持良好泛化能力。

Conclusion: EgoVITA通过结构化规划和验证机制，有效提升了MLLMs在第一人称视角下的推理能力，实现了更连贯和视觉基础化的推理。

Abstract: Reasoning about intentions and actions from a first-person (egocentric) perspective remains a fundamental challenge for multimodal large language models (MLLMs). Unlike third-person (exocentric) videos that capture scenes from an outside observer, egocentric videos reflect the actor's continuously changing viewpoint, introducing partial observability, limited field of view, and self-referenced motion. We introduce $\textbf{EgoVITA}$, a reinforcement learning framework that enables MLLMs to reason through structured planning and verification. Built on Group Relative Policy Optimization (GRPO), EgoVITA alternates between two stages: (1) an $\textbf{egocentric planning phase}$, where the model reasons from a first-person viewpoint to predict a step-by-step plan of future actions, and (2) an $\textbf{exocentric verification phase}$, where it switches to a third-person perspective to check the visual and logical consistency of that plan. Through GRPO, the model learns to make plans that are causally predictive of upcoming visual observations, leading to more coherent and visually grounded reasoning. EgoVITA achieves significant gains on egocentric reasoning tasks, outperforming the baseline Qwen2.5-VL-7B by $\mathbf{+7.7}$ on EgoBlind and $\mathbf{+4.4}$ on EgoOrient, while maintaining strong generalization on exocentric video tasks.

</details>


### [189] [UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization](https://arxiv.org/abs/2511.18254)
*Siyi Li,Qingwen Zhang,Ishan Khatri,Kyle Vedder,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: LiDAR场景流任务中，跨数据集训练能显著提升模型性能，与传统认知相反。作者提出UniFlow模型，通过统一训练多个大规模LiDAR数据集，在多个基准测试中达到新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR任务在跨数据集训练时性能下降，但作者发现运动估计任务可能对传感器配置不敏感，希望通过跨数据集训练学习通用的运动先验知识。

Method: 提出UniFlow模型系列，采用前馈网络架构，统一训练多个具有不同传感器布局和点云密度的大规模LiDAR场景流数据集。

Result: 在Waymo和nuScenes数据集上分别提升5.1%和35.2%，在未见过的TruckScenes数据集上超越该数据集专用模型30.1%。

Conclusion: 运动估计任务能从跨数据集训练中受益，UniFlow证明了学习通用运动先验的可行性，为LiDAR场景流任务提供了新的解决方案。

Abstract: LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.

</details>


### [190] [Sequence-Adaptive Video Prediction in Continuous Streams using Diffusion Noise Optimization](https://arxiv.org/abs/2511.18255)
*Sina Mokhtarzadeh Azar,Emad Bahrami,Enrico Pallotta,Gianpiero Francesca,Radu Timofte,Juergen Gall*

Main category: cs.CV

TL;DR: 提出了SAVi-DNO方法，通过优化扩散噪声而非模型参数，使预训练扩散模型能够持续适应视频流进行预测。


<details>
  <summary>Details</summary>
Motivation: 在连续视频流预测中，模型会不断遇到新的训练样本，需要利用这些样本来改进预测性能，但直接微调大型扩散模型参数成本太高。

Method: 在推理过程中优化扩散噪声，同时保持模型参数冻结，让模型自适应地确定合适的采样噪声。

Result: 在Ego4D、OpenDV-YouTube、UCF-101和SkyTimelapse数据集上，FVD、SSIM和PSNR指标均显示性能提升。

Conclusion: SAVi-DNO方法有效实现了扩散模型对连续视频流的自适应预测，无需昂贵的参数微调。

Abstract: In this work, we investigate diffusion-based video prediction models, which forecast future video frames, for continuous video streams. In this context, the models observe continuously new training samples, and we aim to leverage this to improve their predictions. We thus propose an approach that continuously adapts a pre-trained diffusion model to a video stream. Since fine-tuning the parameters of a large diffusion model is too expensive, we refine the diffusion noise during inference while keeping the model parameters frozen, allowing the model to adaptively determine suitable sampling noise. We term the approach Sequence Adaptive Video Prediction with Diffusion Noise Optimization (SAVi-DNO). To validate our approach, we introduce a new evaluation setting on the Ego4D dataset, focusing on simultaneous adaptation and evaluation on long continuous videos. Empirical results demonstrate improved performance based on FVD, SSIM, and PSNR metrics on long videos of Ego4D and OpenDV-YouTube, as well as videos of UCF-101 and SkyTimelapse, showcasing SAVi-DNO's effectiveness.

</details>


### [191] [MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2511.18262)
*Tao Shen,Xin Wan,Taicai Chen,Rui Zhang,Junwen Pan,Dawei Lu,Fanding Lei,Zhilin Lu,Yunfei Yang,Chen Cheng,Qi She,Chang Liu,Zhenbang Sun*

Main category: cs.CV

TL;DR: Mammoth2是一个统一的AR-Diffusion框架，通过自回归语义规划和扩散生成的有效耦合，实现了高质量图像生成和编辑，同时保持强大的多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 解决统一多模态模型中离散语义推理与高保真视觉合成之间的差距，在单一框架内集成理解和生成能力。

Method: 采用串行设计：自回归路径进行全局语义建模，单流Diffusion Transformer解码器处理图像合成；使用AR-Diffusion特征对齐模块稳定对齐表示；端到端训练结合Next-Token Prediction和Flow Matching目标。

Result: 在公开基准测试中表现优异：GenEval得分0.87，DPGBench得分87.2，ImgEdit得分4.06；与纯理解模型在多模态理解任务上保持竞争力。

Conclusion: 精心耦合的AR-Diffusion架构可以在单一参数和数据高效的模型中提供高保真生成和编辑，同时保持强大的多模态理解能力。

Abstract: Unified multimodal models aim to integrate understanding and generation within a single framework, yet bridging the gap between discrete semantic reasoning and high-fidelity visual synthesis remains challenging. We present MammothModa2 (Mammoth2), a unified autoregressive-diffusion (AR-Diffusion) framework designed to effectively couple autoregressive semantic planning with diffusion-based generation. Mammoth2 adopts a serial design: an AR path equipped with generation experts performs global semantic modeling over discrete tokens, while a single-stream Diffusion Transformer (DiT) decoder handles high-fidelity image synthesis. A carefully designed AR-Diffusion feature alignment module combines multi-layer feature aggregation, unified condition encoding, and in-context conditioning to stably align AR's representations with the diffusion decoder's continuous latents. Mammoth2 is trained end-to-end with joint Next-Token Prediction and Flow Matching objectives, followed by supervised fine-tuning and reinforcement learning over both generation and editing. With roughly 60M supervised generation samples and no reliance on pre-trained generators, Mammoth2 delivers strong text-to-image and instruction-based editing performance on public benchmarks, achieving 0.87 on GenEval, 87.2 on DPGBench, and 4.06 on ImgEdit, while remaining competitive with understanding-only backbones (e.g., Qwen3-VL-8B) on multimodal understanding tasks. These results suggest that a carefully coupled AR-Diffusion architecture can provide high-fidelity generation and editing while maintaining strong multimodal comprehension within a single, parameter- and data-efficient model.

</details>


### [192] [SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors](https://arxiv.org/abs/2511.18264)
*Ruijie Fan,Junyan Ye,Huan Chen,Zilong Huang,Xiaolei Wang,Weijia Li*

Main category: cs.CV

TL;DR: SatSAM2是一个基于SAM2的零样本卫星视频跟踪器，通过引入卡尔曼滤波约束运动模块和运动约束状态机来解决卫星视频跟踪中的泛化性和遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有卫星视频跟踪方法泛化能力差，需要针对特定场景训练，且在遮挡情况下容易丢失跟踪目标。

Method: 基于SAM2构建零样本跟踪器，引入KFCMM模块利用时间运动线索抑制漂移，以及MCSM模块根据运动动态和可靠性调节跟踪状态。

Result: 在两个卫星跟踪基准和MVOT数据集上的实验表明，SatSAM2优于传统和基于基础模型的跟踪器，在OOTB数据集上AUC比最先进方法提升5.84%。

Conclusion: SatSAM2通过将基础模型适应遥感领域，有效解决了卫星视频跟踪的泛化性和遮挡问题，并创建了大规模评估基准MVOT以促进进一步研究。

Abstract: Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.

</details>


### [193] [Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models](https://arxiv.org/abs/2511.18271)
*Tianyang Han,Junhao Su,Junjie Hu,Peizhen Yang,Hengyu Shi,Junfeng Luo,Jialin Gao*

Main category: cs.CV

TL;DR: PicWorld是首个评估文本到图像模型隐式世界知识和物理因果推理能力的基准，包含1100个提示，通过多智能体评估器进行细粒度评估，发现现有模型在这方面存在普遍局限。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注组合对齐或单轮VQA评分，对知识基础、多物理交互和可审计证据等关键维度测试不足，需要更全面的评估基准。

Method: 提出PicWorld基准，包含1100个提示分为三个核心类别；开发PW-Agent多智能体评估器，通过将提示分解为可验证的视觉证据来分层评估图像的物理真实性和逻辑一致性。

Result: 对17个主流T2I模型的分析表明，它们在隐式世界知识和物理因果推理能力方面普遍存在不同程度的根本性局限。

Conclusion: 未来T2I系统需要具备推理感知和知识集成的架构设计。

Abstract: Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems.

</details>


### [194] [Vision Token Masking Alone Cannot Prevent PHI Leakage in Medical Document OCR: A Systematic Evaluation](https://arxiv.org/abs/2511.18272)
*Richard J. Young*

Main category: cs.CV

TL;DR: 评估视觉标记掩码在医疗文档OCR中的隐私保护效果，发现所有掩码策略只能减少42.9%的PHI，对结构化标识符无效，建议结合NLP后处理实现更全面的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉语言模型在医疗OCR中的部署，需要解决处理过程中受保护健康信息泄露的隐私问题。

Method: 使用DeepSeek-OCR，引入7种视觉标记掩码策略，针对不同架构层进行掩码，使用100份合成医疗账单评估PHI减少效果。

Result: 所有掩码策略都只能达到42.9%的PHI减少率，对长格式空间分布标识符有效(100%)，但对短结构化标识符无效(0%)。

Conclusion: 纯视觉隐私干预存在局限性，需要结合NLP后处理和混合架构来实现HIPAA合规的医疗文档处理。

Abstract: Large vision-language models (VLMs) are increasingly deployed for optical character recognition (OCR) in healthcare settings, raising critical concerns about protected health information (PHI) exposure during document processing. This work presents the first systematic evaluation of inference-time vision token masking as a privacy-preserving mechanism for medical document OCR using DeepSeek-OCR. We introduce seven masking strategies (V3-V9) targeting different architectural layers (SAM encoder blocks, compression layers, dual vision encoders, projector fusion) and evaluate PHI reduction across HIPAA-defined categories using 100 synthetic medical billing statements (drawn from a corpus of 38,517 annotated documents) with perfect ground-truth annotations. All masking strategies converge to 42.9% PHI reduction, successfully suppressing long-form spatially-distributed identifiers (patient names, dates of birth, physical addresses at 100% effectiveness) while failing to prevent short structured identifiers (medical record numbers, social security numbers, email addresses, account numbers at 0% effectiveness). Ablation studies varying mask expansion radius (r=1,2,3) demonstrate that increased spatial coverage does not improve reduction beyond this ceiling, indicating that language model contextual inference - not insufficient visual masking - drives structured identifier leakage. A simulated hybrid architecture combining vision masking with NLP post-processing achieves 88.6% total PHI reduction (assuming 80% NLP accuracy on remaining identifiers). This negative result establishes boundaries for vision-only privacy interventions in VLMs, provides guidance distinguishing PHI types amenable to vision-level versus language-level redaction, and redirects future research toward decoder-level fine-tuning and hybrid defense-in-depth architectures for HIPAA-compliant medical document processing.

</details>


### [195] [Point-to-Point: Sparse Motion Guidance for Controllable Video Editing](https://arxiv.org/abs/2511.18277)
*Yeji Song,Jaehyun Lee,Mijin Koo,JunHoo Lee,Nojun Kwak*

Main category: cs.CV

TL;DR: 提出了一种名为锚点令牌的新型运动表示方法，通过少量信息丰富的点轨迹紧凑编码视频动态，实现了更好的视频编辑保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在编辑保真度和运动保真度之间存在权衡，因为它们依赖的运动表示要么过度拟合布局，要么只是隐式定义。

Method: 提出了锚点令牌运动表示，利用视频扩散模型的丰富先验捕获最本质的运动模式，通过少量点轨迹编码视频动态，并灵活重定位以对齐新主体。

Result: 广泛的实验表明，锚点令牌能够实现更可控和语义对齐的视频编辑，在编辑保真度和运动保真度方面都取得了优越性能。

Conclusion: 锚点令牌作为一种紧凑的运动表示方法，能够泛化到多样化场景，解决了视频编辑中保持运动准确性的核心挑战。

Abstract: Accurately preserving motion while editing a subject remains a core challenge in video editing tasks. Existing methods often face a trade-off between edit and motion fidelity, as they rely on motion representations that are either overfitted to the layout or only implicitly defined. To overcome this limitation, we revisit point-based motion representation. However, identifying meaningful points remains challenging without human input, especially across diverse video scenarios. To address this, we propose a novel motion representation, anchor tokens, that capture the most essential motion patterns by leveraging the rich prior of a video diffusion model. Anchor tokens encode video dynamics compactly through a small number of informative point trajectories and can be flexibly relocated to align with new subjects. This allows our method, Point-to-Point, to generalize across diverse scenarios. Extensive experiments demonstrate that anchor tokens lead to more controllable and semantically aligned video edits, achieving superior performance in terms of edit and motion fidelity.

</details>


### [196] [Uni-DAD: Unified Distillation and Adaptation of Diffusion Models for Few-step Few-shot Image Generation](https://arxiv.org/abs/2511.18281)
*Yara Bahram,Melodie Desbos,Mohammadhadi Shateri,Eric Granger*

Main category: cs.CV

TL;DR: Uni-DAD是一个单阶段训练管道，统一了扩散模型的蒸馏和适应过程，通过在训练中耦合双域分布匹配蒸馏目标和多头GAN损失，实现快速高质量的新领域图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要两阶段训练（先适应后蒸馏或先蒸馏后适应），增加了设计复杂性并导致质量或多样性下降。需要一种单阶段方法来实现快速高质量的新领域生成。

Method: 提出Uni-DAD单阶段管道，结合双域分布匹配蒸馏目标（引导学生模型朝向源教师和目标教师的分布）和多头GAN损失（在多个特征尺度上鼓励目标真实性）。

Result: 在少样本图像生成和主题驱动个性化任务上，Uni-DAD在少于4个采样步骤时就能提供比最先进适应方法更高的质量，并在质量和多样性上都优于两阶段训练管道。

Conclusion: Uni-DAD通过统一蒸馏和适应过程，在保持源域多样知识的同时，实现了对新领域的高效适应，在少样本情况下表现优异。

Abstract: Diffusion models (DMs) produce high-quality images, yet their sampling remains costly when adapted to new domains. Distilled DMs are faster but typically remain confined within their teacher's domain. Thus, fast and high-quality generation for novel domains relies on two-stage training pipelines: Adapt-then-Distill or Distill-then-Adapt. However, both add design complexity and suffer from degraded quality or diversity. We introduce Uni-DAD, a single-stage pipeline that unifies distillation and adaptation of DMs. It couples two signals during training: (i) a dual-domain distribution-matching distillation objective that guides the student toward the distributions of the source teacher and a target teacher, and (ii) a multi-head generative adversarial network (GAN) loss that encourages target realism across multiple feature scales. The source domain distillation preserves diverse source knowledge, while the multi-head GAN stabilizes training and reduces overfitting, especially in few-shot regimes. The inclusion of a target teacher facilitates adaptation to more structurally distant domains. We perform evaluations on a variety of datasets for few-shot image generation (FSIG) and subject-driven personalization (SDP). Uni-DAD delivers higher quality than state-of-the-art (SoTA) adaptation methods even with less than 4 sampling steps, and outperforms two-stage training pipelines in both quality and diversity.

</details>


### [197] [RoadSceneVQA: Benchmarking Visual Question Answering in Roadside Perception Systems for Intelligent Transportation System](https://arxiv.org/abs/2511.18286)
*Runwei Guan,Rongsheng Hu,Shangshu Chen,Ningyuan Xiao,Xue Xia,Jiayang Liu,Beibei Chen,Ziren Tang,Ningwei Ouyang,Shaofeng Liang,Yuxuan Fan,Wanjie Sun,Yutao Yue*

Main category: cs.CV

TL;DR: 提出了RoadSceneVQA数据集和RoadMind模型，用于路边场景的视觉问答，结合认知锚定融合和辅助解耦思维链技术提升多模态大语言模型在交通感知和推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前路边感知系统主要关注实例级感知，缺乏自然语言交互能力和上下文交通行为推理能力，需要构建能够理解交通参与者意图、合法性和交互模式的数据集和方法。

Method: 构建了包含34,736个多样化QA对的RoadSceneVQA数据集；提出了CogniAnchor Fusion视觉语言融合模块和Assisted Decoupled Chain-of-Thought方法；开发了RoadMind基线模型。

Result: 在RoadSceneVQA和CODA-LM基准测试中，该管道持续提高了推理精度和计算效率，使MLLM在结构化交通感知和推理任务中达到最先进性能。

Conclusion: RoadSceneVQA数据集和RoadMind模型成功解决了路边场景的视觉问答挑战，通过认知锚定和思维链技术有效提升了多模态大语言模型的交通场景理解能力。

Abstract: Current roadside perception systems mainly focus on instance-level perception, which fall short in enabling interaction via natural language and reasoning about traffic behaviors in context. To bridge this gap, we introduce RoadSceneVQA, a large-scale and richly annotated visual question answering (VQA) dataset specifically tailored for roadside scenarios. The dataset comprises 34,736 diverse QA pairs collected under varying weather, illumination, and traffic conditions, targeting not only object attributes but also the intent, legality, and interaction patterns of traffic participants. RoadSceneVQA challenges models to perform both explicit recognition and implicit commonsense reasoning, grounded in real-world traffic rules and contextual dependencies. To fully exploit the reasoning potential of Multi-modal Large Language Models (MLLMs), we further propose CogniAnchor Fusion (CAF), a vision-language fusion module inspired by human-like scene anchoring mechanisms. Moreover, we propose the Assisted Decoupled Chain-of-Thought (AD-CoT) to enhance the reasoned thinking via CoT prompting and multi-task learning. Based on the above, we propose the baseline model RoadMind. Experiments on RoadSceneVQA and CODA-LM benchmark show that the pipeline consistently improves both reasoning accuracy and computational efficiency, allowing the MLLM to achieve state-of-the-art performance in structural traffic perception and reasoning tasks.

</details>


### [198] [SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes](https://arxiv.org/abs/2511.18290)
*Jungho Lee,Minhyeok Lee,Sunghun Yang,Minseok Kang,Sangyoun Lee*

Main category: cs.CV

TL;DR: SwiftVGGT是一种无需训练的方法，在大规模3D重建中显著减少推理时间同时保持高质量重建，通过消除外部VPR模型依赖和简化点采样对齐方法实现高效重建。


<details>
  <summary>Details</summary>
Motivation: 现有大规模3D重建方法在精度和计算效率之间存在固有权衡，要么速度快但质量低，要么质量高但推理慢。需要一种既能保持高质量又能显著减少推理时间的方法。

Method: 提出SwiftVGGT方法：1）无需外部VPR模型进行闭环检测，减少冗余计算；2）提出简单有效的点采样方法，使用单次Sim(3) SVD对齐相邻块，无需IRLS优化。

Result: 在多个数据集上评估显示，SwiftVGGT达到最先进的重建质量，同时仅需要最近VGGT大规模重建方法33%的推理时间。

Conclusion: SwiftVGGT成功解决了大规模3D重建中精度与效率的权衡问题，在保持高质量的同时显著提升了计算效率，适用于千米级环境的重建。

Abstract: 3D reconstruction in large-scale scenes is a fundamental task in 3D perception, but the inherent trade-off between accuracy and computational efficiency remains a significant challenge. Existing methods either prioritize speed and produce low-quality results, or achieve high-quality reconstruction at the cost of slow inference times. In this paper, we propose SwiftVGGT, a training-free method that significantly reduce inference time while preserving high-quality dense 3D reconstruction. To maintain global consistency in large-scale scenes, SwiftVGGT performs loop closure without relying on the external Visual Place Recognition (VPR) model. This removes redundant computation and enables accurate reconstruction over kilometer-scale environments. Furthermore, we propose a simple yet effective point sampling method to align neighboring chunks using a single Sim(3)-based Singular Value Decomposition (SVD) step. This eliminates the need for the Iteratively Reweighted Least Squares (IRLS) optimization commonly used in prior work, leading to substantial speed-ups. We evaluate SwiftVGGT on multiple datasets and show that it achieves state-of-the-art reconstruction quality while requiring only 33% of the inference time of recent VGGT-based large-scale reconstruction approaches.

</details>


### [199] [DiVE-k: Differential Visual Reasoning for Fine-grained Image Recognition](https://arxiv.org/abs/2511.18305)
*Raja Kumar,Arka Sadhu,Ram Nevatia*

Main category: cs.CV

TL;DR: DiVE-k是一个利用模型自身top-k预测作为训练信号的框架，通过创建多项选择题来训练模型进行细粒度差分推理，显著提升了LVLM在细粒度图像识别任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在细粒度图像识别中难以区分视觉相似的类别，现有基于强化学习的微调方法容易导致记忆训练类别而缺乏泛化能力。

Method: 为每个训练图像创建基于模型top-k输出的多项选择题，使用强化学习训练模型选择正确答案，迫使模型在合理选项间进行差分推理。

Result: 在五个标准细粒度数据集上显著优于现有方法，在基础到新类泛化设置中，DiVE-k在调和平均数指标上分别比QWEN2.5-VL-7B和ViRFT高出10.04%和6.16%。

Conclusion: DiVE-k通过利用模型自身预测作为训练信号，有效缓解了记忆问题并提升了泛化能力，在多种场景下都表现出优越性能。

Abstract: Large Vision Language Models (LVLMs) possess extensive text knowledge but struggles to utilize this knowledge for fine-grained image recognition, often failing to differentiate between visually similar categories. Existing fine-tuning methods using Reinforcement Learning (RL) with exact-match reward signals are often brittle, encourage memorization of training categories, and fail to elicit differential reasoning needed for generalization to unseen classes. To address this, we propose $\textbf{DiVE-k}$, $\textbf{Di}$fferential $\textbf{V}$isual r$\textbf{E}$asoning using top-$\textbf{k}$ generations, framework that leverages model's own top-k predictions as a training signal. For each training image, DiVE-k creates a multiple-choice question from the model's top-k outputs and uses RL to train the model to select the correct answer. This approach requires the model to perform fine-grained differential reasoning among plausible options and provides a simple, verifiable reward signal that mitigates memorization and improves generalization. Experiments on five standard fine-grained datasets show that our method significantly outperforms existing approaches. In the standard base-to-novel generalization setting, DiVE-k surpasses the QWEN2.5-VL-7B and ViRFT by 10.04% and 6.16% on the Harmonic Mean metric, respectively. Further experiments show similar gains in mixed-domain and few-shot scenarios.

</details>


### [200] [ScriptViT: Vision Transformer-Based Personalized Handwriting Generation](https://arxiv.org/abs/2511.18307)
*Sajjan Acharya,Rajendra Baskota*

Main category: cs.CV

TL;DR: 提出一个统一的风格化手写生成框架，使用Vision Transformer编码器学习全局风格模式，通过交叉注意力机制整合风格线索，并利用显著笔画注意力分析提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉作者特定的全局风格模式（如倾斜度、曲率、笔画压力等长距离空间依赖关系），在保持生成文本准确性的同时捕获细微的作者特定特征仍是一个开放问题。

Method: 使用基于Vision Transformer的风格编码器从多个参考图像学习全局风格模式；通过交叉注意力机制将风格线索与目标文本整合；采用显著笔画注意力分析(SSAA)提高模型可解释性。

Result: 该框架能够生成更具风格一致性的手写图像，并更容易理解和分析。

Conclusion: 提出的统一框架能够更好地表示手写体的长距离结构特征，生成更忠实反映目标风格的手写图像，同时提高了模型的可解释性。

Abstract: Styled handwriting generation aims to synthesize handwritten text that looks both realistic and aligned with a specific writer's style. While recent approaches involving GAN, transformer and diffusion-based models have made progress, they often struggle to capture the full spectrum of writer-specific attributes, particularly global stylistic patterns that span long-range spatial dependencies. As a result, capturing subtle writer-specific traits such as consistent slant, curvature or stroke pressure, while keeping the generated text accurate is still an open problem. In this work, we present a unified framework designed to address these limitations. We introduce a Vision Transformer-based style encoder that learns global stylistic patterns from multiple reference images, allowing the model to better represent long-range structural characteristics of handwriting. We then integrate these style cues with the target text using a cross-attention mechanism, enabling the system to produce handwritten images that more faithfully reflect the intended style. To make the process more interpretable, we utilize Salient Stroke Attention Analysis (SSAA), which reveals the stroke-level features the model focuses on during style transfer. Together, these components lead to handwriting synthesis that is not only more stylistically coherent, but also easier to understand and analyze.

</details>


### [201] [Stro-VIGRU: Defining the Vision Recurrent-Based Baseline Model for Brain Stroke Classification](https://arxiv.org/abs/2511.18316)
*Subhajeet Das,Pritam Paul,Rohit Bahadur,Sohan Das*

Main category: cs.CV

TL;DR: 提出基于预训练Vision Transformer的迁移学习框架，用于脑卒中早期识别，通过冻结部分编码器块和微调剩余部分来学习脑卒中特征，结合Bi-GRU分类器，在脑卒中数据集上达到94.06%的准确率。


<details>
  <summary>Details</summary>
Motivation: 脑卒中是全球主要致死和致残原因，早期识别对成功治疗至关重要。CT扫描是常见诊断方法，但人工分析耗时且易出错，需要自动化解决方案。

Method: 使用预训练的Vision Transformer模型，冻结部分编码器块，微调其余部分学习脑卒中特征，提取的特征输入单层双向GRU进行分类，通过数据增强处理类别不平衡问题。

Result: 在脑卒中数据集上实现了94.06%的分类准确率，表明该方法能有效识别脑卒中。

Conclusion: 基于Vision Transformer的迁移学习框架能够有效用于脑卒中早期识别，为临床诊断提供了自动化辅助工具。

Abstract: Stroke majorly causes death and disability worldwide, and early recognition is one of the key elements of successful treatment of the same. It is common to diagnose strokes using CT scanning, which is fast and readily available, however, manual analysis may take time and may result in mistakes. In this work, a pre-trained Vision Transformer-based transfer learning framework is proposed for the early identification of brain stroke. A few of the encoder blocks of the ViT model are frozen, and the rest are allowed to be fine-tuned in order to learn brain stroke-specific features. The features that have been extracted are given as input to a single-layer Bi-GRU to perform classification. Class imbalance is handled by data augmentation. The model has achieved 94.06% accuracy in classifying brain stroke from the Stroke Dataset.

</details>


### [202] [Optimal Pose Guidance for Stereo Calibration in 3D Deformation Measurement](https://arxiv.org/abs/2511.18317)
*Dongcai Tan,Shunkun Liang,Bin Li,Banglei Guan,Ang Su,Yuan Lin,Dapeng Zhang,Minggang Wan,Zibin Liu,Chenglong Wang,Jiajian Zhu,Zhang Li,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种用于3D变形测量的立体标定最优姿态引导方法，通过联合优化相对和绝对外参参数，自动生成最优标定姿态，提高标定效率和精度。


<details>
  <summary>Details</summary>
Motivation: 当前立体标定方法缺乏直观的最优姿态指导，导致变形测量效率低下且精度不理想，需要开发能够自动生成最优姿态的交互式标定框架。

Method: 提出姿态优化方法，引入相对和绝对外参参数的联合优化，采用协方差矩阵迹最小化作为损失函数求解下一个最优姿态，并集成用户友好的图形界面。

Result: 与随机姿态相比，该方法在效率（需要更少图像）和精度（测量误差更低）方面表现优越，在不同视场下保持鲁棒性，热变形测量结果与有限元分析高度一致。

Conclusion: 该方法在3D变形测量领域具有显著应用潜力，仿真实验、真实实验和热变形测量应用均验证了其有效性。

Abstract: Stereo optical measurement techniques, such as digital image correlation (DIC), are widely used in 3D deformation measurement as non-contact, full-field measurement methods, in which stereo calibration is a crucial step. However, current stereo calibration methods lack intuitive optimal pose guidance, leading to inefficiency and suboptimal accuracy in deformation measurements. The aim of this study is to develop an interactive calibration framework that automatically generates the next optimal pose, enabling high-accuracy stereo calibration for 3D deformation measurement. We propose a pose optimization method that introduces joint optimization of relative and absolute extrinsic parameters, with the minimization of the covariance matrix trace adopted as the loss function to solve for the next optimal pose. Integrated with this method is a user-friendly graphical interface, which guides even non-expert users to capture qualified calibration images. Our proposed method demonstrates superior efficiency (requiring fewer images) and accuracy (demonstrating lower measurement errors) compared to random pose, while maintaining robustness across varying FOVs. In the thermal deformation measurement tests on an S-shaped specimen, the results exhibit high agreement with finite element analysis (FEA) simulations in both deformation magnitude and evolutionary trends. We present a pose guidance method for high-precision stereo calibration in 3D deformation measurement. The simulation experiments, real-world experiments, and thermal deformation measurement applications all demonstrate the significant application potential of our proposed method in the field of 3D deformation measurement.
  Keywords: Stereo calibration, Optimal pose guidance, 3D deformation measurement, Digital image correlation

</details>


### [203] [General vs Domain-Specific CNNs: Understanding Pretraining Effects on Brain MRI Tumor Classification](https://arxiv.org/abs/2511.18326)
*Helia Abedini,Saba Rahimi,Reza Vaziri*

Main category: cs.CV

TL;DR: 比较三种预训练CNN在脑肿瘤MRI分类中的表现，发现ConvNeXt-Tiny表现最佳，而医学领域预训练的RadImageNet DenseNet121在小数据集上泛化能力较差


<details>
  <summary>Details</summary>
Motivation: 探讨在小数据集条件下，医学领域预训练模型与通用预训练模型在脑肿瘤检测任务中的性能差异

Method: 在相同条件下系统评估三种预训练CNN架构：RadImageNet DenseNet121（医学领域预训练）、EfficientNetV2S和ConvNeXt-Tiny（通用预训练），使用有限规模的脑MRI数据集进行训练和微调

Result: ConvNeXt-Tiny获得最高准确率，其次是EfficientNetV2S，而RadImageNet DenseNet121尽管经过医学领域预训练，但表现出较差的泛化能力，准确率较低且损失较高

Conclusion: 在小数据条件下，医学领域预训练可能泛化能力不佳，而现代深度通用预训练CNN在大规模数据集上预训练后，在专业医学成像任务中能提供更优越的迁移学习性能

Abstract: Brain tumor detection from MRI scans plays a crucial role in early diagnosis and treatment planning. Deep convolutional neural networks (CNNs) have demonstrated strong performance in medical imaging tasks, particularly when pretrained on large datasets. However, it remains unclear which type of pretrained model performs better when only a small dataset is available: those trained on domain-specific medical data or those pretrained on large general datasets. In this study, we systematically evaluate three pretrained CNN architectures for brain tumor classification: RadImageNet DenseNet121 with medical-domain pretraining, EfficientNetV2S, and ConvNeXt-Tiny, which are modern general-purpose CNNs. All models were trained and fine-tuned under identical conditions using a limited-size brain MRI dataset to ensure a fair comparison. Our results reveal that ConvNeXt-Tiny achieved the highest accuracy, followed by EfficientNetV2S, while RadImageNet DenseNet121, despite being pretrained on domain-specific medical data, exhibited poor generalization with lower accuracy and higher loss. These findings suggest that domain-specific pretraining may not generalize well under small-data conditions. In contrast, modern, deeper general-purpose CNNs pretrained on large-scale datasets can offer superior transfer learning performance in specialized medical imaging tasks.

</details>


### [204] [SciPostLayoutTree: A Dataset for Structural Analysis of Scientific Posters](https://arxiv.org/abs/2511.18329)
*Shohei Tanaka,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 构建了SciPostLayoutTree数据集（约8000张海报），开发了Layout Tree Decoder模型来预测海报的阅读顺序和父子关系，特别改善了空间挑战性关系的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 科学海报在学术交流中很重要，但相比论文，其结构分析研究较少。需要构建专门的数据集和模型来分析海报的阅读顺序和父子关系，以支持结构感知界面的开发。

Method: 构建了包含8000张标注海报的SciPostLayoutTree数据集，开发了Layout Tree Decoder模型，该模型结合视觉特征和边界框特征（位置和类别信息），并使用beam search来预测关系。

Result: 实验结果表明，该模型提高了空间挑战性关系（向上、水平和长距离关系）的预测准确性，为海报结构分析建立了坚实的基线。

Conclusion: SciPostLayoutTree数据集和Layout Tree Decoder模型填补了海报结构分析的研究空白，为开发结构感知界面提供了基础支持。

Abstract: Scientific posters play a vital role in academic communication by presenting ideas through visual summaries. Analyzing reading order and parent-child relations of posters is essential for building structure-aware interfaces that facilitate clear and accurate understanding of research content. Despite their prevalence in academic communication, posters remain underexplored in structural analysis research, which has primarily focused on papers. To address this gap, we constructed SciPostLayoutTree, a dataset of approximately 8,000 posters annotated with reading order and parent-child relations. Compared to an existing structural analysis dataset, SciPostLayoutTree contains more instances of spatially challenging relations, including upward, horizontal, and long-distance relations. As a solution to these challenges, we develop Layout Tree Decoder, which incorporates visual features as well as bounding box features including position and category information. The model also uses beam search to predict relations while capturing sequence-level plausibility. Experimental results demonstrate that our model improves the prediction accuracy for spatially challenging relations and establishes a solid baseline for poster structure analysis. The dataset is publicly available at https://huggingface.co/datasets/omron-sinicx/scipostlayouttree. The code is also publicly available at https://github.com/omron-sinicx/scipostlayouttree.

</details>


### [205] [ConsistCompose: Unified Multimodal Layout Control for Image Composition](https://arxiv.org/abs/2511.18333)
*Xuanke Shi,Boxuan Li,Xiaoyang Han,Zhongang Cai,Lei Yang,Dahua Lin,Quan Wang*

Main category: cs.CV

TL;DR: ConsistCompose是一个统一的多模态框架，通过将布局坐标嵌入语言提示中，实现布局可控的多实例图像生成，无需特定任务分支。


<details>
  <summary>Details</summary>
Motivation: 现有的统一多模态模型主要关注视觉基础（语言与图像区域对齐），而其生成对应部分——基于语言嵌入布局的多实例生成（LELG）仍未被充分探索，限制了精确的组合控制。

Method: 构建ConsistCompose3M数据集（340万多实例生成数据），通过实例-坐标绑定提示和坐标感知的无分类器引导，将语言布局线索转化为精确的空间控制。

Result: 在COCO-Position和MS-Bench上的实验表明，ConsistCompose显著提高了空间准确性，同时保持了身份保真度和竞争力的通用多模态理解能力。

Conclusion: 该框架为布局可控的多模态图像生成建立了一个统一的范式。

Abstract: Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.

</details>


### [206] [A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles](https://arxiv.org/abs/2511.18344)
*Tianyang Xu,Jinjie Gu,Xuefeng Zhu,XiaoJun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 提出了MM-UAV，首个大规模多模态无人机跟踪基准数据集，包含RGB、红外和事件信号三种模态，涵盖30多个挑战性场景，包含1321个同步多模态序列和280万标注帧。同时提出了一个专门针对无人机跟踪的多模态多目标跟踪框架，包含偏移引导自适应对齐模块和自适应动态融合模块。


<details>
  <summary>Details</summary>
Motivation: 随着低空无人机的普及，视觉多目标跟踪成为关键安全技术，但单一视觉模态在低光照、复杂背景和快速运动等挑战性场景中容易失败。多模态跟踪更具鲁棒性，但缺乏专门的公共数据集阻碍了有效解决方案的开发。

Method: 提出了一个多模态多无人机跟踪框架，包含两个关键技术创新：1) 偏移引导自适应对齐模块，解决跨传感器空间不匹配问题；2) 自适应动态融合模块，平衡不同模态的互补信息。还引入了事件增强关联机制，利用事件模态的运动线索进行更可靠的身份维护。

Result: 综合实验表明，所提出的框架在性能上持续优于最先进的方法。

Conclusion: MM-UAV数据集和提出的多模态跟踪框架为多模态无人机跟踪研究提供了重要基础，将促进该领域的进一步发展。数据集和源代码将公开提供。

Abstract: With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.

</details>


### [207] [FlowPortal: Residual-Corrected Flow for Training-Free Video Relighting and Background Replacement](https://arxiv.org/abs/2511.18346)
*Wenshuo Gao,Junyi Fan,Jiangyue Zeng,Shuai Yang*

Main category: cs.CV

TL;DR: FlowPortal是一个无需训练、基于光流的视频重光照框架，通过残差校正光流机制实现背景替换和光照编辑，保证时间一致性和结构保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频重光照方法难以平衡时间一致性、空间保真度和光照自然度，特别是在背景替换场景下。

Method: 提出残差校正光流机制将标准光流模型转换为编辑模型，结合解耦条件设计、高频传输机制和掩码策略实现前景重光照与背景生成分离。

Result: 实验表明FlowPortal在时间一致性、结构保持和光照真实感方面表现优异，同时保持高效率。

Conclusion: FlowPortal为视频重光照与背景替换提供了一种高效且高质量的解决方案，在电影制作和创意媒体应用中具有重要价值。

Abstract: Video relighting with background replacement is a challenging task critical for applications in film production and creative media. Existing methods struggle to balance temporal consistency, spatial fidelity, and illumination naturalness. To address these issues, we introduce FlowPortal, a novel training-free flow-based video relighting framework. Our core innovation is a Residual-Corrected Flow mechanism that transforms a standard flow-based model into an editing model, guaranteeing perfect reconstruction when input conditions are identical and enabling faithful relighting when they differ, resulting in high structural consistency. This is further enhanced by a Decoupled Condition Design for precise lighting control and a High-Frequency Transfer mechanism for detail preservation. Additionally, a masking strategy isolates foreground relighting from background pure generation process. Experiments demonstrate that FlowPortal achieves superior performance in temporal coherence, structural preservation, and lighting realism, while maintaining high efficiency. Project Page: https://gaowenshuo.github.io/FlowPortalProject/.

</details>


### [208] [MagicWand: A Universal Agent for Generation and Evaluation Aligned with User Preference](https://arxiv.org/abs/2511.18352)
*Zitong Xu,Dake Shen,Yaosong Du,Kexiang Hao,Jinghan Huang,Xiande Huang*

Main category: cs.CV

TL;DR: 提出了MagicWand系统，通过UniPrefer-100K数据集和UniPreferBench基准测试，解决了AIGC模型中用户偏好难以表达和保留的问题，实现了基于用户偏好的内容生成与评估。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC模型虽然取得了显著进展，但用户仍难以获得符合个人偏好的内容，主要因为难以编写详细提示词且缺乏偏好保留机制。

Method: 构建UniPrefer-100K大规模数据集，提出MagicWand通用生成与评估代理，该系统能基于用户偏好增强提示词，利用先进生成模型创建高质量内容，并进行偏好对齐的评估与优化。

Result: 在UniPreferBench基准测试上，MagicWand在多种场景下都能生成与用户偏好高度一致的内容和评估结果。

Conclusion: MagicWand系统有效解决了AIGC中的用户偏好对齐问题，为个性化内容生成提供了可行的解决方案。

Abstract: Recent advances in AIGC (Artificial Intelligence Generated Content) models have enabled significant progress in image and video generation. However, users still struggle to obtain content that aligns with their preferences due to the difficulty of crafting detailed prompts and the lack of mechanisms to retain their preferences. To address these challenges, we construct \textbf{UniPrefer-100K}, a large-scale dataset comprising images, videos, and associated text that describes the styles users tend to prefer. Based on UniPrefer-100K, we propose \textbf{MagicWand}, a universal generation and evaluation agent that enhances prompts based on user preferences, leverages advanced generation models for high-quality content, and applies preference-aligned evaluation and refinement. In addition, we introduce \textbf{UniPreferBench}, the first large-scale benchmark with over 120K annotations for assessing user preference alignment across diverse AIGC tasks. Experiments on UniPreferBench demonstrate that MagicWand consistently generates content and evaluations that are well aligned with user preferences across a wide range of scenarios.

</details>


### [209] [TRANSPORTER: Transferring Visual Semantics from VLM Manifolds](https://arxiv.org/abs/2511.18359)
*Alexandros Stergiou*

Main category: cs.CV

TL;DR: 本文提出TRANSPORTER方法，通过将视觉语言模型(VLMs)的logit分数转换为视频，为模型可解释性提供新方向。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型虽然能推理复杂场景，但理解其内部决策过程仍具挑战性。受文本到视频生成模型进展启发，希望开发能揭示VLMs预测背后规则的方法。

Method: 提出logits-to-video(L2V)任务和模型无关的TRANSPORTER方法，通过最优传输耦合将VLM的高语义嵌入空间与T2V模型连接，利用logit分数定义视频生成的嵌入方向。

Result: TRANSPORTER能生成反映字幕变化（对象属性、动作副词、场景上下文）的视频，定量和定性评估表明L2V为模型可解释性提供了保真度丰富的新方向。

Conclusion: L2V任务和TRANSPORTER方法为理解视觉语言模型的内部决策过程开辟了前所未有的可解释性研究路径。

Abstract: How do video understanding models acquire their answers? Although current Vision Language Models (VLMs) reason over complex scenes with diverse objects, action performances, and scene dynamics, understanding and controlling their internal processes remains an open challenge. Motivated by recent advancements in text-to-video (T2V) generative models, this paper introduces a logits-to-video (L2V) task alongside a model-independent approach, TRANSPORTER, to generate videos that capture the underlying rules behind VLMs' predictions. Given the high-visual-fidelity produced by T2V models, TRANSPORTER learns an optimal transport coupling to VLM's high-semantic embedding spaces. In turn, logit scores define embedding directions for conditional video generation. TRANSPORTER generates videos that reflect caption changes over diverse object attributes, action adverbs, and scene context. Quantitative and qualitative evaluations across VLMs demonstrate that L2V can provide a fidelity-rich, novel direction for model interpretability that has not been previously explored.

</details>


### [210] [Alias-free 4D Gaussian Splatting](https://arxiv.org/abs/2511.18367)
*Zilong Chen,Huan-ang Gao,Delin Qu,Haohan Chi,Hao Tang,Kai Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 提出了一种4D高斯溅射的尺度自适应滤波器和尺度损失方法，解决了动态场景重建中因相机焦距或距离变化导致的渲染分辨率调整时出现的高频伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯溅射的动态场景重建方法在调整相机焦距或高斯基元与相机距离以修改渲染分辨率时，会因4D高斯的频率约束和2D膨胀滤波器引起的高斯尺度不匹配而产生强烈伪影。

Method: 推导了4D高斯溅射的最大采样频率公式，引入了4D尺度自适应滤波器和尺度损失，灵活调节4D高斯溅射的采样频率。

Result: 该方法在增加渲染频率时消除了高频伪影，同时有效减少了多视角视频重建中的冗余高斯基元。通过单目和多视角视频重建实验验证了方法的有效性。

Conclusion: 提出的4D尺度自适应滤波方法成功解决了动态场景重建中的频率约束和尺度不匹配问题，实现了无伪影的高分辨率渲染和高效的多视角重建。

Abstract: Existing dynamic scene reconstruction methods based on Gaussian Splatting enable real-time rendering and generate realistic images. However, adjusting the camera's focal length or the distance between Gaussian primitives and the camera to modify rendering resolution often introduces strong artifacts, stemming from the frequency constraints of 4D Gaussians and Gaussian scale mismatch induced by the 2D dilated filter. To address this, we derive a maximum sampling frequency formulation for 4D Gaussian Splatting and introduce a 4D scale-adaptive filter and scale loss, which flexibly regulates the sampling frequency of 4D Gaussian Splatting. Our approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction. We validate the proposed method through monocular and multi-view video reconstruction experiments.Ours project page: https://4d-alias-free.github.io/4D-Alias-free/

</details>


### [211] [MimiCAT: Mimic with Correspondence-Aware Cascade-Transformer for Category-Free 3D Pose Transfer](https://arxiv.org/abs/2511.18370)
*Zenghao Chai,Chen Tang,Yongkang Wong,Xulei Yang,Mohan Kankanhalli*

Main category: cs.CV

TL;DR: MimiCAT是一个用于类别无关3D姿态迁移的级联变换器模型，通过软对应匹配实现跨类别（如人形到四足动物）的姿态迁移，解决了现有方法在结构差异大的角色间迁移失败的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D姿态迁移方法主要局限于相似结构的角色之间，无法泛化到类别无关设置（如人形到四足动物），主要挑战在于不同角色类型的结构和变换多样性导致的区域不匹配和迁移质量差。

Method: 构建百万级跨数百个不同角色的姿态数据集；提出MimiCAT级联变换器模型，利用语义关键点标签学习软对应关系，实现灵活的跨角色多对多匹配；将姿态迁移建模为条件生成过程，先通过软对应匹配将源变换投影到目标，再用形状条件表示进行细化。

Result: 广泛的定性和定量实验表明，MimiCAT能够在不同角色间迁移合理的姿态，显著优于局限于狭窄类别迁移（如人形到人形）的先前方法。

Conclusion: MimiCAT通过软对应匹配和条件生成过程，成功实现了类别无关的3D姿态迁移，解决了跨结构差异大角色的姿态迁移挑战。

Abstract: 3D pose transfer aims to transfer the pose-style of a source mesh to a target character while preserving both the target's geometry and the source's pose characteristic. Existing methods are largely restricted to characters with similar structures and fail to generalize to category-free settings (e.g., transferring a humanoid's pose to a quadruped). The key challenge lies in the structural and transformation diversity inherent in distinct character types, which often leads to mismatched regions and poor transfer quality. To address these issues, we first construct a million-scale pose dataset across hundreds of distinct characters. We further propose MimiCAT, a cascade-transformer model designed for category-free 3D pose transfer. Instead of relying on strict one-to-one correspondence mappings, MimiCAT leverages semantic keypoint labels to learn a novel soft correspondence that enables flexible many-to-many matching across characters. The pose transfer is then formulated as a conditional generation process, in which the source transformations are first projected onto the target through soft correspondence matching and subsequently refined using shape-conditioned representations. Extensive qualitative and quantitative experiments demonstrate that MimiCAT transfers plausible poses across different characters, significantly outperforming prior methods that are limited to narrow category transfer (e.g., humanoid-to-humanoid).

</details>


### [212] [MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models](https://arxiv.org/abs/2511.18373)
*Xiyang Wu,Zongxia Li,Jihui Jin,Guangyao Shi,Gouthaman KV,Vishnu Raj,Nilotpal Sinha,Jingxi Chen,Fan Du,Dinesh Manocha*

Main category: cs.CV

TL;DR: 提出了MASS方法，通过将物理世界上下文线索转化为可解释表示来增强视觉语言模型在物理推理任务上的表现，包括引入MASS-Bench基准和模型无关的时空信号注入方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在标准视频任务上表现良好，但在涉及运动动力学和空间交互的物理驱动推理方面存在局限，这限制了它们解释真实或AI生成内容视频以及生成物理一致内容的能力。

Method: 提出MASS方法：1) 通过基于深度的3D编码和视觉接地将时空信号注入VLM语言空间；2) 使用运动跟踪器捕捉物体动态；3) 应用强化微调来加强跨模态对齐和推理。

Result: 实验表明，经过精炼的VLM在物理推理和理解任务上优于可比和更大的基线模型以及先前的最先进模型，分别提升8.7%和6.0%，性能接近闭源SoTA VLM如Gemini-2.5-Flash。

Conclusion: 该方法有效解决了VLM在物理推理方面的局限性，验证了将物理世界上下文转化为可解释表示的方法的有效性。

Abstract: Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.

</details>


### [213] [Synthetic Curriculum Reinforces Compositional Text-to-Image Generation](https://arxiv.org/abs/2511.18378)
*Shijian Wang,Runhao Fu,Siyi Zhao,Qingqin Zhan,Xingjian Wang,Jiarui Jin,Yuan Lu,Hanqian Wu,Cunjian Chen*

Main category: cs.CV

TL;DR: 提出了CompGen框架，通过场景图建立难度标准，使用自适应MCMC图采样算法生成渐进式训练数据，结合GRPO强化学习来提升文本到图像生成的组合能力。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中的组合合成挑战，特别是多对象场景中准确渲染对象属性、空间关系和语义关系的困难。

Method: 使用场景图建立难度标准，开发自适应MCMC图采样算法生成难度感知的训练课程数据，结合GRPO强化学习进行渐进式优化。

Result: CompGen显著提升了基于扩散和自回归的T2I模型的组合生成能力，easy-to-hard和高斯采样策略表现优于随机采样。

Conclusion: CompGen框架有效改善了组合文本到图像生成系统，展示了不同课程调度策略下的可扩展性。

Abstract: Text-to-Image (T2I) generation has long been an open problem, with compositional synthesis remaining particularly challenging. This task requires accurate rendering of complex scenes containing multiple objects that exhibit diverse attributes as well as intricate spatial and semantic relationships, demanding both precise object placement and coherent inter-object interactions. In this paper, we propose a novel compositional curriculum reinforcement learning framework named CompGen that addresses compositional weakness in existing T2I models. Specifically, we leverage scene graphs to establish a novel difficulty criterion for compositional ability and develop a corresponding adaptive Markov Chain Monte Carlo graph sampling algorithm. This difficulty-aware approach enables the synthesis of training curriculum data that progressively optimize T2I models through reinforcement learning. We integrate our curriculum learning approach into Group Relative Policy Optimization (GRPO) and investigate different curriculum scheduling strategies. Our experiments reveal that CompGen exhibits distinct scaling curves under different curriculum scheduling strategies, with easy-to-hard and Gaussian sampling strategies yielding superior scaling performance compared to random sampling. Extensive experiments demonstrate that CompGen significantly enhances compositional generation capabilities for both diffusion-based and auto-regressive T2I models, highlighting its effectiveness in improving the compositional T2I generation systems.

</details>


### [214] [RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models](https://arxiv.org/abs/2511.18380)
*Timing Yang,Guoyizhe Wei,Alan Yuille,Feng Wang*

Main category: cs.CV

TL;DR: 本文系统分析了Mamba在视觉任务中的表征特性，揭示了其与Softmax Attention的关系，提出了新的激活图评估指标，并展示了Mamba在长距离依赖建模和可解释性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: Mamba作为视觉任务的有效骨干网络，其内在机制在视觉领域仍未被充分理解，需要系统研究其表征特性。

Method: 理论分析Mamba与Softmax和Linear Attention的关系；提出二元分割指标评估激活图；利用DINO进行自监督预训练获得更清晰的激活图。

Result: 确认Mamba可作为Softmax Attention的低秩近似；新指标证明Mamba能建模长距离依赖；自监督方法产生更清晰的激活图；在ImageNet上达到78.5%的线性探测准确率。

Conclusion: 本研究为基于Mamba的视觉架构提供了有价值的见解，展示了其在长距离依赖建模和可解释性方面的潜力。

Abstract: Mamba has recently garnered attention as an effective backbone for vision tasks. However, its underlying mechanism in visual domains remains poorly understood. In this work, we systematically investigate Mamba's representational properties and make three primary contributions. First, we theoretically analyze Mamba's relationship to Softmax and Linear Attention, confirming that it can be viewed as a low-rank approximation of Softmax Attention and thereby bridging the representational gap between Softmax and Linear forms. Second, we introduce a novel binary segmentation metric for activation map evaluation, extending qualitative assessments to a quantitative measure that demonstrates Mamba's capacity to model long-range dependencies. Third, by leveraging DINO for self-supervised pretraining, we obtain clearer activation maps than those produced by standard supervised approaches, highlighting Mamba's potential for interpretability. Notably, our model also achieves a 78.5 percent linear probing accuracy on ImageNet, underscoring its strong performance. We hope this work can provide valuable insights for future investigations of Mamba-based vision architectures.

</details>


### [215] [ViMix-14M: A Curated Multi-Source Video-Text Dataset with Long-Form, High-Quality Captions and Crawl-Free Access](https://arxiv.org/abs/2511.18382)
*Timing Yang,Sucheng Ren,Alan Yuille,Feng Wang*

Main category: cs.CV

TL;DR: ViMix-14M是一个包含约1400万视频-文本对的精心策划数据集，解决了开源视频生成模型面临的数据瓶颈问题，提供无需爬取、可直接下载的高质量长文本描述。


<details>
  <summary>Details</summary>
Motivation: 现有公开视频数据集通常需要手动从YouTube爬取，存在可用数据量少、链接失效、访问限制和版权不确定性等问题，阻碍了开源视频基础模型的训练和发展。

Method: 通过合并多样化的开源视频源，进行统一的去重和质量过滤，并采用多粒度、基于真实标注的重新标注流程，使描述更好地匹配视频的动作、场景和时间结构。

Result: 在多模态检索、文本到视频生成和视频问答任务上的评估显示，相比其他数据集取得了持续改进。

Conclusion: 这项工作有助于消除训练和微调开源视频基础模型的关键障碍，并为构建高质量、可泛化的视频-文本数据集提供了见解。

Abstract: Text-to-video generation has surged in interest since Sora, yet open-source models still face a data bottleneck: there is no large, high-quality, easily obtainable video-text corpus. Existing public datasets typically require manual YouTube crawling, which yields low usable volume due to link rot and access limits, and raises licensing uncertainty. This work addresses this challenge by introducing ViMix-14M, a curated multi-source video-text dataset of around 14 million pairs that provides crawl-free, download-ready access and long-form, high-quality captions tightly aligned to video. ViMix-14M is built by merging diverse open video sources, followed by unified de-duplication and quality filtering, and a multi-granularity, ground-truth-guided re-captioning pipeline that refines descriptions to better match actions, scenes, and temporal structure. We evaluate the dataset by multimodal retrieval, text-to-video generation, and video question answering tasks, observing consistent improvements over counterpart datasets. We hope this work can help removing the key barrier to training and fine-tuning open-source video foundation models, and provide insights of building high-quality and generalizable video-text datasets.

</details>


### [216] [Can a Second-View Image Be a Language? Geometric and Semantic Cross-Modal Reasoning for X-ray Prohibited Item Detection](https://arxiv.org/abs/2511.18385)
*Chuang Peng,Renshuai Tao,Zhongwei Ren,Xianglong Liu,Yunchao Wei*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automatic X-ray prohibited items detection is vital for security inspection and has been widely studied. Traditional methods rely on visual modality, often struggling with complex threats. While recent studies incorporate language to guide single-view images, human inspectors typically use dual-view images in practice. This raises the question: can the second view provide constraints similar to a language modality? In this work, we introduce DualXrayBench, the first comprehensive benchmark for X-ray inspection that includes multiple views and modalities. It supports eight tasks designed to test cross-view reasoning. In DualXrayBench, we introduce a caption corpus consisting of 45,613 dual-view image pairs across 12 categories with corresponding captions. Building upon these data, we propose the Geometric (cross-view)-Semantic (cross-modality) Reasoner (GSR), a multimodal model that jointly learns correspondences between cross-view geometry and cross-modal semantics, treating the second-view images as a "language-like modality". To enable this, we construct the GSXray dataset, with structured Chain-of-Thought sequences: <top>, <side>, <conclusion>. Comprehensive evaluations on DualXrayBench demonstrate that GSR achieves significant improvements across all X-ray tasks, offering a new perspective for real-world X-ray inspection.

</details>


### [217] [SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation](https://arxiv.org/abs/2511.18386)
*Peter Siegel,Federico Tombari,Marc Pollefeys,Daniel Barath*

Main category: cs.CV

TL;DR: SegSplat是一个新颖的3D重建框架，通过多视角2D基础模型特征构建紧凑语义记忆库，为3D高斯预测离散语义索引，实现快速前馈式3D重建与开放词汇语义理解的结合。


<details>
  <summary>Details</summary>
Motivation: 弥合快速前馈3D重建与丰富开放词汇语义理解之间的差距，为机器人交互、增强现实等智能系统提供实用的语义感知3D环境生成。

Method: 构建紧凑语义记忆库，从多视角2D基础模型特征中提取语义信息，为每个3D高斯预测离散语义索引以及几何和外观属性，单次前馈完成。

Result: 在几何保真度上与最先进的前馈3D高斯泼溅方法相当，同时实现鲁棒的开放集语义分割，无需任何逐场景优化的语义特征集成。

Conclusion: 该工作代表了向实用、实时生成语义感知3D环境的重要一步，对推进机器人交互、增强现实等智能系统至关重要。

Abstract: We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.

</details>


### [218] [Exploring Weak-to-Strong Generalization for CLIP-based Classification](https://arxiv.org/abs/2511.18396)
*Jinhao Li,Sarah M. Erfani,Lei Feng,James Bailey,Feng Liu*

Main category: cs.CV

TL;DR: 提出了一种名为类原型学习(CPL)的方法，通过弱监督增强CLIP模型的分类能力，在预训练有限的情况下取得了3.67%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着模型复杂度增加，人工监督变得不切实际。当模型超越人类知识时，提供准确反馈变得困难且低效。需要利用弱模型监督强模型的方法来减少人工监督负担。

Method: 提出类原型学习(CPL)方法，通过弱监督学习更具代表性的类别原型来增强CLIP模型的分类能力。

Result: 在预训练有限的情况下，CPL方法比强基线方法提升了3.67%的性能，在目标场景中表现出稳健的改进。

Conclusion: 弱到强泛化方法在多模态视觉语言模型中同样有效，CPL方法在弱监督下通过简单损失函数实现了显著的性能提升。

Abstract: Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, class prototype learning (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67% improvement over strong baseline methods.

</details>


### [219] [ChineseVideoBench: Benchmarking Multi-modal Large Models for Chinese Video Question Answering](https://arxiv.org/abs/2511.18399)
*Yuxiang Nie,Han Wang,Yongjie Ye,Haiyang Yu,Weitao Jia,Tao Zeng,Hao Feng,Xiang Fei,Yang Li,Xiaohui Lv,Guozhi Tang,Jingqun Tang,Jinghui Lu,Zehui Dai,Jiacong Wang,Dingkang Yang,An-Lan Wang,Can Huang*

Main category: cs.CV

TL;DR: ChineseVideoBench是一个专门用于评估中文视频问答多模态大语言模型的基准测试，包含8个主类和12个子类，测试结果显示当前模型在该基准上表现仍有挑战，Gemini 2.5 Pro表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着对复杂视频分析能力需求的增长，迫切需要全面且具有文化意识的评估框架，特别是在中文视频内容理解方面。

Method: 构建了一个包含8个主类和12个子类的基准数据集，专门针对需要深度视频理解和中文语言文化细微差别的任务。

Result: 实证评估显示该基准对当前MLLMs构成显著挑战，Gemini 2.5 Pro获得最高分77.9%，InternVL-38B是最具竞争力的开源模型。

Conclusion: ChineseVideoBench填补了中文视频问答评估的空白，为MLLMs在复杂中文视频内容上的评估提供了重要基准。

Abstract: This paper introduces ChineseVideoBench, a pioneering benchmark specifically designed for evaluating Multimodal Large Language Models (MLLMs) in Chinese Video Question Answering. The growing demand for sophisticated video analysis capabilities highlights the critical need for comprehensive, culturally-aware evaluation frameworks. ChineseVideoBench addresses this gap by providing a robust dataset and tailored evaluation metrics, enabling rigorous assessment of state-of-the-art MLLMs on complex Chinese video content. Specifically, ChineseVideoBench comprises 8 main classes and 12 sub-classes, encompassing tasks that demand both deep video understanding and nuanced Chinese linguistic and cultural awareness. Our empirical evaluations reveal that ChineseVideoBench presents a significant challenge to current MLLMs. Among the models assessed, Gemini 2.5 Pro achieves the highest performance with an overall score of 77.9%, while InternVL-38B emerges as the most competitive open-source model.

</details>


### [220] [4D-VGGT: A General Foundation Model with SpatioTemporal Awareness for Dynamic Scene Geometry Estimation](https://arxiv.org/abs/2511.18416)
*Haonan Wang,Hanyu Zhou,Haoyue Liu,Luxin Yan*

Main category: cs.CV

TL;DR: 提出了4D-VGGT模型，通过分治策略处理动态场景几何估计中的时空特征表示问题，支持多设置输入、多级表示和多任务预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法将时空特征对齐到统一潜在空间，但由于时空特征的异质性，这种统一范式存在表示不匹配的问题。

Method: 设计自适应视觉网格支持任意视图数和时间步的输入；提出跨视图全局融合用于空间表示和跨时间局部融合用于时间表示；添加多个任务特定头部实现多任务预测。

Result: 在多个动态场景几何基准测试中验证了方法的有效性。

Conclusion: 该统一框架增强了动态场景中特征的可区分性和应用普适性。

Abstract: We investigate a challenging task of dynamic scene geometry estimation, which requires representing both spatial and temporal features. Typically, existing methods align the two features into a unified latent space to model scene geometry. However, this unified paradigm suffers from potential mismatched representation due to the heterogeneous nature between spatial and temporal features. In this work, we propose 4D-VGGT, a general foundation model with divide-and-conquer spatiotemporal representation for dynamic scene geometry. Our model is divided into three aspects: 1) Multi-setting input. We design an adaptive visual grid that supports input sequences with arbitrary numbers of views and time steps. 2) Multi-level representation. We propose a cross-view global fusion for spatial representation and a cross-time local fusion for temporal representation. 3) Multi-task prediction. We append multiple task-specific heads to spatiotemporal representations, enabling a comprehensive visual geometry estimation for dynamic scenes. Under this unified framework, these components enhance the feature discriminability and application universality of our model for dynamic scenes. In addition, we integrate multiple geometry datasets to train our model and conduct extensive experiments to verify the effectiveness of our method across various tasks on multiple dynamic scene geometry benchmarks.

</details>


### [221] [NeuroVascU-Net: A Unified Multi-Scale and Cross-Domain Adaptive Feature Fusion U-Net for Precise 3D Segmentation of Brain Vessels in Contrast-Enhanced T1 MRI](https://arxiv.org/abs/2511.18422)
*Mohammad Jafari Vayeghan,Niloufar Delfan,Mehdi Tale Masouleh,Mansour Parvaresh Rizi,Behzad Moshiri*

Main category: cs.CV

TL;DR: NeuroVascU-Net是一个专门用于从T1CE MRI中分割脑血管的深度学习架构，在保持高精度的同时显著降低了计算成本，为神经外科手术规划提供了实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前脑血管分割方法存在手动分割耗时且易变、自动方法在精度和计算成本之间难以平衡的问题，特别是缺乏专门针对神经肿瘤患者T1CE MRI的深度学习方法。

Method: 基于扩张U-Net架构，集成了两个专门模块：瓶颈处的多尺度上下文特征融合模块（MSC²F）和深层交叉域自适应特征融合模块（CDA²F），分别捕获多尺度信息和动态整合领域特定特征。

Result: 在137名脑肿瘤活检患者的T1CE扫描数据集上，获得了0.8609的Dice分数和0.8841的精确度，仅需12.4M参数，显著少于基于transformer的模型。

Conclusion: NeuroVascU-Net在准确性和效率之间取得了良好平衡，能够准确分割主要和细小血管结构，是计算机辅助神经外科规划的实际可行解决方案。

Abstract: Precise 3D segmentation of cerebral vasculature from T1-weighted contrast-enhanced (T1CE) MRI is crucial for safe neurosurgical planning. Manual delineation is time-consuming and prone to inter-observer variability, while current automated methods often trade accuracy for computational cost, limiting clinical use. We present NeuroVascU-Net, the first deep learning architecture specifically designed to segment cerebrovascular structures directly from clinically standard T1CE MRI in neuro-oncology patients, addressing a gap in prior work dominated by TOF-MRA-based approaches. NeuroVascU-Net builds on a dilated U-Net and integrates two specialized modules: a Multi-Scale Contextual Feature Fusion ($MSC^2F$) module at the bottleneck and a Cross-Domain Adaptive Feature Fusion ($CDA^2F$) module at deeper hierarchical layers. $MSC^2F$ captures both local and global information via multi-scale dilated convolutions, while $CDA^2F$ dynamically integrates domain-specific features, enhancing representation while keeping computation low. The model was trained and validated on a curated dataset of T1CE scans from 137 brain tumor biopsy patients, annotated by a board-certified functional neurosurgeon. NeuroVascU-Net achieved a Dice score of 0.8609 and precision of 0.8841, accurately segmenting both major and fine vascular structures. Notably, it requires only 12.4M parameters, significantly fewer than transformer-based models such as Swin U-NetR. This balance of accuracy and efficiency positions NeuroVascU-Net as a practical solution for computer-assisted neurosurgical planning.

</details>


### [222] [CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images](https://arxiv.org/abs/2511.18424)
*Avishka Perera,Kumal Hewagamage,Saeedha Nazar,Kavishka Abeywardana,Hasitha Gallella,Ranga Rodrigo,Mohamed Afham*

Main category: cs.CV

TL;DR: CrossJEPA是一个跨模态联合嵌入预测架构，通过利用图像基础模型的知识，训练预测器从3D点云推断特定渲染2D视图的嵌入，在3D表示学习中实现了高性能、内存效率和快速训练。


<details>
  <summary>Details</summary>
Motivation: 解决当前利用2D数据的3D表示学习方法模型庞大、训练缓慢、计算成本高的问题，探索JEPA架构在跨模态学习中的潜力。

Method: 提出CrossJEPA架构，利用冻结的图像基础模型作为教师，训练点云编码器预测特定2D视图的嵌入，采用跨域投影信息净化监督信号，并实现一次性目标嵌入缓存机制。

Result: 在ModelNet40上达到94.2%，在ScanObjectNN上达到88.3%的线性探测准确率，仅使用14.1M预训练参数（点编码器8.5M），在单GPU上约6小时完成预训练。

Conclusion: CrossJEPA是一个性能优异、内存高效、训练快速的3D表示学习框架，通过知识蒸馏实现了新的最先进结果。

Abstract: Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.

</details>


### [223] [LungX: A Hybrid EfficientNet-Vision Transformer Architecture with Multi-Scale Attention for Accurate Pneumonia Detection](https://arxiv.org/abs/2511.18425)
*Mansur Yerzhanuly*

Main category: cs.CV

TL;DR: LungX是一种结合EfficientNet多尺度特征、CBAM注意力机制和Vision Transformer全局上下文建模的混合架构，用于肺炎检测，在20,000张胸部X光片上达到86.5%准确率和0.943 AUC，比基准模型提升6.7% AUC。


<details>
  <summary>Details</summary>
Motivation: 肺炎是全球主要死亡原因，及时诊断至关重要。现有方法在肺炎检测性能上仍有提升空间，需要开发更准确的AI诊断辅助工具。

Method: 提出LungX混合架构：结合EfficientNet的多尺度特征提取、CBAM注意力机制增强重要区域关注度、Vision Transformer进行全局上下文建模。

Result: 在RSNA和CheXpert的20,000张胸部X光片上测试，达到86.5%准确率和0.943 AUC，比EfficientNet-B0基准提升6.7% AUC。注意力图显示优秀的病灶定位能力。

Conclusion: LungX在肺炎检测上达到最先进性能，未来方向包括多中心验证和架构优化，目标是达到88%准确率用于临床部署。

Abstract: Pneumonia remains a leading global cause of mortality where timely diagnosis is critical. We introduce LungX, a novel hybrid architecture combining EfficientNet's multi-scale features, CBAM attention mechanisms, and Vision Transformer's global context modeling for enhanced pneumonia detection. Evaluated on 20,000 curated chest X-rays from RSNA and CheXpert, LungX achieves state-of-the-art performance (86.5 percent accuracy, 0.943 AUC), representing a 6.7 percent AUC improvement over EfficientNet-B0 baselines. Visual analysis demonstrates superior lesion localization through interpretable attention maps. Future directions include multi-center validation and architectural optimizations targeting 88 percent accuracy for clinical deployment as an AI diagnostic aid.

</details>


### [224] [DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation](https://arxiv.org/abs/2511.18434)
*Yongkun Du,Pinxuan Chen,Xuye Ying,Zhineng Chen*

Main category: cs.CV

TL;DR: DocPTBench是一个专门针对拍摄文档解析和翻译的基准测试，包含1300多个高分辨率拍摄文档，揭示了现有模型在处理真实世界拍摄文档时的性能显著下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注扫描或数字原生文档，无法充分反映真实世界拍摄条件（如几何变形和光度变化）带来的复杂挑战。

Method: 构建包含1300多个高分辨率拍摄文档的综合基准测试DocPTBench，涵盖多个领域和八种翻译场景，并提供人工验证的解析和翻译标注。

Result: 从数字原生文档切换到拍摄文档时，主流MLLMs在端到端解析中的准确率平均下降18%，翻译下降12%；专业文档解析模型平均下降25%。

Conclusion: 真实世界拍摄文档带来了独特挑战，现有模型的鲁棒性有限，DocPTBench填补了现有基准测试的空白。

Abstract: The advent of Multimodal Large Language Models (MLLMs) has unlocked the potential for end-to-end document parsing and translation. However, prevailing benchmarks such as OmniDocBench and DITrans are dominated by pristine scanned or digital-born documents, and thus fail to adequately represent the intricate challenges of real-world capture conditions, such as geometric distortions and photometric variations. To fill this gap, we introduce DocPTBench, a comprehensive benchmark specifically designed for Photographed Document Parsing and Translation. DocPTBench comprises over 1,300 high-resolution photographed documents from multiple domains, includes eight translation scenarios, and provides meticulously human-verified annotations for both parsing and translation. Our experiments demonstrate that transitioning from digital-born to photographed documents results in a substantial performance decline: popular MLLMs exhibit an average accuracy drop of 18% in end-to-end parsing and 12% in translation, while specialized document parsing models show significant average decrease of 25%. This substantial performance gap underscores the unique challenges posed by documents captured in real-world conditions and reveals the limited robustness of existing models. Dataset and code are available at https://github.com/Topdu/DocPTBench.

</details>


### [225] [When Generative Replay Meets Evolving Deepfakes: Domain-Aware Relative Weighting for Incremental Face Forgery Detection](https://arxiv.org/abs/2511.18436)
*Hao Shen,Jikang Cheng,Renye Yan,Zhongyuan Wang,Wei Peng,Baojin Huang*

Main category: cs.CV

TL;DR: 本文提出了一种用于增量伪造检测的领域感知相对加权策略，通过分析生成回放中的领域风险样本和领域安全样本，动态调整监督策略以提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于样本回放的增量伪造检测方法存在多样性低和隐私问题，生成回放虽能解决这些问题，但其在伪造检测中的可行性尚不明确。

Method: 提出领域感知相对加权策略，对领域安全样本直接监督，对领域风险样本应用相对分离损失，并通过领域混淆分数动态调整监督强度。

Result: 大量实验表明，该方法在不同生成回放设置下持续提升增量学习性能，并减轻了领域重叠的不利影响。

Conclusion: 生成回放在伪造检测中具有可行性，提出的领域感知相对加权策略能有效利用生成回放提升增量检测性能。

Abstract: The rapid advancement of face generation techniques has led to a growing variety of forgery methods. Incremental forgery detection aims to gradually update existing models with new forgery data, yet current sample replay-based methods are limited by low diversity and privacy concerns. Generative replay offers a potential solution by synthesizing past data, but its feasibility for forgery detection remains unclear. In this work, we systematically investigate generative replay and identify two scenarios: when the replay generator closely resembles the new forgery model, generated real samples blur the domain boundary, creating domain-risky samples; when the replay generator differs significantly, generated samples can be safely supervised, forming domain-safe samples. To exploit generative replay effectively, we propose a novel Domain-Aware Relative Weighting (DARW) strategy. DARW directly supervises domain-safe samples while applying a Relative Separation Loss to balance supervision and potential confusion for domain-risky samples. A Domain Confusion Score dynamically adjusts this tradeoff according to sample reliability. Extensive experiments demonstrate that DARW consistently improves incremental learning performance for forgery detection under different generative replay settings and alleviates the adverse impact of domain overlap.

</details>


### [226] [Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning](https://arxiv.org/abs/2511.18437)
*Chi Zhang,Haibo Qiu,Qiming Zhang,Yufei Xu,Zhixiong Zeng,Siqi Yang,Peng Shi,Lin Ma,Jing Zhang*

Main category: cs.CV

TL;DR: PEARL是一种双分支感知-推理协同方法，通过将多模态推理明确锚定到已验证的视觉证据来增强视觉语言模型的推理能力，解决了传统RLVR方法忽视视觉感知的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法仅验证最终文本输出，忽视了视觉感知这一基础步骤，导致视觉幻觉和奖励攻击问题。基于错误感知的推理本质上不可靠。

Method: PEARL为每个推理问题生成感知检查清单——一组可验证答案的感知导向子问题，用于探测模型对关键视觉证据的理解。训练时，通过感知奖励直接增强模型感知能力，并作为推理保真度门控。

Result: 在MathVerse等多模态推理基准测试中，PEARL相比基线提升9.7%，相比GRPO提升6.6%。

Conclusion: PEARL能够无缝集成到GRPO和DAPO等流行RL方法中，通过感知-推理协同显著提升多模态推理性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) and is now being applied to Vision-Language Models (VLMs). However, vanilla RLVR for VLMs verifies only the final textual output, critically neglecting the foundational step of visual perception. This oversight leads to visual hallucinations and reward hacking, as reasoning built upon flawed perception is inherently unreliable. To address this, we propose PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. For each reasoning-oriented QA instance, PEARL first derive a perception checklist -- a set of perception-oriented sub-questions with verifiable answers that probe the model's understanding of key visual evidence. During training, auxiliary rollouts on this checklist yield a perceptual reward that both directly reinforces the model's perception ability and acts as a fidelity gate for reasoning. If the model passes the perception check, its policy update is biased towards evidence-anchored reasoning. Otherwise, the process is halted to prevent reasoning from flawed premises. PEARL can be seamlessly integrated with popular RL methods like GRPO and DAPO. Comprehensive experiments show PEARL achieves substantial gains on multimodal reasoning benchmarks, e.g., a +9.7% improvement over the baseline and +6.6% over GRPO on MathVerse.

</details>


### [227] [ReCoGS: Real-time ReColoring for Gaussian Splatting scenes](https://arxiv.org/abs/2511.18441)
*Lorenzo Rutayisire,Nicola Capodieci,Fabio Pellacini*

Main category: cs.CV

TL;DR: 提出了一种用于高斯溅射场景的实时交互式重新着色方法，支持精确区域选择和实时编辑


<details>
  <summary>Details</summary>
Motivation: 现有基于2D扩散模型的多视图生成方法存在视图不一致、缺乏细粒度控制和高计算需求等问题，需要更高效精确的3D场景编辑方案

Method: 开发用户友好的流水线，支持在预训练高斯溅射场景中进行精确区域选择和重新着色，并提供交互式工具演示实时性能

Result: 实现了实时交互式重新着色功能，代码已在GitHub开源

Conclusion: 该方法为高斯溅射场景编辑提供了高效精确的重新着色解决方案

Abstract: Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions. Beyond view synthesis, this 3D representation has also been explored for editing tasks. Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand. In this work, we focus specifically on the editing task of recoloring. We introduce a user-friendly pipeline that enables precise selection and recoloring of regions within a pre-trained Gaussian Splatting scene. To demonstrate the real-time performance of our method, we also present an interactive tool that allows users to experiment with the pipeline in practice. Code is available at https://github.com/loryruta/recogs.

</details>


### [228] [SineProject: Machine Unlearning for Stable Vision Language Alignment](https://arxiv.org/abs/2511.18444)
*Arpit Garg,Hemanth Saratchandran,Simon Lucey*

Main category: cs.CV

TL;DR: 提出了SineProject方法，通过在冻结投影器中添加正弦调制参数来改善多模态大语言模型的知识遗忘过程，解决了现有方法导致的视觉语言对齐破坏问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型需要遗忘特定知识（如不安全或隐私信息）而不需要完全重新训练，但现有遗忘方法会破坏视觉语言对齐，导致模型拒绝有害和良性查询。

Method: 引入SineProject方法，在冻结的投影器网络中增加正弦调制的可训练参数，改善Jacobian矩阵的谱条件，稳定跨模态嵌入对齐。

Result: 在LLaVA v1.5 7B和13B模型的标准安全和隐私遗忘基准测试中，SineProject减少了良性查询拒绝，同时实现了目标信息的完全遗忘，在遗忘-保留权衡方面达到最先进水平，计算开销可忽略。

Conclusion: SineProject通过改善投影器网络的Jacobian条件，有效解决了多模态大语言模型知识遗忘过程中的对齐破坏问题，实现了更好的遗忘效果和模型性能平衡。

Abstract: Multimodal Large Language Models (MLLMs) increasingly need to forget specific knowledge such as unsafe or private information without requiring full retraining. However, existing unlearning methods often disrupt vision language alignment, causing models to reject both harmful and benign queries. We trace this failure to the projector network during unlearning, its Jacobian becomes severely illconditioned, leading to unstable optimization and drift in cross modal embeddings. We introduce SineProject, a simple method that augments the frozen projector with sinusoidally modulated trainable parameters, improving the Jacobian's spectral conditioning and stabilizing alignment throughout unlearning. Across standard safety and privacy unlearning benchmarks using LLaVA v1.5 7B and 13B, SineProject reduces benign query refusals while achieving complete forgetting of targeted information, yielding state of the art forget retain trade offs with negligible computational overhead.

</details>


### [229] [EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs](https://arxiv.org/abs/2511.18448)
*Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Xiangyang Ji*

Main category: cs.CV

TL;DR: EventBench是一个针对事件视觉多模态大语言模型的综合基准，包含8个多样化任务指标和大规模事件流数据集，用于全面评估模型在事件理解、识别和空间推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在事件视觉方面取得显著进展，但缺乏统一的综合评估基准来全面衡量其能力。

Method: 开发EventBench基准，具有四个关键特点：开放性（发布所有原始事件流和任务指令）、多样性（涵盖理解、识别和空间推理任务）、集成性（首创3D空间推理任务）和规模性（包含超过100万事件-文本对的训练集）。

Result: 评估了GPT-5、Gemini-2.5 Pro等闭源模型，Qwen2.5-VL、InternVL3等开源模型，以及EventGPT等事件专用模型。结果显示当前事件专用MLLM在事件流理解方面表现良好，但在细粒度识别和空间推理方面仍有困难。

Conclusion: EventBench为事件视觉MLLM提供了全面的评估框架，揭示了当前模型在细粒度识别和空间推理方面的局限性，为未来研究指明了方向。

Abstract: Multimodal large language models (MLLMs) have made significant advancements in event-based vision, yet the comprehensive evaluation of their capabilities within a unified benchmark remains largely unexplored. In this work, we introduce EventBench, a benchmark that offers eight diverse task metrics together with a large-scale event stream dataset. EventBench differs from existing event-based benchmarks in four key aspects: (1) openness in accessibility, releasing all raw event streams and task instructions across eight evaluation metrics; (2) diversity in task coverage, spanning understanding, recognition, and spatial reasoning tasks for comprehensive capability assessment; (3) integration in spatial dimensions, pioneering the design of 3D spatial reasoning tasks for event-based MLLMs; and (4) scale in data volume, with an accompanying training set of over one million event-text pairs supporting large-scale training and evaluation. Using EventBench, we evaluate state-of-the-art closed-source models such as GPT-5 and Gemini-2.5 Pro, leading open-source models including Qwen2.5-VL and InternVL3, and event-based MLLMs such as EventGPT that directly process raw event streams. Extensive evaluation reveals that while current event-based MLLMs demonstrate strong performance in event stream understanding, they continue to struggle with fine-grained recognition and spatial reasoning.

</details>


### [230] [NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering](https://arxiv.org/abs/2511.18452)
*Loick Chambon,Paul Couairon,Eloi Zablocki,Alexandre Boulch,Nicolas Thome,Matthieu Cord*

Main category: cs.CV

TL;DR: NAF是一种零射特征上采样方法，通过跨尺度邻域注意力和旋转位置编码学习自适应权重，无需重新训练即可提升任何视觉基础模型的特征分辨率，在多个下游任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有上采样方法面临基本权衡：经典滤波器快速通用但形式固定，现代上采样器准确但需要为每个VFM重新训练。需要一种既能保持高精度又无需重新训练的通用上采样方法。

Method: 提出邻域注意力滤波(NAF)，通过跨尺度邻域注意力和旋转位置编码学习空间和内容自适应权重，仅由高分辨率输入图像引导，实现零射特征上采样。

Result: NAF是首个超越VFM特定上采样器的VFM无关架构，在多个下游任务中达到最先进性能，高效支持2K特征图，以18FPS重建中间分辨率图，并在图像恢复任务中表现优异。

Conclusion: NAF成功解决了特征上采样的精度与通用性权衡问题，为视觉基础模型提供了无需重新训练的高性能上采样解决方案，具有广泛的应用潜力。

Abstract: Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.

</details>


### [231] [RegDeepLab: A Two-Stage Decoupled Framework for Interpretable Embryo Fragmentation Grading](https://arxiv.org/abs/2511.18454)
*Ming-Jhe Lee*

Main category: cs.CV

TL;DR: 提出RegDeepLab双分支多任务学习框架，结合语义分割和回归预测，解决胚胎碎片化程度评估中视觉可解释性与精确分级的平衡问题


<details>
  <summary>Details</summary>
Motivation: 当前IVF胚胎碎片化程度手动评估存在耗时、观察者间差异大和效率瓶颈问题，现有深度学习方案要么缺乏临床所需的视觉可解释性，要么无法直接转换为精确临床分级

Method: RegDeepLab双分支MTL框架，集成DeepLabV3+语义分割与多尺度回归头，采用两阶段解耦训练策略解决梯度冲突和负迁移问题，并引入范围损失进行半监督学习

Result: 端到端MTL训练可最小化分级误差(MAE=0.046)，但会损害分割边界完整性；解耦策略在保持SOTA级分割精度(Dice=0.729)的同时提供稳健高精度分级预测

Conclusion: 研究最终提出了兼具高精度与视觉可解释性的双模块临床辅助解决方案

Abstract: The degree of embryo fragmentation serves as a critical morphological indicator for assessing embryo developmental potential in In Vitro Fertilization (IVF) clinical decision-making. However, current manual grading processes are not only time-consuming but also limited by significant inter-observer variability and efficiency bottlenecks. Although deep learning has demonstrated potential in automated grading in recent years, existing solutions face a significant challenge: pure regression models lack the visual explainability required for clinical practice, while pure segmentation models struggle to directly translate pixel-level masks into precise clinical grades. This study proposes RegDeepLab, a dual-branch Multi-Task Learning (MTL) framework that integrates State-of-the-Art (SOTA) semantic segmentation (DeepLabV3+) with a multi-scale regression head. Addressing the common issues of "Gradient Conflict" and "Negative Transfer" in multi-task training, we propose a "Two-Stage Decoupled Training Strategy." Experimental results demonstrate that while standard end-to-end MTL training can minimize grading error (MAE=0.046) through our designed "Feature Injection" mechanism, it compromises the integrity of segmentation boundaries. In contrast, our decoupled strategy successfully provides robust and high-precision grading predictions while preserving SOTA-level segmentation accuracy (Dice=0.729). Furthermore, we introduce a "Range Loss" to effectively utilize large-scale discrete grading data for semi-supervised learning. This study ultimately presents a dual-module clinical auxiliary solution that combines high accuracy with visual explainability.

</details>


### [232] [Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding](https://arxiv.org/abs/2511.18463)
*Bowei Pu,Chuanbin Liu,Yifan Ge,Peichen Zhou,Yiwei Sun,Zhiyin Lu,Jiankang Wang,Hongtao Xie*

Main category: cs.CV

TL;DR: 提出Video-PLR框架，通过感知循环推理和反幻觉奖励机制解决视频推理中的感知不足和幻觉问题，在3B和7B参数规模上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理大语言模型存在感知捷径问题，采用单步感知范式存在证据不足和幻觉风险。

Method: 1. 感知循环推理(PLR)范式：分循环描述视频片段并分析，决定下一步动作；2. 事实感知评估器(FAE)：评估每个感知结果作为反幻觉奖励。

Result: 在3B和7B参数规模上达到最先进水平，具有最佳数据效率。FAE性能与GPT-4o相当。

Conclusion: Video-PLR框架有效解决了视频推理中的感知不足和幻觉问题，实现了更可靠的视频理解。

Abstract: Sufficient visual perception is the foundation of video reasoning. Nevertheless, existing Video Reasoning LLMs suffer from perception shortcuts, relying on a flawed single-step perception paradigm. This paradigm describes the video and then conducts reasoning, which runs the risk of insufficient evidence and emergent hallucinations. To address these issues, we introduce a new framework that integrates a loop-based paradigm with an anti-hallucination reward. First, to address the insufficient evidence, we introduce the Perception Loop Reasoning (PLR) paradigm. Instead of describing the video at once, each loop requires the model to describe a video segment with precise timestamps, analyze this segment, and decide the next action. Second, for the risk of hallucinations, the Factual-Aware Evaluator (FAE) evaluates each perception result as a reliable anti-hallucination reward. This reward encourages the model to provide sufficient and precise video evidence. Our FAE, which performs comparably to GPT-4o, is tuned on our AnetHallu-117K, a large-scale hallucination judgment preference dataset. Extensive experiments show that our Video-PLR achieves the state-of-the-art in both 3B and 7B parameter scales and has the best data efficiency. Our code, models, and datasets are released on: https://github.com/BoweiPu/VideoPLR.

</details>


### [233] [Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span](https://arxiv.org/abs/2511.18470)
*Heeseung Yun,Joonil Na,Jaeyeon Kim,Calvin Murdock,Gunhee Kim*

Main category: cs.CV

TL;DR: 提出EgoSpanLift方法，将自我中心视觉跨度预测从2D图像平面转换到3D场景，结合3D U-Net和单向变换器实现时空融合，在364.6K样本基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人类基于意图感知环境并指导行为，但现有研究主要关注运动和接触交互，视觉感知预测在AR/VR和辅助技术中具有重要应用价值。

Method: 将SLAM关键点转换为注视兼容几何，提取体积视觉跨度区域，结合3D U-Net和单向变换器进行时空融合，预测3D网格中的未来视觉跨度。

Result: 在自我中心2D注视预测和3D定位方面优于竞争基线，即使投影回2D图像平面也取得可比结果，无需额外2D特定训练。

Conclusion: EgoSpanLift成功将视觉跨度预测提升到3D空间，为自我中心视觉感知预测提供了有效解决方案。

Abstract: People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.

</details>


### [234] [Robust Posterior Diffusion-based Sampling via Adaptive Guidance Scale](https://arxiv.org/abs/2511.18471)
*Liav Hen,Tom Tirer,Raja Giryes,Shady Abu-Hussein*

Main category: cs.CV

TL;DR: 提出了AdaPS方法，通过自适应似然步长策略来平衡扩散先验和数据保真度，在多种图像逆问题中提升重建质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为生成先验在逆问题中面临挑战：过于激进的似然更新会引入伪影，而保守更新会减慢收敛或产生次优重建

Method: 开发了基于两个不同不可处理中间似然梯度近似之间一致性的观测依赖加权方案，自适应扩散调度、时间重采样和注入随机性

Result: 在CelebA-HQ和ImageNet-256验证集上的超分辨率、高斯去模糊和运动去模糊任务中，AdaPS在感知质量上持续超越现有扩散基线，失真损失最小或无损失

Conclusion: AdaPS是超参数无关的方法，对扩散步数、观测噪声水平和不同随机性具有鲁棒性，无需任务特定调优

Abstract: Diffusion models have recently emerged as powerful generative priors for solving inverse problems, achieving state-of-the-art results across various imaging tasks. A central challenge in this setting lies in balancing the contribution of the prior with the data fidelity term: overly aggressive likelihood updates may introduce artifacts, while conservative updates can slow convergence or yield suboptimal reconstructions. In this work, we propose an adaptive likelihood step-size strategy to guide the diffusion process for inverse-problem formulations. Specifically, we develop an observation-dependent weighting scheme based on the agreement between two different approximations of the intractable intermediate likelihood gradients, that adapts naturally to the diffusion schedule, time re-spacing, and injected stochasticity. The resulting approach, Adaptive Posterior diffusion Sampling (AdaPS), is hyperparameter-free and improves reconstruction quality across diverse imaging tasks - including super-resolution, Gaussian deblurring, and motion deblurring - on CelebA-HQ and ImageNet-256 validation sets. AdaPS consistently surpasses existing diffusion-based baselines in perceptual quality with minimal or no loss in distortion, without any task-specific tuning. Extensive ablation studies further demonstrate its robustness to the number of diffusion steps, observation noise levels, and varying stochasticity.

</details>


### [235] [Uncertainty Quantification in HSI Reconstruction using Physics-Aware Diffusion Priors and Optics-Encoded Measurements](https://arxiv.org/abs/2511.18473)
*Juan Romero,Qiang Fu,Matteo Ravasi,Wolfgang Heidrich*

Main category: cs.CV

TL;DR: HSDiff是一个基于贝叶斯推理的高光谱图像重建框架，使用无条件训练的像素级扩散先验和后验扩散采样，通过增强的metameric增强技术提高先验多样性，提供不确定性感知的重建。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的高光谱图像重建方法由于现有数据集缺乏光谱多样性，特别是在评估metamerism现象时存在幻觉问题，需要更好的不确定性建模。

Method: 将高光谱图像重建建模为贝叶斯推理问题，使用无条件训练的像素级扩散先验和后验扩散采样，提出基于区域的metameric黑和分区联合光谱上采样的增强metameric增强技术。

Result: HSDiff提供了完整的高性能不确定性感知HSI重建方法，证明了有效光谱编码在快照高光谱成像中的重要性。

Conclusion: 通过贝叶斯框架，HSDiff为高光谱图像重建提供了校准的信息不确定性，并强调了有效光谱编码的关键作用。

Abstract: Hyperspectral image reconstruction from a compressed measurement is a highly ill-posed inverse problem. Current data-driven methods suffer from hallucination due to the lack of spectral diversity in existing hyperspectral image datasets, particularly when they are evaluated for the metamerism phenomenon. In this work, we formulate hyperspectral image (HSI) reconstruction as a Bayesian inference problem and propose a framework, HSDiff, that utilizes an unconditionally trained, pixel-level diffusion prior and posterior diffusion sampling to generate diverse HSI samples consistent with the measurements of various hyperspectral image formation models. We propose an enhanced metameric augmentation technique using region-based metameric black and partition-of-union spectral upsampling to expand training with physically valid metameric spectra, strengthening the prior diversity and improving uncertainty calibration. We utilize HSDiff to investigate how the studied forward models shape the posterior distribution and demonstrate that guiding with effective spectral encoding provides calibrated informative uncertainty compared to non-encoded models. Through the lens of the Bayesian framework, HSDiff offers a complete, high-performance method for uncertainty-aware HSI reconstruction. Our results also reiterate the significance of effective spectral encoding in snapshot hyperspectral imaging.

</details>


### [236] [Extreme Model Compression for Edge Vision-Language Models: Sparse Temporal Token Fusion and Adaptive Neural Compression](https://arxiv.org/abs/2511.18504)
*Md Tasnin Tanvir,Soumitra Das,Sk Md Abidar Rahaman,Ali Shiri Sichani*

Main category: cs.CV

TL;DR: 提出了两种自适应压缩技术STTF和ANC，将算法创新与硬件感知优化相结合，在边缘设备上实现高效的视觉语言任务处理。


<details>
  <summary>Details</summary>
Motivation: 边缘AI对视觉语言任务的需求需要能够在资源受限设备上实现实时性能的模型，这些设备具有有限的功耗和内存。

Method: STTF通过事件驱动的变化检测动态重用视觉token，ANC通过学习的路由器有条件地激活编码器分支，实现细粒度的场景复杂度适应。

Result: 3B参数的TinyGPT-STTF在COCO 2017测试集上达到CIDEr 131.2，超越LLaVA-1.5 7B 17.6个CIDEr点，同时使用2.3倍更少的参数和62倍更少的设备端FLOPs。STTF在事件视觉任务中减少84%的平均token数量，保持95.6%的准确率。

Conclusion: 这些成果使得能够在真实世界边缘设备上高效部署有能力的视觉语言模型。

Abstract: The demand for edge AI in vision-language tasks requires models that achieve real-time performance on resource-constrained devices with limited power and memory. This paper proposes two adaptive compression techniques -- Sparse Temporal Token Fusion (STTF) and Adaptive Neural Compression (ANC) -- that integrate algorithmic innovations with hardware-aware optimizations. Unlike previous approaches relying on static pruning or uniform scaling, STTF dynamically reuses visual tokens through event-driven change detection, while ANC conditionally activates encoder branches via a learned router, enabling fine-grained adaptation to scene complexity. Our 3B-parameter TinyGPT-STTF achieves CIDEr 131.2, BLEU-4 0.38, METEOR 0.31, and ROUGE-L 0.56 on the COCO 2017 test set, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer on-device FLOPs. TinyGPT-ANC reaches CIDEr 128.5. On event-based vision tasks, STTF reduces average token count by 84% (from 196 to 31 tokens) while preserving 95.6% accuracy on the DVS128 Gesture dataset, and ANC cuts FLOPs by up to 90% in low-motion scenes. Compared to strong baselines, our models improve accuracy by up to 4.4% and reduce latency by up to 13x. These results enable efficient deployment of capable vision-language models on real-world edge devices.

</details>


### [237] [Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives](https://arxiv.org/abs/2511.18507)
*Kai Jiang,Siqi Huang,Xiangyu Chen,Jiawei Shao,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了UNIFIER方法来解决多模态大语言模型在持续学习中的灾难性遗忘问题，特别是在不同场景视角下的视觉理解任务中。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在部署到设备上时需要持续适应动态场景变化，如背景和视角的变化，以有效执行复杂的视觉任务。现有方法在面对真实世界数据流中的场景转换时存在灾难性遗忘问题。

Method: 提出了UNIFIER方法，将不同场景的视觉信息解耦到每个视觉块中的不同分支，并将它们投影到相同的特征空间中。通过一致性约束来保持跨场景视觉表示的稳定性。

Result: 在MSVQA数据集上的大量实验表明，UNIFIER有效缓解了跨场景任务的遗忘问题，并在同一场景内实现了知识积累。

Conclusion: UNIFIER方法能够有效解决多模态大语言模型在持续学习中的灾难性遗忘问题，特别是在处理不同场景视角变化时表现出色。

Abstract: Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.

</details>


### [238] [LRDUN: A Low-Rank Deep Unfolding Network for Efficient Spectral Compressive Imaging](https://arxiv.org/abs/2511.18513)
*He Huang,Yujun Guo,Wei He*

Main category: cs.CV

TL;DR: 提出LRDUN方法，通过低秩分解和深度展开网络解决光谱压缩成像重建问题，显著降低计算成本并提升重建质量


<details>
  <summary>Details</summary>
Motivation: 现有深度展开网络直接处理高维HSI数据，存在计算冗余和从2D残差映射到3D HSI空间的病态问题

Method: 提出两种结合低秩分解的成像模型，开发LRDUN网络在展开近端梯度下降框架中联合求解两个子问题，引入广义特征展开机制

Result: 在模拟和真实数据集上实现最先进的重建质量，同时显著降低计算成本

Conclusion: LRDUN通过低秩分解有效缓解了重建的病态问题，在保持高质量重建的同时大幅提升了计算效率

Abstract: Deep unfolding networks (DUNs) have achieved remarkable success and become the mainstream paradigm for spectral compressive imaging (SCI) reconstruction. Existing DUNs are derived from full-HSI imaging models, where each stage operates directly on the high-dimensional HSI, refining the entire data cube based on the single 2D coded measurement. However, this paradigm leads to computational redundancy and suffers from the ill-posed nature of mapping 2D residuals back to 3D space of HSI. In this paper, we propose two novel imaging models corresponding to the spectral basis and subspace image by explicitly integrating low-rank (LR) decomposition with the sensing model. Compared to recovering the full HSI, estimating these compact low-dimensional components significantly mitigates the ill-posedness. Building upon these novel models, we develop the Low-Rank Deep Unfolding Network (LRDUN), which jointly solves the two subproblems within an unfolded proximal gradient descent (PGD) framework. Furthermore, we introduce a Generalized Feature Unfolding Mechanism (GFUM) that decouples the physical rank in the data-fidelity term from the feature dimensionality in the prior module, enhancing the representational capacity and flexibility of the network. Extensive experiments on simulated and real datasets demonstrate that the proposed LRDUN achieves state-of-the-art (SOTA) reconstruction quality with significantly reduced computational cost.

</details>


### [239] [Unified Deep Learning Platform for Dust and Fault Diagnosis in Solar Panels Using Thermal and Visual Imaging](https://arxiv.org/abs/2511.18514)
*Abishek Karthik,Sreya Mynampati,Pandiyaraju V*

Main category: cs.CV

TL;DR: 开发了一个集中式平台，用于检测太阳能电池板上的灰尘和故障，结合CNN、ResNet和KerNet模型，通过图像处理和热成像分析实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 太阳能电池板输出受多种因素影响，如灰尘、故障等，需要有效的检测系统来维护面板效率和性能。

Method: 使用伽马去除和高斯滤波预处理图像，结合CNN、ResNet和带自注意力机制的KerNet模型进行分类，通过功率输出、正弦波、电压等参数检测灰尘和故障。

Result: 模型在检测灰尘和故障方面表现出比现有模型更高的效率和准确性。

Conclusion: 该多应用模型在检测太阳能电池板灰尘和故障方面高效且优化，适用于从家庭到大型太阳能农场的各种规模需求。

Abstract: Solar energy is one of the most abundant and tapped sources of renewable energies with enormous future potential. Solar panel output can vary widely with factors like intensity, temperature, dirt, debris and so on affecting it. We have implemented a model on detecting dust and fault on solar panels. These two applications are centralized as a single-platform and can be utilized for routine-maintenance and any other checks. These are checked against various parameters such as power output, sinusoidal wave (I-V component of solar cell), voltage across each solar cell and others. Firstly, we filter and preprocess the obtained images using gamma removal and Gaussian filtering methods alongside some predefined processes like normalization. The first application is to detect whether a solar cell is dusty or not based on various pre-determined metrics like shadowing, leaf, droppings, air pollution and from other human activities to extent of fine-granular solar modules. The other one is detecting faults and other such occurrences on solar panels like faults, cracks, cell malfunction using thermal imaging application. This centralized platform can be vital since solar panels have different efficiency across different geography (air and heat affect) and can also be utilized for small-scale house requirements to large-scale solar farm sustentation effectively. It incorporates CNN, ResNet models that with self-attention mechanisms-KerNet model which are used for classification and results in a fine-tuned system that detects dust or any fault occurring. Thus, this multi-application model proves to be efficient and optimized in detecting dust and faults on solar panels. We have performed various comparisons and findings that demonstrates that our model has better efficiency and accuracy results overall than existing models.

</details>


### [240] [Breaking Forgetting: Training-Free Few-Shot Class-Incremental Learning via Conditional Diffusion](https://arxiv.org/abs/2511.18516)
*Haidong Kang,Ketong Qian,Yi Lu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的少样本类增量学习（FSCIL）框架CD-FSCIL，通过条件扩散过程替代传统的梯度优化，有效缓解灾难性遗忘并显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统FSCIL方法依赖梯度优化，随着新类别增加会导致训练成本爆炸性增长，且在极少量样本下不仅造成基础类灾难性遗忘，还阻碍新类适应。

Method: 基于条件扩散过程与梯度优化的内在联系，提出CD-FSCIL框架，用扩散生成转换替代梯度更新；结合LLM自动生成文本描述的多模态学习策略增强少样本下的表示能力。

Result: 在主流FSCIL基准测试中达到最先进性能，同时大幅降低计算和内存开销。

Conclusion: 该方法实现了向无需训练持续适应的范式转变，为FSCIL提供了高效解决方案。

Abstract: Efforts to overcome catastrophic forgetting in Few-Shot Class-Incremental Learning (FSCIL) have primarily focused on developing more effective gradient-based optimization strategies. In contrast, little attention has been paid to the training cost explosion that inevitably arises as the number of novel classes increases, a consequence of relying on gradient learning even under extreme data scarcity. More critically, since FSCIL typically provides only a few samples for each new class, gradient-based updates not only induce severe catastrophic forgetting on base classes but also hinder adaptation to novel ones. This paper seeks to break this long-standing limitation by asking: Can we design a training-free FSCIL paradigm that entirely removes gradient optimization? We provide an affirmative answer by uncovering an intriguing connection between gradient-based optimization and the Conditional Diffusion process. Building on this observation, we propose a Conditional Diffusion-driven FSCIL (CD-FSCIL) framework that substitutes the conventional gradient update process with a diffusion-based generative transition, enabling training-free incremental adaptation while effectively mitigating forgetting. Furthermore, to enhance representation under few-shot constraints, we introduce a multimodal learning strategy that integrates visual features with natural language descriptions automatically generated by Large Language Models (LLMs). This synergy substantially alleviates the sample scarcity issue and improves generalization across novel classes. Extensive experiments on mainstream FSCIL benchmarks demonstrate that our method not only achieves state-of-the-art performance but also drastically reduces computational and memory overhead, marking a paradigm shift toward training-free continual adaptation.

</details>


### [241] [DE-KAN: A Kolmogorov Arnold Network with Dual Encoder for accurate 2D Teeth Segmentation](https://arxiv.org/abs/2511.18533)
*Md Mizanur Rahman Mustakim,Jianwu Li,Sumya Bhuiyan,Mohammad Mehedi Hasan,Bing Han*

Main category: cs.CV

TL;DR: 提出DE-KAN模型，通过双编码器和KAN瓶颈层提升全景X光片中牙齿分割精度，在多个指标上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 全景X光片中牙齿分割面临解剖变异、不规则形状和结构重叠等挑战，传统深度学习模型性能受限

Method: 使用ResNet-18编码器处理增强输入，定制CNN编码器处理原始输入，通过KAN瓶颈层融合全局和局部特征，利用Kolmogorov Arnold表示定理的非线性可学习激活函数

Result: 在两个牙科X光数据集上，mIoU达94.5%，Dice系数97.1%，准确率98.91%，召回率97.36%，Dice系数比现有方法提升4.7%

Conclusion: DE-KAN模型通过双编码器特征互补和KAN非线性激活，显著提升了牙齿分割性能，为牙科影像分析提供了有效解决方案

Abstract: Accurate segmentation of individual teeth from panoramic radiographs remains a challenging task due to anatomical variations, irregular tooth shapes, and overlapping structures. These complexities often limit the performance of conventional deep learning models. To address this, we propose DE-KAN, a novel Dual Encoder Kolmogorov Arnold Network, which enhances feature representation and segmentation precision. The framework employs a ResNet-18 encoder for augmented inputs and a customized CNN encoder for original inputs, enabling the complementary extraction of global and local spatial features. These features are fused through KAN-based bottleneck layers, incorporating nonlinear learnable activation functions derived from the Kolmogorov Arnold representation theorem to improve learning capacity and interpretability. Extensive experiments on two benchmark dental X-ray datasets demonstrate that DE-KAN outperforms state-of-the-art segmentation models, achieving mIoU of 94.5%, Dice coefficient of 97.1%, accuracy of 98.91%, and recall of 97.36%, representing up to +4.7% improvement in Dice compared to existing methods.

</details>


### [242] [HiFi-MambaV2: Hierarchical Shared-Routed MoE for High-Fidelity MRI Reconstruction](https://arxiv.org/abs/2511.18534)
*Pengcheng Fang,Hongli Chen,Guangzhen Yao,Jian Shi,Fangfang Tang,Xiaohao Cai,Shanshan Shan,Feng Liu*

Main category: cs.CV

TL;DR: HiFi-MambaV2是一个用于MRI重建的分层混合专家Mamba架构，通过频率分解和内容自适应计算实现高质量图像重建


<details>
  <summary>Details</summary>
Motivation: 从欠采样的k空间数据重建高保真MRI图像需要恢复高频细节同时保持解剖一致性

Method: 使用可分频率一致性拉普拉斯金字塔进行频率分解，结合分层共享路由MoE进行像素级稀疏调度，并融合全局上下文路径到数据一致性正则化主干网络

Result: 在多个数据集和加速因子下，在PSNR、SSIM和NMSE指标上持续优于CNN、Transformer和现有Mamba基线模型

Conclusion: HiFi-MambaV2实现了可靠且鲁棒的MRI重建，在高频细节和整体结构保真度方面表现优异

Abstract: Reconstructing high-fidelity MR images from undersampled k-space data requires recovering high-frequency details while maintaining anatomical coherence. We present HiFi-MambaV2, a hierarchical shared-routed Mixture-of-Experts (MoE) Mamba architecture that couples frequency decomposition with content-adaptive computation. The model comprises two core components: (i) a separable frequency-consistent Laplacian pyramid (SF-Lap) that delivers alias-resistant, stable low- and high-frequency streams; and (ii) a hierarchical shared-routed MoE that performs per-pixel top-1 sparse dispatch to shared experts and local routers, enabling effective specialization with stable cross-depth behavior. A lightweight global context path is fused into an unrolled, data-consistency-regularized backbone to reinforce long-range reasoning and preserve anatomical coherence. Evaluated on fastMRI, CC359, ACDC, M4Raw, and Prostate158, HiFi-MambaV2 consistently outperforms CNN-, Transformer-, and prior Mamba-based baselines in PSNR, SSIM, and NMSE across single- and multi-coil settings and multiple acceleration factors, consistently surpassing consistent improvements in high-frequency detail and overall structural fidelity. These results demonstrate that HiFi-MambaV2 enables reliable and robust MRI reconstruction.

</details>


### [243] [Zero-Shot Video Deraining with Video Diffusion Models](https://arxiv.org/abs/2511.18537)
*Tuomas Varanka,Juan Luis Gonzalez,Hyeongwoo Kim,Pablo Garrido,Xu Yao*

Main category: cs.CV

TL;DR: 提出了首个零样本视频去雨方法，无需合成数据或模型微调，利用预训练的文本到视频扩散模型，通过负提示和注意力切换机制去除动态场景中的雨滴。


<details>
  <summary>Details</summary>
Motivation: 现有视频去雨方法依赖合成数据或静态相机数据，难以泛化到真实世界的动态场景；扩散模型微调会削弱生成先验，限制泛化能力。

Method: 将输入视频反转到扩散模型的潜在空间，通过负提示干预重建过程去除雨滴概念，并引入注意力切换机制保持动态背景和结构一致性。

Result: 在真实世界雨数据集上的广泛实验显示，相比先前方法有显著改进，展示了无需监督训练的鲁棒泛化能力。

Conclusion: 该方法成功实现了零样本视频去雨，在动态复杂场景中表现出色，为视频恢复任务提供了新的解决方案。

Abstract: Existing video deraining methods are often trained on paired datasets, either synthetic, which limits their ability to generalize to real-world rain, or captured by static cameras, which restricts their effectiveness in dynamic scenes with background and camera motion. Furthermore, recent works in fine-tuning diffusion models have shown promising results, but the fine-tuning tends to weaken the generative prior, limiting generalization to unseen cases. In this paper, we introduce the first zero-shot video deraining method for complex dynamic scenes that does not require synthetic data nor model fine-tuning, by leveraging a pretrained text-to-video diffusion model that demonstrates strong generalization capabilities. By inverting an input video into the latent space of diffusion models, its reconstruction process can be intervened and pushed away from the model's concept of rain using negative prompting. At the core of our approach is an attention switching mechanism that we found is crucial for maintaining dynamic backgrounds as well as structural consistency between the input and the derained video, mitigating artifacts introduced by naive negative prompting. Our approach is validated through extensive experiments on real-world rain datasets, demonstrating substantial improvements over prior methods and showcasing robust generalization without the need for supervised training.

</details>


### [244] [C3Po: Cross-View Cross-Modality Correspondence by Pointmap Prediction](https://arxiv.org/abs/2511.18559)
*Kuan Wei Huang,Brandon Li,Bharath Hariharan,Noah Snavely*

Main category: cs.CV

TL;DR: 本文提出了C3数据集，用于解决地面照片与平面图之间的跨模态对应关系预测问题，并展示了在该任务上现有方法的局限性以及使用新数据集带来的显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有几何模型在处理不同视角（如航拍与地面）或不同模态（如照片与抽象绘图）的输入时表现不佳，特别是在地面照片与平面图对应关系预测这一挑战性问题上。现有数据集在模态多样性或对应关系标注方面存在不足。

Method: 通过从互联网照片集合中利用运动结构重建3D场景，然后手动将重建结果与从互联网收集的平面图进行配准，从而推导出图像与平面图之间的对应关系，创建了包含90K对平面图和照片的C3数据集。

Result: C3数据集包含597个场景的90K对平面图和照片，具有1.53亿像素级对应关系和85K相机姿态。在该任务上，最先进的对应模型表现不佳，但使用新数据训练后，最佳方法的RMSE提高了34%。

Conclusion: C3数据集有助于解决跨模态几何推理中的开放挑战，揭示了现有方法在照片与平面图对应关系预测任务上的局限性，并为改进提供了有价值的数据支持。

Abstract: Geometric models like DUSt3R have shown great advances in understanding the geometry of a scene from pairs of photos. However, they fail when the inputs are from vastly different viewpoints (e.g., aerial vs. ground) or modalities (e.g., photos vs. abstract drawings) compared to what was observed during training. This paper addresses a challenging version of this problem: predicting correspondences between ground-level photos and floor plans. Current datasets for joint photo--floor plan reasoning are limited, either lacking in varying modalities (VIGOR) or lacking in correspondences (WAFFLE). To address these limitations, we introduce a new dataset, C3, created by first reconstructing a number of scenes in 3D from Internet photo collections via structure-from-motion, then manually registering the reconstructions to floor plans gathered from the Internet, from which we can derive correspondence between images and floor plans. C3 contains 90K paired floor plans and photos across 597 scenes with 153M pixel-level correspondences and 85K camera poses. We find that state-of-the-art correspondence models struggle on this task. By training on our new data, we can improve on the best performing method by 34% in RMSE. We also identify open challenges in cross-modal geometric reasoning that our dataset aims to help address.

</details>


### [245] [PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation](https://arxiv.org/abs/2511.18570)
*Samarth Chopra,Jing Liang,Gershom Seneviratne,Dinesh Manocha*

Main category: cs.CV

TL;DR: PhysGS是一个基于贝叶斯推理的3D高斯泼溅扩展方法，能够从视觉线索和视觉-语言先验中估计密集的逐点物理属性，包括摩擦、刚度、硬度和材料组成等。


<details>
  <summary>Details</summary>
Motivation: 现有的3D重建方法主要关注几何和外观，无法推断底层的物理属性，而理解这些物理属性对于机器人安全有效地与环境交互至关重要。

Method: 将属性估计建模为高斯泼溅上的贝叶斯推理，其中材料和属性信念随着新观察的到来而迭代优化，同时建模了任意性和认知不确定性。

Result: 在物体尺度、室内和室外真实世界数据集上，PhysGS相比确定性基线方法，质量估计准确率提升高达22.8%，肖氏硬度误差降低高达61.2%，动摩擦误差降低高达18.1%。

Conclusion: PhysGS在单一的空间连续框架中统一了3D重建、不确定性建模和物理推理，实现了密集物理属性估计。

Abstract: Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.

</details>


### [246] [Zero-Reference Joint Low-Light Enhancement and Deblurring via Visual Autoregressive Modeling with VLM-Derived Modulation](https://arxiv.org/abs/2511.18591)
*Wei Dong,Han Zhou,Junwei Lin,Jun Chen*

Main category: cs.CV

TL;DR: 提出基于视觉自回归建模和视觉语言模型感知先验的生成框架，用于解决真实世界暗光图像的复杂噪声、模糊和低对比度问题，无需配对数据即可实现最先进的恢复性能。


<details>
  <summary>Details</summary>
Motivation: 真实世界暗光图像不仅存在低可见度和对比度问题，还包含复杂噪声和模糊，现有方法依赖配对数据或无法建模动态光照和模糊特性，导致泛化能力差。

Method: 采用视觉自回归建模框架，结合视觉语言模型的感知先验：1）基于VLM可见度分数的自适应曲线估计调节光照；2）集成动态和空间频率感知的旋转位置编码增强模糊结构建模；3）基于VLM模糊分数的有界迭代精化的递归相位域调制策略。

Result: 该框架完全无监督，在基准数据集上实现了最先进的性能表现。

Conclusion: 提出的生成框架通过视觉自回归建模和视觉语言模型感知先验的协同，有效解决了暗光图像恢复中的复杂退化问题，无需配对数据即可获得优异性能。

Abstract: Real-world dark images commonly exhibit not only low visibility and contrast but also complex noise and blur, posing significant restoration challenges. Existing methods often rely on paired data or fail to model dynamic illumination and blur characteristics, leading to poor generalization. To tackle this, we propose a generative framework based on visual autoregressive (VAR) modeling, guided by perceptual priors from the vision-language model (VLM). Specifically, to supply informative conditioning cues for VAR models, we deploy an adaptive curve estimation scheme to modulate the diverse illumination based on VLM-derived visibility scores. In addition, we integrate dynamic and spatial-frequency-aware Rotary Positional Encodings (SF-RoPE) into VAR to enhance its ability to model structures degraded by blur. Furthermore, we propose a recursive phase-domain modulation strategy that mitigates blur-induced artifacts in the phase domain via bounded iterative refinement guided by VLM-assessed blur scores. Our framework is fully unsupervised and achieves state-of-the-art performance on benchmark datasets.

</details>


### [247] [Stage-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up MRI](https://arxiv.org/abs/2511.18595)
*Wenhao Guo,Golrokh Mirzaei*

Main category: cs.CV

TL;DR: 本文首次对胶质母细胞瘤随访MRI的深度学习模型进行了分期特异性基准测试，发现在第二次随访时模型性能更好，Mamba+CNN混合模型在准确性和效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 区分胶质母细胞瘤的真实进展与治疗相关假性进展具有挑战性，特别是在早期随访阶段，需要建立分期感知的基准测试方法。

Method: 使用Burdenko GBM进展队列(n=180)，在统一的质量控制驱动流程下训练11个代表性深度学习家族，采用患者级交叉验证，独立分析不同放疗后扫描时间点。

Result: 两个阶段的准确率相当(~0.70-0.74)，但第二次随访时的区分度更好，F1和AUC值提高。Mamba+CNN混合模型提供最佳准确率-效率权衡，而transformer变体在计算成本显著更高的情况下提供竞争性AUC。

Conclusion: 研究建立了分期感知基准，表明绝对区分度总体仍有限，反映了TP与PsP区分的固有难度，未来工作需要整合纵向建模、多序列MRI和更大的多中心队列。

Abstract: Differentiating true tumor progression (TP) from treatment-related pseudoprogression (PsP) in glioblastoma remains challenging, especially at early follow-up. We present the first stage-specific, cross-sectional benchmarking of deep learning models for follow-up MRI using the Burdenko GBM Progression cohort (n = 180). We analyze different post-RT scans independently to test whether architecture performance depends on time-point. Eleven representative DL families (CNNs, LSTMs, hybrids, transformers, and selective state-space models) were trained under a unified, QC-driven pipeline with patient-level cross-validation. Across both stages, accuracies were comparable (~0.70-0.74), but discrimination improved at the second follow-up, with F1 and AUC increasing for several models, indicating richer separability later in the care pathway. A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off, while transformer variants delivered competitive AUCs at substantially higher computational cost and lightweight CNNs were efficient but less reliable. Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols. Notably, absolute discrimination remained modest overall, reflecting the intrinsic difficulty of TP vs. PsP and the dataset's size imbalance. These results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts.

</details>


### [248] [NeAR: Coupled Neural Asset-Renderer Stack](https://arxiv.org/abs/2511.18600)
*Hong Li,Chongjie Ye,Houyuan Chen,Weiqing Xiao,Ziyang Yan,Lixing Xiao,Zhaoxi Chen,Jianfeng Xiang,Shaocong Xu,Xuhui Liu,Yikai Wang,Baochang Zhang,Xiaoguang Han,Jiaolong Yang,Hao Zhao*

Main category: cs.CV

TL;DR: NeAR是一个耦合的神经资产-渲染器堆栈，通过联合设计资产表示和神经渲染器，实现了端到端可学习的图形管线，在保真度、一致性和效率方面都有优势。


<details>
  <summary>Details</summary>
Motivation: 神经资产创作和神经渲染目前是分离的两个领域，作者认为将它们耦合设计可以解锁端到端可学习的图形堆栈，带来保真度、一致性和效率方面的优势。

Method: 在资产侧使用Trellis风格的3D结构化潜空间和光照均匀化神经资产，通过整流流主干从普通光照输入预测光照均匀化的SLAT；在渲染器侧设计光照感知神经渲染器，使用神经资产、显式视角嵌入和HDR环境贴图实现实时可重光照渲染。

Result: NeAR在四个任务上验证：基于G-buffer的前向渲染、随机光照单图像重建、未知光照单图像重光照、新视角重光照，在定量指标和感知质量上都超越了现有最佳基线方法。

Conclusion: 这种耦合的资产-渲染器视角为未来图形堆栈提供了新思路，应将神经资产和渲染器视为协同设计的组件而非独立实体。

Abstract: Neural asset authoring and neural rendering have emerged as fundamentally disjoint threads: one generates digital assets using neural networks for traditional graphics pipelines, while the other develops neural renderers that map conventional assets to images. However, the potential of jointly designing the asset representation and renderer remains largely unexplored. We argue that coupling them can unlock an end-to-end learnable graphics stack with benefits in fidelity, consistency, and efficiency. In this paper, we explore this possibility with NeAR: a Coupled Neural Asset-Renderer Stack. On the asset side, we build on Trellis-style Structured 3D Latents and introduce a lighting-homogenized neural asset: from a casually lit input, a rectified-flow backbone predicts a Lighting-Homogenized SLAT that encodes geometry and intrinsic material cues in a compact, view-agnostic latent. On the renderer side, we design a lighting-aware neural renderer that uses this neural asset, along with explicit view embeddings and HDR environment maps, to achieve real-time, relightable rendering. We validate NeAR on four tasks: (1) G-buffer-based forward rendering, (2) random-lit single-image reconstruction, (3) unknown-lit single-image relighting, and (4) novel-view relighting. Our coupled stack surpasses state-of-the-art baselines in both quantitative metrics and perceptual quality. We hope this coupled asset-renderer perspective inspires future graphics stacks that view neural assets and renderers as co-designed components instead of independent entities.

</details>


### [249] [RigAnyFace: Scaling Neural Facial Mesh Auto-Rigging with Unlabeled Data](https://arxiv.org/abs/2511.18601)
*Wenchao Ma,Dario Kneubuehler,Maurice Chu,Ian Sachs,Haomiao Jiang,Sharon Xiaolei Huang*

Main category: cs.CV

TL;DR: RigAnyFace (RAF) 是一个可扩展的神经自动绑定框架，能够处理多种拓扑结构的面部网格，包括具有多个断开组件的网格。该框架通过2D监督策略增强泛化能力，支持眼球等断开组件的详细表情动画。


<details>
  <summary>Details</summary>
Motivation: 传统手动绑定成本高昂且数据集有限，限制了模型的泛化能力。需要一种能够处理多样化拓扑结构并增强泛化性的自动绑定方法。

Method: 使用三角化无关的表面学习网络，结合定制架构设计来处理FACS参数和断开组件。采用2D监督策略对未标记的中性网格进行训练，增加数据多样性。

Result: RAF能够在艺术家制作的资产和野外样本上对各种拓扑结构的网格进行绑定，在准确性和泛化性方面优于先前工作，并支持眼球等断开组件。

Conclusion: RAF提供了一个可扩展且通用的面部自动绑定解决方案，能够处理复杂拓扑结构，并通过数据增强策略显著提升模型的泛化能力。

Abstract: In this paper, we present RigAnyFace (RAF), a scalable neural auto-rigging framework for facial meshes of diverse topologies, including those with multiple disconnected components. RAF deforms a static neutral facial mesh into industry-standard FACS poses to form an expressive blendshape rig. Deformations are predicted by a triangulation-agnostic surface learning network augmented with our tailored architecture design to condition on FACS parameters and efficiently process disconnected components. For training, we curated a dataset of facial meshes, with a subset meticulously rigged by professional artists to serve as accurate 3D ground truth for deformation supervision. Due to the high cost of manual rigging, this subset is limited in size, constraining the generalization ability of models trained exclusively on it. To address this, we design a 2D supervision strategy for unlabeled neutral meshes without rigs. This strategy increases data diversity and allows for scaled training, thereby enhancing the generalization ability of models trained on this augmented data. Extensive experiments demonstrate that RAF is able to rig meshes of diverse topologies on not only our artist-crafted assets but also in-the-wild samples, outperforming previous works in accuracy and generalizability. Moreover, our method advances beyond prior work by supporting multiple disconnected components, such as eyeballs, for more detailed expression animation. Project page: https://wenchao-m.github.io/RigAnyFace.github.io

</details>


### [250] [Functional Localization Enforced Deep Anomaly Detection Using Fundus Images](https://arxiv.org/abs/2511.18627)
*Jan Benedikt Ruhland,Thorsten Papenbrock,Jan-Peter Sowa,Ali Canbay,Nicole Eter,Bernd Freisleben,Dominik Heider*

Main category: cs.CV

TL;DR: 本研究系统评估了Vision Transformer在眼底图像视网膜疾病检测中的性能，结合多种数据增强策略，在多个数据集上获得稳定优异的表现，同时开发了基于GANomaly的异常检测器提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 眼底图像视网膜疾病检测面临成像质量差异、早期病变细微表现以及数据集间域偏移等挑战，需要开发可靠且可泛化的检测方法。

Method: 使用Vision Transformer分类器，结合几何增强、颜色增强、直方图均衡化等多种数据增强策略，在多个公共数据集和自建AEyeDB数据集上进行评估，并开发GANomaly异常检测器提供可解释性。

Result: ViT在不同数据集和疾病上表现稳定，准确率0.789-0.843，糖尿病视网膜病变和年龄相关性黄斑变性检测可靠，青光眼最易误诊。几何和颜色增强效果最稳定，直方图均衡化对结构细微病变有益。在Papila数据集上AUC达0.91，优于卷积集成基线。GANomaly异常检测器AUC为0.76。

Conclusion: Transformer架构和多数据集训练具有优势，结合概率校准可为未来临床应用提供阈值无关的决策支持，异常检测器提供重建可解释性和对未见数据的鲁棒泛化能力。

Abstract: Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings.
  On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.

</details>


### [251] [Health system learning achieves generalist neuroimaging models](https://arxiv.org/abs/2511.18640)
*Akhil Kondepudi,Akshay Rao,Chenhui Zhao,Yiwei Lyu,Samir Harake,Soumyanil Banerjee,Rushikesh Joshi,Anna-Katharina Meissner,Renly Hou,Cheng Jiang,Asadur Chowdury,Ashok Srinivasan,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: NeuroVFM是一个基于医疗系统数据的视觉基础模型，通过在524万临床MRI和CT扫描上训练，实现了神经影像任务的先进性能，超越了前沿AI模型。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型缺乏对私有临床数据的访问，神经影像在公共领域代表性不足，限制了在临床医学中的性能表现。

Method: 采用可扩展的体素联合嵌入预测架构，在524万临床MRI和CT体积数据上进行训练，通过轻量级视觉指令调优与开源语言模型配对。

Result: 在放射诊断和报告生成等多项临床任务中达到最先进性能，生成比前沿模型更准确的放射学报告，减少幻觉发现和关键错误。

Conclusion: 医疗系统学习是构建通用医学AI的可行范式，为临床基础模型提供了可扩展框架。

Abstract: Frontier artificial intelligence (AI) models, such as OpenAI's GPT-5 and Meta's DINOv3, have advanced rapidly through training on internet-scale public data, yet such systems lack access to private clinical data. Neuroimaging, in particular, is underrepresented in the public domain due to identifiable facial features within MRI and CT scans, fundamentally restricting model performance in clinical medicine. Here, we show that frontier models underperform on neuroimaging tasks and that learning directly from uncurated data generated during routine clinical care at health systems, a paradigm we call health system learning, yields high-performance, generalist neuroimaging models. We introduce NeuroVFM, a visual foundation model trained on 5.24 million clinical MRI and CT volumes using a scalable volumetric joint-embedding predictive architecture. NeuroVFM learns comprehensive representations of brain anatomy and pathology, achieving state-of-the-art performance across multiple clinical tasks, including radiologic diagnosis and report generation. The model exhibits emergent neuroanatomic understanding and interpretable visual grounding of diagnostic findings. When paired with open-source language models through lightweight visual instruction tuning, NeuroVFM generates radiology reports that surpass frontier models in accuracy, clinical triage, and expert preference. Through clinically grounded visual understanding, NeuroVFM reduces hallucinated findings and critical errors, offering safer clinical decision support. These results establish health system learning as a paradigm for building generalist medical AI and provide a scalable framework for clinical foundation models.

</details>


### [252] [From Healthy Scans to Annotated Tumors: A Tumor Fabrication Framework for 3D Brain MRI Synthesis](https://arxiv.org/abs/2511.18654)
*Nayu Dong,Townim Chowdhury,Hieu Phan,Mark Jenkinson,Johan Verjans,Zhibin Liao*

Main category: cs.CV

TL;DR: 提出TF框架，通过两阶段方法合成3D脑肿瘤数据，解决医学影像中标注数据稀缺问题，显著提升低数据量下的肿瘤分割性能


<details>
  <summary>Details</summary>
Motivation: MRI肿瘤标注数据稀缺限制了自动化肿瘤分割的准确性。现有合成方法存在局限性：手动建模需要专家知识且耗时，深度生成模型需要大量训练数据，这在临床数据有限的环境中不实用

Method: 提出TF框架，包含粗粒度肿瘤合成和生成模型驱动的精炼过程。完全自动化，仅使用健康图像扫描和少量真实标注数据来合成大量配对的合成数据

Result: 使用合成的图像-标签对作为数据增强，在低数据量情况下显著提高了下游肿瘤分割任务的性能

Conclusion: TF为医学图像增强提供了可扩展且可靠的解决方案，解决了临床AI应用中数据稀缺的关键挑战

Abstract: The scarcity of annotated Magnetic Resonance Imaging (MRI) tumor data presents a major obstacle to accurate and automated tumor segmentation. While existing data synthesis methods offer promising solutions, they often suffer from key limitations: manual modeling is labor intensive and requires expert knowledge. Deep generative models may be used to augment data and annotation, but they typically demand large amounts of training pairs in the first place, which is impractical in data limited clinical settings. In this work, we propose Tumor Fabrication (TF), a novel two-stage framework for unpaired 3D brain tumor synthesis. The framework comprises a coarse tumor synthesis process followed by a refinement process powered by a generative model. TF is fully automated and leverages only healthy image scans along with a limited amount of real annotated data to synthesize large volumes of paired synthetic data for enriching downstream supervised segmentation training. We demonstrate that our synthetic image-label pairs used as data enrichment can significantly improve performance on downstream tumor segmentation tasks in low-data regimes, offering a scalable and reliable solution for medical image enrichment and addressing critical challenges in data scarcity for clinical AI applications.

</details>


### [253] [Robust Physical Adversarial Patches Using Dynamically Optimized Clusters](https://arxiv.org/abs/2511.18656)
*Harrison Bagley,Will Meakin,Simon Lucey,Yee Wei Law,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 提出了一种基于超像素的正则化方法，通过SLIC算法动态聚类对抗性补丁中的像素，使用隐函数定理反向传播梯度来更新超像素边界和颜色，从而生成对尺度变化具有鲁棒性的对抗补丁。


<details>
  <summary>Details</summary>
Motivation: 物理对抗攻击在深度学习系统中令人担忧，但现有方法很少关注尺度变化问题。当补丁被缩放时，插值引起的颜色混合会平滑像素值，导致高频模式丢失和对抗信号退化。

Method: 使用SLIC算法在对抗补丁优化过程中动态聚类像素，应用隐函数定理反向传播梯度来更新超像素边界和颜色，生成对尺度变化具有鲁棒性的结构。

Result: 该方法在数字域取得了更好的性能，当物理实现时，这些性能增益得以保留，导致改进的物理性能。使用新的物理评估协议客观评估了真实世界性能。

Conclusion: 提出的超像素正则化方法能够生成对尺度变化具有鲁棒性的对抗补丁，在数字和物理域都表现出更好的性能。

Abstract: Physical adversarial attacks on deep learning systems is concerning due to the ease of deploying such attacks, usually by placing an adversarial patch in a scene to manipulate the outcomes of a deep learning model. Training such patches typically requires regularization that improves physical realizability (e.g., printability, smoothness) and/or robustness to real-world variability (e.g. deformations, viewing angle, noise). One type of variability that has received little attention is scale variability. When a patch is rescaled, either digitally through downsampling/upsampling or physically through changing imaging distances, interpolation-induced color mixing occurs. This smooths out pixel values, resulting in a loss of high-frequency patterns and degrading the adversarial signal. To address this, we present a novel superpixel-based regularization method that guides patch optimization to scale-resilient structures. Our ap proach employs the Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels in an adversarial patch during optimization. The Implicit Function Theorem is used to backpropagate gradients through SLIC to update the superpixel boundaries and color. This produces patches that maintain their structure over scale and are less susceptible to interpolation losses. Our method achieves greater performance in the digital domain, and when realized physically, these performance gains are preserved, leading to improved physical performance. Real-world performance was objectively assessed using a novel physical evaluation protocol that utilizes screens and cardboard cut-outs to systematically vary real-world conditions.

</details>


### [254] [Data Augmentation Strategies for Robust Lane Marking Detection](https://arxiv.org/abs/2511.18668)
*Flora Lian,Dinh Quang Huynh,Hector Penades,J. Stephany Berrio Perez,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 提出基于生成式AI的数据增强方法，通过几何透视变换、AI修复和车身叠加来模拟特定视角，提升车道检测模型在侧置摄像头场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决车道检测模型在公开数据集（如CULane）上训练后，无法适应不同摄像头视角（特别是侧置摄像头）的领域偏移问题。

Method: 使用生成式AI数据增强流水线，包含几何透视变换、AI驱动的修复技术和车辆车身叠加，以模拟部署特定的视角同时保持车道连续性。

Result: 在SCNN和UFLDv2模型上验证，增强数据训练后模型在不同条件（包括阴影）下表现更稳健，精确率、召回率和F1分数均有提升。

Conclusion: 该方法通过弥合公开数据集与部署特定场景之间的差距，为车道检测在试点部署场景中提供了可扩展且实用的可靠性提升框架。

Abstract: Robust lane detection is essential for advanced driver assistance and autonomous driving, yet models trained on public datasets such as CULane often fail to generalise across different camera viewpoints. This paper addresses the challenge of domain shift for side-mounted cameras used in lane-wheel monitoring by introducing a generative AI-based data enhancement pipeline. The approach combines geometric perspective transformation, AI-driven inpainting, and vehicle body overlays to simulate deployment-specific viewpoints while preserving lane continuity. We evaluated the effectiveness of the proposed augmentation in two state-of-the-art models, SCNN and UFLDv2. With the augmented data trained, both models show improved robustness to different conditions, including shadows. The experimental results demonstrate gains in precision, recall, and F1 score compared to the pre-trained model.
  By bridging the gap between widely available datasets and deployment-specific scenarios, our method provides a scalable and practical framework to improve the reliability of lane detection in a pilot deployment scenario.

</details>


### [255] [Sphinx: Efficiently Serving Novel View Synthesis using Regression-Guided Selective Refinement](https://arxiv.org/abs/2511.18672)
*Yuchen Xia,Souvik Kundu,Mosharaf Chowdhury,Nishil Talati*

Main category: cs.CV

TL;DR: Sphinx是一个无需训练的混合推理框架，通过回归式快速初始化和选择性细化，在保持扩散模型质量的同时显著降低计算成本，实现1.8倍加速且感知质量下降小于5%。


<details>
  <summary>Details</summary>
Motivation: 解决扩散式NVS计算成本过高而回归式NVS生成质量不足的矛盾，填补高质量、高效推理NVS框架的设计空白。

Method: 使用回归式快速初始化引导扩散模型降噪工作，结合选择性细化和自适应噪声调度，将更多计算资源分配给不确定区域和帧。

Result: 相比扩散模型推理实现平均1.8倍加速，感知质量下降小于5%，在质量和延迟之间建立了新的帕累托边界。

Conclusion: Sphinx框架成功实现了扩散级质量的高效NVS推理，为动态变化的推理场景提供了灵活的性能-质量权衡方案。

Abstract: Novel View Synthesis (NVS) is the task of generating new images of a scene from viewpoints that were not part of the original input. Diffusion-based NVS can generate high-quality, temporally consistent images, however, remains computationally prohibitive. Conversely, regression-based NVS offers suboptimal generation quality despite requiring significantly lower compute; leaving the design objective of a high-quality, inference-efficient NVS framework an open challenge. To close this critical gap, we present Sphinx, a training-free hybrid inference framework that achieves diffusion-level fidelity at a significantly lower compute. Sphinx proposes to use regression-based fast initialization to guide and reduce the denoising workload for the diffusion model. Additionally, it integrates selective refinement with adaptive noise scheduling, allowing more compute to uncertain regions and frames. This enables Sphinx to provide flexible navigation of the performance-quality trade-off, allowing adaptation to latency and fidelity requirements for dynamically changing inference scenarios. Our evaluation shows that Sphinx achieves an average 1.8x speedup over diffusion model inference with negligible perceptual degradation of less than 5%, establishing a new Pareto frontier between quality and latency in NVS serving.

</details>


### [256] [Edit2Perceive: Image Editing Diffusion Models Are Strong Dense Perceivers](https://arxiv.org/abs/2511.18673)
*Yiqing Shi,Yiren Song,Mike Zheng Shou*

Main category: cs.CV

TL;DR: Edit2Perceive：一个统一的扩散框架，将图像编辑模型适配于深度、法线和抠图等密集感知任务，在保持结构一致性的同时实现更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有密集感知方法大多依赖为随机生成设计的文本到图像生成器，而图像编辑扩散模型天然具有图像到图像的一致性，更适合作为密集感知任务的基础。

Method: 基于FLUX.1 Kontext架构，采用全参数微调和像素空间一致性损失，在中间去噪状态间强制执行结构保持细化，并使用单步确定性推理。

Result: 在深度、法线和抠图三个任务上均取得最先进的综合结果，推理速度提升高达8倍，且仅需在相对较小的数据集上训练。

Conclusion: 面向编辑的扩散变换器在几何感知任务中展现出强大潜力，为密集感知提供了更合适的基础架构。

Abstract: Recent advances in diffusion transformers have shown remarkable generalization in visual synthesis, yet most dense perception methods still rely on text-to-image (T2I) generators designed for stochastic generation. We revisit this paradigm and show that image editing diffusion models are inherently image-to-image consistent, providing a more suitable foundation for dense perception task. We introduce Edit2Perceive, a unified diffusion framework that adapts editing models for depth, normal, and matting. Built upon the FLUX.1 Kontext architecture, our approach employs full-parameter fine-tuning and a pixel-space consistency loss to enforce structure-preserving refinement across intermediate denoising states. Moreover, our single-step deterministic inference yields up to faster runtime while training on relatively small datasets. Extensive experiments demonstrate comprehensive state-of-the-art results across all three tasks, revealing the strong potential of editing-oriented diffusion transformers for geometry-aware perception.

</details>


### [257] [MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis](https://arxiv.org/abs/2511.18676)
*Yongcheng Yao,Yongshuo Zong,Raman Dutt,Yongxin Yang,Sotirios A Tsaftaris,Timothy Hospedales*

Main category: cs.CV

TL;DR: MedVision是一个专门用于评估和改进医学视觉语言模型定量分析能力的大规模数据集和基准测试，涵盖22个公共数据集，包含3080万图像-标注对，专注于检测、肿瘤大小估计和角度/距离测量三个定量任务。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型主要设计用于分类问答或定性描述任务，但临床决策往往依赖于定量评估（如测量肿瘤大小或关节角度），这种定量推理能力在现有模型中尚未得到充分探索和支持。

Method: 构建MedVision大规模数据集，涵盖22个公共数据集和3080万图像-标注对，专注于三个代表性定量任务：解剖结构和异常检测、肿瘤/病变大小估计、角度/距离测量。通过监督微调提升模型性能。

Result: 现成的视觉语言模型在这些定量任务上表现不佳，但在MedVision上进行监督微调后，在检测、肿瘤大小估计和角度/距离测量方面的性能显著提升，错误率降低，精度提高。

Conclusion: 这项工作为开发具有强大定量推理能力的医学成像视觉语言模型奠定了基础，MedVision数据集和基准测试为相关研究提供了重要资源。

Abstract: Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., "Is this normal or abnormal?") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at https://medvision-vlm.github.io.

</details>


### [258] [A Theory-Inspired Framework for Few-Shot Cross-Modal Sketch Person Re-Identification](https://arxiv.org/abs/2511.18677)
*Yunpeng Gong,Yongjie Hou,Jiangming Shi,Kim Long Diep,Min Jiang*

Main category: cs.CV

TL;DR: 提出了KTCAA框架，通过理论驱动的对齐增强和知识转移催化剂来解决基于草图的行人重识别中的模态差异和少样本问题。


<details>
  <summary>Details</summary>
Motivation: 基于草图的行人重识别面临显著的模态差异和有限标注数据挑战，需要开发能够有效进行跨模态泛化的方法。

Method: 提出对齐增强(AA)模块进行局部草图风格变换模拟目标分布，以及知识转移催化剂(KTC)模块引入最坏情况扰动增强不变性，在元学习框架下联合优化。

Result: 在多个基准测试中达到最先进性能，特别是在数据稀缺条件下表现优异。

Conclusion: KTCAA框架通过理论指导的跨模态对齐和不变性增强，有效解决了草图-图像行人重识别的少样本泛化问题。

Abstract: Sketch based person re-identification aims to match hand-drawn sketches with RGB surveillance images, but remains challenging due to significant modality gaps and limited annotated data. To address this, we introduce KTCAA, a theoretically grounded framework for few-shot cross-modal generalization. Motivated by generalization theory, we identify two key factors influencing target domain risk: (1) domain discrepancy, which quantifies the alignment difficulty between source and target distributions; and (2) perturbation invariance, which evaluates the model's robustness to modality shifts. Based on these insights, we propose two components: (1) Alignment Augmentation (AA), which applies localized sketch-style transformations to simulate target distributions and facilitate progressive alignment; and (2) Knowledge Transfer Catalyst (KTC), which enhances invariance by introducing worst-case perturbations and enforcing consistency. These modules are jointly optimized under a meta-learning paradigm that transfers alignment knowledge from data-rich RGB domains to sketch-based scenarios. Experiments on multiple benchmarks demonstrate that KTCAA achieves state-of-the-art performance, particularly in data-scarce conditions.

</details>


### [259] [Neural Geometry Image-Based Representations with Optimal Transport (OT)](https://arxiv.org/abs/2511.18679)
*Xiang Gao,Yuanpeng Liu,Xinmu Wang,Jiazhi Li,Minghao Guo,Yu Guo,Xiyun Song,Heather Yu,Zhiqiang Lao,Xianfeng David Gu*

Main category: cs.CV

TL;DR: 提出了一种基于几何图像的神经表示方法，通过将不规则网格转换为规则图像网格，实现高效的单次前向传递重建高质量网格，无需复杂解码器网络。


<details>
  <summary>Details</summary>
Motivation: 现有3D网格神经表示方法依赖神经过拟合和渐进解码，计算成本高；而图像具有规则结构便于处理但难以直接应用于网格。关键洞察是将不规则网格转换为规则图像网格以应用图像处理优势。

Method: 使用几何图像表示将网格转换为规则图像网格，结合最优传输(OT)解决平坦区域过采样和特征区域欠采样问题，通过几何图像mipmapping实现连续细节层次(LoD)。

Result: 实验结果显示在存储效率和重建精度方面达到最先进水平，通过压缩比(CR)、Chamfer距离(CD)和Hausdorff距离(HD)等指标验证。

Conclusion: 提出的神经几何图像表示是无解码器的、存储高效的，且天然适合神经处理，能够通过单次前向传递从低分辨率几何图像mipmap重建高质量网格。

Abstract: Neural representations for 3D meshes are emerging as an effective solution for compact storage and efficient processing. Existing methods often rely on neural overfitting, where a coarse mesh is stored and progressively refined through multiple decoder networks. While this can restore high-quality surfaces, it is computationally expensive due to successive decoding passes and the irregular structure of mesh data. In contrast, images have a regular structure that enables powerful super-resolution and restoration frameworks, but applying these advantages to meshes is difficult because their irregular connectivity demands complex encoder-decoder architectures. Our key insight is that a geometry image-based representation transforms irregular meshes into a regular image grid, making efficient image-based neural processing directly applicable. Building on this idea, we introduce our neural geometry image-based representation, which is decoder-free, storage-efficient, and naturally suited for neural processing. It stores a low-resolution geometry-image mipmap of the surface, from which high-quality meshes are restored in a single forward pass. To construct geometry images, we leverage Optimal Transport (OT), which resolves oversampling in flat regions and undersampling in feature-rich regions, and enables continuous levels of detail (LoD) through geometry-image mipmapping. Experimental results demonstrate state-of-the-art storage efficiency and restoration accuracy, measured by compression ratio (CR), Chamfer distance (CD), and Hausdorff distance (HD).

</details>


### [260] [Hierarchical GraphCut Phase Unwrapping based on Invariance of Diffeomorphisms Framework](https://arxiv.org/abs/2511.18682)
*Xiang Gao,Xinmu Wang,Zhou Zhao,Junqi Huang,Xianfeng David Gu*

Main category: cs.CV

TL;DR: 提出了一种基于微分同胚和GraphCut的相位展开框架，通过像素标记问题重构相位展开，实现了45.5倍加速和更低的L2误差，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有相位展开方法在速度和精度之间存在权衡：快速方法精度不足，而精确算法速度太慢，无法满足实时应用需求。噪声、遮挡和复杂3D几何使得相位展开成为不适定问题。

Method: 将基于GraphCut的相位展开重新表述为像素标记问题，利用微分同胚的不变性特性，通过共形和最优传输映射在图像空间中应用。预计算奇数个微分同胚，在每个域中应用分层GraphCut算法，通过多数投票融合标签图来稳健估计相位计数k。

Result: 实验结果显示在真实实验和仿真中实现了45.5倍的速度提升和更低的L2误差，展示了在实时应用中的潜力。

Conclusion: 该框架成功解决了相位展开中速度与精度的权衡问题，通过微分同胚和分层GraphCut的结合，为实时3D扫描应用提供了有效的解决方案。

Abstract: Recent years have witnessed rapid advancements in 3D scanning technologies, with applications spanning VR/AR, digital human creation, and medical imaging. Structured-light scanning with phase-shifting techniques is preferred for its use of low-intensity visible light and high accuracy, making it well suited for capturing 4D facial dynamics. A key step is phase unwrapping, which recovers continuous phase values from measurements wrapped modulo 2pi. The goal is to estimate the unwrapped phase count k in the equation Phi = phi + 2pi k, where phi is the wrapped phase and Phi is the true phase. Noise, occlusions, and complex 3D geometry make recovering the true phase challenging because phase unwrapping is ill-posed: measurements only provide modulo 2pi values, and estimating k requires assumptions about surface continuity. Existing methods trade speed for accuracy: fast approaches lack precision, while accurate algorithms are too slow for real-time use. To overcome these limitations, this work proposes a phase unwrapping framework that reformulates GraphCut-based unwrapping as a pixel-labeling problem. This framework improves the estimation of the unwrapped phase count k through the invariance property of diffeomorphisms applied in image space via conformal and optimal transport (OT) maps. An odd number of diffeomorphisms are precomputed from the input phase data, and a hierarchical GraphCut algorithm is applied in each domain. The resulting label maps are fused via majority voting to robustly estimate k at each pixel. Experimental results demonstrate a 45.5x speedup and lower L2 error in real experiments and simulations, showing potential for real-time applications.

</details>


### [261] [Now You See It, Now You Don't - Instant Concept Erasure for Safe Text-to-Image and Video Generation](https://arxiv.org/abs/2511.18684)
*Shristi Das Biswas,Arani Roy,Kaushik Roy*

Main category: cs.CV

TL;DR: ICE是一种无需训练、模态无关的一次性权重修改方法，用于文本到图像和文本到视频模型的概念擦除，通过各向异性能量加权缩放定义擦除和保留子空间，并使用重叠投影器实现精确、持久的遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法存在需要重新训练、推理开销大或易受对抗攻击的问题，且很少建模目标擦除概念与周围内容之间的潜在语义重叠，导致擦除后产生附带损害，同时很少有方法能在T2I和T2V领域都可靠工作。

Method: ICE使用各向异性能量加权缩放定义擦除和保留子空间，通过独特的闭式重叠投影器显式正则化它们的交集，提出凸且Lipschitz有界的谱遗忘目标，平衡擦除保真度和交集保留，获得稳定唯一的解析解。

Result: ICE在艺术风格、物体、身份和显式内容的目标移除中，高效实现了强擦除效果，提高了对红队测试的鲁棒性，同时对原始生成能力仅造成最小退化，在T2I和T2V模型中都表现良好。

Conclusion: ICE是一种训练免费、模态无关的一次性权重修改方法，能够实现精确、持久的无学习，在文本到图像和文本到视频模型中表现出色，具有零开销和强鲁棒性。

Abstract: Robust concept removal for text-to-image (T2I) and text-to-video (T2V) models is essential for their safe deployment. Existing methods, however, suffer from costly retraining, inference overhead, or vulnerability to adversarial attacks. Crucially, they rarely model the latent semantic overlap between the target erase concept and surrounding content -- causing collateral damage post-erasure -- and even fewer methods work reliably across both T2I and T2V domains. We introduce Instant Concept Erasure (ICE), a training-free, modality-agnostic, one-shot weight modification approach that achieves precise, persistent unlearning with zero overhead. ICE defines erase and preserve subspaces using anisotropic energy-weighted scaling, then explicitly regularises against their intersection using a unique, closed-form overlap projector. We pose a convex and Lipschitz-bounded Spectral Unlearning Objective, balancing erasure fidelity and intersection preservation, that admits a stable and unique analytical solution. This solution defines a dissociation operator that is translated to the model's text-conditioning layers, making the edit permanent and runtime-free. Across targeted removals of artistic styles, objects, identities, and explicit content, ICE efficiently achieves strong erasure with improved robustness to red-teaming, all while causing only minimal degradation of original generative abilities in both T2I and T2V models.

</details>


### [262] [Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents](https://arxiv.org/abs/2511.18685)
*Dayong Liu,Chao Xu,Weihong Chen,Suyu Zhang,Juncheng Wang,Jiankang Deng,Baigui Sun,Yang Liu*

Main category: cs.CV

TL;DR: CFG-Bench是一个新的基准测试，旨在系统评估多模态大语言模型在具身物理交互中的细粒度动作智能，包含1,368个视频和19,562个多模态问答对，涵盖四种认知能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注高级规划或空间推理，而忽视了具身物理交互所需的细粒度动作智能，因此需要专门评估这一关键能力。

Method: 构建CFG-Bench基准，包含四个认知维度：物理交互、时间因果关系、意图理解和评估判断，通过多模态问答对系统评估模型能力。

Result: 主流MLLMs在生成物理交互的详细指令方面表现不佳，在意图和评估的高阶推理方面存在显著局限性。在CFG-Bench数据上进行监督微调可显著提升现有具身基准测试的性能。

Conclusion: CFG-Bench揭示了当前MLLMs在具身物理交互方面的局限性，为开发更强大和接地气的具身智能体提供了重要见解。

Abstract: Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.

</details>


### [263] [EVCC: Enhanced Vision Transformer-ConvNeXt-CoAtNet Fusion for Classification](https://arxiv.org/abs/2511.18691)
*Kazi Reyazul Hasan,Md Nafiu Rahman,Wasif Jalal,Sadif Ahmed,Shahriar Raj,Mubasshira Musarrat,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: EVCC是一种结合Vision Transformer、轻量级ConvNeXt和CoAtNet的多分支混合视觉架构，通过自适应token剪枝、门控双向交叉注意力等创新技术，在多个数据集上实现SOTA精度，同时减少25-35%的计算量。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer-CNN混合架构虽然提升了图像分类性能，但计算成本过高。需要开发既能保持高精度又能显著降低计算复杂度的新型架构。

Method: 1) 带信息保留的自适应token剪枝；2) 门控双向交叉注意力用于特征细化；3) 多任务学习的辅助分类头；4) 上下文感知置信度驱动的动态路由门机制

Result: 在CIFAR-100、Tobacco3482、CelebA和Brain Cancer数据集上，EVCC相比DeiT-Base、MaxViT-Base和CrossViT-Base等模型，精度提升达2个百分点，同时FLOPs减少25-35%。

Conclusion: EVCC通过动态调整计算需求，有效平衡了精度与效率的权衡，结合了全局上下文、局部细节和层次特征，适用于实际应用场景。

Abstract: Hybrid vision architectures combining Transformers and CNNs have significantly advanced image classification, but they usually do so at significant computational cost. We introduce EVCC (Enhanced Vision Transformer-ConvNeXt-CoAtNet), a novel multi-branch architecture integrating the Vision Transformer, lightweight ConvNeXt, and CoAtNet through key innovations: (1) adaptive token pruning with information preservation, (2) gated bidirectional cross-attention for enhanced feature refinement, (3) auxiliary classification heads for multi-task learning, and (4) a dynamic router gate employing context-aware confidence-driven weighting. Experiments across the CIFAR-100, Tobacco3482, CelebA, and Brain Cancer datasets demonstrate EVCC's superiority over powerful models like DeiT-Base, MaxViT-Base, and CrossViT-Base by consistently achieving state-of-the-art accuracy with improvements of up to 2 percentage points, while reducing FLOPs by 25 to 35%. Our adaptive architecture adjusts computational demands to deployment needs by dynamically reducing token count, efficiently balancing the accuracy-efficiency trade-off while combining global context, local details, and hierarchical features for real-world applications. The source code of our implementation is available at https://anonymous.4open.science/r/EVCC.

</details>


### [264] [Exploring Surround-View Fisheye Camera 3D Object Detection](https://arxiv.org/abs/2511.18695)
*Changcai Li,Wenwei Lin,Zuoxun Hou,Gang Chen,Wei Zhang,Huihui Zhou,Weishi Zheng*

Main category: cs.CV

TL;DR: 该论文研究了在环视鱼眼相机系统中实现端到端3D物体检测的技术可行性，提出了两种兼容鱼眼几何的检测方法，并发布了新的鱼眼3D检测数据集。


<details>
  <summary>Details</summary>
Motivation: 传统基于针孔相机模型的3D物体检测器在鱼眼图像上性能下降严重，需要开发专门针对鱼眼几何特性的检测方法。

Method: 提出了两种方法：基于鸟瞰图范式的FisheyeBEVDet和基于查询范式的FisheyePETR，两者都采用球面空间表示来有效捕捉鱼眼几何特性。

Result: 在自建的Fisheye3DOD数据集上实验表明，鱼眼兼容建模相比基线方法准确率提升最高达6.2%。

Conclusion: 通过专门设计的球面空间表示，可以有效提升鱼眼相机系统中的3D物体检测性能，证明了鱼眼几何建模的重要性。

Abstract: In this work, we explore the technical feasibility of implementing end-to-end 3D object detection (3DOD) with surround-view fisheye camera system. Specifically, we first investigate the performance drop incurred when transferring classic pinhole-based 3D object detectors to fisheye imagery. To mitigate this, we then develop two methods that incorporate the unique geometry of fisheye images into mainstream detection frameworks: one based on the bird's-eye-view (BEV) paradigm, named FisheyeBEVDet, and the other on the query-based paradigm, named FisheyePETR. Both methods adopt spherical spatial representations to effectively capture fisheye geometry. In light of the lack of dedicated evaluation benchmarks, we release Fisheye3DOD, a new open dataset synthesized using CARLA and featuring both standard pinhole and fisheye camera arrays. Experiments on Fisheye3DOD show that our fisheye-compatible modeling improves accuracy by up to 6.2% over baseline methods.

</details>


### [265] [Dendritic Convolution for Noise Image Recognition](https://arxiv.org/abs/2511.18699)
*Jiarui Xue,Dongjian Yang,Ye Sun,Gang Liu*

Main category: cs.CV

TL;DR: 提出一种抗噪声神经元卷积（DDC），模拟神经元树突结构，通过非线性交互处理邻域信息，降低噪声干扰，在图像分类和目标检测任务中显著提升模型在噪声环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过调整网络或训练策略来处理噪声图像识别，抗噪声性能已达瓶颈。本文从神经元角度探索抗干扰解决方案，模拟生物神经元的树突结构来改进卷积操作。

Method: 提出抗噪声神经元卷积（DDC），模仿神经元树突结构，将树突的邻域交互计算逻辑整合到卷积操作底层设计中，通过输入特征间的非线性交互模拟生物树突的XOR逻辑预处理功能，重构特征提取的数学范式。

Result: 在图像分类任务（YOLOv11-cls、VGG16、EfficientNet-B0）和目标检测任务（YOLOv11、YOLOv8、YOLOv5）中，用DDC替换传统卷积后，EfficientNet-B0在噪声数据集上的准确率相对提升11.23%，YOLOv8的mAP提升19.80%。

Conclusion: DDC的计算方式与生物神经元树突的一致性使其在复杂噪声环境中表现显著优于传统卷积，通过关注邻域信息交互有效减轻了噪声影响。

Abstract: In real-world scenarios of image recognition, there exists substantial noise interference. Existing works primarily focus on methods such as adjusting networks or training strategies to address noisy image recognition, and the anti-noise performance has reached a bottleneck. However, little is known about the exploration of anti-interference solutions from a neuronal perspective.This paper proposes an anti-noise neuronal convolution. This convolution mimics the dendritic structure of neurons, integrates the neighborhood interaction computation logic of dendrites into the underlying design of convolutional operations, and simulates the XOR logic preprocessing function of biological dendrites through nonlinear interactions between input features, thereby fundamentally reconstructing the mathematical paradigm of feature extraction. Unlike traditional convolution where noise directly interferes with feature extraction and exerts a significant impact, DDC mitigates the influence of noise by focusing on the interaction of neighborhood information. Experimental results demonstrate that in image classification tasks (using YOLOv11-cls, VGG16, and EfficientNet-B0) and object detection tasks (using YOLOv11, YOLOv8, and YOLOv5), after replacing traditional convolution with the dendritic convolution, the accuracy of the EfficientNet-B0 model on noisy datasets is relatively improved by 11.23%, and the mean Average Precision (mAP) of YOLOv8 is increased by 19.80%. The consistency between the computation method of this convolution and the dendrites of biological neurons enables it to perform significantly better than traditional convolution in complex noisy environments.

</details>


### [266] [ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction](https://arxiv.org/abs/2511.18701)
*Mustafa Munir,Harsh Goel,Xiwen Wei,Minkyu Choi,Sahil Shah,Kartikeya Bhardwaj,Paul Whatmough,Sandeep Chinchali,Radu Marculescu*

Main category: cs.CV

TL;DR: ObjectAlign是一个结合感知指标和符号推理的视频编辑框架，用于检测和纠正对象不一致性问题，如帧闪烁和身份漂移。


<details>
  <summary>Details</summary>
Motivation: 解决视频编辑和合成中常见的对象不一致问题，如帧闪烁和身份漂移，这些会降低感知质量。

Method: 提出可学习阈值用于对象一致性指标；引入神经符号验证器，结合SMT检查和概率模型检查；基于神经网络的插值方法进行自适应帧修复。

Result: 在DAVIS和Pexels数据集上，相比SOTA基线，CLIP Score提升1.4分，warp error提升6.1分。

Conclusion: ObjectAlign通过结合感知指标和符号推理，有效解决了视频编辑中的对象不一致问题，在多个指标上显著优于现有方法。

Abstract: Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed "consistent" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.

</details>


### [267] [CoD: A Diffusion Foundation Model for Image Compression](https://arxiv.org/abs/2511.18706)
*Zhaoyang Jia,Zihan Zheng,Naifu Xue,Jiahao Li,Bin Li,Zongyu Guo,Xiaoyi Zhang,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: CoD是首个专为压缩设计的扩散基础模型，相比基于文本到图像扩散模型（如Stable Diffusion）的现有编解码器，在超低码率下具有更高的压缩效率，训练成本降低300倍，并提供像素空间扩散可实现VTM级PSNR的新见解。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本到图像扩散模型的编解码器在压缩角度上存在次优问题，特别是在超低码率下限制了扩散编解码器的潜力。

Method: 从头训练CoD压缩导向扩散基础模型，实现压缩和生成的端到端优化，支持多种基于扩散的编解码器。

Result: 在DiffC等下游编解码器中用CoD替换Stable Diffusion，在超低码率（如0.0039 bpp）下达到SOTA结果；训练速度比Stable Diffusion快300倍（约20 vs. 6,250 A100 GPU天）；像素空间扩散可实现VTM级PSNR且感知质量高。

Conclusion: CoD为未来扩散编解码器研究奠定了基础，展示了压缩导向扩散模型的优势，包括高压缩效率、低成本训练和新见解提供。

Abstract: Existing diffusion codecs typically build on text-to-image diffusion foundation models like Stable Diffusion. However, text conditioning is suboptimal from a compression perspective, hindering the potential of downstream diffusion codecs, particularly at ultra-low bitrates. To address it, we introduce \textbf{CoD}, the first \textbf{Co}mpression-oriented \textbf{D}iffusion foundation model, trained from scratch to enable end-to-end optimization of both compression and generation. CoD is not a fixed codec but a general foundation model designed for various diffusion-based codecs. It offers several advantages: \textbf{High compression efficiency}, replacing Stable Diffusion with CoD in downstream codecs like DiffC achieves SOTA results, especially at ultra-low bitrates (e.g., 0.0039 bpp); \textbf{Low-cost and reproducible training}, 300$\times$ faster training than Stable Diffusion ($\sim$ 20 vs. $\sim$ 6,250 A100 GPU days) on entirely open image-only datasets; \textbf{Providing new insights}, e.g., We find pixel-space diffusion can achieve VTM-level PSNR with high perceptual quality and can outperform GAN-based codecs using fewer parameters. We hope CoD lays the foundation for future diffusion codec research. Codes will be released.

</details>


### [268] [Modality-Collaborative Low-Rank Decomposers for Few-Shot Video Domain Adaptation](https://arxiv.org/abs/2511.18711)
*Yuyang Wanyan,Xiaoshan Yang,Weiming Dong,Changsheng Xu*

Main category: cs.CV

TL;DR: 提出MC-LRD框架解决少样本视频域自适应问题，通过模态协同低秩分解器分离模态独特和共享特征，提升跨域对齐效果。


<details>
  <summary>Details</summary>
Motivation: 视频多模态特性在少样本场景下面临域对齐和模态协作的双重挑战，现有方法忽略了不同模态特征组件具有不同域偏移的问题，限制了跨域泛化性能。

Method: 使用多模态分解路由器和渐进共享参数的分解器，通过正交解相关约束和跨域激活一致性损失，选择性激活产生模态独特和共享特征。

Result: 在三个公开基准测试中，模型相比现有方法取得了显著改进。

Conclusion: MC-LRD框架有效解决了少样本视频域自适应中模态协作和域对齐的挑战，通过特征分解和一致性约束提升了跨域性能。

Abstract: In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative LowRank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and subrouters, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.

</details>


### [269] [DriveFlow: Rectified Flow Adaptation for Robust 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2511.18713)
*Hongbin Lin,Yiming Yang,Chaoda Zheng,Yifan Zhang,Shuaicheng Niu,Zilu Guo,Yafeng Li,Gui Gui,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: DriveFlow提出了一种基于预训练文本到图像流模型的校正流适应方法，用于自动驾驶中的训练数据增强，通过频率分解策略解决OOD问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中视觉中心的3D物体检测面临标注成本高和室外场景多样的问题，导致训练数据无法覆盖所有测试场景（OOD问题）。训练免费的图像编辑提供了一种有前景的解决方案，但现有方法存在效果有限或难以保持准确3D几何的问题。

Method: 基于频率分解，DriveFlow引入两种策略来适应文本条件速度导出的无噪声编辑路径：1）高频前景保护：通过高频对齐损失保持精确的3D物体几何；2）双频背景优化：进行双频优化以平衡编辑灵活性和语义一致性。

Result: 综合实验验证了DriveFlow的有效性和效率，在OOD场景的所有类别上都展示了全面的性能提升。

Conclusion: DriveFlow通过频率分解和校正流适应方法，成功解决了自动驾驶中3D物体检测的OOD问题，实现了训练数据增强的显著改进。

Abstract: In autonomous driving, vision-centric 3D object detection recognizes and localizes 3D objects from RGB images. However, due to high annotation costs and diverse outdoor scenes, training data often fails to cover all possible test scenarios, known as the out-of-distribution (OOD) issue. Training-free image editing offers a promising solution for improving model robustness by training data enhancement without any modifications to pre-trained diffusion models. Nevertheless, inversion-based methods often suffer from limited effectiveness and inherent inaccuracies, while recent rectified-flow-based approaches struggle to preserve objects with accurate 3D geometry. In this paper, we propose DriveFlow, a Rectified Flow Adaptation method for training data enhancement in autonomous driving based on pre-trained Text-to-Image flow models. Based on frequency decomposition, DriveFlow introduces two strategies to adapt noise-free editing paths derived from text-conditioned velocities. 1) High-Frequency Foreground Preservation: DriveFlow incorporates a high-frequency alignment loss for foreground to maintain precise 3D object geometry. 2) Dual-Frequency Background Optimization: DriveFlow also conducts dual-frequency optimization for background, balancing editing flexibility and semantic consistency. Comprehensive experiments validate the effectiveness and efficiency of DriveFlow, demonstrating comprehensive performance improvements on all categories across OOD scenarios. Code is available at https://github.com/Hongbin98/DriveFlow.

</details>


### [270] [Seeing What Matters: Visual Preference Policy Optimization for Visual Generation](https://arxiv.org/abs/2511.18719)
*Ziqi Ni,Yuanzhi Liang,Rui Li,Yi Zhou,Haibing Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: ViPO是一种改进的GRPO方法，将标量奖励提升为像素级优势图，通过感知结构化模块实现空间和时间感知的优化，在图像和视频生成中优于传统GRPO。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法使用单一标量奖励处理整个图像或视频，忽略了视觉内容的空间和时间结构，导致难以纠正局部伪影和建模细粒度感知线索。

Method: 引入视觉偏好策略优化(ViPO)，使用预训练视觉骨干构建空间和时间感知的优势图，将优化压力重新分配到感知重要区域，同时保持标准GRPO的稳定性。

Result: 在图像和视频基准测试中，ViPO始终优于传统GRPO，提高了与人类偏好奖励的领域内对齐，并增强了领域外评估的泛化能力。

Conclusion: ViPO是一种架构无关、轻量级且与现有GRPO训练管道完全兼容的方法，为视觉生成提供了更具表达力和信息量的学习信号。

Abstract: Reinforcement learning (RL) has become a powerful tool for post-training visual generative models, with Group Relative Policy Optimization (GRPO) increasingly used to align generators with human preferences. However, existing GRPO pipelines rely on a single scalar reward per sample, treating each image or video as a holistic entity and ignoring the rich spatial and temporal structure of visual content. This coarse supervision hinders the correction of localized artifacts and the modeling of fine-grained perceptual cues. We introduce Visual Preference Policy Optimization (ViPO), a GRPO variant that lifts scalar feedback into structured, pixel-level advantages. ViPO employs a Perceptual Structuring Module that uses pretrained vision backbones to construct spatially and temporally aware advantage maps, redistributing optimization pressure toward perceptually important regions while preserving the stability of standard GRPO. Across both image and video benchmarks, ViPO consistently outperforms vanilla GRPO, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations. The method is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines, providing a more expressive and informative learning signal for visual generation.

</details>


### [271] [GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.18729)
*Lin Liu,Caiyan Jia,Guanyi Yu,Ziying Song,JunQiao Li,Feiyang Jia,Peiliang Wu,Xiaoshuai Hao,Yandan Luo*

Main category: cs.CV

TL;DR: GuideFlow是一个基于约束流匹配的新型自动驾驶规划框架，通过显式约束建模和能量模型训练，解决了现有端到端规划器的多模态轨迹崩溃和约束集成问题，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有模仿式端到端规划器存在多模态轨迹模式崩溃问题，无法产生多样化的轨迹提议；而生成式端到端规划器难以在生成过程中直接集成安全和物理约束，需要额外的优化阶段来完善输出。

Method: 利用约束流匹配技术，显式地在流匹配生成过程中强制执行约束，而不是依赖隐式约束编码；通过将流匹配与能量模型统一训练，增强模型自主优化能力以鲁棒地满足物理约束；将驾驶攻击性参数化为生成过程中的控制信号。

Result: 在多个主要驾驶基准测试（Bench2Drive、NuScenes、NavSim和ADV-NuScenes）上验证了有效性，在NavSim测试困难分割（Navhard）上达到SOTA，EPDMS得分为43.0。

Conclusion: GuideFlow框架通过约束流匹配有效解决了端到端自动驾驶规划中的多模态轨迹生成和约束集成问题，实现了高性能的规划性能。

Abstract: Driving planning is a critical component of end-to-end (E2E) autonomous driving. However, prevailing Imitative E2E Planners often suffer from multimodal trajectory mode collapse, failing to produce diverse trajectory proposals. Meanwhile, Generative E2E Planners struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. In this paper, we propose \textit{\textbf{GuideFlow}}, a novel planning framework that leverages Constrained Flow Matching. Concretely, \textit{\textbf{GuideFlow}} explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our core contribution lies in directly enforcing explicit constraints within the flow matching generation process, rather than relying on implicit constraint encoding. Crucially, \textit{\textbf{GuideFlow}} unifies the training of the flow matching with the Energy-Based Model (EBM) to enhance the model's autonomous optimization capability to robustly satisfy physical constraints. Secondly, \textit{\textbf{GuideFlow}} parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim and ADV-NuScenes) validate the effectiveness of \textit{\textbf{GuideFlow}}. Notably, on the NavSim test hard split (Navhard), \textit{\textbf{GuideFlow}} achieved SOTA with an EPDMS score of 43.0. The code will be released.

</details>


### [272] [Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion](https://arxiv.org/abs/2511.18734)
*Keyang Lu,Sifan Zhou,Hongbin Xu,Gang Xu,Zhifei Yang,Yikai Wang,Zhen Xiao,Jieyi Long,Ming Li*

Main category: cs.CV

TL;DR: Yo'City是一个基于大型模型的智能框架，通过分层规划和迭代优化实现用户定制化、无限扩展的3D城市生成，在语义、几何、纹理和布局等多维度评估中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D城市生成方法大多依赖单一扩散模型，无法实现个性化定制和无限扩展的城市规模场景生成。

Method: 采用分层"城市-区域-网格"规划策略，通过全局规划器和局部设计器定义结构，使用"生成-优化-评估"等距图像合成循环实现网格级3D生成，并引入基于场景图的距离和语义感知布局优化机制。

Result: 在构建的多样化基准数据集上，Yo'City在语义、几何、纹理和布局等六个多维度指标上均优于现有最先进方法。

Conclusion: Yo'City框架成功实现了用户定制化和无限扩展的3D城市生成，为虚拟现实和数字孪生等应用提供了有效的解决方案。

Abstract: Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.

</details>


### [273] [Thinking Ahead: Foresight Intelligence in MLLMs and World Models](https://arxiv.org/abs/2511.18735)
*Zhantao Gong,Liaoyuan Fan,Qing Guo,Xun Xu,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: 提出了Foresight Intelligence概念和FSU-QA数据集，用于评估和增强视觉语言模型的前瞻推理能力。实验表明当前模型在预测未来事件方面仍有困难，但通过FSU-QA微调可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍忽视了对未来事件的预测和解释能力（Foresight Intelligence），而这种能力在自动驾驶等应用中至关重要。

Method: 引入FSU-QA视觉问答数据集，用于评估视觉语言模型的前瞻推理能力，并通过世界模型生成的预测来增强模型性能。

Result: 当前最先进的视觉语言模型在预测未来事件方面表现不佳，但经过FSU-QA微调的小型模型性能大幅提升，甚至超越更大型的先进模型。

Conclusion: FSU-QA为开发能够真正预测和理解未来事件的下一代模型提供了理论基础和评估基准。

Abstract: In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.

</details>


### [274] [ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion](https://arxiv.org/abs/2511.18742)
*Zhenghan Fang,Jian Zheng,Qiaozi Gao,Xiaofeng Gao,Jeremias Sulam*

Main category: cs.CV

TL;DR: 提出了一种基于后向离散化的文本到图像扩散模型ProxT2I，使用学习的条件近端算子替代分数函数，结合强化学习优化采样器，并创建了包含1500万高质量人脸图像的新数据集LAION-Face-T2I-15M。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用前向离散化和基于分数的采样方法存在采样速度慢、稳定性差的问题，需要大量采样步骤才能生成高质量样本。

Method: 开发基于后向离散化的文本到图像扩散模型ProxT2I，使用学习的条件近端算子替代分数函数，并利用强化学习和策略优化技术优化采样器以获得任务特定奖励。

Result: 相比基于分数的基线方法，该方法显著提高了采样效率和人类偏好对齐度，在计算需求和模型规模更小的情况下，性能与现有最先进的文本到图像模型相当。

Conclusion: ProxT2I为人类文本到图像生成提供了一个轻量级但高性能的解决方案，通过后向离散化和近端算子的创新方法改进了扩散模型的采样效率。

Abstract: Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.

</details>


### [275] [Any4D: Open-Prompt 4D Generation from Natural Language and Images](https://arxiv.org/abs/2511.18746)
*Hao Li,Qiao Sun*

Main category: cs.CV

TL;DR: 提出PEWM框架，通过将视频生成限制在较短的时间范围内，解决了基于视频生成的具身世界模型对大规模交互数据的依赖问题，实现了语言与动作的细粒度对齐，并降低了学习复杂度和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 基于视频生成的具身世界模型依赖大规模具身交互数据，但这类数据稀缺、收集困难且维度高，限制了语言与动作的对齐粒度，并加剧了长时视频生成的挑战。

Method: 提出PEWM框架，将视频生成限制在固定较短的时间范围内，配备模块化视觉语言模型规划器和起点-目标热图引导机制，支持原始级策略的组合泛化。

Result: 实现了语言概念与机器人动作视觉表示的细粒度对齐，降低了学习复杂度，提高了具身数据收集的效率，减少了推理延迟，支持灵活闭环控制和复杂任务的组合泛化。

Conclusion: PEWM框架利用视频模型中的时空视觉先验和视觉语言模型的语义感知能力，弥合了细粒度物理交互与高层推理之间的差距，为可扩展、可解释和通用具身智能铺平了道路。

Abstract: While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \textit{"GPT moment"} in the embodied domain. There is a naive observation: \textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \textit{2) reduces} learning complexity, \textit{3) improves} data efficiency in embodied data collection, and \textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.

</details>


### [276] [From Features to Reference Points: Lightweight and Adaptive Fusion for Cooperative Autonomous Driving](https://arxiv.org/abs/2511.18757)
*Yongqi Zhu,Morui Zhu,Qi Chen,Deyuan Qu,Song Fu,Qing Yang*

Main category: cs.CV

TL;DR: RefPtsFusion是一个轻量级、可解释的协同自动驾驶框架，通过交换紧凑的参考点（如物体位置、速度、尺寸信息）而非大型特征图，将通信带宽从数百MB/s降至几KB/s，同时保持稳定的感知性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统协同自动驾驶方法中特征图共享带来的高通信带宽需求问题，创建传感器和模型无关的接口，适应异构感知模型的车辆协同。

Method: 使用参考点交换替代特征图共享，开发选择性Top-K查询融合，从发送方选择性添加高置信度查询，在准确性和通信成本之间取得平衡。

Result: 在M3CAD数据集上，通信开销降低了五个数量级（从数百MB/s降至几KB/s），同时保持稳定的感知性能，表现出强大的鲁棒性和一致的传输行为。

Conclusion: RefPtsFusion展示了在可扩展、实时协同驾驶系统中的潜力，为异构车辆协同提供了高效解决方案。

Abstract: We present RefPtsFusion, a lightweight and interpretable framework for cooperative autonomous driving. Instead of sharing large feature maps or query embeddings, vehicles exchange compact reference points, e.g., objects' positions, velocities, and size information. This approach shifts the focus from "what is seen" to "where to see", creating a sensor- and model-independent interface that works well across vehicles with heterogeneous perception models while greatly reducing communication bandwidth. To enhance the richness of shared information, we further develop a selective Top-K query fusion that selectively adds high-confidence queries from the sender. It thus achieves a strong balance between accuracy and communication cost. Experiments on the M3CAD dataset show that RefPtsFusion maintains stable perception performance while reducing communication overhead by five orders of magnitude, dropping from hundreds of MB/s to only a few KB/s at 5 FPS (frame per second), compared to traditional feature-level fusion methods. Extensive experiments also demonstrate RefPtsFusion's strong robustness and consistent transmission behavior, highlighting its potential for scalable, real-time cooperative driving systems.

</details>


### [277] [VAOT: Vessel-Aware Optimal Transport for Retinal Fundus Enhancement](https://arxiv.org/abs/2511.18763)
*Xuanzhao Dong,Wenhui Zhu,Yujian Xiong,Xiwen Chen,Hao Wang,Xin Li,Jiajun Cheng,Zhipeng Wang,Shao Tang,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: VAOT是一个用于彩色眼底图像增强的框架，结合最优传输目标和结构保留正则化器，在无配对设置中保持血管结构完整性。


<details>
  <summary>Details</summary>
Motivation: 传统GAN-based增强方法会扭曲临床关键的血管结构，改变血管拓扑和端点完整性，这促使开发能够保持血管结构完整性的增强方法。

Method: 提出VAOT框架，结合最优传输目标和两个结构保留正则化器：基于骨架的损失函数维护全局血管连通性，端点感知损失稳定局部端点。

Result: 在合成退化基准和下游血管及病变分割评估中，该方法优于多个最先进基线方法。

Conclusion: VAOT框架在减少噪声的同时有效保持了血管结构，在无配对图像增强中表现出优越性能。

Abstract: Color fundus photography (CFP) is central to diagnosing and monitoring retinal disease, yet its acquisition variability (e.g., illumination changes) often degrades image quality, which motivates robust enhancement methods. Unpaired enhancement pipelines are typically GAN-based, however, they can distort clinically critical vasculature, altering vessel topology and endpoint integrity. Motivated by these structural alterations, we propose Vessel-Aware Optimal Transport (\textbf{VAOT}), a framework that combines an optimal-transport objective with two structure-preserving regularizers: (i) a skeleton-based loss to maintain global vascular connectivity and (ii) an endpoint-aware loss to stabilize local termini. These constraints guide learning in the unpaired setting, reducing noise while preserving vessel structure. Experimental results on synthetic degradation benchmark and downstream evaluations in vessel and lesion segmentation demonstrate the superiority of the proposed methods against several state-of-the art baselines. The code is available at https://github.com/Retinal-Research/VAOT

</details>


### [278] [NI-Tex: Non-isometric Image-based Garment Texture Generation](https://arxiv.org/abs/2511.18765)
*Hui Shan,Ming Li,Haitao Yang,Kai Zheng,Sizhe Zheng,Yanwei Fu,Xiangru Huang*

Main category: cs.CV

TL;DR: 提出了一种非等距图像到3D服装纹理生成方法，通过物理模拟数据集和纳米香蕉编辑技术，实现跨姿态的PBR纹理生成。


<details>
  <summary>Details</summary>
Motivation: 现有工业3D服装网格几何覆盖广泛，但纹理多样性有限。传统方法需要严格的拓扑一致性或精确的网格变形，限制了纹理生成质量和灵活性。

Method: 构建3D服装视频数据集提供几何和材质监督；使用纳米香蕉进行高质量非等距图像编辑；提出基于不确定性引导视图选择的迭代烘焙方法。

Result: 通过广泛实验证明，该方法能生成适用于工业级3D服装设计的多样化且空间对齐的PBR材质。

Conclusion: 该方法解决了非等距图像到3D服装纹理生成的挑战，提供了生产就绪的PBR纹理，适用于工业应用。

Abstract: Existing industrial 3D garment meshes already cover most real-world clothing geometries, yet their texture diversity remains limited. To acquire more realistic textures, generative methods are often used to extract Physically-based Rendering (PBR) textures and materials from large collections of wild images and project them back onto garment meshes. However, most image-conditioned texture generation approaches require strict topological consistency between the input image and the input 3D mesh, or rely on accurate mesh deformation to match to the image poses, which significantly constrains the texture generation quality and flexibility. To address the challenging problem of non-isometric image-based garment texture generation, we construct 3D Garment Videos, a physically simulated, garment-centric dataset that provides consistent geometry and material supervision across diverse deformations, enabling robust cross-pose texture learning. We further employ Nano Banana for high-quality non-isometric image editing, achieving reliable cross-topology texture generation between non-isometric image-geometry pairs. Finally, we propose an iterative baking method via uncertainty-guided view selection and reweighting that fuses multi-view predictions into seamless, production-ready PBR textures. Through extensive experiments, we demonstrate that our feedforward dual-branch architecture generates versatile and spatially aligned PBR materials suitable for industry-level 3D garment design.

</details>


### [279] [Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment](https://arxiv.org/abs/2511.18766)
*Xintao Chen,Xiaohao Xu,Bozhong Zheng,Yun Liu,Yingna Wu*

Main category: cs.CV

TL;DR: VSAD是一个新颖的多视角异常检测框架，通过显式建模跨视角的几何一致性来学习视角不变表示，在真实工业数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多视角图像中由于视角变化导致的良性外观变化与真实缺陷难以区分的问题，现有单视角方法处理多视角时特征表示不一致且误报率高。

Method: 提出ViewSense-AD框架，包含多视角对齐模块(MVAM)利用单应性投影对齐相邻视角特征，集成到View-Align潜在扩散模型(VALDM)中进行渐进式多阶段对齐，并采用轻量级融合精炼模块(FRM)增强全局一致性。

Result: 在RealIAD和MANTA数据集上的广泛实验表明，VSAD在像素、视角和样本级别的视觉异常检测中显著优于现有方法，证明其对大视角偏移和复杂纹理的鲁棒性。

Conclusion: VSAD通过几何一致性建模和多阶段对齐策略，能够从多视角图像中构建对物体表面的连贯整体理解，有效解决了多视角异常检测的挑战。

Abstract: Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.

</details>


### [280] [Rethinking Garment Conditioning in Diffusion-based Virtual Try-On](https://arxiv.org/abs/2511.18775)
*Kihyun Na,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: Re-CatVTON是一个高效的虚拟试穿模型，通过单UNet架构实现高性能，相比双UNet模型显著减少了计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 虽然基于扩散模型的双UNet虚拟试穿模型具有更好的保真度，但其计算和内存开销过大，需要开发更高效的模型。

Method: 通过可视化分析和理论分析提出三个关于上下文特征学习的假设，开发单UNet模型Re-CatVTON，引入改进的分类器自由引导策略，并直接注入干净服装潜变量来防止预测误差累积。

Result: 相比前身CatVTON显著提升性能，计算和内存需求低于高性能双UNet模型Leffa，在FID、KID和LPIPS指标上表现更好，仅SSIM略有下降。

Conclusion: Re-CatVTON为单UNet虚拟试穿模型建立了新的效率-性能平衡点，在保持高质量的同时显著降低了计算成本。

Abstract: Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.

</details>


### [281] [ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection](https://arxiv.org/abs/2511.18780)
*Ruize Ma,Minghong Cai,Yilei Jiang,Jiaming Han,Yi Feng,Yingshui Tan,Xiaoyong Zhu,Bo Zhang,Bo Zheng,Xiangyu Yue*

Main category: cs.CV

TL;DR: ConceptGuard是一个统一的安全框架，用于主动检测和缓解多模态视频生成中的不安全语义，通过对比检测和语义抑制机制，在融合的图像-文本输入中识别潜在安全风险并引导生成过程远离不安全概念。


<details>
  <summary>Details</summary>
Motivation: 现有的安全方法通常是仅文本的、需要预先了解风险类别或作为后生成审计器，难以主动缓解多模态组合风险。视频生成模型从多模态提示创建高质量视频的能力带来了新的安全风险。

Method: ConceptGuard采用两阶段方法：1）对比检测模块将融合的图像-文本输入投影到结构化概念空间来识别潜在安全风险；2）语义抑制机制通过干预提示的多模态条件来引导生成过程远离不安全概念。

Result: 在两个新基准上的综合实验表明，ConceptGuard在风险检测和安全视频生成方面始终优于现有基线方法，达到了最先进的性能。

Conclusion: ConceptGuard为多模态视频生成提供了一个有效的统一安全框架，能够主动检测和缓解不安全语义，解决了现有方法在处理多模态组合风险方面的局限性。

Abstract: Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation.

</details>


### [282] [A Novel Dual-Stream Framework for dMRI Tractography Streamline Classification with Joint dMRI and fMRI Data](https://arxiv.org/abs/2511.18781)
*Haotian Yan,Bocheng Guo,Jianzhong He,Nir A. Sochen,Ofer Pasternak,Lauren J O'Donnell,Fan Zhang*

Main category: cs.CV

TL;DR: 提出了一种新颖的双流束流分类框架，联合分析dMRI和fMRI数据，通过增强功能一致性来改进白质束分割。


<details>
  <summary>Details</summary>
Motivation: 当前束流分类方法主要依赖束流轨迹的几何特征，无法区分具有相似路径但功能不同的纤维束。

Method: 设计了一个新颖网络，使用预训练主干模型处理完整束流轨迹，同时通过辅助网络处理纤维端点区域的fMRI信号。

Result: 通过将皮质脊髓束分割为四个躯体定位亚区，消融研究和与最先进方法的比较证明了该方法的优越性能。

Conclusion: 该方法通过联合分析结构和功能数据，显著提高了束流分类的功能一致性。

Abstract: Streamline classification is essential to identify anatomically meaningful white matter tracts from diffusion MRI (dMRI) tractography. However, current streamline classification methods rely primarily on the geometric features of the streamline trajectory, failing to distinguish between functionally distinct fiber tracts with similar pathways. To address this, we introduce a novel dual-stream streamline classification framework that jointly analyzes dMRI and functional MRI (fMRI) data to enhance the functional coherence of tract parcellation. We design a novel network that performs streamline classification using a pretrained backbone model for full streamline trajectories, while augmenting with an auxiliary network that processes fMRI signals from fiber endpoint regions. We demonstrate our method by parcellating the corticospinal tract (CST) into its four somatotopic subdivisions. Experimental results from ablation studies and comparisons with state-of-the-art methods demonstrate our approach's superior performance.

</details>


### [283] [STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution](https://arxiv.org/abs/2511.18786)
*Junyang Chen,Jiangxin Dong,Long Sun,Yixin Yang,Jinshan Pan*

Main category: cs.CV

TL;DR: STCDiT是一个基于预训练视频扩散模型的视频超分辨率框架，通过运动感知VAE重建和锚帧引导方法，在复杂相机运动下恢复结构保真和时间稳定的视频。


<details>
  <summary>Details</summary>
Motivation: 解决视频超分辨率中保持时间稳定性和结构保真度的挑战，特别是在复杂相机运动场景下。

Method: 1. 运动感知VAE重建：分段重建具有统一运动特征的视频片段；2. 锚帧引导：利用第一帧潜在空间的结构信息约束生成过程。

Result: 在结构保真度和时间一致性方面优于现有最先进方法。

Conclusion: 结合运动感知重建和锚帧引导的视频扩散模型能够实现高质量的视频超分辨率。

Abstract: We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions. The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation. To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions. Moreover, we observe that the first-frame latent extracted by the VAE encoder in each clip, termed the anchor-frame latent, remains unaffected by temporal compression and retains richer spatial structural information than subsequent frame latents. We further develop an anchor-frame guidance approach that leverages structural information from anchor frames to constrain the generation process and improve structural fidelity of video features. Coupling these two designs enables the video diffusion model to achieve high-quality video super-resolution. Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.

</details>


### [284] [Understanding Task Transfer in Vision-Language Models](https://arxiv.org/abs/2511.18787)
*Bhuvan Sachdeva,Karan Uppal,Abhinav Java,Vineeth N. Balasubramanian*

Main category: cs.CV

TL;DR: 本文提出了Perfection Gap Factor (PGF)指标来系统研究视觉语言模型在感知任务间的迁移性，揭示了任务间的正负迁移模式，为高效训练提供指导。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态基准测试中表现良好，但在深度估计、物体计数等视觉感知任务上落后于人类和专业模型。针对单个任务的微调会对其他任务产生不可预测的影响，这使得任务特定微调具有挑战性。

Method: 通过系统研究任务迁移性，引入PGF指标来量化迁移的广度和幅度。使用三个开源权重VLM在13个感知任务上进行评估，构建任务迁移图来揭示任务间的关系。

Result: 分析发现了正负迁移模式，识别了相互影响的任务组，根据迁移行为将任务组织成角色，并展示了PGF如何指导数据选择以实现更高效训练。

Conclusion: 这些发现既突出了正迁移的机会，也揭示了负干扰的风险，为推进VLM发展提供了可操作的指导。

Abstract: Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.

</details>


### [285] [StereoDETR: Stereo-based Transformer for 3D Object Detection](https://arxiv.org/abs/2511.18788)
*Shiyi Mu,Zichong Gu,Zhiqi Ai,Anqi Liu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: StereoDETR是一个基于DETR的高效立体3D物体检测框架，通过单目DETR分支和立体分支的结合，在保持竞争性精度的同时实现了实时推理，首次在速度上超越单目方法。


<details>
  <summary>Details</summary>
Motivation: 立体3D检测方法相比单目方法精度更高，但存在计算开销大、延迟高的问题。现有最优立体方法精度是单目方法的两倍，但推理速度只有单目方法的一半。

Method: StereoDETR包含两个分支：单目DETR分支（基于2D DETR，增加预测物体尺度、方向和采样点的通道）和立体分支（利用低成本多尺度视差特征预测物体级深度图）。两个分支通过可微分深度采样策略耦合，并引入约束监督策略处理遮挡问题。

Result: StereoDETR实现了实时推理，是首个在速度上超越单目方法的立体检测方法。在KITTI基准测试中取得了竞争性精度，在行人和自行车子集上创造了新的最优结果。

Conclusion: StereoDETR成功解决了立体3D检测的计算效率问题，在保持高精度的同时实现了实时性能，为立体视觉的实际应用提供了可行方案。

Abstract: Compared to monocular 3D object detection, stereo-based 3D methods offer significantly higher accuracy but still suffer from high computational overhead and latency. The state-of-the-art stereo 3D detection method achieves twice the accuracy of monocular approaches, yet its inference speed is only half as fast. In this paper, we propose StereoDETR, an efficient stereo 3D object detection framework based on DETR. StereoDETR consists of two branches: a monocular DETR branch and a stereo branch. The DETR branch is built upon 2D DETR with additional channels for predicting object scale, orientation, and sampling points. The stereo branch leverages low-cost multi-scale disparity features to predict object-level depth maps. These two branches are coupled solely through a differentiable depth sampling strategy. To handle occlusion, we introduce a constrained supervision strategy for sampling points without requiring extra annotations. StereoDETR achieves real-time inference and is the first stereo-based method to surpass monocular approaches in speed. It also achieves competitive accuracy on the public KITTI benchmark, setting new state-of-the-art results on pedestrian and cyclist subsets. The code is available at https://github.com/shiyi-mu/StereoDETR-OPEN.

</details>


### [286] [Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing](https://arxiv.org/abs/2511.18792)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Chun Tung Chou,Wen Hu*

Main category: cs.CV

TL;DR: 该论文通过大规模MAE预训练方法，在14个数据集、130万样本上验证了数据规模对Wi-Fi感知领域泛化的重要性，发现数据多样性比模型容量对跨域性能影响更大。


<details>
  <summary>Details</summary>
Motivation: 解决Wi-Fi感知在实际应用中因环境、硬件、用户变化导致的领域偏移问题，当前数据集规模有限且碎片化，限制了模型的泛化能力。

Method: 采用掩码自编码器(MAE)预训练方法，在14个Wi-Fi CSI数据集上使用4种设备、多个频段(2.4/5/6 GHz)和带宽(20-160 MHz)收集的130万样本进行大规模预训练。

Result: 实验显示：1) 预训练数据量增加带来未见领域性能的对数线性提升；2) 当前数据量下，更大模型仅带来边际收益；3) 大规模预训练在人类活动识别、手势识别和用户识别任务上比监督学习基线提升2.2%-15.7%的跨域准确率。

Conclusion: 数据规模而非模型容量是当前Wi-Fi感知泛化的主要瓶颈，为设计实际可部署的鲁棒Wi-Fi感知系统提供了重要方向。

Abstract: While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical "domain shift" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.

</details>


### [287] [PartDiffuser: Part-wise 3D Mesh Generation via Discrete Diffusion](https://arxiv.org/abs/2511.18801)
*Yichen Yang,Hong Li,Haodong Zhu,Linin Yang,Guojun Lei,Sheng Xu,Baochang Zhang*

Main category: cs.CV

TL;DR: PartDiffuser是一个半自回归扩散框架，通过部件级生成平衡全局结构和局部细节，解决现有自回归方法在网格生成中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归方法在生成艺术家设计的网格时难以平衡全局结构一致性和高保真局部细节，且容易产生误差累积。

Method: 首先对网格进行语义分割，然后在部件间采用自回归确保全局拓扑，在部件内使用并行离散扩散过程精确重建高频几何特征，基于DiT架构并引入部件感知交叉注意力机制。

Result: 实验表明该方法在生成具有丰富细节的3D网格方面显著优于最先进模型，展现出适合实际应用的卓越细节表示能力。

Conclusion: PartDiffuser通过解耦全局和局部生成任务，有效解决了网格生成中的结构一致性和细节保真度平衡问题。

Abstract: Existing autoregressive (AR) methods for generating artist-designed meshes struggle to balance global structural consistency with high-fidelity local details, and are susceptible to error accumulation. To address this, we propose PartDiffuser, a novel semi-autoregressive diffusion framework for point-cloud-to-mesh generation. The method first performs semantic segmentation on the mesh and then operates in a "part-wise" manner: it employs autoregression between parts to ensure global topology, while utilizing a parallel discrete diffusion process within each semantic part to precisely reconstruct high-frequency geometric features. PartDiffuser is based on the DiT architecture and introduces a part-aware cross-attention mechanism, using point clouds as hierarchical geometric conditioning to dynamically control the generation process, thereby effectively decoupling the global and local generation tasks. Experiments demonstrate that this method significantly outperforms state-of-the-art (SOTA) models in generating 3D meshes with rich detail, exhibiting exceptional detail representation suitable for real-world applications.

</details>


### [288] [TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging](https://arxiv.org/abs/2511.18806)
*Qinglei Cao,Ziyao Tang,Xiaoqin Tang*

Main category: cs.CV

TL;DR: 提出了一种基于目标先验的3D CT重建框架，通过从投影数据中提取目标先验来增强隐式学习，在超稀疏视图场景下显著提升了重建精度和学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF的隐式3D重建方法往往忽略了物体解剖先验的重要性，限制了重建精度和学习效率，特别是在超稀疏视图场景下。

Method: 采用目标先验引导的隐式学习框架，结合位置编码和结构编码进行体素级隐式重建，利用目标先验指导体素采样并丰富结构编码。

Result: 在复杂腹部数据集上的实验表明，学习效率比当前领先模型NAF提升10倍，重建质量优于最准确模型NeRP，在10、20、30个投影下PSNR分别提升3.57dB、5.42dB、5.70dB。

Conclusion: 提出的目标先验引导隐式学习框架有效解决了超稀疏视图CT重建的挑战，显著提升了重建质量和学习效率。

Abstract: X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.

</details>


### [289] [Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache](https://arxiv.org/abs/2511.18811)
*Yuqiu Jiang,Xiaozhen Qiao,Tianyu Mei,Haojian Huang,Yifan Chen,Ye Zheng,Zhe Sun*

Main category: cs.CV

TL;DR: 提出Adaptive Diversity Cache (ADC)模块，一种无需训练即插即用的机制，通过构建类别特定的缓存来积累高置信度和多样化的特征表示，缓解HOI检测中的长尾偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有的VLM-based HOI检测方法严重依赖额外训练或提示调优，导致计算开销大且可扩展性有限，特别是在长尾场景中罕见交互严重不足的问题。

Method: ADC模块构建类别特定的缓存，在推理过程中积累高置信度和多样化的特征表示，采用频率感知的缓存自适应机制，优先考虑稀有类别，无需额外训练或微调即可实现鲁棒的预测校准。

Result: 在HICO-DET和V-COCO数据集上的实验表明，ADC持续改进现有HOI检测器，在稀有类别上获得高达+8.57% mAP增益，在完整数据集上获得+4.39% mAP增益。

Conclusion: ADC有效缓解了长尾偏差问题，同时保持了整体性能，是一种高效且可扩展的HOI检测改进方法。

Abstract: Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\% mAP gain on rare categories and +4.39\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.

</details>


### [290] [DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video](https://arxiv.org/abs/2511.18814)
*Jiawei Hou,Shenghao Zhang,Can Wang,Zheng Gu,Yonggen Ling,Taiping Zeng,Xiangyang Xue,Jingbo Zhang*

Main category: cs.CV

TL;DR: 提出了DetAny4D，一个端到端的开放集4D物体检测框架，直接从序列输入预测3D边界框，解决了现有方法的时间不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有4D物体检测方法要么逐帧预测缺乏时间一致性建模，要么采用复杂多阶段流程容易产生误差传播，且缺乏大规模连续可靠3D边界框标注数据集。

Method: 融合预训练基础模型的多模态特征，设计几何感知的时空解码器捕捉时空动态，采用多任务学习架构和专用训练策略保持序列全局一致性。

Result: 在DA4D数据集上实验表明，DetAny4D达到竞争性检测精度，显著提升时间稳定性，有效解决4D检测中的抖动和不一致问题。

Conclusion: DetAny4D框架在4D物体检测中实现了准确且时间稳定的性能，为开放集场景理解提供了有效解决方案。

Abstract: Reliable 4D object detection, which refers to 3D object detection in streaming video, is crucial for perceiving and understanding the real world. Existing open-set 4D object detection methods typically make predictions on a frame-by-frame basis without modeling temporal consistency, or rely on complex multi-stage pipelines that are prone to error propagation across cascaded stages. Progress in this area has been hindered by the lack of large-scale datasets that capture continuous reliable 3D bounding box (b-box) annotations. To overcome these challenges, we first introduce DA4D, a large-scale 4D detection dataset containing over 280k sequences with high-quality b-box annotations collected under diverse conditions. Building on DA4D, we propose DetAny4D, an open-set end-to-end framework that predicts 3D b-boxes directly from sequential inputs. DetAny4D fuses multi-modal features from pre-trained foundational models and designs a geometry-aware spatiotemporal decoder to effectively capture both spatial and temporal dynamics. Furthermore, it adopts a multi-task learning architecture coupled with a dedicated training strategy to maintain global consistency across sequences of varying lengths. Extensive experiments show that DetAny4D achieves competitive detection accuracy and significantly improves temporal stability, effectively addressing long-standing issues of jitter and inconsistency in 4D object detection. Data and code will be released upon acceptance.

</details>


### [291] [SupLID: Geometrical Guidance for Out-of-Distribution Detection in Semantic Segmentation](https://arxiv.org/abs/2511.18816)
*Nimeshika Udayangani,Sarah Erfani,Christopher Leckie*

Main category: cs.CV

TL;DR: SupLID是一个新颖的语义分割OOD检测框架，通过利用语义空间的几何结构和线性内在维度(LID)来指导分类器衍生的OOD分数，在像素级异常检测中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语义分割OOD检测方法主要基于分类器置信度分数，存在过度自信的脆弱性，需要更有效的几何结构指导来提升检测能力。

Method: SupLID构建几何核心集捕捉ID子空间的内在结构，在超像素级别计算OOD分数，结合LID分析距离分布来表征高维数据的局部结构。

Result: SupLID显著增强了现有基于分类器的OOD分数，在AUR、FPR和AUP等关键评估指标上达到了最先进的性能。

Conclusion: SupLID提供的几何线索与传统分类器置信度形成互补信号，能够无缝集成到任何语义分割分类器中，有效检测多样化的OOD场景。

Abstract: Out-of-Distribution (OOD) detection in semantic segmentation aims to localize anomalous regions at the pixel level, advancing beyond traditional image-level OOD techniques to better suit real-world applications such as autonomous driving. Recent literature has successfully explored the adaptation of commonly used image-level OOD methods--primarily based on classifier-derived confidence scores (e.g., energy or entropy)--for this pixel-precise task. However, these methods inherit a set of limitations, including vulnerability to overconfidence. In this work, we introduce SupLID, a novel framework that effectively guides classifier-derived OOD scores by exploiting the geometrical structure of the underlying semantic space, particularly using Linear Intrinsic Dimensionality (LID). While LID effectively characterizes the local structure of high-dimensional data by analyzing distance distributions, its direct application at the pixel level remains challenging. To overcome this, SupLID constructs a geometrical coreset that captures the intrinsic structure of the in-distribution (ID) subspace. It then computes OOD scores at the superpixel level, enabling both efficient real-time inference and improved spatial smoothness. We demonstrate that geometrical cues derived from SupLID serve as a complementary signal to traditional classifier confidence, enhancing the model's ability to detect diverse OOD scenarios. Designed as a post-hoc scoring method, SupLID can be seamlessly integrated with any semantic segmentation classifier at deployment time. Our results demonstrate that SupLID significantly enhances existing classifier-based OOD scores, achieving state-of-the-art performance across key evaluation metrics, including AUR, FPR, and AUP. Code is available at https://github.com/hdnugit/SupLID.

</details>


### [292] [Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring](https://arxiv.org/abs/2511.18817)
*Siyuan Wei,Chunjie Wang,Xiao Liu,Xiaosheng Yan,Zhishan Zhou,Rui Huang*

Main category: cs.CV

TL;DR: 提出了一个完全自动化的流水线，将原始3D扫描转换为高质量、无歧义的对话数据，解决了3D MLLMs数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 3D多模态大语言模型落后于2D模型，主要因为缺乏大规模高质量的3D场景对话数据集，且现有方法存在视角歧义和对象指代歧义问题。

Method: 开发了包含四个阶段的自动化流水线：元标注收集、场景图构建与关系校正、区分性对象指代、多任务数据生成，结合规则约束与2D MLLMs/LLMs。

Result: 生成了Disc3D数据集，包含25K混合3D场景中的200多万样本，涵盖多种任务。实验表明使用Disc3D训练能在多个基准测试中带来显著改进。

Conclusion: 该自动化流水线能以低成本生成高质量3D对话数据，有效解决现有数据集的固有缺陷，推动3D MLLMs的发展。

Abstract: 3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.

</details>


### [293] [DiP: Taming Diffusion Models in Pixel Space](https://arxiv.org/abs/2511.18822)
*Zhennan Chen,Junwei Zhu,Xu Chen,Jiangning Zhang,Xiaobin Hu,Hanzhen Zhao,Chengjie Wang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DiP是一个高效的像素空间扩散框架，通过解耦全局结构和局部细节生成，在保持高质量的同时实现与潜在扩散模型相当的效率。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在生成质量和计算效率之间的权衡问题，避免潜在扩散模型的信息损失和非端到端训练，同时克服像素空间模型的计算瓶颈。

Method: 将生成过程解耦为全局和局部两个阶段：使用扩散变换器处理大块区域构建全局结构，同时通过轻量级补丁细节器恢复局部细节。

Result: 推理速度比先前方法快10倍，仅增加0.3%的参数数量，在ImageNet 256×256上达到1.90 FID分数。

Conclusion: DiP框架成功平衡了生成质量和计算效率，无需依赖VAE即可实现高效的高分辨率合成。

Abstract: Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\times$256.

</details>


### [294] [VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models](https://arxiv.org/abs/2511.18823)
*Fufangchen Zhao,Liao Zhang,Daiqi Shi,Yuanjun Gao,Chen Ye,Yang Cai,Jian Gao,Danfeng Yan*

Main category: cs.CV

TL;DR: VideoPerceiver是一个新颖的视频多模态大语言模型，通过两阶段训练框架增强视频理解中的细粒度感知能力，特别针对短片段中的瞬时动作和长视频中的罕见事件进行优化。


<details>
  <summary>Details</summary>
Motivation: 解决当前视频多模态大语言模型在理解短片段中的瞬时动作和长视频中的罕见瞬态事件方面的局限性，提升对细粒度运动线索的感知能力。

Method: 采用两阶段训练框架：1）监督微调阶段构建"关键信息缺失"视频，通过替换关键帧并引入辅助对比损失来增强对细粒度运动线索的敏感性；2）强化学习阶段使用相对奖励机制，确保完整视频生成的描述优于降级输入。

Result: 在细粒度动作理解和罕见事件描述基准测试中显著优于最先进的视频多模态大语言模型，同时在标准任务上保持强大性能。

Conclusion: 通过优先处理任务相关的视觉特征，重新定义了视频语言模型的细粒度感知训练方法，为视频理解提供了更精确的解决方案。

Abstract: We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct "key-information-missing" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.

</details>


### [295] [Q-Save: Towards Scoring and Attribution for Generated Video Evaluation](https://arxiv.org/abs/2511.18825)
*Xiele Wu,Zicheng Zhang,Mingtao Chen,Yixian Liu,Yiming Liu,Shushi Wang,Zhichao Hu,Yuhong Liu,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 提出了Q-Save基准数据集和模型，用于对AI生成视频进行整体和可解释的质量评估，包含近10000个视频的多维度标注和统一评估模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够对AI生成视频进行整体质量评估并提供可解释性理由的基准数据集和模型，需要建立多维度、可解释的评估体系。

Method: 采用SlowFast框架区分快慢帧处理，使用多阶段训练策略：先进行监督微调，然后使用分组相对策略优化增强模型，最后再次进行监督微调以提高稳定性。

Result: 模型在视频质量预测方面达到了最先进的性能，同时提供与人类对齐的可解释性理由。

Conclusion: Q-Save为生成视频研究中的可解释评估建立了坚实基础，有助于多模态生成和可信AI的发展。

Abstract: We present Q-Save, a new benchmark dataset and model for holistic and explainable evaluation of AI-generated video (AIGV) quality. The dataset contains near 10000 videos, each annotated with a scalar mean opinion score (MOS) and fine-grained attribution labels along three core dimensions: visual quality, dynamic quality, and text-video alignment. These multi-aspect annotations enable both accurate quality assessment and interpretable reasoning behind the scores. To leverage this data, we propose a unified evaluation model that jointly performs quality scoring and attribution-based explanation. The model adopts the SlowFast framework to distinguish between fast frames and slow frames - slow frames are processed with high resolution while fast frames use low resolution, balancing evaluation accuracy and computational efficiency. For training, we use data formatted in Chain-of-Thought (COT) style and employ a multi-stage strategy: we first conduct Supervised Fine-Tuning (SFT), then further enhance the model with Grouped Relative Policy Optimization (GRPO), and finally perform SFT again to improve model stability. Experimental results demonstrate that our model achieves state-of-the-art performance in video quality prediction while also providing human-aligned, interpretable justifications. Our dataset and model establish a strong foundation for explainable evaluation in generative video research, contributing to the development of multimodal generation and trustworthy AI. Code and dataset will be released upon publication.

</details>


### [296] [Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification](https://arxiv.org/abs/2511.18826)
*Aakash Gore,Anoushka Dey,Aryan Mishra*

Main category: cs.CV

TL;DR: 提出了一种不确定性感知的双学生知识蒸馏框架，利用教师预测不确定性来选择性指导学生学习，通过异构学生架构的协作学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法对所有教师预测一视同仁，忽略了教师预测的不确定性，这可能导致学生模型学习到不可靠的知识。

Method: 采用不确定性感知的双学生知识蒸馏框架，使用ResNet-18和MobileNetV2作为异构学生架构，通过同伴学习机制相互协作学习。

Result: 在ImageNet-100上，ResNet-18达到83.84% top-1准确率，MobileNetV2达到81.46% top-1准确率，相比传统单学生蒸馏方法分别提升2.04%和0.92%。

Conclusion: 不确定性感知的双学生知识蒸馏框架能有效提升模型压缩性能，异构学生架构的协作学习机制有助于知识传递的优化。

Abstract: Knowledge distillation has emerged as a powerful technique for model compression, enabling the transfer of knowledge from large teacher networks to compact student models. However, traditional knowledge distillation methods treat all teacher predictions equally, regardless of the teacher's confidence in those predictions. This paper proposes an uncertainty-aware dual-student knowledge distillation framework that leverages teacher prediction uncertainty to selectively guide student learning. We introduce a peer-learning mechanism where two heterogeneous student architectures, specifically ResNet-18 and MobileNetV2, learn collaboratively from both the teacher network and each other. Experimental results on ImageNet-100 demonstrate that our approach achieves superior performance compared to baseline knowledge distillation methods, with ResNet-18 achieving 83.84\% top-1 accuracy and MobileNetV2 achieving 81.46\% top-1 accuracy, representing improvements of 2.04\% and 0.92\% respectively over traditional single-student distillation approaches.

</details>


### [297] [Leveraging Metaheuristic Approaches to Improve Deep Learning Systems for Anxiety Disorder Detection](https://arxiv.org/abs/2511.18827)
*Mohammadreza Amiri,Monireh Hosseini*

Main category: cs.CV

TL;DR: 本研究提出了一种结合深度学习与群体智能优化的混合模型，用于通过多模态可穿戴传感器数据自动检测焦虑症，相比单一深度学习方法显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统焦虑症诊断主要依赖主观评估方法（如临床访谈和自评问卷），存在耗时、评估者依赖性强等局限性。人工智能技术的发展为更一致、自动化的焦虑检测提供了新机会。

Method: 开发了一个综合模型，将深度学习架构与群体智能优化策略（包括遗传算法和粒子群优化）相结合。使用多模态可穿戴传感器数据集分析生理、情绪和行为信号，通过优化算法精炼特征空间和超参数，深度学习组件从序列化多源输入中提取分层判别性表征。

Result: 评估显示，这两种计算范式的融合显著提升了检测性能，相比单独使用深度网络，混合模型在准确率方面取得了显著改善，并在不同个体间表现出更强的泛化能力。

Conclusion: 研究结果强调了将元启发式优化与深度学习相结合，开发可扩展、客观且具有临床意义的焦虑障碍评估解决方案的潜力。

Abstract: Despite being among the most common psychological disorders, anxiety-related conditions are still primarily identified through subjective assessments, such as clinical interviews and self-evaluation questionnaires. These conventional methods often require significant time and may vary depending on the evaluator. However, the emergence of advanced artificial intelligence techniques has created new opportunities for detecting anxiety in a more consistent and automated manner. To address the limitations of traditional approaches, this study introduces a comprehensive model that integrates deep learning architectures with optimization strategies inspired by swarm intelligence. Using multimodal and wearable-sensor datasets, the framework analyzes physiological, emotional, and behavioral signals. Swarm intelligence techniques including genetic algorithms and particle swarm optimization are incorporated to refine the feature space and optimize hyperparameters. Meanwhile, deep learning components are tasked with deriving layered and discriminative representations from sequential, multi-source inputs. Our evaluation shows that the fusion of these two computational paradigms significantly enhances detection performance compared with using deep networks alone. The hybrid model achieves notable improvements in accuracy and demonstrates stronger generalization across various individuals. Overall, the results highlight the potential of combining metaheuristic optimization with deep learning to develop scalable, objective, and clinically meaningful solutions for assessing anxiety disorders

</details>


### [298] [VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction](https://arxiv.org/abs/2511.18831)
*Shaobo Wang,Tianle Niu,Runkang Yang,Deshan Liu,Xu He,Zichen Wen,Conghui He,Xuming Hu,Linfeng Zhang*

Main category: cs.CV

TL;DR: VideoCompressa通过动态潜在压缩框架解决视频数据集存储和计算成本高的问题，通过联合优化关键帧选择器和VAE压缩，仅需极少量数据即可达到甚至超越全数据训练效果。


<details>
  <summary>Details</summary>
Motivation: 视频理解模型的可扩展性受到大规模视频数据集高昂存储和计算成本的限制，现有数据合成方法难以扩展到视频领域，因为视频存在时间冗余和复杂的时空动态特性。

Method: 提出VideoCompressa框架，包含可微分关键帧选择器（轻量级ConvNet+Gumbel-Softmax采样）和预训练冻结VAE，联合优化关键帧选择和潜在代码压缩，最大化保留任务相关信息。

Result: 在UCF101上仅使用0.13%原始数据就超越全数据训练2.34个百分点，速度提升5800倍；在HMDB51上仅用0.41%训练数据即可匹配全数据性能，比零样本基线提升10.61%。

Conclusion: VideoCompressa通过解决视频数据中的帧级冗余问题，实现了前所未有的数据效率，为大规模视频理解提供了高效的解决方案。

Abstract: The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets. While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics. In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy. To leverage this insight, we introduce VideoCompressa, a novel framework for video data synthesis that reframes the problem as dynamic latent compression. Specifically, VideoCompressa jointly optimizes a differentiable keyframe selector-implemented as a lightweight ConvNet with Gumbel-Softmax sampling-to identify the most informative frames, and a pretrained, frozen Variational Autoencoder (VAE) to compress these frames into compact, semantically rich latent codes. These latent representations are then fed into a compression network, enabling end-to-end backpropagation. Crucially, the keyframe selector and synthetic latent codes are co-optimized to maximize retention of task-relevant information. Experiments show that our method achieves unprecedented data efficiency: on UCF101 with ConvNets, VideoCompressa surpasses full-data training by 2.34\% points using only 0.13\% of the original data, with over 5800x speedup compared to traditional synthesis method. Moreover, when fine-tuning Qwen2.5-7B-VL on HMDB51, VideoCompressa matches full-data performance using just 0.41\% of the training data-outperforming zero-shot baseline by 10.61\%.

</details>


### [299] [FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories](https://arxiv.org/abs/2511.18834)
*Lei Ke,Hubery Yin,Gongye Liu,Zhengyao Lv,Jingcai Guo,Chen Li,Wenhan Luo,Yujiu Yang,Jing Lyu*

Main category: cs.CV

TL;DR: FlowSteer方法通过引导学生模型沿着教师模型的真实生成轨迹来释放ReFlow蒸馏的潜力，解决了ReFlow在实践中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 尽管ReFlow在理论上与流匹配一致，但在实际场景中性能不如一致性蒸馏和分数蒸馏，采样效率成为流匹配实际应用的关键瓶颈。

Method: 提出FlowSteer方法：1）在线轨迹对齐(OTA)解决分段ReFlow训练中的分布不匹配问题；2）在ODE轨迹上应用对抗蒸馏目标，提高学生对教师生成轨迹的遵循度；3）修复FlowMatchEulerDiscreteScheduler中的缺陷。

Result: 在SD3上的实验结果表明该方法有效提升了性能。

Conclusion: FlowSteer成功解锁了基于ReFlow的蒸馏潜力，通过轨迹对齐和对抗蒸馏显著提升了采样效率。

Abstract: With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application. Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching. This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation. In this work, we investigate this issue within the ReFlow framework and propose FlowSteer, a method unlocks the potential of ReFlow-based distillation by guiding the student along teacher's authentic generation trajectories. We first identify that Piecewised ReFlow's performance is hampered by a critical distribution mismatch during the training and propose Online Trajectory Alignment(OTA) to resolve it. Then, we introduce a adversarial distillation objective applied directly on the ODE trajectory, improving the student's adherence to the teacher's generation trajectory. Furthermore, we find and fix a previously undiscovered flaw in the widely-used FlowMatchEulerDiscreteScheduler that largely degrades few-step inference quality. Our experiment result on SD3 demonstrates our method's efficacy.

</details>


### [300] [FVAR: Visual Autoregressive Modeling via Next Focus Prediction](https://arxiv.org/abs/2511.18838)
*Xiaofan Li,Chenming Wu,Yanpeng Sun,Jiaming Zhou,Delin Qu,Yansong Qu,Weihao Bo,Haibao Yu,Dingkang Liang*

Main category: cs.CV

TL;DR: FVAR提出了一种新的视觉自回归模型范式，将传统的"下一尺度预测"转变为"下一焦点预测"，通过渐进式去模糊而非简单下采样来消除混叠伪影，提升图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统视觉自回归模型使用均匀尺度下采样构建多尺度金字塔，导致混叠伪影，损害细节并引入锯齿和摩尔纹。需要一种能消除这些伪影的方法。

Method: 1) 下一焦点预测范式：渐进减少模糊而非下采样；2) 渐进重聚焦金字塔构建：使用物理一致的散焦核构建无混叠多尺度表示；3) 高频残差学习：使用专门的残差教师网络在训练中有效整合混叠信息。

Result: 在ImageNet上的实验表明，FVAR显著减少了混叠伪影，改善了细节保留和文本可读性，性能优于现有方法。

Conclusion: FVAR通过焦点预测范式和物理一致的散焦处理，有效解决了视觉自回归模型中的混叠问题，与现有VAR框架完美兼容。

Abstract: Visual autoregressive models achieve remarkable generation quality through next-scale predictions across multi-scale token pyramids. However, the conventional method uses uniform scale downsampling to build these pyramids, leading to aliasing artifacts that compromise fine details and introduce unwanted jaggies and moiré patterns. To tackle this issue, we present \textbf{FVAR}, which reframes the paradigm from \emph{next-scale prediction} to \emph{next-focus prediction}, mimicking the natural process of camera focusing from blur to clarity. Our approach introduces three key innovations: \textbf{1) Next-Focus Prediction Paradigm} that transforms multi-scale autoregression by progressively reducing blur rather than simply downsampling; \textbf{2) Progressive Refocusing Pyramid Construction} that uses physics-consistent defocus kernels to build clean, alias-free multi-scale representations; and \textbf{3) High-Frequency Residual Learning} that employs a specialized residual teacher network to effectively incorporate alias information during training while maintaining deployment simplicity. Specifically, we construct optical low-pass views using defocus point spread function (PSF) kernels with decreasing radius, creating smooth blur-to-clarity transitions that eliminate aliasing at its source. To further enhance detail generation, we introduce a High-Frequency Residual Teacher that learns from both clean structure and alias residuals, distilling this knowledge to a vanilla VAR deployment network for seamless inference. Extensive experiments on ImageNet demonstrate that FVAR substantially reduces aliasing artifacts, improves fine detail preservation, and enhances text readability, achieving superior performance with perfect compatibility to existing VAR frameworks.

</details>


### [301] [Enhancing Multi-Label Thoracic Disease Diagnosis with Deep Ensemble-Based Uncertainty Quantification](https://arxiv.org/abs/2511.18839)
*Yasiru Laksara,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 该研究通过集成深度集成方法，在胸部X光疾病诊断中实现了优越的不确定性量化，解决了深度学习模型在临床应用中缺乏可靠置信度的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型如CheXNet在临床环境中因缺乏可靠的不确定性量化而受限，无法提供预测置信度，这在高风险医疗决策中至关重要。

Method: 从蒙特卡洛dropout方法转向9成员深度集成方法，该方法能稳定性能并实现总不确定性的可分解量化。

Result: 深度集成方法实现了SOTA性能：平均AUROC为0.8559，平均F1分数为0.3857，平均ECE为0.0728，NLL为0.1916，平均认知不确定性为0.0240。

Conclusion: 深度集成方法将模型从概率工具转变为可靠的临床决策支持系统，提供了可信赖和可解释的诊断平台。

Abstract: The utility of deep learning models, such as CheXNet, in high stakes clinical settings is fundamentally constrained by their purely deterministic nature, failing to provide reliable measures of predictive confidence. This project addresses this critical gap by integrating robust Uncertainty Quantification (UQ) into a high performance diagnostic platform for 14 common thoracic diseases on the NIH ChestX-ray14 dataset. Initial architectural development failed to stabilize performance and calibration using Monte Carlo Dropout (MCD), yielding an unacceptable Expected Calibration Error (ECE) of 0.7588. This technical failure necessitated a rigorous architectural pivot to a high diversity, 9-member Deep Ensemble (DE). This resulting DE successfully stabilized performance and delivered superior reliability, achieving a State-of-the-Art (SOTA) average Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.8559 and an average F1 Score of 0.3857. Crucially, the DE demonstrated superior calibration (Mean ECE of 0.0728 and Negative Log-Likelihood (NLL) of 0.1916) and enabled the reliable decomposition of total uncertainty into its Aleatoric (irreducible data noise) and Epistemic (reducible model knowledge) components, with a mean Epistemic Uncertainty (EU) of 0.0240. These results establish the Deep Ensemble as a trustworthy and explainable platform, transforming the model from a probabilistic tool into a reliable clinical decision support system.

</details>


### [302] [Personalized Federated Segmentation with Shared Feature Aggregation and Boundary-Focused Calibration](https://arxiv.org/abs/2511.18847)
*Ishmam Tashdeed,Md. Atiqur Rahman,Sabrina Islam,Md. Azam Hossain*

Main category: cs.CV

TL;DR: 提出FedOAP方法，通过解耦交叉注意力和扰动边界损失，解决个性化联邦学习中器官无关肿瘤分割问题，在非IID数据下提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法大多忽视了利用客户端间共享特征的潜在优势，特别是在不同器官分割数据场景下。

Method: 使用解耦交叉注意力(DCA)让客户端保留本地查询同时关注全局共享键值对，引入扰动边界损失(PBL)提升边界分割一致性。

Result: 在不同器官的肿瘤分割任务中，FedOAP持续优于现有最先进的联邦和个性化分割方法。

Conclusion: FedOAP通过建模跨客户端共享特征的长程依赖和边界一致性优化，有效提升了器官无关肿瘤分割的性能。

Abstract: Personalized federated learning (PFL) possesses the unique capability of preserving data confidentiality among clients while tackling the data heterogeneity problem of non-independent and identically distributed (Non-IID) data. Its advantages have led to widespread adoption in domains such as medical image segmentation. However, the existing approaches mostly overlook the potential benefits of leveraging shared features across clients, where each client contains segmentation data of different organs. In this work, we introduce a novel personalized federated approach for organ agnostic tumor segmentation (FedOAP), that utilizes cross-attention to model long-range dependencies among the shared features of different clients and a boundary-aware loss to improve segmentation consistency. FedOAP employs a decoupled cross-attention (DCA), which enables each client to retain local queries while attending to globally shared key-value pairs aggregated from all clients, thereby capturing long-range inter-organ feature dependencies. Additionally, we introduce perturbed boundary loss (PBL) which focuses on the inconsistencies of the predicted mask's boundary for each client, forcing the model to localize the margins more precisely. We evaluate FedOAP on diverse tumor segmentation tasks spanning different organs. Extensive experiments demonstrate that FedOAP consistently outperforms existing state-of-the-art federated and personalized segmentation methods.

</details>


### [303] [Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization](https://arxiv.org/abs/2511.18851)
*Yilin Wen,Kechuan Dong,Yusuke Sugano*

Main category: cs.CV

TL;DR: 提出了一种基于运动离散化的在线测试时自适应方法，通过无监督聚类获得锚点运动，结合软重置机制来缓解3D人体姿态估计中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 在线测试时自适应在3D人体姿态估计中面临误差累积的挑战，因为依赖不完美预测的自监督会导致性能随时间下降。

Method: 使用潜在运动表示空间的无监督聚类获得锚点运动，引入软重置机制将姿态估计器恢复到指数移动平均值，并利用连续自适应捕捉个人形状和运动特征。

Result: 实验表明该方法优于之前的在线测试时自适应方法，验证了设计选择的有效性。

Conclusion: 通过缓解误差累积，该方法能够稳健地利用个人特征来提高3D人体姿态估计的准确性。

Abstract: Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.

</details>


### [304] [Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos](https://arxiv.org/abs/2511.18856)
*Sana Alamgeer*

Main category: cs.CV

TL;DR: 设计了一个混合显著性模型来预测360度视频中的感兴趣区域(ROI)，以优化视频流传输和提升观看体验。


<details>
  <summary>Details</summary>
Motivation: 360度视频中的ROI检测对于优化视频流传输至关重要，可以预测视口、智能裁剪视频以减少带宽使用，并减少头戴设备观看时的头部移动。

Method: 预处理视频获取帧，开发混合显著性模型预测ROI，后处理模型输出得到每帧的ROI。

Result: 将所提方法与360RAT数据集的主观标注进行了性能比较。

Conclusion: 该研究成功开发了一个用于360度视频ROI预测的混合显著性模型，有助于提升视频流传输效率和观看体验。

Abstract: The main goal of the project is to design a new model that predicts regions of interest in 360$^{\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.

</details>


### [305] [Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling](https://arxiv.org/abs/2511.18858)
*Xiao Cui,Yulei Qin,Xinyue Li,Wengang Zhou,Hongsheng Li,Houqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种针对长尾数据集蒸馏的新方法，通过统计对齐视角解决模型偏差和BN统计估计问题，在多个长尾基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法在平衡数据集上表现良好，但在长尾分布下表现不佳，因为类别不平衡会导致模型表示偏差和BN统计估计错误。

Method: 采用统计对齐视角，包含三个核心组件：增强专家模型用于可靠统计估计和软标签生成；通过动态调整动量的完整前向传递重新校准BN统计以减少表示偏差；通过多轮机制增量选择高置信度和多样化的增强来初始化合成图像。

Result: 在四个长尾基准测试上均取得一致改进，在CIFAR-100-LT和Tiny-ImageNet-LT上分别提升15.6%和11.8%的top-1准确率。

Conclusion: 该方法有效解决了长尾数据集蒸馏中的模型偏差和统计估计问题，显著提升了蒸馏性能。

Abstract: Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance.Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.

</details>


### [306] [DualGazeNet: A Biologically Inspired Dual-Gaze Query Network for Salient Object Detection](https://arxiv.org/abs/2511.18865)
*Yu Zhang,Haoan Ping,Yuchen Li,Zhenshan Bing,Fuchun Sun,Alois Knoll*

Main category: cs.CV

TL;DR: DualGazeNet是一个受生物视觉启发的纯Transformer框架，通过模拟人类视觉系统的双通路处理机制，在显著目标检测任务中实现了SOTA性能，同时具有更高的计算效率和更强的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测方法通过复杂的多阶段架构、专门融合模块和注意力机制来提升性能，但这种复杂性反而引入了特征冗余和交叉干扰，导致性能瓶颈。相比之下，人类视觉系统能够高效识别显著目标而无需复杂架构，这启发了设计一个既受生物启发又架构简单的框架。

Method: 提出DualGazeNet，一个受生物启发的纯Transformer框架，模拟人类视觉系统中的稳健表示学习和巨细胞-小细胞双通路处理机制，结合皮层注意力调制。

Result: 在五个RGB显著目标检测基准测试中，DualGazeNet持续超越25个SOTA的CNN和Transformer方法。相比四个容量相似的Transformer基线，推理速度提升约60%，FLOPs减少53.4%。在伪装和水下显著目标检测基准上也表现出强大的跨域泛化能力。

Conclusion: DualGazeNet证明了通过模拟生物视觉原理可以设计出既简单又高效的显著目标检测框架，在保持高精度的同时大幅提升计算效率，并具备优秀的跨域泛化性能。

Abstract: Recent salient object detection (SOD) methods aim to improve performance in four key directions: semantic enhancement, boundary refinement, auxiliary task supervision, and multi-modal fusion. In pursuit of continuous gains, these approaches have evolved toward increasingly sophisticated architectures with multi-stage pipelines, specialized fusion modules, edge-guided learning, and elaborate attention mechanisms. However, this complexity paradoxically introduces feature redundancy and cross-component interference that obscure salient cues, ultimately reaching performance bottlenecks. In contrast, human vision achieves efficient salient object identification without such architectural complexity. This contrast raises a fundamental question: can we design a biologically grounded yet architecturally simple SOD framework that dispenses with most of this engineering complexity, while achieving state-of-the-art accuracy, computational efficiency, and interpretability? In this work, we answer this question affirmatively by introducing DualGazeNet, a biologically inspired pure Transformer framework that models the dual biological principles of robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation in the human visual system. Extensive experiments on five RGB SOD benchmarks show that DualGazeNet consistently surpasses 25 state-of-the-art CNN- and Transformer-based methods. On average, DualGazeNet achieves about 60\% higher inference speed and 53.4\% fewer FLOPs than four Transformer-based baselines of similar capacity (VST++, MDSAM, Sam2unet, and BiRefNet). Moreover, DualGazeNet exhibits strong cross-domain generalization, achieving leading or highly competitive performance on camouflaged and underwater SOD benchmarks without relying on additional modalities.

</details>


### [307] [HunyuanVideo 1.5 Technical Report](https://arxiv.org/abs/2511.18870)
*Bing Wu,Chang Zou,Changlin Li,Duojun Huang,Fang Yang,Hao Tan,Jack Peng,Jianbing Wu,Jiangfeng Xiong,Jie Jiang,Linus,Patrol,Peizhen Zhang,Peng Chen,Penghao Zhao,Qi Tian,Songtao Liu,Weijie Kong,Weiyan Wang,Xiao He,Xin Li,Xinchi Deng,Xuefei Zhe,Yang Li,Yanxin Long,Yuanbo Peng,Yue Wu,Yuhong Liu,Zhenyu Wang,Zuozhuo Dai,Bo Peng,Coopers Li,Gu Gong,Guojian Xiao,Jiahe Tian,Jiaxin Lin,Jie Liu,Jihong Zhang,Jiesong Lian,Kaihang Pan,Lei Wang,Lin Niu,Mingtao Chen,Mingyang Chen,Mingzhe Zheng,Miles Yang,Qiangqiang Hu,Qi Yang,Qiuyong Xiao,Runzhou Wu,Ryan Xu,Rui Yuan,Shanshan Sang,Shisheng Huang,Siruis Gong,Shuo Huang,Weiting Guo,Xiang Yuan,Xiaojia Chen,Xiawei Hu,Wenzhi Sun,Xiele Wu,Xianshun Ren,Xiaoyan Yuan,Xiaoyue Mi,Yepeng Zhang,Yifu Sun,Yiting Lu,Yitong Li,You Huang,Yu Tang,Yixuan Li,Yuhang Deng,Yuan Zhou,Zhichao Hu,Zhiguang Liu,Zhihe Yang,Zilin Yang,Zhenzhi Lu,Zixiang Zhou,Zhao Zhong*

Main category: cs.CV

TL;DR: HunyuanVideo 1.5是一个轻量级但强大的开源视频生成模型，仅用83亿参数就实现了最先进的视觉质量和运动连贯性，可在消费级GPU上高效推理。


<details>
  <summary>Details</summary>
Motivation: 为社区提供一个高性能的视频生成基础模型，降低视频创作和研究的门槛，让更广泛的受众能够使用先进的视频生成技术。

Method: 采用精心策划的数据、先进的DiT架构（包含选择性滑动瓦片注意力SSTA）、通过字形感知文本编码增强双语理解、渐进式预训练和后训练，以及高效的视频超分辨率网络。

Result: 该紧凑而高效的模型在开源视频生成模型中建立了新的最先进水平，能够跨多种时长和分辨率进行高质量的文本到视频和图像到视频生成。

Conclusion: 通过发布代码和模型权重，为社区提供了一个高性能的基础，使先进的视频生成技术更加普及和易于使用。

Abstract: We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.

</details>


### [308] [Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction](https://arxiv.org/abs/2511.18873)
*Yiming Wang,Shaofei Wang,Marko Mihajlovic,Siyu Tang*

Main category: cs.CV

TL;DR: 提出了Neural Texture Splatting (NTS)方法，通过全局神经场为每个3D高斯基元预测局部外观和几何场，显著提升了3D高斯泼溅在各种重建任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然在新视角合成方面表现出色，但其表示能力受限于使用3D高斯核建模局部变化。现有方法通过每个基元的纹理增强表达能力，但在更一般的重建场景中效果有限。

Method: 引入全局神经场（三平面和神经解码器的混合表示）为每个基元预测局部外观和几何场，利用共享全局表示减少模型大小并促进全局信息交换。

Result: 在多个基准测试中，Neural Texture Splatting一致地改进了模型性能，并在稀疏和密集输入设置下实现了最先进的结果。

Conclusion: 该方法通过神经建模局部纹理场引入了表达性的视角和时间依赖效果，在多种重建任务中实现了显著的性能提升。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.

</details>


### [309] [Parallel Vision Token Scheduling for Fast and Accurate Multimodal LMMs Inference](https://arxiv.org/abs/2511.18875)
*Wengyi Zhan,Mingbao Lin,Zhihang Lin,Rongrong Ji*

Main category: cs.CV

TL;DR: ParVTS是一种无需训练的多模态大语言模型推理加速框架，通过将视觉token划分为主体和非主体组并行处理，并在推理中期丢弃非主体路径来减少计算量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理高分辨率图像时会产生大量视觉token，导致推理延迟显著增加。简单地剪枝视觉token会丢失重要的上下文信息，影响模型精度。

Method: 提出ParVTS框架：1）将视觉token划分为主体和非主体组；2）并行处理两组token以将语义信息转移到问题token中；3）在推理中期丢弃非主体路径以减少计算。

Result: 在多个MLLM骨干网络上实验表明，ParVTS最多可剪枝88.9%的视觉token，性能下降极小，实现1.77倍加速和70%的FLOPs减少。

Conclusion: ParVTS是一种有效的训练免费调度框架，能够显著减少多模态大语言模型的计算复杂度，同时保持模型性能，且兼容多种现有MLLM架构。

Abstract: Multimodal large language models (MLLMs) deliver impressive vision-language reasoning but suffer steep inference latency because self-attention scales quadratically with sequence length and thousands of visual tokens contributed by high-resolution images. Naively pruning less-informative visual tokens reduces this burden, yet indiscriminate removal can strip away contextual cues essential for background or fine-grained questions, undermining accuracy. In this paper, we present ParVTS (Parallel Vision Token Scheduling), a training-free scheduling framework that partitions visual tokens into subject and non-subject groups, processes them in parallel to transfer their semantics into question tokens, and discards the non-subject path mid-inference to reduce computation. This scheduling reduces computational complexity, requires no heuristics or additional modules, and is compatible with diverse existing MLLM architectures. Experiments across multiple MLLM backbones show that ParVTS prunes up to 88.9% of visual tokens with minimal performance drop, achieving 1.77x speedup and 70% FLOPs reduction.

</details>


### [310] [Facade Segmentation for Solar Photovoltaic Suitability](https://arxiv.org/abs/2511.18882)
*Ayca Duran,Christoph Waibel,Bernd Bickel,Iro Armeni,Arno Schlueter*

Main category: cs.CV

TL;DR: 该论文提出了一种自动化管道，通过整合建筑立面详细信息来识别适合光伏应用的表面并估算太阳能潜力，使用SegFormer-B5在CMP Facades数据集上进行微调，并将语义预测转换为立面级光伏适用性掩码和光伏面板布局。


<details>
  <summary>Details</summary>
Motivation: 建筑集成光伏立面是实现城市脱碳的有前景途径，特别是在屋顶面积不足且地面安装阵列不可行的情况下。虽然基于机器学习的屋顶光伏规划方法已有深入研究，但针对立面的自动化方法仍然稀缺且过于简化。

Method: 提出一个管道，在CMP Facades数据集上微调SegFormer-B5，将语义预测转换为考虑模块尺寸和间隙的立面级光伏适用性掩码和光伏面板布局。

Result: 应用于来自10个城市的373个已知尺寸立面数据集，结果显示可安装BIPV潜力显著低于理论潜力，为可靠的城市能源规划提供了宝贵见解。

Conclusion: 随着立面图像的日益可用性，所提出的管道可以扩展到支持全球城市的BIPV规划。

Abstract: Building integrated photovoltaic (BIPV) facades represent a promising pathway towards urban decarbonization, especially where roof areas are insufficient and ground-mounted arrays are infeasible. Although machine learning-based approaches to support photovoltaic (PV) planning on rooftops are well researched, automated approaches for facades still remain scarce and oversimplified. This paper therefore presents a pipeline that integrates detailed information on the architectural composition of the facade to automatically identify suitable surfaces for PV application and estimate the solar energy potential. The pipeline fine-tunes SegFormer-B5 on the CMP Facades dataset and converts semantic predictions into facade-level PV suitability masks and PV panel layouts considering module sizes and clearances. Applied to a dataset of 373 facades with known dimensions from ten cities, the results show that installable BIPV potential is significantly lower than theoretical potential, thus providing valuable insights for reliable urban energy planning. With the growing availability of facade imagery, the proposed pipeline can be scaled to support BIPV planning in cities worldwide.

</details>


### [311] [MagicWorld: Interactive Geometry-driven Video World Exploration](https://arxiv.org/abs/2511.18886)
*Guangyuan Li,Siming Zheng,Shuolin Xu,Jinwei Chen,Bo Li,Xiaobin Hu,Lei Zhao,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: MagicWorld是一个交互式视频世界模型，通过整合3D几何先验和历史检索机制，解决了现有方法在视角变化下的结构不稳定性和多步交互中的历史遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有交互式视频世界模型存在两个关键限制：1）未能充分利用指令驱动场景运动与底层3D几何的对应关系，导致视角变化时结构不稳定；2）多步交互中容易遗忘历史信息，导致错误累积和场景语义结构漂移。

Method: 提出MagicWorld模型，包含两个核心模块：1）动作引导3D几何模块（AG3D），从每轮交互的首帧构建点云，为视角转换提供显式几何约束；2）历史缓存检索机制（HCR），在生成时检索相关历史帧作为条件信号，帮助模型利用过去场景信息。

Result: 实验结果表明，MagicWorld在交互迭代过程中显著提升了场景稳定性和连续性。

Conclusion: MagicWorld通过整合3D几何先验和历史检索机制，有效解决了交互式视频生成中的结构一致性和历史信息保持问题，为构建更稳定的动态场景演化模型提供了有效方案。

Abstract: Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.

</details>


### [312] [MFmamba: A Multi-function Network for Panchromatic Image Resolution Restoration Based on State-Space Model](https://arxiv.org/abs/2511.18888)
*Qian Jiang,Qianqian Wang,Xin Jin,Michal Wozniak,Shaowen Yao,Wei Zhou*

Main category: cs.CV

TL;DR: 提出了一种多功能模型MFmamba，通过单一PAN图像输入实现超分辨率、光谱恢复以及两者联合任务，解决了传统方法需要多传感器输入的限制。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像中单一传感器只能获取高空间分辨率的灰度PAN图像或低空间分辨率的彩色MS图像的问题，期望通过单一输入获得高空间分辨率的彩色图像。

Method: 使用UNet++作为主干网络，结合Mamba上采样块(MUB)、双池注意力(DPA)替换跳跃连接，以及多尺度混合交叉块(MHCB)进行初始特征提取。

Result: 实验表明MFmamba在评估指标和视觉效果上具有竞争力，在三种任务中表现良好。

Conclusion: MFmamba模型能够仅使用输入PAN图像有效完成超分辨率、光谱恢复及其联合任务，为遥感图像处理提供了集成解决方案。

Abstract: Remote sensing images are becoming increasingly widespread in military, earth resource exploration. Because of the limitation of a single sensor, we can obtain high spatial resolution grayscale panchromatic (PAN) images and low spatial resolution color multispectral (MS) images. Therefore, an important issue is to obtain a color image with high spatial resolution when there is only a PAN image at the input. The existing methods improve spatial resolution using super-resolution (SR) technology and spectral recovery using colorization technology. However, the SR technique cannot improve the spectral resolution, and the colorization technique cannot improve the spatial resolution. Moreover, the pansharpening method needs two registered inputs and can not achieve SR. As a result, an integrated approach is expected. To solve the above problems, we designed a novel multi-function model (MFmamba) to realize the tasks of SR, spectral recovery, joint SR and spectral recovery through three different inputs. Firstly, MFmamba utilizes UNet++ as the backbone, and a Mamba Upsample Block (MUB) is combined with UNet++. Secondly, a Dual Pool Attention (DPA) is designed to replace the skip connection in UNet++. Finally, a Multi-scale Hybrid Cross Block (MHCB) is proposed for initial feature extraction. Many experiments show that MFmamba is competitive in evaluation metrics and visual results and performs well in the three tasks when only the input PAN image is used.

</details>


### [313] [MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting](https://arxiv.org/abs/2511.18894)
*Chenyu Mu,Guihai Chen,Xun Yang,Erkun Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出MetaDCSeg框架，通过动态学习像素级权重来抑制噪声标注的影响，特别针对医学图像分割中边界模糊问题，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割常受噪声标注和模糊边界干扰，导致模型训练不稳定。现有方法基于全局噪声假设或置信度样本选择，难以有效处理边界区域的噪声问题。

Method: 提出动态中心距离(DCD)机制，通过加权特征距离建模边界不确定性，关注模糊边界附近的难分割像素，动态学习像素级权重来抑制噪声标签同时保留可靠标注。

Result: 在四个基准数据集上的大量实验表明，MetaDCSeg在不同噪声水平下均优于现有最先进方法，显著提升了分割性能。

Conclusion: MetaDCSeg通过动态像素级权重学习和边界不确定性建模，有效解决了医学图像分割中的噪声标注问题，特别在边界区域表现优异。

Abstract: Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.

</details>


### [314] [Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation](https://arxiv.org/abs/2511.18919)
*Ruiying Liu,Yuanzhi Liang,Haibin Huang,Tianshu Yu,Chi Zhang*

Main category: cs.CV

TL;DR: BPGO通过引入贝叶斯先验锚点来建模奖励不确定性，在GRPO框架基础上改进视觉生成模型的后训练优化，解决了文本-视觉对应关系模糊导致的奖励信号不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统GRPO框架受限于文本-视觉对应关系的模糊性：单个提示可能对应多种视觉输出，单个图像/视频可能有多种正确解释。这种多对多关系导致奖励模型产生不确定和弱区分性的信号，使GRPO无法充分利用可靠反馈并过度拟合噪声信号。

Method: 提出贝叶斯先验引导优化(BPGO)，在GRPO基础上通过语义先验锚点显式建模奖励不确定性。采用两级自适应优化信任调节：组间贝叶斯信任分配强调与先验一致的组更新，降低模糊组的权重；组内先验锚定重归一化通过扩展自信偏差和压缩不确定分数来锐化样本区分。

Result: 在图像和视频生成任务中，BPGO相比标准GRPO和近期变体，在语义对齐、感知保真度和收敛速度方面都表现出更一致且更强的性能。

Conclusion: BPGO通过显式建模奖励不确定性并引入贝叶斯先验指导，有效解决了GRPO框架中的模糊性问题，为视觉生成模型的后训练优化提供了更可靠的解决方案。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.

</details>


### [315] [EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models](https://arxiv.org/abs/2511.18920)
*Wenhao Xu,Xin Dong,Yue Li,Haoyuan Shi,Zhiwei Xiong*

Main category: cs.CV

TL;DR: EventSTU是一个基于事件相机的训练免费框架，通过粗到细的关键帧采样和自适应token剪枝，在保持性能的同时显著降低视频理解的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在长视频理解时面临高推理成本问题，因为长视频包含大量token。受事件视觉启发，作者希望利用事件相机的特性来消除冗余帧和token。

Method: 1. 时间域：设计粗到细关键帧采样算法，利用事件相机的变化触发特性消除冗余帧；2. 空间域：设计自适应token剪枝算法，利用事件视觉显著性作为零成本先验；3. 时空整合：结合问题相关性自适应分配token剪枝预算。

Result: EventSTU实现了3.01倍FLOPs减少和3.10倍预填充加速，同时性能仍优于最强基线。构建了首个事件包含的多模态基准EventBench。

Conclusion: EventSTU框架有效解决了视频大语言模型的高计算成本问题，不仅支持物理事件相机，也支持使用模拟事件进行通用视频理解，在效率和性能上都有显著提升。

Abstract: Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.

</details>


### [316] [BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models](https://arxiv.org/abs/2511.18921)
*Juncheng Li,Yige Li,Hanxun Huang,Yunhao Chen,Xin Wang,Yixu Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: BackdoorVLM是首个针对视觉语言模型(VLMs)的全面后门攻击基准，系统评估了5类多模态后门威胁，在12种攻击方法、2个开源VLMs和3个数据集上的测试显示，VLMs对文本指令高度敏感，仅需1%的中毒率即可实现90%以上的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 虽然后门攻击在单模态环境中已被广泛研究，但其对多模态基础模型特别是视觉语言模型的影响仍未充分探索，需要系统性的评估框架。

Method: 采用统一视角，在图像描述和视觉问答等核心视觉语言任务中注入和分析后门，将多模态后门威胁组织为5个代表性类别，使用12种代表性攻击方法跨越文本、图像和双模态触发器。

Result: VLMs对文本指令表现出强烈敏感性，在双模态后门中文本触发器通常压倒图像触发器；涉及文本模态的后门攻击效果极强，仅1%的中毒率即可在大多数任务中实现超过90%的成功率。

Conclusion: 当前VLMs存在显著且先前未充分探索的漏洞，BackdoorVLM可作为分析和缓解多模态后门威胁的有用基准。

Abstract: Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\% yielding over 90\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .

</details>


### [317] [One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control](https://arxiv.org/abs/2511.18922)
*Zhenxing Mi,Yuxin Wang,Dan Xu*

Main category: cs.CV

TL;DR: One4D是一个统一的4D生成和重建框架，通过统一掩码条件机制处理不同稀疏度的输入帧，能够从单张图像生成4D内容、从完整视频重建4D内容，或在稀疏帧下混合生成和重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不同稀疏度输入帧时存在困难，且常用的扩散微调策略在联合RGB和点云图生成时容易导致基础视频模型性能下降。

Method: 采用统一掩码条件机制处理不同稀疏度输入，设计解耦LoRA控制方法，使用两个模态特定的LoRA适配器分别处理RGB帧和点云图，通过轻量级零初始化控制链接学习像素级一致性。

Result: 在合成和真实4D数据集上训练，One4D在生成和重建任务中都能产生高质量的RGB帧和准确的点云图。

Conclusion: 这项工作代表了使用视频扩散模型进行通用、高质量基于几何的4D世界建模的重要一步。

Abstract: We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D

</details>


### [318] [AttenDence: Maximizing Attention Confidence for Test Time Adaptation](https://arxiv.org/abs/2511.18925)
*Yash Mali*

Main category: cs.CV

TL;DR: 提出一种基于注意力熵最小化的测试时自适应方法，通过最小化CLS令牌到图像补丁的注意力分布熵来提升模型在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的测试时自适应主要依赖输出分布的熵最小化，而transformer的注意力机制提供了额外的无监督学习信号，可以更好地适应分布偏移。

Method: 最小化CLS令牌到图像补丁的注意力分布熵，鼓励模型在分布偏移下更自信地关注相关图像区域，即使在单张测试图像下也有效。

Result: 该方法在各种损坏类型下都提高了鲁棒性，同时在干净数据上不会损害性能，适用于测试时的单样本流图像。

Conclusion: 注意力熵最小化是一种有效的测试时自适应方法，能够利用transformer的注意力机制来提升模型在分布偏移下的性能。

Abstract: Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective.This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.

</details>


### [319] [FineXtrol: Controllable Motion Generation via Fine-Grained Text](https://arxiv.org/abs/2511.18927)
*Keming Shen,Bizhu Wu,Junliang Chen,Xiaoqin Wang,Linlin Shen*

Main category: cs.CV

TL;DR: FineXtrol是一个新颖的运动生成控制框架，通过时间感知、精确、用户友好的细粒度文本控制信号来指导特定身体部位随时间运动，解决了现有方法中细节错位、缺乏时间线索和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动运动生成方法存在两个主要问题：基于大语言模型的方法会产生错位细节且缺乏明确时间线索，而基于3D坐标序列的方法在转换为标准运动表示时计算成本高昂。

Method: 提出了FineXtrol框架，使用时间感知的细粒度文本控制信号描述特定身体部位随时间运动；设计了分层对比学习模块，使文本编码器能为新的控制信号生成更具区分度的嵌入，从而提升运动可控性。

Result: 定量结果显示FineXtrol在可控运动生成方面表现强劲，定性分析证明了其在指导特定身体部位运动方面的灵活性。

Conclusion: FineXtrol通过细粒度文本控制信号和分层对比学习，有效解决了现有运动生成方法的局限性，实现了高效且精确的可控运动生成。

Abstract: Recent works have sought to enhance the controllability and precision of text-driven motion generation. Some approaches leverage large language models (LLMs) to produce more detailed texts, while others incorporate global 3D coordinate sequences as additional control signals. However, the former often introduces misaligned details and lacks explicit temporal cues, and the latter incurs significant computational cost when converting coordinates to standard motion representations. To address these issues, we propose FineXtrol, a novel control framework for efficient motion generation guided by temporally-aware, precise, user-friendly, and fine-grained textual control signals that describe specific body part movements over time. In support of this framework, we design a hierarchical contrastive learning module that encourages the text encoder to produce more discriminative embeddings for our novel control signals, thereby improving motion controllability. Quantitative results show that FineXtrol achieves strong performance in controllable motion generation, while qualitative analysis demonstrates its flexibility in directing specific body part movements.

</details>


### [320] [Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search](https://arxiv.org/abs/2511.18929)
*Zijian Song,Xiaoxin Lin,Tao Pu,Zhenlong Yuan,Guangrun Wang,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了人类中心开放未来任务发现（HOTD）问题，开发了包含2000+真实视频的HOTD-Bench基准，并提出了协作多智能体搜索树（CMAST）框架来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型在机器人技术和具身AI中取得进展，但在开放未来场景中，如何让模型发现能直接帮助人类的任务仍是一个未充分探索的挑战，特别是当人类意图高度并发和动态变化时。

Method: 提出了协作多智能体搜索树（CMAST）框架，通过多智能体系统分解复杂推理，并使用可扩展的搜索树模块结构化推理过程。

Result: CMAST在HOTD-Bench上取得了最佳性能，显著超越了现有的大型多模态模型，并且能很好地与现有模型集成，持续提升性能。

Conclusion: HOTD问题框架和CMAST方法为解决开放未来场景中的人类中心任务发现提供了有效解决方案，在基准测试中表现出色。

Abstract: Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that directly assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across multiple plausible futures. To facilitate this study, we propose an HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes the complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.

</details>


### [321] [VeCoR - Velocity Contrastive Regularization for Flow Matching](https://arxiv.org/abs/2511.18942)
*Zong-Wei Hong,Jing-lun Li,Lin-Ze Li,Shen Zhang,Yao Tang*

Main category: cs.CV

TL;DR: 提出了Velocity Contrastive Regularization (VeCoR)方法，通过对比学习增强流匹配模型，在正向监督基础上增加负向监督，提升生成稳定性和图像质量。


<details>
  <summary>Details</summary>
Motivation: 标准流匹配方法可能沿轨迹累积误差并使样本偏离数据流形，导致感知质量下降，特别是在轻量级或低步数配置下。

Method: 将流匹配扩展为平衡的吸引-排斥方案，通过对比性双向监督增强标准目标，既对齐预测速度与稳定参考方向，又将其推离不一致的离流形方向。

Result: 在ImageNet-1K 256×256上，VeCoR在SiT-XL/2和REPA-SiT-XL/2骨干网络上分别实现22%和35%的相对FID降低，在MS-COCO文本到图像生成上获得32%的相对FID增益。

Conclusion: VeCoR通过对比性正则化将流匹配从纯吸引的单边目标转变为双边训练信号，显著提升了稳定性、收敛性和图像质量，特别是在低步数和轻量级设置下。

Abstract: Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models. Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.
  To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both "where to go" and "where not to go." To be formal, we propose \textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision. VeCoR not only aligns the predicted velocity with a stable reference direction (positive supervision) but also pushes it away from inconsistent, off-manifold directions (negative supervision). This contrastive formulation transforms FM from a purely attractive, one-sided objective into a two-sided training signal, regularizing trajectory evolution and improving perceptual fidelity across datasets and backbones.
  On ImageNet-1K 256$\times$256, VeCoR yields 22\% and 35\% relative FID reductions on SiT-XL/2 and REPA-SiT-XL/2 backbones, respectively, and achieves further FID gains (32\% relative) on MS-COCO text-to-image generation, demonstrating consistent improvements in stability, convergence, and image quality, particularly in low-step and lightweight settings. Project page: https://p458732.github.io/VeCoR_Project_Page/

</details>


### [322] [Leveraging Adversarial Learning for Pathological Fidelity in Virtual Staining](https://arxiv.org/abs/2511.18946)
*José Teixeira,Pascal Klöckner,Diana Montezuma,Melis Erdal Cesur,João Fraga,Hugo M. Horlings,Jaime S. Cardoso,Sara P. Oliveira*

Main category: cs.CV

TL;DR: 本文开发了CSSP2P GAN模型用于虚拟染色，通过盲法病理专家评估证明其具有更高的病理保真度，并研究了对抗性损失对虚拟染色质量的关键影响，同时指出了当前评估指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 免疫组化染色成本高且劳动密集，虚拟染色作为图像到图像转换任务提供了有前景的替代方案。现有研究大多使用条件生成对抗网络，但忽视了对抗性损失对虚拟染色质量的影响，且评估指标不够稳健。

Method: 开发了CSSP2P GAN模型，在迭代开发过程中研究了对抗性损失的影响，并通过盲法病理专家评估验证模型性能。

Result: CSSP2P GAN实现了更高的病理保真度，对抗性损失被证明对虚拟染色质量至关重要，模型性能优于该领域的参考工作。

Conclusion: 虚拟染色是一个新兴研究领域，需要更稳健的评估方法，CSSP2P GAN在病理保真度方面表现优异，对抗性损失在虚拟染色中起着关键作用。

Abstract: In addition to evaluating tumor morphology using H&E staining, immunohistochemistry is used to assess the presence of specific proteins within the tissue. However, this is a costly and labor-intensive technique, for which virtual staining, as an image-to-image translation task, offers a promising alternative. Although recent, this is an emerging field of research with 64% of published studies just in 2024. Most studies use publicly available datasets of H&E-IHC pairs from consecutive tissue sections. Recognizing the training challenges, many authors develop complex virtual staining models based on conditional Generative Adversarial Networks, but ignore the impact of adversarial loss on the quality of virtual staining. Furthermore, overlooking the issues of model evaluation, they claim improved performance based on metrics such as SSIM and PSNR, which are not sufficiently robust to evaluate the quality of virtually stained images. In this paper, we developed CSSP2P GAN, which we demonstrate to achieve heightened pathological fidelity through a blind pathological expert evaluation. Furthermore, while iteratively developing our model, we study the impact of the adversarial loss and demonstrate its crucial role in the quality of virtually stained images. Finally, while comparing our model with reference works in the field, we underscore the limitations of the currently used evaluation metrics and demonstrate the superior performance of CSSP2P GAN.

</details>


### [323] [Eevee: Towards Close-up High-resolution Video-based Virtual Try-on](https://arxiv.org/abs/2511.18957)
*Jianhao Zeng,Yancheng Bai,Ruidong Chen,Xuanpu Zhang,Lei Sun,Dongyang Jin,Ryan Xu,Nannan Zhang,Dan Song,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出了一个高分辨率视频虚拟试穿数据集，包含详细服装图像和文本描述，以及全身和特写试穿视频。同时提出了VGID指标来评估服装一致性，解决了现有方法在纹理细节捕捉和特写视频生成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视频虚拟试穿技术存在两个关键限制：1) 依赖单一服装图像输入，难以准确捕捉真实纹理细节；2) 只关注全身试穿视频，忽略了商业对特写视频的需求。

Method: 构建高分辨率数据集，包含详细服装信息（高保真图像、特写镜头、文本描述）和真实人体模型的全身及特写试穿视频。提出VGID指标评估服装纹理和结构一致性。

Result: 实验证明，利用数据集中的详细图像，现有视频生成模型能够提取并融入纹理特征，显著提升虚拟试穿结果的真实感和细节保真度。基准测试有效识别了当前方法在纹理和结构保持方面的问题。

Conclusion: 该数据集和VGID指标为视频虚拟试穿技术提供了重要支持，解决了纹理细节捕捉和特写视频生成的挑战，推动了该技术在电商营销中的实际应用。

Abstract: Video virtual try-on technology provides a cost-effective solution for creating marketing videos in fashion e-commerce. However, its practical adoption is hindered by two critical limitations. First, the reliance on a single garment image as input in current virtual try-on datasets limits the accurate capture of realistic texture details. Second, most existing methods focus solely on generating full-shot virtual try-on videos, neglecting the business's demand for videos that also provide detailed close-ups. To address these challenges, we introduce a high-resolution dataset for video-based virtual try-on. This dataset offers two key features. First, it provides more detailed information on the garments, which includes high-fidelity images with detailed close-ups and textual descriptions; Second, it uniquely includes full-shot and close-up try-on videos of real human models. Furthermore, accurately assessing consistency becomes significantly more critical for the close-up videos, which demand high-fidelity preservation of garment details. To facilitate such fine-grained evaluation, we propose a new garment consistency metric VGID (Video Garment Inception Distance) that quantifies the preservation of both texture and structure. Our experiments validate these contributions. We demonstrate that by utilizing the detailed images from our dataset, existing video generation models can extract and incorporate texture features, significantly enhancing the realism and detail fidelity of virtual try-on results. Furthermore, we conduct a comprehensive benchmark of recent models. The benchmark effectively identifies the texture and structural preservation problems among current methods.

</details>


### [324] [CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery](https://arxiv.org/abs/2511.18968)
*Bhuvan Sachdeva,Sneha Kumari,Rudransh Agarwal,Shalaka Kumaraswamy,Niharika Singri Prasad,Simon Mueller,Raphael Lechtenboehmer,Maximilian W. M. Wintergerst,Thomas Schultz,Kaushik Murali,Mohit Jain*

Main category: cs.CV

TL;DR: CataractCompDetect是一个白内障手术并发症检测框架，结合了相位感知定位、SAM 2跟踪、并发症特定风险评分和视觉语言推理，在首个白内障手术并发症数据集CataComp上实现了70.63%的平均F1分数。


<details>
  <summary>Details</summary>
Motivation: 白内障手术是全球最常见的手术之一，但术中并发症如虹膜脱垂、后囊破裂和玻璃体丢失仍是导致不良结果的主要原因。自动检测这些事件可以实现早期预警系统和客观培训反馈。

Method: 提出CataractCompDetect框架，结合相位感知定位、SAM 2跟踪、并发症特定风险评分和视觉语言推理进行最终分类。

Result: 在CataComp数据集上，平均F1得分为70.63%，各并发症检测性能为：虹膜脱垂81.8%、后囊破裂60.87%、玻璃体丢失69.23%。

Conclusion: 结果表明将结构化手术先验知识与视觉语言推理相结合对于识别罕见但高影响的术中事件具有重要价值。

Abstract: Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.

</details>


### [325] [Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs](https://arxiv.org/abs/2511.18976)
*Huaming Ling,Ying Wang,Si Chen,Junfeng Fan*

Main category: cs.CV

TL;DR: 提出单阶段微调策略和广义交错打包方案，解决全同态加密CNN推理中的非线性激活近似和密文容量限制问题，实现高效端到端FHE推理。


<details>
  <summary>Details</summary>
Motivation: 解决深度CNN在全同态加密推理中的两个核心挑战：用低阶多项式近似非线性激活函数（如ReLU）以减少精度损失，以及克服密文容量限制以支持高分辨率图像处理。

Method: 1. 单阶段微调策略：将预训练CNN直接转换为FHE友好形式；2. 广义交错打包方案：兼容任意空间分辨率的特征图，配合精心设计的同态算子保持GIP形式加密。

Result: 在CIFAR-10、ImageNet和MS COCO上的实验表明，通过SFT策略获得的FHE友好CNN达到与使用ReLU或SiLU激活的基线相当的精度，并首次展示了基于FHE的YOLO架构目标检测推理。

Conclusion: 该工作实现了跨多种CNN架构的高效端到端FHE推理，为全同态加密在深度学习中的实际应用提供了重要进展。

Abstract: We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.

</details>


### [326] [Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models](https://arxiv.org/abs/2511.18978)
*Santiago Moreno,Pablo Meseguer,Rocío del Amor,Valery Naranjo*

Main category: cs.CV

TL;DR: ZEUS是一个零样本视觉语言分割框架，利用类别特定的文本提示集合和冻结的VLM编码器，在组织病理学全玻片图像中生成高分辨率肿瘤分割掩码，无需像素级标注。


<details>
  <summary>Details</summary>
Motivation: 皮肤肿瘤活检的准确标注面临巨大挑战，包括形态学变异大、组织学模式重叠以及良恶性病变的细微区别。现有视觉语言模型在组织病理学中主要局限于玻片级任务或依赖粗糙交互提示，难以在千兆像素全玻片图像上生成细粒度分割。

Method: 将每个全玻片图像分割成重叠的图块，提取视觉嵌入，通过计算与文本提示的余弦相似度来生成最终分割掩码。使用类别特定的文本提示集合和冻结的VLM编码器。

Result: 在两个内部数据集（原发性梭形细胞肿瘤和皮肤转移瘤）上展示了竞争性性能，突出了提示设计、领域偏移和机构变异对组织病理学VLM的影响。

Conclusion: ZEUS显著减少了标注负担，同时为下游诊断工作流程提供了可扩展、可解释的肿瘤分割方法。

Abstract: Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.

</details>


### [327] [UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection](https://arxiv.org/abs/2511.18983)
*Ching-Yi Lai,Chih-Yu Jian,Pei-Cheng Chuang,Chia-Ming Lee,Chih-Chung Hsu,Chiou-Ting Hsu,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 提出了一种单模态生成多模态对比学习框架，通过将单一视觉模态转换为三个互补特征，解决社交媒体压缩环境下深度伪造检测的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台采用不同程度的压缩给深度伪造检测模型带来泛化和可靠性挑战，现有单模态方法在数据压缩下特征退化，多模态方法需要昂贵的数据收集且面临模态质量不一致问题。

Method: UMCL框架将单视觉模态转换为三个互补特征：压缩鲁棒的rPPG信号、时间地标动态和预训练视觉语言模型的语义嵌入，通过亲和力驱动的语义对齐策略和跨质量相似性学习策略进行特征对齐和鲁棒性增强。

Result: 实验表明该方法在各种压缩率和操作类型下均取得优越性能，建立了鲁棒深度伪造检测的新基准，即使在单个特征退化时仍保持高检测精度。

Conclusion: 该方法通过显式特征对齐提供了可解释的特征关系洞察，为压缩环境下的深度伪造检测提供了有效的解决方案。

Abstract: In deepfake detection, the varying degrees of compression employed by social media platforms pose significant challenges for model generalization and reliability. Although existing methods have progressed from single-modal to multimodal approaches, they face critical limitations: single-modal methods struggle with feature degradation under data compression in social media streaming, while multimodal approaches require expensive data collection and labeling and suffer from inconsistent modal quality or accessibility in real-world scenarios. To address these challenges, we propose a novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate (CCR) deepfake detection. In the training stage, our approach transforms a single visual modality into three complementary features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings from pre-trained vision-language models. These features are explicitly aligned through an affinity-driven semantic alignment (ASA) strategy, which models inter-modal relationships through affinity matrices and optimizes their consistency through contrastive learning. Subsequently, our cross-quality similarity learning (CQSL) strategy enhances feature robustness across compression rates. Extensive experiments demonstrate that our method achieves superior performance across various compression rates and manipulation types, establishing a new benchmark for robust deepfake detection. Notably, our approach maintains high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships through explicit alignment.

</details>


### [328] [Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning](https://arxiv.org/abs/2511.18989)
*Wassim Benabbas,Mohammed Brahimi,Samir Akhrouf,Bilal Fortas*

Main category: cs.CV

TL;DR: 本研究探讨注意力架构和零样本学习方法能否弥合植物病害分类中学术数据集与现实农业条件之间的差距，发现CLIP零样本模型在无需任务特定训练的情况下直接根据自然语言描述分类病害，展现出强大的适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于PlantVillage数据集的研究虽然精度高，但无法泛化到现实田间图像，在已发表研究与实际应用需求之间存在显著差距，需要研究解决这一问题的方法。

Method: 评估三类模型：卷积神经网络(CNNs)、视觉变换器(Vision Transformers)和基于对比语言-图像预训练(CLIP)的零样本模型，比较它们在领域转移下的性能表现。

Result: CNNs在领域转移下鲁棒性有限，视觉变换器通过捕捉全局上下文特征展现出更强的泛化能力，而CLIP模型无需任务特定训练即可直接根据自然语言描述分类病害，提供强大的适应性和可解释性。

Conclusion: 零样本学习作为植物健康诊断的领域适应策略具有实际和可扩展的潜力，特别适用于多样化的田间环境。

Abstract: Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.

</details>


### [329] [View-Consistent Diffusion Representations for 3D-Consistent Video Generation](https://arxiv.org/abs/2511.18991)
*Duolikun Danier,Ge Gao,Steven McDonagh,Changjian Li,Hakan Bilen,Oisin Mac Aodha*

Main category: cs.CV

TL;DR: ViCoDR通过改进视频扩散模型的多视角一致性表示，显著提升了生成视频的3D一致性，解决了现有模型在相机姿态变化时出现的物体和结构变形问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在相机姿态变化时会产生3D不一致的视觉伪影，如物体和结构变形，这影响了用户体验和仿真保真度。

Method: 提出ViCoDR方法，通过学习多视角一致的扩散表示来改进视频模型的3D一致性，在相机控制的图像到视频、文本到视频和多视角生成模型上进行评估。

Result: ViCoDR在多个视频生成任务中显著提升了生成视频的3D一致性，证明了3D一致表示与视频质量之间的强相关性。

Conclusion: 改进视频扩散模型的多视角一致性表示是提升生成视频3D一致性的有效途径，ViCoDR方法在各种视频生成场景中都取得了显著改进。

Abstract: Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.

</details>


### [330] [AuViRe: Audio-visual Speech Representation Reconstruction for Deepfake Temporal Localization](https://arxiv.org/abs/2511.18993)
*Christos Koutlis,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 提出AuViRe方法，通过音频-视觉语音表示重建来定位深度伪造视频中的篡改片段，利用跨模态重建在篡改区域产生更大差异的原理实现精确的时间定位。


<details>
  <summary>Details</summary>
Motivation: 随着合成音视频内容的快速发展，确保数字媒体完整性变得至关重要，需要有效检测恶意篡改内容。

Method: 基于音频-视觉语音表示重建(AuViRe)，从一个模态(如唇部运动)重建另一个模态(如音频波形)的表示，篡改视频段中跨模态重建更困难，产生放大差异。

Result: 在LAV-DF数据集上AP@0.95提升+8.9，在AV-Deepfake1M上AP@0.5提升+9.6，在真实场景实验中AUC提升+5.1。

Conclusion: AuViRe方法通过跨模态重建差异有效定位深度伪造视频中的篡改片段，在多个数据集上显著优于现有方法。

Abstract: With the rapid advancement of sophisticated synthetic audio-visual content, e.g., for subtle malicious manipulations, ensuring the integrity of digital media has become paramount. This work presents a novel approach to temporal localization of deepfakes by leveraging Audio-Visual Speech Representation Reconstruction (AuViRe). Specifically, our approach reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform). Cross-modal reconstruction is significantly more challenging in manipulated video segments, leading to amplified discrepancies, thereby providing robust discriminative cues for precise temporal forgery localization. AuViRe outperforms the state of the art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on an in-the-wild experiment. Code available at https://github.com/mever-team/auvire.

</details>


### [331] [A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation](https://arxiv.org/abs/2511.19004)
*Wentao Qu,Guofeng Mei,Yang Wu,Yongshun Gong,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: 提出T2LDM文本到LiDAR扩散模型，通过自条件表示引导(SCRG)解决文本-LiDAR数据稀缺问题，生成高质量3D场景，并支持多种条件生成任务。


<details>
  <summary>Details</summary>
Motivation: 文本到LiDAR生成可以为下游任务提供丰富的3D数据，但文本-LiDAR数据对稀缺导致训练先验不足，生成场景过于平滑，且低质量文本描述会降低生成质量和可控性。

Method: 提出T2LDM扩散模型，采用自条件表示引导(SCRG)在训练时提供软监督，推理时解耦；构建T2nuScenes基准和可控性指标；设计方向位置先验减少街道扭曲；通过冻结去噪网络学习条件编码器支持多条件任务。

Result: 在无条件和条件生成实验中，T2LDM优于现有方法，实现了最先进的场景生成效果。

Conclusion: T2LDM通过SCRG机制有效感知几何结构，生成详细场景对象，同时支持多种条件生成任务，在文本到LiDAR生成方面表现优异。

Abstract: Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.

</details>


### [332] [Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting](https://arxiv.org/abs/2511.19021)
*Qiyang Yu,Yu Fang,Tianrui Li,Xuemei Cao,Yan Chen,Jianghao Li,Fan Min*

Main category: cs.CV

TL;DR: 提出了Grc-ViT，一种动态粗到细的视觉Transformer框架，通过自适应调整视觉粒度来解决ViT在细粒度细节表示上的不足，平衡了全局依赖和局部感知。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在捕获全局依赖方面表现出色，但难以高效表示细粒度局部细节。现有的多尺度方法依赖固定补丁大小并引入冗余计算。

Method: 包含两个关键模块：粗粒度评估模块（使用边缘密度、熵和频域线索评估视觉复杂度）和细粒度精炼模块（根据选定粒度精炼注意力计算）。通过两个可学习参数α和β端到端优化。

Result: 综合评估表明，Grc-ViT在增强细粒度区分能力的同时，在准确性和计算效率之间实现了优越的权衡。

Conclusion: Grc-ViT通过动态调整视觉粒度，有效解决了ViT在细粒度细节表示上的局限性，实现了高效且精确的特征学习。

Abstract: Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, α and \b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.

</details>


### [333] [Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling](https://arxiv.org/abs/2511.19024)
*Long Tang,Guoquan Zhen,Jie Hao,Jianbo Zhang,Huiyu Duan,Liang Yuan,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种名为Life-IQA的盲图像质量评估方法，通过GCN增强的层间交互和MoE特征解耦来改进质量特征解码，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有BIQA方法融合浅层和深层特征时忽视了它们对质量预测的不等贡献，且质量解码架构研究不足。

Method: 使用GCN增强的层间交互模块进行跨注意力特征交互，以及MoE特征解耦模块通过不同专家解耦特定失真类型或质量维度的特征表示。

Result: 在多个BIQA基准测试中，Life-IQA比普通Transformer解码器在准确性和成本之间表现出更优的平衡，并达到最先进性能。

Conclusion: Life-IQA通过创新的特征解码框架有效解决了BIQA中特征贡献不均和解码架构不足的问题，取得了优异性能。

Abstract: Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \underline{l}ayer\underline{i}nteraction and MoE-based \underline{f}eature d\underline{e}coupling, termed \textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\texttt{Life-IQA}}.

</details>


### [334] [Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric](https://arxiv.org/abs/2511.19032)
*Xiangjie Sui,Songyang Li,Hanwei Zhu,Baoliang Chen,Yuming Fang,Xin Sun*

Main category: cs.CV

TL;DR: 提出了Bench-C基准测试和RAS指标，用于评估大视觉语言模型在视觉损坏下的鲁棒性，强调判别性样本和预测结构退化分析。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在两个主要局限：1) 当前数据集中低判别性样本占主导，掩盖了模型间的真实鲁棒性差距；2) 传统基于准确率的指标无法捕捉底层预测结构的退化。

Method: 引入Bench-C基准测试，通过考虑损坏下预测不一致性和语义多样性的选择策略来强调判别性样本；提出RAS统一指标，通过预测不确定性和校准对齐的变化来测量logit级预测结构的退化。

Result: 实验发现：1) 模型在损坏下表现出不同行为模式，如错误置信和犹豫；2) 轻微损坏可能导致准确率略有提升，但整体预测结构仍会退化；3) 将鲁棒性分解为破坏性和纠正性组件，可揭示不同模型的失败和恢复模式。

Conclusion: Bench-C和RAS提供了更全面的模型鲁棒性评估框架，能够揭示传统指标无法捕捉的预测结构退化问题。

Abstract: Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.

</details>


### [335] [ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay](https://arxiv.org/abs/2511.19033)
*Gengyuan Zhang,Mingcong Ding,Jingpei Wu,Ruotong Liao,Volker Tresp*

Main category: cs.CV

TL;DR: ReEXplore是一个无需训练的方法，通过回顾性经验回放和分层边界选择来解决MLLM在具身探索中的问题，显著提升了探索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM具身探索方法存在三个主要问题：依赖过时的预训练知识、训练成本高、边界探索动作空间过大导致决策不可靠。

Method: 使用回顾性经验回放注入蒸馏的抽象经验，以及分层边界选择将边界排序分解为从粗到细的决策过程。

Result: 在多个具身探索基准测试中，ReEXplore相比强MLLM基线取得了显著提升，成功率和导航效率最高提升了3倍。

Conclusion: ReEXplore框架能够实现鲁棒、可追踪且高效的具身探索，解决了MLLM在探索新环境时的关键挑战。

Abstract: Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.

</details>


### [336] [CSD: Change Semantic Detection with only Semantic Change Masks for Damage Assessment in Conflict Zones](https://arxiv.org/abs/2511.19035)
*Kai Zhenga,Zhenkai Wu,Fupeng Wei,Miaolan Zhou,Kai Lie,Haitao Guo,Lei Ding,Wei Zhang,Hang-Cheng Dong*

Main category: cs.CV

TL;DR: 提出了基于DINOv3预训练模型的多尺度交叉注意力差异孪生网络(MC-DiSNet)，用于解决冲突地区小范围、模糊边界损伤检测的挑战，并定义了新的CSD任务。


<details>
  <summary>Details</summary>
Motivation: 冲突地区损伤评估对人道主义援助和地区稳定至关重要，但损伤区域通常面积小、边界模糊，导致数据有限、标注困难，存在高类内相似性和模糊语义变化等识别挑战。

Method: 使用DINOv3作为骨干网络提取双时相遥感图像特征，提出多尺度交叉注意力差异孪生网络MC-DiSNet，并发布了包含2023-2024年高分辨率卫星图像的Gaza-change数据集。

Result: 在Gaza-Change和SECOND数据集上的实验表明，该方法能有效解决CSD任务，为冲突地区快速损伤评估的实际应用铺平了道路。

Conclusion: 提出的CSD任务直接扩展了二元变化检测，由于语义区域空间范围有限，比传统SCD任务更具挑战性，但该方法表现出色，具有实际应用价值。

Abstract: Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. Unlike conventional semantic change detection (SCD), our approach eliminates the need for large-scale semantic annotations of bi-temporal images, instead focusing directly on the changed regions. We term this new task change semantic detection (CSD). The CSD task represents a direct extension of binary change detection (BCD). Due to the limited spatial extent of semantic regions, it presents greater challenges than traditional SCD tasks. We evaluated our method under the CSD framework on both the Gaza-Change and SECOND datasets. Experimental results demonstrate that our proposed approach effectively addresses the CSD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.

</details>


### [337] [MedSAM3: Delving into Segment Anything with Medical Concepts](https://arxiv.org/abs/2511.19046)
*Anglin Liu,Rundong Xue,Xu R. Cao,Yifan Shen,Yi Lu,Xiang Li,Qianqian Chen,Jintai Chen*

Main category: cs.CV

TL;DR: MedSAM-3是一个基于文本提示的医学图像分割模型，通过微调SAM 3架构并引入多模态大语言模型，实现了开放词汇的医学概念分割，在各种医学成像模态上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法缺乏泛化性，需要大量耗时的手动标注，无法适应新的临床应用需求。

Method: 在医学图像上微调SAM 3架构，结合语义概念标签，实现医学可提示概念分割；引入MedSAM-3 Agent框架，集成多模态大语言模型进行复杂推理和迭代优化。

Result: 在X射线、MRI、超声、CT和视频等多种医学成像模态上的综合实验表明，该方法显著优于现有的专业模型和基础模型。

Conclusion: MedSAM-3通过文本提示和智能代理框架，为医学图像分割提供了更通用、高效的解决方案，具有重要的临床应用价值。

Abstract: Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.

</details>


### [338] [Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation](https://arxiv.org/abs/2511.19049)
*Ruojun Xu,Yu Kai,Xuhua Ren,Jiaxiang Cheng,Bing Ma,Tianxiang Zheng,Qinhlin Lu*

Main category: cs.CV

TL;DR: PG-DPO通过自适应拒绝缩放和隐式偏好正则化解决DPO在扩散模型中的似然位移问题，在视频生成任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DPO在扩散模型中的似然位移问题尚未被充分研究，这导致视频生成任务性能不佳。

Method: 在扩散框架下分析DPO损失，提出PG-DPO方法，结合自适应拒绝缩放(ARS)和隐式偏好正则化(IPR)。

Result: PG-DPO在定量指标和定性评估中均优于现有方法。

Conclusion: PG-DPO为视频生成任务中的偏好对齐提供了稳健解决方案。

Abstract: Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.

</details>


### [339] [LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space](https://arxiv.org/abs/2511.19057)
*Hai Wu,Shuai Tang,Jiale Wang,Longkun Zou,Mingyue Guo,Rongqin Liang,Ke Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: 提出了LAA3D数据集，包含15,000张真实图像和600,000帧合成数据，用于低空飞行器的3D检测和跟踪，并建立了基准测试和单目3D检测基线方法MonoLAA。


<details>
  <summary>Details</summary>
Motivation: 目前针对低空飞行器3D感知的专用数据集稀缺，需要填补这一空白以推动3D目标定位和行为理解研究。

Method: 构建大规模LAA3D数据集，包含真实和合成数据，涵盖多种飞行器类别；建立统一评估协议的基准测试；提出单目3D检测基线方法MonoLAA。

Result: 数据集支持3D目标检测、多目标跟踪和6自由度姿态估计；MonoLAA方法在变焦相机上实现稳健的3D定位；合成数据预训练模型通过微调在真实数据上表现良好。

Conclusion: LAA3D为低空3D目标感知研究提供了全面基础，展示了从合成到真实数据的良好泛化能力。

Abstract: Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.

</details>


### [340] [Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation](https://arxiv.org/abs/2511.19062)
*Qiyang Yu,Yu Fang,Tianrui Li,Xuemei Cao,Yan Chen,Jianghao Li,Fan Min,Yi Zhang*

Main category: cs.CV

TL;DR: Grc-SAM是一个基于粒度计算的粗到细框架，通过多粒度注意力机制实现无需手动提示的图像分割，解决了现有方法的局部定位和可扩展性限制。


<details>
  <summary>Details</summary>
Motivation: 解决现有提示无关图像分割方法（如SAM）的两个主要限制：(1) 缺乏自主区域定位机制；(2) 在高分辨率下细粒度建模能力有限。

Method: 采用粗到细三阶段框架：粗阶段自适应提取高响应区域实现前景定位；细阶段使用稀疏局部注意力增强细节建模；将精炼掩码编码为潜在提示嵌入替代手工提示。

Result: 大量实验结果表明Grc-SAM在准确性和可扩展性方面优于基线方法。

Conclusion: Grc-SAM为提示无关分割提供了独特的粒度计算视角，成功将粒度计算与视觉变换器相结合。

Abstract: Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.

</details>


### [341] [Understanding, Accelerating, and Improving MeanFlow Training](https://arxiv.org/abs/2511.19065)
*Jin-Young Kim,Hyojun Go,Lea Bogensperger,Julius Erbach,Nikolai Kalischek,Federico Tombari,Konrad Schindler,Dominik Narnhofer*

Main category: cs.CV

TL;DR: MeanFlow通过联合学习瞬时和平均速度场实现少步高质量生成，但训练动态不明确。研究发现瞬时速度是学习平均速度的前提，小时间间隔下平均速度有助于瞬时速度学习，但大间隔会降低效果。基于此设计了先加速瞬时速度形成、再转向长间隔平均速度的训练方案，显著提升了少步生成性能。


<details>
  <summary>Details</summary>
Motivation: MeanFlow虽然承诺在少步内实现高质量生成建模，但其训练动态机制尚不明确，需要深入分析两种速度场的相互作用以优化训练过程。

Method: 分析瞬时速度和平均速度的相互作用，发现瞬时速度是学习平均速度的前提条件，小时间间隔下平均速度有助于瞬时速度学习。基于这些观察设计了分阶段训练方案：先加速瞬时速度形成，然后逐步转向长间隔平均速度学习。

Result: 增强的MeanFlow训练实现了更快的收敛和显著更好的少步生成性能：在DiT-XL骨干网络上，1步生成ImageNet 256x256的FID达到2.87，相比传统MeanFlow基线的3.43有明显提升。或者可以用2.5倍更短的训练时间达到基线性能，或用更小的DiT-L骨干网络达到相同性能。

Conclusion: 瞬时速度和平均速度之间存在复杂的相互作用关系，通过分阶段优化训练策略可以显著提升MeanFlow的少步生成性能，证明了训练动态分析对优化生成模型的重要性。

Abstract: MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.

</details>


### [342] [DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling](https://arxiv.org/abs/2511.19067)
*Timur Mamedov,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: DynaMix是一种新颖的可泛化行人重识别方法，通过动态结合手动标记的多摄像头数据和大规模伪标记的单摄像头数据，在三个核心组件的协同作用下实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法严重依赖有限的多摄像头标记数据，而DynaMix旨在有效利用大规模单摄像头数据来提升模型在未见摄像头和环境下的泛化能力。

Method: 包含三个核心组件：动态重标记模块优化单摄像头伪标签；高效质心模块在大规模身份空间中维护稳健身份表示；数据采样模块平衡学习复杂度和批内多样性。

Result: 在可泛化行人重识别任务中，DynaMix始终优于现有最先进方法。

Conclusion: DynaMix通过动态适应训练数据的结构和噪声，实现了在大规模数据上的高效训练，显著提升了行人重识别的泛化性能。

Abstract: Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.

</details>


### [343] [DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation](https://arxiv.org/abs/2511.19071)
*Fangda Chen,Jintao Tang,Pancheng Wang,Ting Wang,Shasha Li,Ting Deng*

Main category: cs.CV

TL;DR: 提出了DEAP-3DSAM模型，通过特征增强解码器和双注意力提示器解决SAM在3D医学图像分割中的空间特征损失和手动提示依赖问题，在腹部肿瘤分割数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: SAM在3D医学图像分割中存在两个主要问题：伪3D处理导致空间特征损失，以及依赖手动提示难以在实际场景中应用。

Method: 提出特征增强解码器融合原始图像特征和空间信息，设计双注意力提示器通过空间注意力和通道注意力自动获取提示信息。

Result: 在四个公共腹部肿瘤分割数据集上的实验表明，DEAP-3DSAM在3D图像分割中达到最先进性能，优于或匹配现有手动提示方法。

Conclusion: 定量和定性消融研究证实了所提模块的有效性，DEAP-3DSAM成功解决了SAM在3D医学图像分割中的关键限制。

Abstract: The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.

</details>


### [344] [Graph-based 3D Human Pose Estimation using WiFi Signals](https://arxiv.org/abs/2511.19105)
*Jichao Chen,YangYang Qu,Ruibo Tang,Dirk Slock*

Main category: cs.CV

TL;DR: GraphPose-Fi是一个基于图神经网络的WiFi人体姿态估计框架，通过显式建模骨骼拓扑结构，在MM-Fi数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的WiFi人体姿态估计方法通常使用回归网络直接将CSI映射到3D关节坐标，忽略了人体关节之间的固有拓扑关系。

Method: 框架包含跨天线共享的CNN编码器用于子载波-时间特征提取，轻量级注意力模块自适应重加权时间和天线维度的特征，以及结合GCN层和自注意力的图回归头来捕捉局部拓扑和全局依赖。

Result: 在MM-Fi数据集的各种设置下，所提方法显著优于现有方法。

Conclusion: GraphPose-Fi通过显式建模骨骼拓扑关系，有效提升了WiFi人体姿态估计的性能。

Abstract: WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods. However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints. In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE. Our framework comprises a CNN encoder shared across antennas for subcarrier-time feature extraction, a lightweight attention module that adaptively reweights features over time and across antennas, and a graph-based regression head that combines GCN layers with self-attention to capture local topology and global dependencies. Our proposed method significantly outperforms existing methods on the MM-Fi dataset in various settings. The source code is available at: https://github.com/Cirrick/GraphPose-Fi.

</details>


### [345] [HABIT: Human Action Benchmark for Interactive Traffic in CARLA](https://arxiv.org/abs/2511.19109)
*Mohan Ramesh,Mark Azer,Fabian B. Flohr*

Main category: cs.CV

TL;DR: HABIT是一个高保真自动驾驶仿真基准，通过整合真实世界人类运动数据到CARLA模拟器中，解决了现有基准在模拟复杂行人行为方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶仿真在模拟真实多样的人类行为方面存在严重局限，现有基准往往简化行人交互，无法捕捉对系统部署至关重要的复杂动态意图和多样化响应。

Method: 开发了模块化、可扩展且物理一致的运动重定向流水线，将来自动作捕捉和视频的真实人类运动整合到CARLA模拟器中，从30,000个重定向运动中筛选出4,730个交通兼容的行人运动。

Result: 评估三个最先进的自动驾驶代理（InterFuser、TransFuser、BEVDriver）显示，HABIT暴露了在脚本化仿真中被隐藏的规划器弱点，碰撞率高达7.43次/公里，AIS 3+伤害风险达12.94%，不必要刹车率高达33%。

Conclusion: HABIT能够揭示最先进自动驾驶代理的关键失效模式，这些模式在之前的评估中被遗漏，所有组件已公开发布以支持可复现的行人感知AI研究。

Abstract: Current autonomous driving (AD) simulations are critically limited by their inadequate representation of realistic and diverse human behavior, which is essential for ensuring safety and reliability. Existing benchmarks often simplify pedestrian interactions, failing to capture complex, dynamic intentions and varied responses critical for robust system deployment. To overcome this, we introduce HABIT (Human Action Benchmark for Interactive Traffic), a high-fidelity simulation benchmark. HABIT integrates real-world human motion, sourced from mocap and videos, into CARLA (Car Learning to Act, a full autonomous driving simulator) via a modular, extensible, and physically consistent motion retargeting pipeline. From an initial pool of approximately 30,000 retargeted motions, we curate 4,730 traffic-compatible pedestrian motions, standardized in SMPL format for physically consistent trajectories. HABIT seamlessly integrates with CARLA's Leaderboard, enabling automated scenario generation and rigorous agent evaluation. Our safety metrics, including Abbreviated Injury Scale (AIS) and False Positive Braking Rate (FPBR), reveal critical failure modes in state-of-the-art AD agents missed by prior evaluations. Evaluating three state-of-the-art autonomous driving agents, InterFuser, TransFuser, and BEVDriver, demonstrates how HABIT exposes planner weaknesses that remain hidden in scripted simulations. Despite achieving close or equal to zero collisions per kilometer on the CARLA Leaderboard, the autonomous agents perform notably worse on HABIT, with up to 7.43 collisions/km and a 12.94% AIS 3+ injury risk, and they brake unnecessarily in up to 33% of cases. All components are publicly released to support reproducible, pedestrian-aware AI research.

</details>


### [346] [DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection](https://arxiv.org/abs/2511.19111)
*Hai Ci,Ziheng Peng,Pei Yang,Yingxin Xuan,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出了DiffSeg30k数据集，包含3万张扩散编辑图像及像素级标注，将AIGC检测从二分类扩展到语义分割任务，支持编辑区域定位和编辑模型识别。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC检测基准主要关注整图分类，忽视了扩散编辑的定位问题。扩散编辑能够对局部图像区域进行逼真修改，使得AI生成内容更难检测。

Method: 构建DiffSeg30k数据集：1) 使用COCO图像反映真实世界内容多样性；2) 采用8种SOTA扩散模型进行局部编辑；3) 每张图像最多进行三轮顺序编辑；4) 基于视觉语言模型自动识别有意义区域并生成上下文感知提示，覆盖添加、删除和属性更改。

Result: 基准测试显示语义分割任务面临显著挑战，特别是在图像失真鲁棒性方面。分割模型在整图分类上表现出色，优于现有伪造分类器，并在跨生成器泛化方面显示出巨大潜力。

Conclusion: DiffSeg30k通过展示基于分割方法的潜力和局限性，将推动AI生成内容细粒度定位研究的发展。

Abstract: Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k

</details>


### [347] [3M-TI: High-Quality Mobile Thermal Imaging via Calibration-free Multi-Camera Cross-Modal Diffusion](https://arxiv.org/abs/2511.19117)
*Minchong Chen,Xiaoyun Yuan,Junzhe Wan,Jianing Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 3M-TI是一个无需校准的多相机跨模态扩散框架，用于解决移动热成像传感器空间分辨率低的问题，通过跨模态自注意力模块在去噪过程中自适应对齐热成像和RGB特征。


<details>
  <summary>Details</summary>
Motivation: 移动平台热成像传感器的小型化限制了其空间分辨率和纹理保真度，导致图像模糊且信息量少。现有热成像超分辨率方法要么难以恢复精细结构，要么依赖繁琐的跨相机校准，影响实际部署和鲁棒性。

Method: 提出3M-TI框架，在扩散UNet中集成跨模态自注意力模块(CSM)，替代原始自注意力层，在去噪过程中自适应对齐热成像和RGB特征，无需显式相机校准。

Result: 在真实移动热成像相机和公共基准测试上的广泛评估验证了其优越性能，在视觉质量和定量指标上均达到最先进水平。增强后的热成像在目标检测和分割等关键下游任务中带来显著提升。

Conclusion: 3M-TI通过生成先验有效提升了超分辨率热成像的空间分辨率、结构保真度和纹理细节，为鲁棒的移动热感知系统提供了实用价值。

Abstract: The miniaturization of thermal sensors for mobile platforms inherently limits their spatial resolution and textural fidelity, leading to blurry and less informative images. Existing thermal super-resolution (SR) methods can be grouped into single-image and RGB-guided approaches: the former struggles to recover fine structures from limited information, while the latter relies on accurate and laborious cross-camera calibration, which hinders practical deployment and robustness. Here, we propose 3M-TI, a calibration-free Multi-camera cross-Modality diffusion framework for Mobile Thermal Imaging. At its core, 3M-TI integrates a cross-modal self-attention module (CSM) into the diffusion UNet, replacing the original self-attention layers to adaptively align thermal and RGB features throughout the denoising process, without requiring explicit camera calibration. This design enables the diffusion network to leverage its generative prior to enhance spatial resolution, structural fidelity, and texture detail in the super-resolved thermal images. Extensive evaluations on real-world mobile thermal cameras and public benchmarks validate our superior performance, achieving state-of-the-art results in both visual quality and quantitative metrics. More importantly, the thermal images enhanced by 3M-TI lead to substantial gains in critical downstream tasks like object detection and segmentation, underscoring its practical value for robust mobile thermal perception systems. More materials: https://github.com/work-submit/3MTI.

</details>


### [348] [MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images](https://arxiv.org/abs/2511.19119)
*Qirui Wang,Jingyi He,Yining Pan,Si Yong Yeo,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: 提出了MonoSR数据集，用于单目空间推理，涵盖室内、室外和物体中心场景，支持多种问题类型，为开放世界单目空间推理提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有空间推理研究主要关注室内环境和多视角观察，限制了在室外场景的泛化能力和在单目图像（最常见真实世界设置）中的应用。

Method: 构建大规模单目空间推理数据集MonoSR，评估先进视觉语言模型在该任务上的表现，分析辅助信息的重要性，并为未来模型设计提供指导。

Result: 建立了涵盖多样化场景的单目空间推理数据集，揭示了现有模型在这一挑战性任务上的局限性。

Conclusion: 为在真实世界、开放世界环境中推进单目空间推理建立了基础，提供了数据集和实用指导。

Abstract: Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.

</details>


### [349] [When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP](https://arxiv.org/abs/2511.19126)
*Beilin Chu,Weike You,Mengtao Li,Tingting Zheng,Kehan Zhao,Xuan Xu,Zhigao Lu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CV

TL;DR: 本文提出SemAnti方法，通过冻结CLIP的语义子空间并在打乱语义下仅适配伪影敏感层，显著提升了AI生成图像检测的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的检测器过度依赖语义线索而非生成器伪影，导致在分布偏移下性能脆弱。需要解决语义偏差问题以实现更鲁棒的检测。

Method: 提出语义对抗微调范式SemAnti：利用Patch Shuffle破坏全局语义连续性但保留局部伪影线索，冻结语义子空间，仅微调对伪影敏感的层。

Result: 在AIGCDetectBenchmark和GenImage基准测试中达到最先进的跨域泛化性能，证明调节语义是释放CLIP在AI生成图像检测中全部潜力的关键。

Conclusion: 通过抑制语义偏差并专注于生成器伪影，CLIP可以成为更鲁棒的AI生成图像检测器，语义调节是实现这一目标的有效策略。

Abstract: The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.

</details>


### [350] [MambaRefine-YOLO: A Dual-Modality Small Object Detector for UAV Imagery](https://arxiv.org/abs/2511.19134)
*Shuyu Cao,Minxin Chen,Yucheng Song,Zhaozhong Chen,Xinyou Zhang*

Main category: cs.CV

TL;DR: MambaRefine-YOLO：一种用于无人机图像中小目标检测的双模态融合方法，通过双门控互补Mamba融合模块和分层特征聚合颈部，在精度和速度间取得优越平衡


<details>
  <summary>Details</summary>
Motivation: 解决无人机图像中小目标检测的挑战，包括低分辨率和背景干扰，同时平衡跨模态交互的有效性和计算效率

Method: 提出DGC-MFM模块通过光照感知和差异感知门控机制自适应平衡RGB和红外模态，以及HFAN颈部采用"先精炼后融合"策略增强多尺度特征

Result: 在DroneVehicle数据集上达到83.2%的mAP，比基线提升7.9%；在VisDrone数据集上仅使用HFAN也显示显著提升

Conclusion: 该方法在精度和速度间实现了优越平衡，非常适合实际无人机应用

Abstract: Small object detection in Unmanned Aerial Vehicle (UAV) imagery is a persistent challenge, hindered by low resolution and background clutter. While fusing RGB and infrared (IR) data offers a promising solution, existing methods often struggle with the trade-off between effective cross-modal interaction and computational efficiency. In this letter, we introduce MambaRefine-YOLO. Its core contributions are a Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities through illumination-aware and difference-aware gating mechanisms, and a Hierarchical Feature Aggregation Neck (HFAN) that uses a ``refine-then-fuse'' strategy to enhance multi-scale features. Our comprehensive experiments validate this dual-pronged approach. On the dual-modality DroneVehicle dataset, the full model achieves a state-of-the-art mAP of 83.2%, an improvement of 7.9% over the baseline. On the single-modality VisDrone dataset, a variant using only the HFAN also shows significant gains, demonstrating its general applicability. Our work presents a superior balance between accuracy and speed, making it highly suitable for real-world UAV applications.

</details>


### [351] [FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation](https://arxiv.org/abs/2511.19137)
*Zhifeng Xie,Keyi Zhang,Yiye Yan,Yuling Guo,Fan Yang,Jiting Zhou,Mengtian Li*

Main category: cs.CV

TL;DR: FilmSceneDesigner是一个自动化电影场景生成系统，通过基于代理的链式框架将自然语言描述转换为结构化参数，并使用程序化生成管道创建完整的电影场景。


<details>
  <summary>Details</summary>
Motivation: 传统电影场景设计依赖专家手动建模，过程耗时耗力。需要自动化系统来提升效率。

Method: 基于代理的链式框架将语言描述转换为结构化参数，程序化生成管道执行地板规划、结构生成、材质分配、门窗布置和物体检索布局等功能。构建了SetDepot-Pro数据集（6,862个3D资产和733种材质）。

Result: 实验和人工评估表明，系统能生成结构合理、具有强烈电影真实感的场景，支持虚拟预演、施工图纸和情绪板创建等下游任务。

Conclusion: FilmSceneDesigner成功实现了自动化电影场景设计，显著提升了设计效率和质量。

Abstract: Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere. However, the traditional process depends on expert-driven manual modeling, which is labor-intensive and time-consuming. To address this issue, we introduce FilmSceneDesigner, an automated scene generation system that emulates professional film set design workflow. Given a natural language description, including scene type, historical period, and style, we design an agent-based chaining framework to generate structured parameters aligned with film set design workflow, guided by prompt strategies that ensure parameter accuracy and coherence. On the other hand, we propose a procedural generation pipeline which executes a series of dedicated functions with the structured parameters for floorplan and structure generation, material assignment, door and window placement, and object retrieval and layout, ultimately constructing a complete film scene from scratch. Moreover, to enhance cinematic realism and asset diversity, we construct SetDepot-Pro, a curated dataset of 6,862 film-specific 3D assets and 733 materials. Experimental results and human evaluations demonstrate that our system produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks such as virtual previs, construction drawing and mood board creation.

</details>


### [352] [ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation](https://arxiv.org/abs/2511.19145)
*Dongha Lee,Jinhee Park,Minjun Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: ABM-LoRA是一种通过对齐预训练模型和适配器激活边界来加速低秩适配器收敛的初始化策略，显著减少信息损失并提高训练效率。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然参数效率高，但其随机初始化导致梯度更新在失配的切空间中进行，造成显著信息损失并阻碍早期收敛。

Method: 在微调训练前，将适配器的激活边界与预训练模型对齐，最大化全参数梯度在适配器子空间中的投影。

Result: 在语言理解、对话生成和视觉识别任务中均表现出色，在VTAB-1K上达到最高准确率，在需要几何理解的结构化推理任务上获得显著提升。

Conclusion: ABM-LoRA通过激活边界对齐有效解决了LoRA初始化问题，显著加速收敛并提高性能，在各种架构和任务中表现出强大效果。

Abstract: We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.

</details>


### [353] [Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation](https://arxiv.org/abs/2511.19147)
*Huisoo Lee,Jisu Han,Hyunsouk Cho,Wonjun Hwang*

Main category: cs.CV

TL;DR: 提出了CoMA框架，利用两个互补的基础模型（如CLIP和BLIP）进行源自由域自适应，通过双向适应机制和分解互信息实现稳定的跨域知识迁移。


<details>
  <summary>Details</summary>
Motivation: 单一基础模型在源自由域自适应中语义覆盖有限，无法充分捕捉域偏移下的多样化上下文线索，需要利用多个互补基础模型来提升适应效果。

Method: CoMA框架：1）双向适应机制对齐不同基础模型与目标模型；2）分解互信息（DMI）在mini-batch训练中增强真实依赖关系，抑制虚假依赖；3）从基础模型向目标模型传递互补知识。

Result: 在Office-31、Office-Home、DomainNet-126和VisDA四个基准测试中均优于现有最先进的SFDA方法，在闭集、部分集和开集设置下都取得了最佳结果。

Conclusion: 通过协同利用多个互补基础模型，CoMA框架能够有效克服单一模型的语义偏差问题，在源自由域自适应任务中实现稳定且优越的性能。

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.

</details>


### [354] [Test-Time Preference Optimization for Image Restoration](https://arxiv.org/abs/2511.19169)
*Bingchen Li,Xin Li,Jiaqi Xu,Jiaming Guo,Wenbo Li,Renjing Pei,Zhibo Chen*

Main category: cs.CV

TL;DR: 提出了首个测试时偏好优化（TTPO）范式，用于图像恢复任务，通过在线生成偏好数据并指导扩散去噪过程，使恢复图像更符合人类偏好，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练和零样本图像恢复方法往往无法与人类偏好对齐，恢复的图像质量可能不被用户青睐，需要在不重新训练模型和不收集大量偏好数据的情况下提升恢复质量。

Method: 设计了一个无需训练的三阶段流程：1）基于初始恢复图像使用扩散反演和去噪在线生成候选偏好图像；2）使用自动偏好对齐指标或人工反馈选择偏好和非偏好图像；3）将选定的偏好图像作为奖励信号指导扩散去噪过程，优化恢复图像。

Result: 在多种图像恢复任务和模型上的广泛实验证明了该方法的有效性和灵活性。

Conclusion: TTPO范式能够有效提升图像恢复的感知质量，生成符合人类偏好的恢复结果，且与任何图像恢复模型主干兼容。

Abstract: Image restoration (IR) models are typically trained to recover high-quality images using L1 or LPIPS loss. To handle diverse unknown degradations, zero-shot IR methods have also been introduced. However, existing pre-trained and zero-shot IR approaches often fail to align with human preferences, resulting in restored images that may not be favored. This highlights the critical need to enhance restoration quality and adapt flexibly to various image restoration tasks or backbones without requiring model retraining and ideally without labor-intensive preference data collection. In this paper, we propose the first Test-Time Preference Optimization (TTPO) paradigm for image restoration, which enhances perceptual quality, generates preference data on-the-fly, and is compatible with any IR model backbone. Specifically, we design a training-free, three-stage pipeline: (i) generate candidate preference images online using diffusion inversion and denoising based on the initially restored image; (ii) select preferred and dispreferred images using automated preference-aligned metrics or human feedback; and (iii) use the selected preference images as reward signals to guide the diffusion denoising process, optimizing the restored image to better align with human preferences. Extensive experiments across various image restoration tasks and models demonstrate the effectiveness and flexibility of the proposed pipeline.

</details>


### [355] [MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes](https://arxiv.org/abs/2511.19172)
*Kehua Chen,Tianlu Mao,Zhuxin Ma,Hao Jiang,Zehao Li,Zihan Liu,Shuqi Gao,Honglong Zhao,Feng Dai,Yucheng Zhang,Zhaoqi Wang*

Main category: cs.CV

TL;DR: MetroGS是一个用于复杂城市环境高效稳健重建的新型高斯泼溅框架，通过分布式2D高斯泼溅表示、结构化密集增强、渐进混合几何优化和深度引导外观建模，在大规模城市场景中实现优越的几何精度和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅及其衍生方法在大规模场景重建中取得了重大突破，但如何高效稳定地实现高质量几何保真度仍然是一个核心挑战。

Method: 1. 基于分布式2D高斯泼溅表示作为核心基础；2. 结构化密集增强方案，利用SfM先验和点图模型实现更密集初始化；3. 渐进混合几何优化策略，整合单目和多视图优化；4. 深度引导外观建模方法，学习具有3D一致性的空间特征。

Result: 在大规模城市数据集上的实验表明，MetroGS实现了优越的几何精度和渲染质量。

Conclusion: MetroGS为高保真大规模场景重建提供了一个统一的解决方案，能够有效处理复杂城市环境中的重建挑战。

Abstract: Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.

</details>


### [356] [Evaluating Deep Learning and Traditional Approaches Used in Source Camera Identification](https://arxiv.org/abs/2511.19180)
*Mansur Ozaman*

Main category: cs.CV

TL;DR: 本文对三种相机来源识别技术进行了比较分析：PRNU、JPEG压缩伪影分析和CNN，评估了它们在设备分类准确性方面的表现，并讨论了实际应用中所需的科学进展。


<details>
  <summary>Details</summary>
Motivation: 相机来源识别是计算机视觉中的重要任务，有助于对图像进行更全面的分析。

Method: 比较分析了三种技术：Photo Response Non-Uniformity (PRNU)、JPEG压缩伪影分析和卷积神经网络(CNNs)。

Result: 评估了每种方法在设备分类准确性方面的表现。

Conclusion: 讨论了这些方法在实际场景应用中所需的科学进展和发展需求。

Abstract: One of the most important tasks in computer vision is identifying the device using which the image was taken, useful for facilitating further comprehensive analysis of the image. This paper presents comparative analysis of three techniques used in source camera identification (SCI): Photo Response Non-Uniformity (PRNU), JPEG compression artifact analysis, and convolutional neural networks (CNNs). It evaluates each method in terms of device classification accuracy. Furthermore, the research discusses the possible scientific development needed for the implementation of the methods in real-life scenarios.

</details>


### [357] [nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation](https://arxiv.org/abs/2511.19183)
*Carsten T. Lüth,Jeremias Traub,Kim-Celine Kahl,Till J. Bungert,Lukas Klein,Lars Krämer,Paul F. Jaeger,Fabian Isensee,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: nnActive是一个开源主动学习框架，用于3D生物医学图像分割，通过改进随机采样策略和提出前景效率指标，解决了当前评估中的四个陷阱，发现在生物医学图像分割中主动学习方法虽然优于标准随机采样，但未能可靠超越改进的前景感知随机采样。


<details>
  <summary>Details</summary>
Motivation: 生物医学图像语义分割依赖大量标注数据，但手动标注成本高且需要专业知识。主动学习旨在通过选择信息量最大的样本来减少标注工作量，但在3D生物医学成像领域，主动学习是否始终优于随机采样尚无共识，存在四个评估陷阱阻碍方法评估。

Method: 开发nnActive框架，通过(1)大规模研究覆盖四个生物医学成像数据集和三种标注机制；(2)扩展nnU-Net，使用部分标注进行3D基于补丁的查询选择训练；(3)提出前景感知随机采样策略处理医学图像的前景-背景类别不平衡；(4)提出前景效率指标，捕捉背景区域低标注成本。

Result: (A)所有主动学习方法都优于标准随机采样，但未能可靠超越改进的前景感知随机采样；(B)主动学习的优势取决于任务特定参数；(C)预测熵是整体表现最好的主动学习方法，但可能需要最多的标注工作量；(D)通过更计算密集的设计选择可以改进主动学习性能。

Conclusion: nnActive作为一个整体开源框架，可以作为3D生物医学成像中主动学习研究和应用的催化剂。前景感知随机采样在生物医学图像分割中是一个强有力的基线，主动学习的实际效益需要仔细评估。

Abstract: Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive

</details>


### [358] [SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection](https://arxiv.org/abs/2511.19187)
*Nithira Jayarathne,Naveen Basnayake,Keshawa Jayasundara,Pasindu Dodampegama,Praveen Wijesinghe,Hirushika Pelagewatta,Kavishka Abeywardana,Sandushan Ranaweera,Chamira Edussooriya*

Main category: cs.CV

TL;DR: 提出基于EfficientNet-B6的轻量级深度伪造图像检测模型，通过变换技术和优化策略解决类别不平衡问题，实现高准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造图像检测对于打击虚假信息至关重要，需要开发轻量且通用的检测方法，使非专家也能有效识别深度伪造内容。

Method: 使用EfficientNet-B6架构进行微调，结合变换技术处理类别不平衡，采用鲁棒预处理、过采样和优化策略，并探索傅里叶变换特征。

Result: 模型实现了高准确率、稳定性和泛化能力，但傅里叶变换的相位和振幅特征影响有限。

Conclusion: 该框架为非专家提供了有效的深度伪造图像检测工具，在可访问和可靠的检测技术方面取得了重要进展。

Abstract: Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.

</details>


### [359] [Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks](https://arxiv.org/abs/2511.19198)
*Ann-Sophia Müller,Moonkwang Jeong,Meng Zhang,Jiyuan Tian,Arkadiusz Miernik,Stefanie Speidel,Tian Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于物理器官模型的自动化3D解剖数据生成工作流，用于解决手术规划和训练中3D解剖模型数据不足的问题，特别针对成像对比度差的软组织器官如前列腺。


<details>
  <summary>Details</summary>
Motivation: 解决手术规划和训练中3D解剖模型数据获取的瓶颈问题，特别是针对成像对比度差的软组织器官，避免从真实患者获取数据面临的法律、伦理和技术挑战。

Method: 使用生物模拟水凝胶制作人工前列腺模型，在多区域具有成像对比度；通过定制超声扫描仪记录手术前后数据；训练神经网络分割超声图像；基于分割结果重建3D网格模型；使用3D GAN生成更多3D模型。

Result: 神经网络分割方法在交并比(IoU)指标上优于传统的非学习型计算机视觉技术，能够有效重建3D网格模型并提供性能反馈。

Conclusion: 该工作流能够自动化生成高质量的3D解剖数据，为依赖3D数据的下游机器学习任务提供支持，特别是在软组织器官的手术规划和训练中具有重要应用价值。

Abstract: Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.

</details>


### [360] [CLASH: A Benchmark for Cross-Modal Contradiction Detection](https://arxiv.org/abs/2511.19199)
*Teodora Popordanoska,Jiameng Li,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: CLASH是一个用于多模态矛盾检测的新基准，包含COCO图像与包含对象级或属性级矛盾的矛盾标题配对，评估模型识别跨模态冲突的能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中存在大量矛盾的多模态输入，但现有基准通常假设输入一致性，无法评估跨模态矛盾检测这一防止幻觉和确保可靠性的基本能力。

Method: 引入CLASH基准，包含COCO图像与矛盾标题配对，包含对象级和属性级矛盾，提供经过自动质量检查的微调集和人工验证的诊断集，评估采用多项选择和开放式问题格式。

Result: 对最先进模型的分析显示它们在识别跨模态冲突方面存在显著限制，暴露出系统性的模态偏见和类别特定弱点。针对CLASH的微调显著提升了冲突检测能力。

Conclusion: CLASH基准揭示了当前多模态模型在矛盾检测方面的严重不足，并证明通过针对性微调可以显著改善这一关键能力。

Abstract: Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.

</details>


### [361] [Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?](https://arxiv.org/abs/2511.19200)
*Itay Cohen,Ethan Fetaya,Amir Rosenfeld*

Main category: cs.CV

TL;DR: 该论文研究了CLIP等视觉语言模型是否能区分真实物体和相似物（如玩具、雕像、绘画等），提出了RoLA数据集和一种在CLIP嵌入空间中估计真实与相似物方向的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管计算机视觉模型在识别基准上表现良好，但与人类感知相比仍存在差距，特别是判断图像是否看起来像某个物体而不一定是该物体的实例的能力。

Method: 创建了RoLA数据集包含真实和相似物样本，首先评估基于提示的基线方法，然后在CLIP嵌入空间中估计真实与相似物之间的方向向量。

Result: 将该方向向量应用于图像和文本嵌入后，在Conceptual12M上的跨模态检索性能得到提升，同时增强了CLIP前缀字幕生成器的字幕质量。

Conclusion: CLIP模型能够捕捉真实物体与相似物之间的细微区别，通过估计嵌入空间中的方向向量可以改善跨模态检索和字幕生成任务。

Abstract: Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired "real"/"lookalike" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.

</details>


### [362] [NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting](https://arxiv.org/abs/2511.19202)
*Brent Zoomers,Florian Hahlbohm,Joni Vanherck,Lode Jorissen,Marcus Magnor,Nick Michiels*

Main category: cs.CV

TL;DR: 提出一种使用小型共享MLP学习3D高斯模型中高斯可见性函数的方法，通过遮挡剔除加速渲染，在组合场景中在VRAM使用和图像质量方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然可以利用视锥剔除和细节层次策略加速渲染，但高斯的半透明特性阻碍了遮挡剔除这一高效技术的应用。

Method: 使用小型共享MLP学习训练模型中所有高斯的视点相关可见性函数，在光栅化前查询视锥内高斯的可见性，丢弃被遮挡的基元，并集成到新型实例化软件光栅器中。

Result: 在组合场景中，该方法在VRAM使用和图像质量方面优于当前最先进技术，并与现有LoD技术具有互补特性。

Conclusion: 提出的神经查询遮挡剔除方法有效解决了3D高斯渲染中的遮挡剔除问题，显著提升了渲染效率。

Abstract: 3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.

</details>


### [363] [ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment](https://arxiv.org/abs/2511.19217)
*Wanjiang Weng,Xiaofeng Tan,Junbo Wang,Guo-Sen Xie,Pan Zhou,Hongsong Wang*

Main category: cs.CV

TL;DR: 提出ReAlign方法，通过奖励引导采样来改善文本到动作生成中的对齐问题，提升语义一致性和动作质量


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型中文本与动作分布不对齐的问题，该问题导致生成的动作语义不一致或质量低下

Method: 提出Reward-guided sampling Alignment (ReAlign)，包含步感知奖励模型和奖励引导策略，整合文本对齐模块和动作对齐模块

Result: 在动作生成和检索任务中显著提升了文本-动作对齐度和动作质量，优于现有最先进方法

Conclusion: ReAlign方法有效解决了文本与动作分布不对齐问题，在保持概率密度的同时提升了语义一致性和动作真实性

Abstract: Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.

</details>


### [364] [Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering](https://arxiv.org/abs/2511.19220)
*Federico Felizzi,Olivia Riccomi,Michele Ferramola,Francesco Andrea Causio,Manuel Del Medico,Vittorio De Vita,Lorenzo De Mori,Alessandra Piscitelli Pietro Eric Risuleo,Bianca Destro Castaniti,Antonio Cristiano Alessia Longo,Luigi De Angelis,Mariapia Vassalli,Marcello Di Pumpo*

Main category: cs.CV

TL;DR: 研究发现大型视觉语言模型在意大利医学问答中视觉依赖程度差异显著：GPT-4o视觉依赖性最强，准确率下降27.9个百分点，而其他模型仅小幅下降。所有模型都会为虚构的视觉内容生成自信解释，表明存在不同程度的文本捷径依赖。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型在医学视觉问答基准上表现优异，但其对视觉信息的真实依赖程度尚不明确。研究旨在探究前沿模型在回答意大利医学问题时是否真正基于视觉信息进行推理。

Method: 使用欧洲医学问答意大利数据集的60个明确需要图像解释的问题，将正确的医学图像替换为空白占位符，测试模型是否真正整合视觉和文本信息。评估了Claude Sonnet 4.5、GPT-4o、GPT-5-mini和Gemini 2.0 flash exp四个最先进模型。

Result: GPT-4o表现出最强的视觉依赖性，准确率从83.2%降至55.3%（下降27.9个百分点）；GPT-5-mini、Gemini和Claude仅分别下降8.5、2.4和5.6个百分点。所有模型都会为虚构的视觉解释生成自信的推理过程。

Conclusion: 不同模型在视觉依赖性方面存在显著差异，揭示了模型鲁棒性的关键差异。在临床部署前需要进行严格评估，因为模型可能过度依赖文本捷径而非真正的视觉分析。

Abstract: Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.

</details>


### [365] [Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving](https://arxiv.org/abs/2511.19221)
*Jianhua Han,Meng Tian,Jiangtong Zhu,Fan He,Huixin Zhang,Sitong Guo,Dechang Zhu,Hao Tang,Pei Xu,Yuze Guo,Minzhe Niu,Haojie Zhu,Qichao Dong,Xuechao Yan,Siyuan Dong,Lu Hou,Qingqiu Huang,Xiaosong Jia,Hang Xu*

Main category: cs.CV

TL;DR: Percept-WAM是首个在单个视觉语言模型中隐式集成2D/3D场景理解能力的感知增强世界感知-动作模型，通过统一的World-PV和World-BEV token编码空间坐标和置信度，在长尾、远距离和小物体场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在空间定位和理解方面较弱，导致自动驾驶系统在长尾场景和复杂交互中感知和定位能力有限。

Method: 提出网格条件预测机制，包含IoU感知评分和并行自回归解码，将2D/3D感知任务统一到World-PV和World-BEV token中，并利用预训练VLM参数保持通用智能。

Result: 在COCO 2D检测和nuScenes BEV 3D检测上分别达到51.7/58.9 mAP，与轨迹解码器集成后在NAVSIM上超越DiffusionDrive 2.1 PMDS，展现出强大的开放词汇和长尾泛化能力。

Conclusion: Percept-WAM成功将2D/3D感知能力集成到单一VLM中，显著提升了自动驾驶系统的空间感知稳定性和准确性，特别是在挑战性场景中。

Abstract: Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.

</details>


### [366] [Learning Plug-and-play Memory for Guiding Video Diffusion Models](https://arxiv.org/abs/2511.19229)
*Selena Song,Ziming Xu,Zijun Zhang,Kun Zhou,Jiaxian Guo,Lianhui Qin,Biwei Huang*

Main category: cs.CV

TL;DR: 提出了DiT-Mem方法，通过可学习的记忆编码器为扩散变换器视频生成模型注入世界知识，改善物理规则遵循和视频保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的DiT视频生成模型虽然视觉质量好，但经常违反基本物理定律和常识动态，缺乏显式的世界知识。

Method: 使用堆叠的3D CNN、低通/高通滤波器和自注意力层构建记忆编码器，将参考视频映射为紧凑的记忆标记，在DiT自注意力层中作为记忆使用。训练时冻结扩散主干，仅优化记忆编码器。

Result: 方法在少量训练参数(150M)和10K数据样本上实现高效训练，推理时可即插即用，显著提升了物理规则遵循和视频保真度。

Conclusion: DiT-Mem通过记忆注入机制有效增强了视频生成模型的世界知识，改善了物理合理性和生成质量。

Abstract: Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.

</details>


### [367] [IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2511.19235)
*Carl Lindström,Mahan Rafidashti,Maryam Fatemi,Lars Hammarstrand,Martin R. Oswald,Lennart Svensson*

Main category: cs.CV

TL;DR: IDSplat是一个自监督的3D高斯泼溅框架，无需人工标注即可重建动态驾驶场景，实现显式的实例分解和可学习的运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖昂贵的人工标注来获取物体轨迹，要么使用没有显式物体级分解的时变表示，导致静态和动态元素交织，阻碍场景分离。

Method: 将动态物体建模为经历刚性变换的连贯实例，使用零样本、基于语言的视频跟踪结合激光雷达进行3D锚定，通过特征对应估计一致位姿，并引入协调转向平滑方案获得时间物理一致的运动轨迹。

Result: 在Waymo Open Dataset上的实验表明，该方法实现了有竞争力的重建质量，同时保持实例级分解，并能跨不同序列和视图密度泛化而无需重新训练。

Conclusion: IDSplat为大规模自动驾驶应用提供了一种实用的动态场景重建解决方案，无需人工标注即可实现高质量的实例分解重建。

Abstract: Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.

</details>


### [368] [Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation](https://arxiv.org/abs/2511.19254)
*Mohamed Rissal Hedna,Sesugh Samuel Nder*

Main category: cs.CV

TL;DR: 本文研究了在物流系统中针对货物占用率分类器的物理对抗性补丁攻击，通过3D模拟环境优化补丁纹理，在拒绝服务攻击中达到84.94%成功率。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉系统在现代物流中广泛应用，但可能受到物理对抗性攻击的威胁，特别是可打印并放置在内部表面的对抗性补丁。

Method: 使用Mitsuba 3进行可微分渲染，在几何、光照和视点变化下优化补丁纹理，并与2D合成基线进行比较。

Result: 3D优化补丁在拒绝服务攻击（空到满）中达到84.94%成功率，隐蔽攻击（满到空）达到30.32%成功率。

Conclusion: 这是首个在物理真实的完全模拟3D场景中研究货物占用率估计对抗性补丁攻击的工作，强调了自动化物流管道安全性的重要性。

Abstract: Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.

</details>


### [369] [LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models](https://arxiv.org/abs/2511.19261)
*Shuai Wang,Daoan Zhang,Tianyi Bai,Shitong Shao,Jiebo Luo,Jiaheng Wei*

Main category: cs.CV

TL;DR: LAST方法通过让视觉语言模型在给出最终答案前进行空间和时间维度的视觉思考，联合提升3D空间理解和长视频理解能力，仅使用2D图像作为输入。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视觉语言模型在3D空间理解和长视频理解方面仍然表现不佳，虽然它们在典型的视觉语言任务上很强大。现有方法通常依赖专门的架构设计来分别改进3D任务和视频理解任务的性能。

Method: 提出LAST方法，让VLMs在空间和时间维度进行思考，构建3D空间和时间维度的视觉思考轨迹，而不是仅依赖文本给出最终答案。支持两种场景：零样本直接提示专有模型，以及使用包含空间和时间思考轨迹的数据微调通用VLMs。

Result: LAST在各种基准测试中带来了显著提升，包括3个空间理解、4个视频理解和3个图像理解任务。特别地，在EgoSchema上使用GPT-4o零样本获得15.8%的提升，在VSI-Bench上相比Qwen2.5-VL-7B获得8.3的提升。

Conclusion: LAST方法能够有效联合提升视觉语言模型在3D空间和长视频理解方面的能力，仅使用2D图像输入即可实现显著性能改进。

Abstract: Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.

</details>


### [370] [BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment](https://arxiv.org/abs/2511.19268)
*Dewei Zhou,Mingwei Li,Zongxin Yang,Yu Lu,Yunqiu Xu,Zhizhong Wang,Zeyi Huang,Yi Yang*

Main category: cs.CV

TL;DR: 提出了BideDPO框架，通过双向解耦的偏好优化方法解决条件图像生成中文本与条件图像之间的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 当前条件图像生成方法在处理文本提示与条件图像之间的冲突时面临挑战，包括输入级冲突和模型偏置冲突，标准监督微调难以有效解决这些问题。

Method: 提出双向解耦DPO框架，创建两个解耦的偏好对（一个针对条件，一个针对文本），使用自适应损失平衡策略管理偏好对的影响，并开发自动数据管道生成冲突感知数据。

Result: 实验表明BideDPO显著提高了文本成功率（如+35%）和条件遵循度，在构建的DualAlign基准和COCO数据集上验证了方法的有效性。

Conclusion: BideDPO框架通过解耦优化策略有效解决了条件图像生成中的冲突问题，为多约束任务提供了新的解决方案。

Abstract: Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.

</details>


### [371] [Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection](https://arxiv.org/abs/2511.19274)
*Mingyang Chen,Jiawei Du,Bo Huang,Yi Wang,Xiaobo Zhang,Wei Wang*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型重建偏差的数据选择方法，通过部分反向去噪估计数据似然，在ImageNet上仅用50%数据即可达到全数据训练性能


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法主要依赖启发式评分信号，缺乏对数据似然的显式建模，可能无法捕捉支撑有效模型训练的分布结构

Method: 利用扩散模型通过部分反向去噪诱导的重建偏差来估计数据似然，基于马尔可夫扩散过程的ELBO建立重建误差与数据似然的正式联系

Result: 在ImageNet上的大量实验表明，重建偏差提供了有效的评分标准，在不同选择比例下始终优于现有基线，仅用50%数据即可接近全数据训练性能

Conclusion: 基于似然的方法揭示了数据选择中的信息洞察，阐明了数据分布特征与模型学习偏好之间的相互作用

Abstract: Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.

</details>


### [372] [ReMatch: Boosting Representation through Matching for Multimodal Retrieval](https://arxiv.org/abs/2511.19278)
*Qianying Liu,Xiao Liang,Zhiqiang Zhang,Yibo Chen,Xu Tang,Zhongfei Qing,Fengfan Zhou,Yao Hu,Paul Henderson*

Main category: cs.CV

TL;DR: ReMatch是一个利用多模态大语言模型生成能力进行多模态检索的框架，通过端到端训练和生成式匹配阶段，实现了在多模态嵌入基准上的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将MLLM视为简单编码器，忽略了其生成特性，未能充分利用其组合推理和世界知识。

Method: 使用聊天式生成匹配阶段端到端训练嵌入MLLM，通过多视角输入自回归判断相关性，并采用多个可学习token增强多模态嵌入。

Result: 在Massive Multimodal Embedding Benchmark上达到新的最先进水平，在五个数据集上显示出强大的零样本泛化能力。

Conclusion: ReMatch框架通过充分利用MLLM的生成能力，实现了更强大的多模态检索性能，具有出色的鲁棒性和可迁移性。

Abstract: We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.

</details>


### [373] [DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting](https://arxiv.org/abs/2511.19294)
*Phurtivilai Patt,Leyang Huang,Yinqiang Zhang,Yang Lei*

Main category: cs.CV

TL;DR: 提出了一种新的3D高斯泼溅方法，通过预先密集化策略结合LiDAR数据和单目深度估计，避免了传统自适应密度控制带来的浮动伪影和资源浪费问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法依赖自适应密度控制，容易产生浮动伪影和资源使用效率低下的问题。

Method: 采用预先密集化方法，结合稀疏LiDAR数据和RGB图像的单目深度估计，使用ROI感知采样方案优先处理语义和几何重要区域。

Result: 在四个新收集的数据集上验证，实现了与最先进技术相当的结果，同时显著降低了资源消耗和训练时间。

Conclusion: 该方法有效绕过了自适应密度控制，减少了高斯基元重叠，提升了视觉质量，在复杂场景中能更好地保留感兴趣区域。

Abstract: This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.

</details>


### [374] [IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection](https://arxiv.org/abs/2511.19301)
*Johannes Meier,Florian Günther,Riccardo Marin,Oussema Dhaouadi,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: IDEAL-M3D是首个用于单目3D检测的实例级主动学习框架，通过显式多样性集成方法解决现有方法选择整张图像和偏向深度模糊的问题，仅需60%标注就能达到全数据集训练的性能。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测标注成本高，现有主动学习方法存在两个问题：选择整张图像效率低，以及基于不确定性的选择偏向深度模糊的远距离物体。

Method: 提出实例级主动学习管道，使用异构骨干网络、任务无关特征、损失权重扰动和时间相关bagging来构建显式多样性集成。

Result: 在KITTI验证集和测试集上，仅使用60%标注就能达到或超过全数据集训练的AP3D性能。

Conclusion: IDEAL-M3D通过实例级选择和多样性集成，显著提高了单目3D检测主动学习的效率和性能。

Abstract: Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.
  To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset.

</details>


### [375] [Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection](https://arxiv.org/abs/2511.19306)
*Zixuan Wang,Haoran Sun,Jiaming Lu,Wenxuan Wang,Zhongling Huang,Dingwen Zhang,Xuelin Qian,Junwei Han*

Main category: cs.CV

TL;DR: 提出了DGSPNet，一种端到端的语言提示驱动框架，通过双粒度语义提示（粗粒度文本先验和细粒度个性化语义描述）和文本引导注意力机制，显著提升了红外小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有红外小目标检测方法面临特征表示有限和背景干扰严重的问题，而基于CLIP的方法又受到文本描述不准确和依赖人工标注的限制。

Method: DGSPNet框架集成双粒度语义提示：粗粒度文本先验和通过视觉到文本映射获得的细粒度个性化语义描述，并引入文本引导通道注意力和空间注意力机制。

Result: 在三个基准数据集上的大量实验表明，该方法显著提高了检测精度，达到了最先进的性能。

Conclusion: DGSPNet通过语言提示驱动的方法有效解决了红外小目标检测中的挑战，无需依赖任何标注要求，实现了优异的检测性能。

Abstract: Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.

</details>


### [376] [Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach](https://arxiv.org/abs/2511.19316)
*Xincheng Wang,Hanchi Sun,Wenjun Sun,Kejun Xue,Wangqiu Zhou,Jianbo Zhang,Wei Sun,Dandan Zhu,Xiongkuo Min,Jun Jia,Zhijun Fang*

Main category: cs.CV

TL;DR: 本文提出了一个统一的评估框架来分析扩散模型数据集水印方法，发现现有方法在通用性和可传递性方面表现良好，但在真实威胁场景下的鲁棒性不足，并提出了一种实用的水印去除方法。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型微调技术能够复制特定图像集，但带来了版权和安全风险。数据集水印技术被提出用于确保可追溯性，但缺乏统一的评估框架。

Method: 建立通用威胁模型，引入包含通用性、可传递性和鲁棒性的综合评估框架，并通过实验验证现有方法的性能，同时提出一种实用的水印去除方法。

Result: 实验表明现有方法在通用性和可传递性方面表现良好，对常见图像处理操作具有一定鲁棒性，但在真实威胁场景下仍存在不足。提出的水印去除方法能够完全消除数据集水印而不影响微调效果。

Conclusion: 当前数据集水印方法在真实威胁场景下存在脆弱性，这为未来研究提出了重要挑战，需要开发更鲁棒的水印技术。

Abstract: Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.

</details>


### [377] [SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis](https://arxiv.org/abs/2511.19319)
*Lingwei Dang,Zonghan Li,Juntong Li,Hongwen Zhang,Liang An,Yebin Liu,Qingyao Wu*

Main category: cs.CV

TL;DR: SyncMV4D是首个联合生成同步多视角手物交互视频和4D运动的模型，通过统一视觉先验、运动动力学和多视角几何，解决了现有单视角方法几何失真和3D方法泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前手物交互生成方法存在局限性：单视角视频方法缺乏3D几何感知导致几何失真，而3D方法依赖实验室环境的高质量数据，难以泛化到真实场景。

Method: 提出两个核心创新：(1)多视角联合扩散模型共同生成HOI视频和中间运动；(2)扩散点对齐器将粗糙中间运动细化为全局对齐的4D度量点轨迹，通过闭环循环耦合2D外观和4D动态。

Result: 实验表明，该方法在视觉真实性、运动合理性和多视角一致性方面优于现有最先进方法。

Conclusion: SyncMV4D通过统一多视角视频和4D运动生成，有效解决了手物交互生成中的几何失真和泛化性问题，为动画和机器人应用提供了更可靠的解决方案。

Abstract: Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.

</details>


### [378] [SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation](https://arxiv.org/abs/2511.19320)
*Jiaming Zhang,Shengming Cao,Rui Li,Xiaotong Zhao,Yutao Cui,Xinglin Hou,Gangshan Wu,Haolan Chen,Yu Xu,Limin Wang,Kai Ma*

Main category: cs.CV

TL;DR: SteadyDancer是一个基于图像到视频(I2V)范式的人类图像动画框架，通过条件协调机制、协同姿态调制模块和分阶段解耦目标训练管道，实现了第一帧身份保持和精确运动控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有参考到视频(R2V)范式在图像到运动绑定过程中忽视时空不对齐问题，导致身份漂移和视觉伪影的挑战。

Method: 1. 条件协调机制协调冲突条件；2. 协同姿态调制模块生成自适应姿态表示；3. 分阶段解耦目标训练管道分层优化模型。

Result: 在外观保真度和运动控制方面达到最先进性能，且训练资源需求显著少于同类方法。

Conclusion: SteadyDancer是首个能够稳健确保第一帧保持的框架，实现了协调一致的人类图像动画。

Abstract: Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.

</details>


### [379] [MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation](https://arxiv.org/abs/2511.19326)
*Farnoosh Koleini,Hongfei Xue,Ahmed Helmy,Pu Wang*

Main category: cs.CV

TL;DR: MonoMSK是一个从单目视频中重建生物力学真实3D人体运动的混合框架，通过结合数据驱动学习和物理模拟，同时恢复运动学（运动）和动力学（力/力矩）。


<details>
  <summary>Details</summary>
Motivation: 现有单目方法使用解剖学不准确的简化模型（如SMPL）并忽略物理约束，限制了生物力学保真度。需要开发能够同时恢复运动学和动力学的生物力学真实方法。

Method: 结合基于transformer的逆动力学与可微分前向运动学和动力学层，通过ODE模拟建立物理调节的逆-前向循环，使用前向-逆一致性损失对齐运动重建与动力学推理。

Result: 在BML-MoVi、BEDLAM和OpenCap数据集上，MonoMSK在运动学精度上显著优于现有方法，并首次实现了精确的单目动力学估计。

Conclusion: MonoMSK通过整合数据驱动学习和物理模拟，实现了生物力学真实的3D人体运动重建，为单目运动分析开辟了新方向。

Abstract: Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.

</details>


### [380] [POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse](https://arxiv.org/abs/2511.19339)
*Anjie Le,Can Peng,Yuyuan Liu,J. Alison Noble*

Main category: cs.CV

TL;DR: 本文提出POUR方法，通过几何投影在表示层面实现机器遗忘，基于神经坍缩理论证明正交投影保持ETF结构，提供可证明最优的遗忘操作。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法主要修改分类器而保留内部表示，导致遗忘不彻底。本文旨在将遗忘扩展到表示层面，实现更完整的遗忘效果。

Method: 基于神经坍缩理论，提出POUR方法：POUR-P使用闭合形式的几何投影，POUR-D在蒸馏框架下进行特征级遗忘。引入表示遗忘评分(RUS)量化表示层面的遗忘和保留保真度。

Result: 在CIFAR-10/100和PathMNIST数据集上的实验表明，POUR在分类层面和表示层面指标上均优于现有最先进的遗忘方法，能有效遗忘同时保留知识。

Conclusion: POUR方法通过表示层面的几何投影实现了可证明最优的遗忘，解决了现有方法遗忘不彻底的问题，为机器遗忘提供了新的理论框架和实践方案。

Abstract: In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.

</details>


### [381] [Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning](https://arxiv.org/abs/2511.19343)
*Qihan Huang,Haofei Zhang,Rong Wei,Yi Wang,Rui Tang,Mingli Song,Jie Song*

Main category: cs.CV

TL;DR: 提出Syn-GRPO方法，通过在线数据生成器合成高质量训练数据，解决MLLM强化学习中数据质量低、响应多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法面临数据质量低的问题，数据样本无法激发MLLM产生多样化响应，限制了强化学习的探索范围。

Method: Syn-GRPO包含数据服务器和GRPO工作流两个组件。数据服务器使用图像生成模型从现有样本合成新样本，采用解耦异步方案提高生成效率。GRPO工作流提供新图像描述，并利用多样性奖励监督MLLM预测图像描述以合成多样化响应样本。

Result: 在三个视觉感知任务上的实验结果表明，Syn-GRPO大幅提高了数据质量，性能显著优于现有MLLM感知方法。

Conclusion: Syn-GRPO在提升数据质量方面表现出色，为长期自演化强化学习展示了良好潜力。

Abstract: RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.

</details>


### [382] [CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting](https://arxiv.org/abs/2511.19351)
*Abdurahman Ali Mohammed,Catherine Fonder,Ying Wei,Wallapak Tavanapong,Donald S Sakaguchi,Qi Li,Surya K. Mallapragada*

Main category: cs.CV

TL;DR: 提出了一个包含3023张图像、超过43万个细胞标注的大规模细胞计数数据集，并评估了多种计数方法，其中基于SAM的SAM-Counter方法在MAE指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 细胞计数在生物医学研究和临床应用中至关重要，但手动计数耗时且易出错。现有数据集规模有限（通常少于500张图像），难以训练可靠的深度学习模型。

Method: 构建了大规模免疫细胞化学实验数据集，包含3023张图像和43万+手动标注的细胞位置。评估了回归方法、人群计数方法和细胞计数方法三类技术，并基于Segment Anything Model (SAM)开发了SAM-Counter方法。

Result: 在细胞数量从10到2126个/图像范围的测试集上，SAM-Counter的MAE为22.12，优于现有最佳方法的27.46。

Conclusion: 该数据集和基准测试框架对推动自动细胞计数进展具有重要价值，为未来研究提供了坚实基础。

Abstract: Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.

</details>


### [383] [Growing with the Generator: Self-paced GRPO for Video Generation](https://arxiv.org/abs/2511.19356)
*Rui Li,Yuanzhi Liang,Ziqi Ni,Haibing Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Self-Paced GRPO是一种自适应奖励机制的强化学习方法，通过让奖励模型与生成器协同进化，解决传统GRPO中静态奖励模型导致的分布偏差和奖励饱和问题。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法依赖静态奖励模型，其评估行为在训练期间被冻结，导致分布偏差、奖励快速饱和，限制了强化学习对齐的稳定性和有效性。

Method: 提出自我调节的GRPO框架，引入渐进式奖励机制，随着生成质量提高自动将奖励重点从粗粒度视觉保真度转向时间连贯性和细粒度文本-视频语义对齐。

Result: 在VBench基准测试中，使用多种视频生成骨干网络进行实验，相比静态奖励的GRPO基线，在视觉质量和语义对齐方面均获得一致提升。

Conclusion: Self-Paced GRPO通过奖励-策略协同进化缓解了奖励-策略不匹配问题，减轻了奖励利用，实现了更稳定的优化，验证了该方法的有效性和通用性。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.

</details>


### [384] [DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation](https://arxiv.org/abs/2511.19365)
*Zehong Ma,Longhui Wei,Shuai Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: 提出频率解耦的像素扩散框架DeCo，通过分离高频细节和低频语义的生成，提升像素扩散模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有像素扩散模型在单一扩散变换器中同时建模高频信号和低频语义，导致训练和推理速度慢。需要更高效的像素扩散范式。

Method: 使用轻量级像素解码器生成高频细节，条件于DiT提供的语义指导，让DiT专注于低频语义建模。引入频率感知流匹配损失，强调视觉显著频率。

Result: 在ImageNet上达到FID 1.62(256×256)和2.22(512×512)，在像素扩散模型中表现最优，缩小了与潜在扩散方法的差距。文本到图像模型在GenEval上获得0.86的领先分数。

Conclusion: DeCo框架通过频率解耦实现了高效的像素扩散，在保持高模型容量的同时提升了性能，代码已开源。

Abstract: Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.

</details>


### [385] [An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification](https://arxiv.org/abs/2511.19367)
*Saniah Kayenat Chowdhury,Rusab Sarmun,Muhammad E. H. Chowdhury,Sohaib Bassam Zoghoul,Israa Al-Hashimi,Adam Mushtak,Amith Khandakar*

Main category: cs.CV

TL;DR: 提出一种基于医学原理的混合管道方法，通过显式测量肿瘤大小和距离属性来进行肺癌分期，而不是将其视为纯图像分类任务。该方法在Lung-PET-CT-Dx数据集上达到91.36%的总体分类准确率。


<details>
  <summary>Details</summary>
Motivation: 端到端的深度学习方法往往忽视肿瘤-淋巴结-转移系统所依赖的空间和解剖信息，而肿瘤分期依赖于多个定量标准，包括肿瘤大小及其与最近解剖结构的距离，微小变化都可能改变分期结果。

Method: 使用专门的编码器-解码器网络精确分割肺部和邻近解剖结构（包括肺叶、肿瘤、纵隔和膈肌），然后通过分割掩模的定量分析提取必要的肿瘤属性（测量最大肿瘤尺寸和计算肿瘤与邻近解剖结构的距离），最后应用基于规则的肿瘤分期方法。

Result: 在Lung-PET-CT-Dx数据集上表现优于传统深度学习模型，总体分类准确率达到91.36%，各阶段F1分数分别为：T1 0.93、T2 0.89、T3 0.96、T4 0.90。

Conclusion: 这是首个将显式临床背景嵌入肿瘤分期分类的研究，与标准卷积神经网络的黑盒操作不同，该方法既提供了最先进的性能，又提供了透明的决策支持。

Abstract: Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable "black box" manner, our method offers both state-of-the-art performance and transparent decision support.

</details>


### [386] [UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval](https://arxiv.org/abs/2511.19380)
*Maroun Ayli,Youssef Bakouny,Tushar Sharma,Nader Jalloul,Hani Seifeddine,Rima Kilany*

Main category: cs.CV

TL;DR: 提出了一种基于图结构的UI表示方法，将UI截图转换为编码层次关系和空间布局的属性图，通过对比图自编码器学习多级相似性嵌入，在金融软件UI数据集上实现了高精度和低延迟的搜索。


<details>
  <summary>Details</summary>
Motivation: 企业软件拥有数千个UI界面，现有方法依赖视觉相似性或文本语义，缺乏对UI组合结构属性的显式建模，导致设计一致性、模式发现和合规检查面临挑战。

Method: 使用图基表示将UI截图转换为属性图，编码层次关系和空间布局；采用对比图自编码器学习保留视觉、结构和语义多级相似性的嵌入表示；构建UISearch多模态搜索框架，结合结构嵌入和语义搜索。

Result: 在20,396个金融软件UI上，UISearch达到0.92的Top-5准确率，中位延迟47.5ms（P95：124ms），可扩展到20,000+个界面；结构嵌入比最先进的视觉编码器具有更好的区分能力。

Conclusion: 该方法代表了UI表示表达能力的基本进步，混合索引架构支持复杂查询和细粒度UI区分，这是纯视觉方法无法实现的。

Abstract: Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.

</details>


### [387] [BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation](https://arxiv.org/abs/2511.19394)
*Rachit Saluja,Asli Cihangir,Ruining Deng,Johannes C. Paetzold,Fengbei Liu,Mert R. Sabuncu*

Main category: cs.CV

TL;DR: BackSplit：通过细粒度背景标签提升小病灶分割性能的新范式，将背景细分为多个解剖结构类别，不增加推理成本但显著改善分割效果


<details>
  <summary>Details</summary>
Motivation: 传统病灶分割将所有非病灶像素归为单一"背景"类，忽略了丰富的解剖背景信息。背景实际上是高度异质的，包含各种组织、器官等结构

Method: BackSplit方法：将背景类细分为多个细粒度标签，使用手动标注或预训练分割模型自动生成的辅助标签进行训练

Result: 在多个数据集和架构上的实验表明，BackSplit能持续提升小病灶分割性能，即使使用自动生成的辅助标签也有效

Conclusion: BackSplit是一种简单而强大的范式，通过细粒度背景标签显著提升小病灶分割性能，具有鲁棒性、简单性和广泛适用性

Abstract: Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single "background" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.
  In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.

</details>


### [388] [In-Video Instructions: Visual Signals as Generative Control](https://arxiv.org/abs/2511.19401)
*Gongfan Fang,Xinyin Ma,Xinchao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为"视频内指令"的新范式，通过将用户指导直接嵌入视频帧中的视觉元素（如文字、箭头、轨迹）来实现可控的图像到视频生成，相比基于文本提示的方法能提供更明确、空间感知的指令对应关系。


<details>
  <summary>Details</summary>
Motivation: 利用大规模视频生成模型强大的视觉能力，探索如何将其用于可控的图像到视频生成，通过将视觉信号解释为指令来提供比文本提示更精确的空间控制。

Method: 提出In-Video Instruction范式，在视频帧中嵌入视觉指令元素（文字、箭头、轨迹等），让视频模型能够直接解释这些视觉信号并执行相应的动作生成。

Result: 在三个最先进的生成器（Veo 3.1、Kling 2.5、Wan 2.2）上的广泛实验表明，视频模型能够可靠地解释和执行这种视觉嵌入指令，特别是在复杂的多对象场景中表现突出。

Conclusion: 视频生成模型具备理解和执行视觉嵌入指令的能力，In-Video Instruction范式为可控图像到视频生成提供了一种更精确、空间感知的替代方案。

Abstract: Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.

</details>


### [389] [Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens](https://arxiv.org/abs/2511.19418)
*Yiming Qin,Bomin Wei,Jiaxin Ge,Konstantinos Kallidromitis,Stephanie Fu,Trevor Darrell,Xudong Wang*

Main category: cs.CV

TL;DR: 提出了Chain-of-Visual-Thought (COVT)框架，使视觉语言模型能够通过连续视觉令牌进行推理，解决现有模型在密集视觉感知方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在语言空间推理表现出色，但在需要密集视觉感知的任务（如空间推理和几何意识）上表现不佳，因为缺乏捕捉跨空间维度密集视觉信息的机制。

Method: COVT框架让VLMs通过连续视觉令牌（紧凑的潜在表示）进行推理，在约20个令牌的预算内从轻量级视觉专家中提取知识，捕捉2D外观、3D几何、空间布局和边缘结构等互补属性。训练时模型自回归预测这些视觉令牌来重建密集监督信号；推理时直接在连续视觉令牌空间进行推理。

Result: 在超过十个不同的感知基准测试中，将COVT集成到Qwen2.5-VL和LLaVA等强VLMs中，性能持续提升3%到16%，证明紧凑的连续视觉思维能够实现更精确、更基础和更可解释的多模态智能。

Conclusion: COVT框架通过引入连续视觉令牌推理机制，显著提升了视觉语言模型在密集视觉感知任务上的性能，为多模态智能提供了更精确、基础和可解释的推理能力。

Abstract: Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.

</details>


### [390] [SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation](https://arxiv.org/abs/2511.19425)
*Tianrun Chen,Runlong Cao,Xinda Yu,Lanyun Zhu,Chaotao Ding,Deyi Ji,Cheng Chen,Qi Zhu,Chunyan Xu,Papa Mao,Ying Zang*

Main category: cs.CV

TL;DR: SAM3-Adapter是首个针对SAM3设计的适配器框架，通过减少计算开销并提升分割精度，在医学影像、伪装物体分割和阴影检测等多个下游任务中取得新的最先进结果。


<details>
  <summary>Details</summary>
Motivation: 解决SAM及其后续版本在细粒度、低层次分割任务（如伪装物体检测、医学图像分割、细胞图像分割和阴影检测）中的局限性。

Method: 基于原始SAM-Adapter的模块化和可组合设计理念，构建专门针对SAM3的适配器框架，解锁其全部分割能力。

Result: 在多个下游任务中一致超越SAM和SAM2解决方案，建立了新的最先进结果，提供更强的泛化性、更丰富的任务适应性和显著改进的分割精度。

Conclusion: SAM3-Adapter可以作为未来研究和实际分割应用的基础，集成SAM3与适配器相比所有先前的SAM适配方法具有更高的准确性、鲁棒性和效率。

Abstract: The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.

</details>


### [391] [Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction](https://arxiv.org/abs/2511.19426)
*Yun Zhou,Yaoting Wang,Guangquan Jie,Jinyu Liu,Henghui Ding*

Main category: cs.CV

TL;DR: Ref-SAM3D是SAM3D的扩展，通过引入文本描述作为高级先验，实现从单张RGB图像的文本引导3D重建，解决了SAM3D无法根据文本描述重建特定对象的问题。


<details>
  <summary>Details</summary>
Motivation: SAM3D虽然具有强大的3D重建能力，但无法根据文本描述重建特定对象，这在3D编辑、游戏开发和虚拟环境等实际应用中至关重要。

Method: 通过将文本描述作为高级先验整合到SAM3D中，实现仅基于自然语言和单张2D视图的文本引导3D重建。

Result: Ref-SAM3D在零样本重建任务中表现出竞争力和高保真度，有效弥合了2D视觉线索与3D几何理解之间的差距。

Conclusion: Ref-SAM3D为参考引导的3D重建提供了一个更灵活和易于使用的范式，代码已开源。

Abstract: SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.

</details>


### [392] [Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution](https://arxiv.org/abs/2511.19430)
*Dingkang Liang,Cheng Zhang,Xiaopeng Xu,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: ORS3D是一个新的任务调度基准，结合了语言理解、3D空间定位和效率优化，要求智能体利用并行子任务最小化总完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在任务规划中忽略了运筹学知识和3D空间定位，无法满足真实世界任务调度的需求。

Method: 构建了ORS3D-60K数据集，包含6万个复合任务和4千个真实场景；提出了GRANT模型，采用调度令牌机制生成高效的任务调度和定位动作。

Result: 在ORS3D-60K上的广泛实验验证了GRANT在语言理解、3D定位和调度效率方面的有效性。

Conclusion: ORS3D任务和GRANT模型为具身AI中的任务调度提供了新的研究基准和有效解决方案。

Abstract: Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT

</details>


### [393] [Cloud4D](https://arxiv.org/abs/2511.19431)
*Jacob Lin,Edward Gryspeerdt,Ronald Clark*

Main category: cs.CV

TL;DR: Cloud4D是首个基于学习的框架，仅使用同步地面相机重建物理一致的4D云状态，提供25米空间分辨率和5秒时间分辨率的3D液态水含量分布，并估计水平风向量。


<details>
  <summary>Details</summary>
Motivation: 当前全球数值天气预报和气候模型大多在公里尺度运行，难以模拟单个云层和极端降水、阵风、湍流、地表辐照度等因素，需要更高分辨率的模型，而现有仪器难以获得高分辨率真实观测数据。

Method: 使用同步地面相机，通过同形引导的2D到3D变换器推断完整的3D液态水含量分布，并通过追踪3D液态水含量反演随时间变化来估计水平风向量。

Result: 在包含六个向上拍摄相机的两个月部署中，系统相对于最先进的卫星测量提供了数量级的时空分辨率改进，同时相对于并置雷达测量保持个位数相对误差（<10%）。

Conclusion: Cloud4D框架能够仅使用地面相机实现高分辨率的4D云状态重建，为改进天气和气候模型提供了新的观测能力。

Abstract: There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.

</details>


### [394] [Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts](https://arxiv.org/abs/2511.19434)
*Yasin Esfandiari,Stefan Bauer,Sebastian U. Stich,Andrea Dittadi*

Main category: cs.CV

TL;DR: 提出一种简单即插即用的采样方法，通过在高噪声水平使用图像质量专家塑造全局结构，在低噪声水平切换至似然专家优化像素统计，无需重新训练即可打破扩散模型中似然与质量的权衡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中存在感知样本质量与数据似然之间的权衡：强调高噪声去噪步骤的训练目标能产生真实图像但似然较差，而似然导向的训练会过度加权低噪声步骤并损害视觉保真度。

Method: 结合两个预训练扩散专家，在去噪轨迹中切换使用：高噪声水平应用图像质量专家塑造全局结构，低噪声水平切换至似然专家细化像素统计，仅需选择中间切换步骤而无需重新训练。

Result: 在CIFAR-10和ImageNet32上，合并模型始终匹配或优于其基础组件，相对于每个单独专家都改善或保持了似然和样本质量。

Conclusion: 在噪声水平间进行专家切换是打破图像扩散模型中似然-质量权衡的有效方法。

Abstract: Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.

</details>


### [395] [Are Image-to-Video Models Good Zero-Shot Image Editors?](https://arxiv.org/abs/2511.19435)
*Zechuan Zhang,Zhenyuan Chen,Zongxin Yang,Yi Yang*

Main category: cs.CV

TL;DR: IF-Edit是一个无需调优的框架，将预训练的图像到视频扩散模型重新用于指令驱动的图像编辑，解决了提示不对齐、冗余时间潜在变量和模糊后期帧三个关键挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模视频扩散模型展现出强大的世界模拟和时间推理能力，但其作为零样本图像编辑器的应用尚未充分探索。

Method: 包括思维链提示增强模块、时间潜在变量丢弃策略和自一致后精炼步骤，分别解决提示不对齐、冗余潜在变量和模糊帧问题。

Result: 在四个公共基准测试中，IF-Edit在推理中心任务上表现强劲，同时在通用编辑任务上保持竞争力。

Conclusion: 该研究为将视频扩散模型作为图像编辑器提供了系统视角，并展示了统一视频-图像生成推理的简单方法。

Abstract: Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.

</details>


### [396] [VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection](https://arxiv.org/abs/2511.19436)
*Qiang Wang,Xinyuan Gao,SongLin Dong,Jizhou Han,Jiangyang Li,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: VDC-Agent是一个自演进的视频详细描述框架，无需人工标注或大型教师模型。通过闭环的生成-评分-优化流程自动构建训练数据，在VDC基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频详细描述任务中依赖人工标注和大型教师模型的问题，实现无需外部监督的自演进学习。

Method: 构建闭环系统：视频描述生成→基于原则的评分（分数和文本建议）→提示优化。当质量下降时启用自反思路径修正。将轨迹转换为偏好元组，通过课程直接偏好优化进行微调。

Result: 在VDC基准测试中达到49.08%平均准确率和2.50分数，超越专用视频描述器，相比基础模型提升+5.13%准确率和+0.27分数，推理成本相近。

Conclusion: VDC-Agent证明了自演进框架在视频详细描述任务中的有效性，能够自动构建高质量训练数据并显著提升模型性能。

Abstract: We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.

</details>


### [397] [LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context](https://arxiv.org/abs/2511.19437)
*Jingzhi Bao,Hongze Chen,Lingting Zhu,Chenyu Liu,Runze Zhang,Keyang Luo,Zeyu Hu,Weikai Chen,Yingda Yin,Xin Wang,Zehong Lin,Jun Zhang,Xiaoguang Han*

Main category: cs.CV

TL;DR: LumiTex是一个端到端的PBR纹理生成框架，通过多分支生成、光照感知材料注意力和几何引导修复三个关键组件，解决了材料分解和纹理补全的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有PBR纹理生成方法未能解决两个基本挑战：1）在有限光照线索下从图像提示进行材料分解，2）无缝且视角一致的纹理补全。

Method: 包含三个关键组件：1）多分支生成方案，在共享光照先验下解耦反照率和金属粗糙度；2）光照感知材料注意力机制，将光照上下文注入解码过程；3）基于大视角合成模型的几何引导修复模块。

Result: 大量实验表明，LumiTex在纹理质量方面达到了最先进的性能，超越了现有的开源和商业方法。

Conclusion: LumiTex通过创新的多组件设计，成功解决了PBR纹理生成中的材料分解和纹理补全问题，实现了高质量的纹理生成。

Abstract: Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [398] [Pier: Efficient Large Language Model pretraining with Relaxed Global Communication](https://arxiv.org/abs/2511.17849)
*Shuyuan Fan,Zhao Zhang*

Main category: cs.DC

TL;DR: Pier是一个高效的分布式优化器，通过减少全局通信来加速大语言模型预训练，在GPT模型系列上实现了2.7x-3.7x的加速，且不损失模型性能。


<details>
  <summary>Details</summary>
Motivation: 全局通信（如all-reduce和allgather）是大语言模型预训练的主要性能瓶颈，需要设计更高效的优化器来减少通信开销。

Method: 基于DiLoCo框架，在处理器组内使用内部优化器，全局通信使用外部优化器，并引入动量预热和动量衰减技术来保持收敛性。

Result: 在256个A100 GPU上，GPT-2 XL训练加速2.7x-3.7x；在64个GH200 Superchips上加速1.2x-1.9x；在128个A100上，GPT-2 7B训练时间减少54.5%，且验证损失和下游任务性能无下降。

Conclusion: Pier通过减少全局通信有效加速了大语言模型预训练，在保持模型性能的同时显著提升了训练效率。

Abstract: Global communication, such as all-reduce and allgather, is the prominent performance bottleneck in large language model (LLM) pretraining. To address this issue, we present Pier, an efficient and scalable optimizer with relaxed global communication. Pier is built upon DiLoCo, which leverages an inner optimizer within groups of processors and an outer optimizer that requires global communication. To preserve the convergence and model performance, Pier incorporates two key techniques for the outer optimizer: momentum warmup and momentum decay. Pier employs an efficient and scalable system architecture to enable complex parallelization strategies in LLM pretraining. We examine the model performance and runtime reduction of Pier using the GPT model family (e.g., small, medium, XL, and 7B) and the OpenWebText dataset with a suite of thirteen downstream tasks. With data parallel strategy, Pier speeds up GPT-2 XL training by up to 2.7x-3.7x on 256 NVIDIA A100 GPUs and 1.2x-1.9x on 64 GH200 Superchips, respectively, without degradation of validation loss or downstream task performance. With data parallel and tensor parallel, Pier reduces the time cost GPT-2 7B model training by 54.5% on 128 A100s.

</details>


### [399] [SAGkit: A Python SAG Toolkit for Response Time Analysis of Hybrid-Triggered Jobs](https://arxiv.org/abs/2511.17882)
*Ruide Cao,Zhuyun Qi,Qinyang He,Chenxi Ling,Yi Wang,Guoming Tang*

Main category: cs.DC

TL;DR: SAGkit是一个Python工具包，实现了调度抽象图(SAG)框架，用于分布式控制系统的精确可持续响应时间分析，特别解决了非抢占式系统中状态空间爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现代延迟关键应用对实时性和鲁棒性要求越来越高，传统响应时间分析方法在处理非抢占式系统、释放抖动和执行时间变化时面临状态空间爆炸问题。

Method: 开发了SAGkit工具包，基于调度抽象图框架，通过在SAG基础上允许作业缺失，实现了混合触发作业的精确可持续响应时间分析。

Result: 实验表明SAGkit在可接受的运行时和内存开销下实现了精确性，能够有效分析复杂分布式控制系统。

Conclusion: SAGkit作为一个轻量级开源工具包，为研究人员分析复杂分布式控制系统提供了有力支持，并具有进一步开发的潜力。

Abstract: For distributed control systems, modern latency-critical applications are increasingly demanding real-time guarantees and robustness. Response-time analysis (RTA) is useful for this purpose, as it helps analyze and guarantee timing bounds. However, conventional RTA methods struggle with the state-space explosion problem, especially in non-preemptive systems with release jitter and execution time variations. In this paper, we introduce SAGkit, a Python toolkit that implements the schedule-abstraction graph (SAG) framework. SAGkit novelly enables exact and sustainable RTA of hybrid-triggered jobs by allowing job absence on the SAG basis. Our experiments demonstrate that SAGkit achieves exactness with acceptable runtime and memory overhead. This lightweight toolkit empowers researchers to analyze complex distributed control systems and is open-access for further development.

</details>


### [400] [MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale](https://arxiv.org/abs/2511.18124)
*Sangam Ghimire,Nigam Niraula,Nirjal Bhurtel,Paribartan Timalsina,Bishal Neupane,James Bhattarai,Sudan Jha*

Main category: cs.DC

TL;DR: MIDAS是一个自适应中间件层，通过命名空间感知负载均衡、协作缓存层和自稳定控制循环来解决元数据热点问题，显著降低队列长度和热点影响。


<details>
  <summary>Details</summary>
Motivation: 元数据热点是高性能计算和云存储环境中可扩展I/O的主要障碍，会导致长队列、尾部延迟增加和系统吞吐量降低。现有解决方案过于僵化、部署侵入性强或在动态工作负载下不稳定。

Method: MIDAS采用三种机制：命名空间感知负载均衡器（增强一致性哈希）、协作缓存层（通过租约、失效或自适应超时保持后端语义）、自稳定控制循环（动态调整路由攻击性和缓存生命周期）。

Result: 与轮询调度相比，MIDAS平均队列长度减少约23%，最坏情况热点缓解达80%，提高了突发场景下的可扩展性、尾部延迟预测性和整体系统性能。

Conclusion: 基于中间件的稳定性感知策略可以为元数据管理提供后端无关的改进，在突发场景中实现更好的可扩展性、更可预测的尾部延迟和更强的整体系统性能。

Abstract: Metadata hotspots remain one of the key obstacles to scalable Input/Output (I/O) in both High-Performance Computing (HPC) and cloud-scale storage environments. Situations such as job start-ups, checkpoint storms, or heavily skewed namespace access can trigger thousands of concurrent metadata requests against a small subset of servers. The result is long queues, inflated tail latencies, and reduced system throughput. Prior efforts including static namespace partitioning, backend-specific extensions, and kernel-level modifications address parts of the problem, but they often prove too rigid, intrusive to deploy, or unstable under shifting workloads. We present MIDAS, an adaptive middleware layer that operates transparently between clients and metadata servers, requiring no changes to kernels or storage backends. The design brings together three mechanisms: (i) a namespace-aware load balancer that enhances consistent hashing with power-of-d sampling informed by live telemetry, (ii) a cooperative caching layer that preserves backend semantics through leases, invalidations, or adaptive timeouts, and (iii) a self-stabilizing control loop that dynamically adjusts routing aggressiveness and cache lifetimes while avoiding oscillations under bursty workloads. Analysis of the model and controlled experiments show that MIDAS reduces average queue lengths by roughly 23% and mitigates worst-case hotspots by up to 80% when compared to round-robin scheduling. These findings highlight that a stability-aware, middleware-based strategy can provide backend-agnostic improvements to metadata management, enabling better scalability in bursty scenarios, more predictable tail latencies, and stronger overall system performance.

</details>


### [401] [Simulating Dynamic Cloud Marketspaces: Modeling Spot Instance Behavior and Scheduling with CloudSim Plus](https://arxiv.org/abs/2511.18137)
*Christoph Goldgruber,Benedikt Pittl,Erich Schikuta*

Main category: cs.DC

TL;DR: 扩展CloudSim Plus模拟框架以支持动态云定价环境，并评估HLEM-VMP算法在波动工作负载下的性能表现


<details>
  <summary>Details</summary>
Motivation: 公共云环境中动态定价模型（如竞价实例）的日益普及给工作负载调度和可靠性带来了新挑战，现有分配算法和模拟工具未能充分处理这些模型的波动性和不确定性

Method: 扩展CloudSim Plus模拟框架以支持真实的竞价实例生命周期管理，包括中断、终止、休眠和重新分配，并使用Google Cluster Trace数据集进行大规模模拟验证

Result: HLEM-VMP算法在动态竞价市场条件下相比基线策略减少了竞价实例中断次数和最大中断持续时间

Conclusion: 该工作提供了模拟动态云行为的框架，并为虚拟机分配性能和市场风险提供了分析见解，有助于实现更稳健和成本效益的云计算资源管理

Abstract: The increasing reliance on dynamic pricing models, such as spot instances, in public cloud environments presents new challenges for workload scheduling and reliability. While these models offer cost advantages, they introduce volatility and uncertainty that are not fully addressed by current allocation algorithms or simulation tools. This work contributes to the modeling and evaluation of such environments by extending the CloudSim Plus simulation framework to support realistic spot instance lifecycle management, including interruption, termination, hibernation, and reallocation. The enhanced simulator is validated using synthetic scenarios and large-scale simulations based on the Google Cluster Trace dataset. Building on this foundation, the HLEM-VMP allocation algorithm, originally proposed in earlier research, was adapted to operate under dynamic spot market conditions. Its performance was evaluated against baseline allocation strategies to assess its efficiency and resilience in volatile workload environments. The comparison demonstrated a reduction in the number of spot instance interruptions as well as a decrease in the maximum interruption duration. Overall, this work provides both a simulation framework for simulating dynamic cloud behavior and analytical insights into virtual machine allocation performance and market risk, contributing to more robust and cost-effective resource management in cloud computing.

</details>


### [402] [AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems](https://arxiv.org/abs/2511.18151)
*Rajat Bhattacharjya,Sing-Yao Wu,Hyunwoo Oh,Chaewon Nam,Suyeon Koo,Mohsen Imani,Elaheh Bozorgzadeh,Nikil Dutt*

Main category: cs.DC

TL;DR: AVERY是一个自适应分割计算框架，通过认知启发的双流分割方法，将视觉语言模型分为上下文流和洞察流，实现在灾难响应等低带宽环境下资源受限平台上的实时可查询智能分析。


<details>
  <summary>Details</summary>
Motivation: 无人机在灾难响应中需要复杂的可查询智能分析，但传统CNN无法提供语义推理，而视觉语言模型资源需求过高难以在设备上部署，云端卸载在低带宽灾难区域网络下效果不佳。

Method: 提出功能性的认知启发双流分割方法：高频低分辨率的上下文流用于实时感知，低频高保真的洞察流用于深度分析。轻量级自感知控制器根据网络条件和操作意图动态选择预训练压缩模型。

Result: 在边缘-云场景下使用LISA-7B VLM进行测试，AVERY始终优于静态配置，相比原始图像压缩准确率提高11.2%，相比全边缘执行能耗降低93.98%。

Conclusion: AVERY框架显著提升了任务效率，在动态环境中为资源受限平台实现了实时可查询智能分析能力。

Abstract: Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution "context stream" for real-time awareness and a low-frequency, high-fidelity "insight stream" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.

</details>


### [403] [Monotone Decontamination of Arbitrary Dynamic Graphs with Mobile Agents](https://arxiv.org/abs/2511.18315)
*Rajashree Bar,Daibik Barik,Adri Bhattacharya,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 研究动态图中的单调去污问题，提出了两种动态性模型，给出了所需智能体数量的上下界。


<details>
  <summary>Details</summary>
Motivation: 网络去污问题在静态图中已有研究，但在动态图中尚未探索。本文旨在研究任意动态图中的单调去污问题，优化所需智能体数量。

Method: 设计了两种基于边消失后重新出现时间的动态性模型，为每个模型提出了所需智能体数量的下界和上界。

Result: 在两种动态性模型中，分别给出了单调去污所需智能体数量的理论界限，揭示了边突然消失或重新出现带来的困难。

Conclusion: 本文首次研究了动态图中的单调去污问题，通过两种动态模型的分析，为优化所需智能体数量提供了理论依据。

Abstract: Network decontamination is a well-known problem, in which the aim of the mobile agents should be to decontaminate the network (i.e., both nodes and edges). This problem comes with an added constraint, i.e., of \emph{monotonicity}, in which whenever a node or an edge is decontaminated, it must not get recontaminated. Hence, the name comes \emph{monotone decontamination}. This problem has been relatively explored in static graphs, but nothing is known yet in dynamic graphs. We, in this paper, study the \emph{monotone decontamination} problem in arbitrary dynamic graphs. We designed two models of dynamicity, based on the time within which a disappeared edge must reappear. In each of these two models, we proposed lower bounds as well as upper bounds on the number of agents, required to fully decontaminate the underlying dynamic graph, monotonically. Our results also highlight the difficulties faced due to the sudden disappearance or reappearance of edges. Our aim in this paper has been to primarily optimize the number of agents required to solve monotone decontamination in these dynamic networks.

</details>


### [404] [An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds](https://arxiv.org/abs/2511.18906)
*Marco Zambianco,Lorenzo Fasol,Roberto Doriguzzi-Corin*

Main category: cs.DC

TL;DR: 提出了一种用于MIG GPU云的新型调度框架，通过最小化碎片化来最大化工作负载接受率，在重负载条件下平均提高10%的工作负载调度数量。


<details>
  <summary>Details</summary>
Motivation: MIG GPU的固定分区导致严重的GPU碎片化问题，使得GPU资源利用率低下，限制了可容纳的工作负载数量。

Method: 引入碎片化度量指标来量化资源效率，并基于此设计贪心调度算法，为每个进入的工作负载选择最小化碎片化增长的GPU和MIG切片。

Result: 在多样化工作负载分布下，该方法相比基准策略始终实现更高的工作负载接受率，在重负载条件下平均提高10%的调度工作负载数量，同时使用大致相同数量的GPU。

Conclusion: 所提出的调度框架能有效缓解MIG GPU云中的碎片化问题，显著提高资源利用效率和工作负载调度能力。

Abstract: The explosive growth of AI applications has created unprecedented demand for GPU resources. Cloud providers meet this demand through GPU-as-a-Service platforms that offer rentable GPU resources for running AI workloads. In this context, the sharing of GPU resources between different tenants is essential to maximize the number of scheduled workloads. Among the various GPU sharing technologies, NVIDIA's Multi-Instance GPU (MIG) stands out by partitioning GPUs at hardware level into isolated slices with dedicated compute and memory, ensuring strong tenant isolation, preventing resource contention, and enhancing security. Despite these advantages, MIG's fixed partitioning introduces scheduling rigidity, leading to severe GPU fragmentation in multi-tenant environments, where workloads are continuously deployed and terminated. Fragmentation leaves GPUs underutilized, limiting the number of workloads that can be accommodated. To overcome this challenge, we propose a novel scheduling framework for MIG-based clouds that maximizes workload acceptance while mitigating fragmentation in an online, workload-agnostic setting. We introduce a fragmentation metric to quantify resource inefficiency and guide allocation decisions. Building on this metric, our greedy scheduling algorithm selects GPUs and MIG slices that minimize fragmentation growth for each incoming workload. We evaluate our approach against multiple baseline strategies under diverse workload distributions. Results demonstrate that our method consistently achieves higher workload acceptance rates, leading to an average 10% increase in the number of scheduled workloads in heavy load conditions, while using approximately the same number of GPUs as the benchmark methods.

</details>


### [405] [AME: An Efficient Heterogeneous Agentic Memory Engine for Smartphones](https://arxiv.org/abs/2511.19192)
*Xinkui Zhao,Qingyu Ma,Yifan Zhang,Hengxuan Lou,Guanjie Cheng,Shuiguang Deng,Jianwei Yin*

Main category: cs.DC

TL;DR: AME是一个专为智能手机SoC设计的设备端智能体内存引擎，解决了现有向量数据库在移动设备上的性能瓶颈，通过硬件感知的矩阵流水线和智能调度机制，显著提升了查询、索引构建和插入操作的性能。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库主要针对服务器环境设计，直接移植到智能手机上存在两大问题：(1)移动SoC的计算资源限制与向量数据库假设不匹配；(2)设备端使用模式需要频繁的插入、删除和索引维护操作，与服务器工作负载不同。

Method: AME采用两种关键技术：1)硬件感知的高效矩阵流水线，最大化计算单元利用率并利用多级片上存储维持高吞吐量；2)硬件和工作负载感知的调度方案，协调查询、插入和索引重建以最小化延迟。

Result: 在Snapdragon 8系列SoC上的实验表明，AME在保持相同召回率的情况下将查询吞吐量提升1.4倍，索引构建速度提升7倍，在并发查询工作负载下插入吞吐量提升6倍。

Conclusion: AME成功解决了移动设备上向量数据库的性能瓶颈，为设备端智能体提供了高效、隐私保护的内存管理解决方案。

Abstract: On-device agents on smartphones increasingly require continuously evolving memory to support personalized, context-aware, and long-term behaviors. To meet both privacy and responsiveness demands, user data is embedded as vectors and stored in a vector database for fast similarity search. However, most existing vector databases target server-class environments. When ported directly to smartphones, two gaps emerge: (G1) a mismatch between mobile SoC constraints and vector-database assumptions, including tight bandwidth budgets, limited on-chip memory, and stricter data type and layout constraints; and (G2) a workload mismatch, because on-device usage resembles a continuously learning memory, in which queries must coexist with frequent inserts, deletions, and ongoing index maintenance. To address these challenges, we propose AME, an on-device Agentic Memory Engine co-designed with modern smartphone SoCs. AME introduces two key techniques: (1) a hardware-aware, high-efficiency matrix pipeline that maximizes compute-unit utilization and exploits multi-level on-chip storage to sustain high throughput; and (2) a hardware- and workload-aware scheduling scheme that coordinates querying, insertion, and index rebuilding to minimize latency. We implement AME on Snapdragon 8-series SoCs and evaluate it on HotpotQA. In our experiments, AME improves query throughput by up to 1.4x at matched recall, achieves up to 7x faster index construction, and delivers up to 6x higher insertion throughput under concurrent query workloads.

</details>


### [406] [Constant-Size Certificates for Leader Election in Chordal Graphs and Related Classes](https://arxiv.org/abs/2511.19208)
*Jérémie Chalopin,Maria Kokkou*

Main category: cs.DC

TL;DR: 提出了针对弦图和K4-free可拆解图的领导者选举，以及可拆解图的生成树构造的常数大小局部认证方案，并给出了将认证方案转换为自稳定算法的自动转换方法。


<details>
  <summary>Details</summary>
Motivation: 在分布式计算中，需要高效的认证方案来验证问题解决方案的正确性。本文关注领导者选举和生成树构造这两个基本问题，旨在为特定图类提供常数大小的局部认证方案。

Method: 为弦图和K4-free可拆解图设计了领导者选举的常数大小局部认证方案，为可拆解图设计了生成树构造的认证方案。还提出了将认证方案转换为自稳定算法的自动转换方法。

Result: 成功构建了针对特定图类的常数大小局部认证方案，其中弦图的领导者选举方案还能确保无环定向。转换方法只需在认证方案状态集中添加一个额外状态即可实现自稳定。

Conclusion: 这些是首个针对这些图类的局部认证结果，可能揭示了验证其他问题有用的结构特性。认证方案到自稳定算法的转换方法具有独立的研究价值。

Abstract: In distributed computing a certification scheme consists of a set of states and conditions over those states that enable each node of a graph to efficiently verify the correctness of a solution to a given problem. This work focuses on two fundamental problems: leader election and spanning tree construction. For each problem, we present a constant-size (per edge), local certification scheme, where the conditions available to each node can only refer to the graph induced by its one-hop neighborhood. In particular, we provide certification schemes for leader election in chordal and $K_4$-free dismantlable graphs and for spanning tree construction in dismantlable graphs, assuming a root is given. For chordal graphs, our leader election certification scheme additionally ensures an acyclic orientation, a property that is not generally verifiable using constant-size certificates in arbitrary graphs. To the best of our knowledge, these are the first local certification results tailored to these graph classes, potentially highlighting structural properties useful for verifying additional problems. Finally, we propose an algorithm that automatically transforms any certification scheme into a silent self-stabilizing algorithm (i.e., an algorithm that automatically recovers from faults) by adding only one extra state to the set of states of the certification scheme, assuming a Gouda fair scheduler. This transformation may be of independent interest.

</details>


### [407] [IOMMU Support for Virtual-Address Remote DMA in an ARMv8 environment](https://arxiv.org/abs/2511.19258)
*Antonis Psistakis*

Main category: cs.DC

TL;DR: 该论文成功测试并验证了ARM的SMMU（IOMMU）在Xilinx Zynq UltraScale+ MPSoC平台上的功能，通过开发自定义内核模块实现了虚拟地址到物理地址的转换，支持了Unimem系统的全局地址空间虚拟化方法。


<details>
  <summary>Details</summary>
Motivation: 在具有多个计算节点的复杂系统中，维持节点间高效正确的内存一致性是一个关键挑战。Unimem系统通过虚拟化全局地址空间来解决这个问题，需要依赖每个节点的IOMMU。由于Linux对SMMU的文档有限且不清晰，需要测试和使用单个节点的IOMMU功能来支持这种方法。

Method: 开发自定义内核模块来测试ARM SMMU功能：1）在Processing System中插入虚拟到物理地址映射并触发DMA传输；2）从Programmable Logic发起DMA事务验证SMMU翻译；3）开发无需预映射的模块，通过配置SMMU使用用户进程页表指针来动态翻译所有相关虚拟地址。

Result: 在所有测试场景中成功证明了SMMU的正确运行：DMA请求通过SMMU进行地址翻译，从PS和PL发起的DMA事务都能被正确翻译，动态地址翻译功能也正常工作。

Conclusion: 成功验证了SMMU在Xilinx Zynq UltraScale+ MPSoC平台上的功能，为Unimem系统的全局地址空间虚拟化方法提供了支持。由于时间限制，高级SMMU功能的进一步探索留待未来工作。

Abstract: In complex systems with many compute nodes containing multiple CPUs that are coherent within each node, a key challenge is maintaining efficient and correct coherence between nodes. The Unimem system addresses this by proposing a virtualized global address space that enables such coherence, relying on the I/O Memory Management Unit (IOMMU) in each node. The goal of this thesis is to support this approach by successfully testing and using the IOMMU of a single node. For this purpose, we used ARM's IOMMU, known as the System Memory Management Unit (SMMU), which translates virtual addresses to physical addresses. Because Linux documentation for the SMMU is limited and unclear, we implemented custom kernel modules to test and use its functionality.
  First, we tested the SMMU in the Processing System (PS) of the Xilinx Zynq UltraScale+ MPSoC by developing a module that inserted virtual-to-physical address mappings into the SMMU. We then triggered a DMA transfer to a virtual address and observed that the request passed through the SMMU for address translation. We repeated this experiment by initiating DMA transactions from the Programmable Logic (PL) and similarly confirmed that the transactions were translated by the SMMU. Finally, we developed a module that enables transactions from the PL without requiring explicit pre-mapping of virtual and physical address pairs. This was achieved by configuring the SMMU with the page table pointer of a user process, allowing it to translate all relevant virtual addresses dynamically.
  Overall, we successfully demonstrated the correct operation of the SMMU across all tested scenarios. Due to time constraints, further exploration of advanced SMMU features is left for future work.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [408] [Optimized Memory Tagging on AmpereOne Processors](https://arxiv.org/abs/2511.17773)
*Shiv Kaushik,Mahesh Madhav,Nagi Aboulenein,Jason Bessette,Sandeep Brahmadathan,Ben Chaffin,Matthew Erler,Stephan Jourdan,Thomas Maciukenas,Ramya Masti,Jon Perry,Massimo Sutera,Scott Tetrick,Bret Toll,David Turley,Carl Worth,Atiq Bajwa*

Main category: cs.AR

TL;DR: AmpereOne处理器是首个支持ARM MTE内存标签扩展的数据中心处理器，通过同步标签检查提供内存安全保护，在广泛的数据中心工作负载中仅产生个位数性能影响，且无内存容量开销。


<details>
  <summary>Details</summary>
Motivation: 解决C/C++等指针语言中的内存安全问题，这些问题是广泛安全攻击的根源。现有编译器扩展和ISA扩展在开销和适用性方面存在限制，阻碍了广泛生产部署。

Method: 采用ARM AArch64 ISA的Memory Tagging Extension(MTE)，在AmpereOne处理器上实现优化的MTE硬件支持，提供同步标签检查功能，无需额外内存容量存储标签。

Result: AmpereOne处理器在广泛的数据中心工作负载中实现同步标签检查，仅产生个位数性能影响，且无内存容量开销。分析显示应用内存管理是主要剩余开销来源，存在明确的软件优化机会。

Conclusion: AmpereOne处理器的高效MTE实现与明确的软件优化路径相结合，使其在生产云环境中部署极具吸引力，为内存安全提供了实用解决方案。

Abstract: Memory-safety escapes continue to form the launching pad for a wide range of security attacks, especially for the substantial base of deployed software that is coded in pointer-based languages such as C/C++. Although compiler and Instruction Set Architecture (ISA) extensions have been introduced to address elements of this issue, the overhead and/or comprehensive applicability have limited broad production deployment. The Memory Tagging Extension (MTE) to the ARM AArch64 Instruction Set Architecture is a valuable tool to address memory-safety escapes; when used in synchronous tag-checking mode, MTE provides deterministic detection and prevention of sequential buffer overflow attacks, and probabilistic detection and prevention of exploits resulting from temporal use-after-free pointer programming bugs. The AmpereOne processor, launched in 2024, is the first datacenter processor to support MTE. Its optimized MTE implementation uniquely incurs no memory capacity overhead for tag storage and provides synchronous tag-checking with single-digit performance impact across a broad range of datacenter class workloads. Furthermore, this paper analyzes the complete hardware-software stack, identifying application memory management as the primary remaining source of overhead and highlighting clear opportunities for software optimization. The combination of an efficient hardware foundation and a clear path for software improvement makes the MTE implementation of the AmpereOne processor highly attractive for deployment in production cloud environments.

</details>


### [409] [Comprehensive Design Space Exploration for Tensorized Neural Network Hardware Accelerators](https://arxiv.org/abs/2511.17971)
*Jinsong Zhang,Minghe Li,Jiayi Tian,Jinming Lu,Zheng Zhang*

Main category: cs.AR

TL;DR: 提出了一种统一的协同探索框架，将张量分解模型的收缩路径、硬件架构和数据流映射联合优化，以最大化边缘设备上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 现有高阶张量分解研究主要关注算法优势，忽视了硬件部署效率，导致张量化模型在真实设备上的延迟和能耗潜力未能充分发挥。

Method: 构建统一的协同探索框架，将收缩路径、硬件架构和数据流映射整合到统一设计空间中，通过面向延迟的搜索目标进行全局探索。

Result: 在可配置FPGA内核上实现优化配置，相比密集基线模型，推理和训练延迟分别降低4倍和3.85倍。

Conclusion: 通过联合优化张量分解模型的算法和硬件维度，可以显著提升边缘设备上的端到端模型效率。

Abstract: High-order tensor decomposition has been widely adopted to obtain compact deep neural networks for edge deployment. However, existing studies focus primarily on its algorithmic advantages such as accuracy and compression ratio-while overlooking the hardware deployment efficiency. Such hardware-unaware designs often obscure the potential latency and energy benefits of tensorized models. Although several works attempt to reduce computational cost by optimizing the contraction sequence based on the number of multiply-accumulate operations, they typically neglect the underlying hardware characteristics, resulting in suboptimal real-world performance. We observe that the contraction path, hardware architecture, and dataflow mapping are tightly coupled and must be optimized jointly within a unified design space to maximize deployment efficiency on real devices. To this end, we propose a co-exploration framework that unifies these dimensions within a unified design space for efficient training and inference of tensorized neural networks on edge platforms. The framework formulates a latency oriented search objective and solves it via a global latency-driven exploration across the unified design space to achieve end-to-end model efficiency. The optimized configurations are implemented on a configurable FPGA kernel, achieving up to 4 and 3.85 lower inference and training latency compared with the dense baseline.

</details>


### [410] [HDDB: Efficient In-Storage SQL Database Search Using Hyperdimensional Computing on Ferroelectric NAND Flash](https://arxiv.org/abs/2511.18234)
*Quanling Zhao,Yanru Chen,Runyang Tian,Sumukh Pinge,Weihong Xu,Augusto Vega,Steven Holmes,Saransh Gupta,Tajana Rosing*

Main category: cs.AR

TL;DR: HDDB结合超维计算与铁电NAND存储器，实现存储内SQL谓词评估和分析，具有高并行性和低数据移动特性，在噪声环境下保持正确性。


<details>
  <summary>Details</summary>
Motivation: 利用HDC的噪声容忍特性与FeNAND存储器的高密度和存储内计算能力，解决传统SQL数据库在大型事实表上能耗高、延迟大的问题。

Method: 提出HDDB软硬件协同设计，结合HDC与FeNAND MLC，开发新的HDC编码技术用于SQL数据表，将谓词过滤和聚合转化为高效的HDC操作。

Result: 在TPC-DS事实表上，HDDB相比传统CPU/GPU SQL数据库引擎，延迟降低80.6倍，能耗降低12,636倍，在10%随机损坏TLC单元下仍能保持正确性。

Conclusion: HDDB为噪声鲁棒、内存中心数据库处理提供了实用基础，显著提升了SQL查询的能效和性能。

Abstract: Hyperdimensional Computing (HDC) encodes information and data into high-dimensional distributed vectors that can be manipulated using simple bitwise operations and similarity searches, offering parallelism, low-precision hardware friendliness, and strong robustness to noise. These properties are a natural fit for SQL database workloads dominated by predicate evaluation and scans, which demand low energy and low latency over large fact tables. Notably, HDC's noise-tolerance maps well onto emerging ferroelectric NAND (FeNAND) memories, which provide ultra-high density and in-storage compute capability but suffer from elevated raw bit-error rates. In this work, we propose HDDB, a hardware-software co-design that combines HDC with FeNAND multi-level cells (MLC) to perform in-storage SQL predicate evaluation and analytics with massive parallelism and minimal data movement. Particularly, we introduce novel HDC encoding techniques for standard SQL data tables and formulate predicate-based filtering and aggregation as highly efficient HDC operations that can happen in-storage. By exploiting the intrinsic redundancy of HDC, HDDB maintains correct predicate and decode outcomes under substantial device noise (up to 10% randomly corrupted TLC cells) without explicit error-correction overheads. Experiments on TPC-DS fact tables show that HDDB achieves up to 80.6x lower latency and 12,636x lower energy consumption compared to conventional CPU/GPU SQL database engines, suggesting that HDDB provides a practical substrate for noise-robust, memory-centric database processing.

</details>


### [411] [Evaluation of NVENC Split-Frame Encoding (SFE) for UHD Video Transcoding](https://arxiv.org/abs/2511.18687)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.AR

TL;DR: NVIDIA Split-Frame Encoding (SFE) 技术通过将单个UHD帧分割到多个NVENC芯片并行编码，显著提升编码吞吐量，在实时应用中几乎翻倍吞吐量且RD性能损失可忽略，使实时8K编码成为可能。


<details>
  <summary>Details</summary>
Motivation: 随着消费设备能够拍摄4K和8K超高清视频，需要高性能视频转码器进行互联网传输。NVIDIA在高端GPU中集成了多个NVENC芯片，SFE技术旨在利用这些硬件资源提升编码效率。

Method: 使用SFE技术将单个UHD帧分割到多个物理NVENC编码器进行并行编码，然后拼接结果。通过标准化测试序列评估其对RD性能、编码吞吐量、功耗和端到端延迟的影响。

Result: SFE在实时应用中几乎使编码吞吐量翻倍，RD性能损失可忽略，使4K编码可使用更高质量预设，并使实时8K编码成为可能。在4K下不增加延迟，在8K下甚至能减少延迟。

Conclusion: SFE是实现高吞吐量实时UHD转码的关键技术，在数据中心和生成式AI应用中具有重要价值，能够有效平衡性能与效率的权衡。

Abstract: NVIDIA Encoder (NVENC) features in modern NVIDIA GPUs, offer significant advantages over software encoders by providing comparable Rate-Distortion (RD) performance while consuming considerably less power. The increasing capability of consumer devices to capture footage in Ultra High-Definition (UHD) at 4K and 8K resolutions necessitates high-performance video transcoders for internet-based delivery. To address this demand, NVIDIA introduced Split-Frame Encoding (SFE), a technique that leverages multiple on-die NVENC chips available in high-end GPUs. SFE splits a single UHD frame for parallel encoding across these physical encoders and subsequently stitches the results, which significantly improves encoding throughput. However, this approach is known to incur an RD performance penalty. The widespread adoption of NVIDIA GPUs in data centers, driven by the rise of Generative AI, means NVENC is poised to play a critical role in transcoding UHD video. To better understand the performance-efficiency tradeoff of SFE, this paper evaluates SFE's impact on RD performance, encoding throughput, power consumption, and end-to-end latency using standardized test sequences. The results show that for real-time applications, SFE nearly doubles encoding throughput with a negligible RD performance penalty, which enables the use of higher-quality presets for 4K and makes real-time 8K encoding feasible, effectively offsetting the minor RD penalty. Moreover, SFE adds no latency at 4K and can reduce it at 8K, positioning it as a key enabler for high-throughput, real-time UHD transcoding.

</details>


### [412] [Evaluation of GPU Video Encoder for Low-Latency Real-Time 4K UHD Encoding](https://arxiv.org/abs/2511.18688)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.AR

TL;DR: 评估NVIDIA、Intel和AMD GPU上的低延迟编码模式，比较硬件编码器与软件编码器的性能，发现硬件编码器在实现显著更低端到端延迟的同时，RD性能略优于软件解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着4K UHD成为实时视频流的新标准，以及对6G时代低延迟需求的增长，需要深入了解GPU硬件编码器的低延迟编码性能。

Method: 从率失真性能和延迟角度评估NVIDIA、Intel和AMD GPU的低延迟编码模式，并与硬件编码器的正常延迟调优和领先软件编码器进行比较。

Result: 硬件编码器比软件解决方案实现显著更低的端到端延迟，RD性能略优；超低延迟模式将E2E延迟降至83毫秒（5帧）且不影响RD性能；硬件编码器延迟对质量预设不敏感。

Conclusion: 硬件编码器能够实现高质量、低延迟的视频流传输，为6G时代的实时视频应用提供了可靠的技术基础。

Abstract: The demand for high-quality, real-time video streaming has grown exponentially, with 4K Ultra High Definition (UHD) becoming the new standard for many applications such as live broadcasting, TV services, and interactive cloud gaming. This trend has driven the integration of dedicated hardware encoders into modern Graphics Processing Units (GPUs). Nowadays, these encoders support advanced codecs like HEVC and AV1 and feature specialized Low-Latency and Ultra Low-Latency tuning, targeting end-to-end latencies of < 2 seconds and < 500 ms, respectively. As the demand for such capabilities grows toward the 6G era, a clear understanding of their performance implications is essential. In this work, we evaluate the low-latency encoding modes on GPUs from NVIDIA, Intel, and AMD from both Rate-Distortion (RD) performance and latency perspectives. The results are then compared against both the normal-latency tuning of hardware encoders and leading software encoders. Results show hardware encoders achieve significantly lower E2E latency than software solutions with slightly better RD performance. While standard Low-Latency tuning yields a poor quality-latency trade-off, the Ultra Low-Latency mode reduces E2E latency to 83 ms (5 frames) without additional RD impact. Furthermore, hardware encoder latency is largely insensitive to quality presets, enabling high-quality, low-latency streams without compromise.

</details>


### [413] [Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing](https://arxiv.org/abs/2511.18755)
*Xiaotong Huang,He Zhu,Tianrui Ma,Yuxiang Xiong,Fangxin Liu,Zhezhi He,Yiming Gan,Zihan Liu,Jingwen Leng,Yu Feng,Minyi Guo*

Main category: cs.AR

TL;DR: Splatonic是一个稀疏高效的3DGS-SLAM算法-硬件协同设计，通过自适应稀疏像素采样和像素级渲染流水线，在移动设备上实现实时性能，相比移动GPU获得274.9倍加速和4738.5倍能耗节省。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)在SLAM中展现出高保真重建和快速收敛的优势，但其高计算成本使其在移动平台上不实用，特别是跟踪过程。

Method: 提出自适应稀疏像素采样算法减少渲染像素数量达256倍；设计新型像素级渲染流水线，通过高斯并行渲染和预检α检查提高硬件利用率；提出流水线架构简化设计并解决投影和聚合中的新瓶颈。

Result: 在四个3DGS-SLAM算法上评估，相比移动GPU实现274.9倍加速和4738.5倍能耗节省，相比最先进加速器实现25.2倍加速和241.1倍能耗节省，同时保持可比较的精度。

Conclusion: Splatonic通过算法-硬件协同设计成功解决了3DGS-SLAM在移动设备上的计算瓶颈，实现了高效的实时性能。

Abstract: 3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence. However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.
  This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices. Inspired by classical SLAMs, we propose an adaptive sparse pixel sampling algorithm that reduces the number of rendered pixels by up to 256$\times$ while retaining accuracy. To unlock this performance potential on mobile GPUs, we design a novel pixel-based rendering pipeline that improves hardware utilization via Gaussian-parallel rendering and preemptive $α$-checking. Together, these optimizations yield up to 121.7$\times$ speedup on the bottleneck stages and 14.6$\times$ end-to-end speedup on off-the-shelf GPUs. To further address new bottlenecks introduced by our rendering pipeline, we propose a pipelined architecture that simplifies the overall design while addressing newly emerged bottlenecks in projection and aggregation. Evaluated across four 3DGS-SLAM algorithms, Splatonic achieves up to 274.9$\times$ speedup and 4738.5$\times$ energy savings over mobile GPUs and up to 25.2$\times$ speedup and 241.1$\times$ energy savings over state-of-the-art accelerators, all with comparable accuracy.

</details>


### [414] [HeLEx: A Heterogeneous Layout Explorer for Spatial Elastic Coarse-Grained Reconfigurable Arrays](https://arxiv.org/abs/2511.19366)
*Alan Jia Bao Du,Tarek S. Abdelrahman*

Main category: cs.AR

TL;DR: HeLEx框架用于优化异构粗粒度可重构阵列的功能布局，通过分支定界搜索减少处理单元支持的操作数量，显著降低CGRA面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有CGRA设计通常采用全功能布局，导致资源浪费。需要一种方法在保证映射成功率的同时，优化CGRA的功能布局以减少面积和功耗。

Method: 使用分支定界搜索算法，从全功能布局开始逐步消除处理单元中的冗余操作，确保输入数据流图仍能成功映射到优化后的CGRA上。

Result: 实验显示平均减少68.7%的操作数量，CGRA面积减少近70%，功耗降低超过51%，生成的CGRA接近理论最小值的6.2%以内。

Conclusion: HeLEx框架能有效优化CGRA功能布局，在保持映射能力的同时显著降低硬件开销，性能优于现有方法。

Abstract: We present HeLEx, a framework for determining the functional layout of heterogeneous spatially-configured elastic Coarse-Grained Reconfigurable Arrays (CGRAs). Given a collection of input data flow graphs (DFGs) and a target CGRA, the framework starts with a full layout in which every processing element (PE) supports every operation in the DFGs. It then employs a branch-and-bound (BB) search to eliminate operations out of PEs, ensuring that the input DFGs successfully map onto the resulting CGRAs, eventually returning an optimized heterogeneous CGRA. Experimental evaluation with 12 DFGs and 9 target CGRA sizes reveals that the framework reduces the number of operations by 68.7% on average, resulting in a reduction of CGRA area by almost 70% and of power by over 51%, all compared to the initial full layout. HeLEx generates CGRAs that are on average only within 6.2% of theoretically minimum CGRAs that support exactly the number of operations needed by the input DFGs. A comparison with functional layouts produced by two state-of-the-art frameworks indicates that HeLEx achieves better reduction in the number of operations, by up to 2.6X.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [415] [Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration](https://arxiv.org/abs/2511.18674)
*Alfredo Metere*

Main category: cs.PF

TL;DR: 提出了一种基于低秩矩阵近似的矩阵乘法方法Low-Rank GEMM，通过FP8精度和智能内核选择实现次二次复杂度，在NVIDIA RTX 4090上达到378 TFLOPS性能，相比PyTorch FP32节省75%内存并获得7.8倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵乘法方法存在立方计算复杂度问题，限制了大规模机器学习工作负载的性能。

Method: 利用低秩矩阵近似实现次二次复杂度，结合FP8精度和智能内核选择，系统自动根据矩阵特性和硬件能力选择最优分解方法（SVD、随机SVD）和精度级别。

Result: 在NVIDIA RTX 4090上，对N=20480的矩阵达到378 TFLOPS性能，节省75%内存，相比PyTorch FP32获得7.8倍加速。当N≥10240时，该方法超越传统cuBLAS实现。

Conclusion: Low-Rank GEMM通过内存带宽优化而非计算捷径，成为大规模矩阵乘法的最快方法，特别适用于大尺寸矩阵运算。

Abstract: Large matrix multiplication is a cornerstone of modern machine learning workloads, yet traditional approaches suffer from cubic computational complexity (e.g., $\mathcal{O}(n^3)$ for a matrix of size $n\times n$). We present Low-Rank GEMM, a novel approach that leverages low-rank matrix approximations to achieve sub-quadratic complexity while maintaining hardware-accelerated performance through FP8 precision and intelligent kernel selection. On a NVIDIA RTX 4090, our implementation achieves up to 378 TFLOPS on matrices up to $N=20480$, providing 75\% memory savings and $7.8\times$ speedup over PyTorch FP32 for large matrices. The system automatically adapts to hardware capabilities, selecting optimal decomposition methods (SVD, randomized SVD) and precision levels based on matrix characteristics and available accelerators. Comprehensive benchmarking on NVIDIA RTX 4090 demonstrates that Low-Rank GEMM becomes the fastest approach for matrices $N\geq10240$, surpassing traditional cuBLAS implementations through memory bandwidth optimization rather than computational shortcuts.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [416] [Multimodal Real-Time Anomaly Detection and Industrial Applications](https://arxiv.org/abs/2511.18698)
*Aman Verma,Keshav Samdani,Mohd. Samiuddin Shafi*

Main category: cs.SD

TL;DR: 该论文介绍了一个多模态房间监控系统的设计和演进，从轻量级版本（YOLOv8+ByteTrack+AST）发展到高级版本（多模型音频集成、混合目标检测、双向跨模态注意力、多方法异常检测），显著提升了准确性、鲁棒性和工业适用性。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够实时进行活动识别和异常检测的综合多模态监控系统，通过整合同步的视频和音频处理来提高监控效果和工业安全性。

Method: 初始版本使用YOLOv8、ByteTrack和音频频谱图变换器(AST)；高级版本整合了三个音频模型(AST、Wav2Vec2、HuBERT)、双目标检测器(YOLO和DETR)、双向跨模态注意力和多方法异常检测。

Result: 实验评估显示系统在通用监控场景和工业安全应用中均表现有效，在标准硬件上实现实时性能的同时保持高准确率。

Conclusion: 系统的演进证明了多模态集成和先进融合机制能够显著提升监控系统的性能，为工业安全应用提供了实用的解决方案。

Abstract: This paper presents the design, implementation, and evolution of a comprehensive multimodal room-monitoring system that integrates synchronized video and audio processing for real-time activity recognition and anomaly detection. We describe two iterations of the system: an initial lightweight implementation using YOLOv8, ByteTrack, and the Audio Spectrogram Transformer (AST), and an advanced version that incorporates multi-model audio ensembles, hybrid object detection, bidirectional cross-modal attention, and multi-method anomaly detection. The evolution demonstrates significant improvements in accuracy, robustness, and industrial applicability. The advanced system combines three audio models (AST, Wav2Vec2, and HuBERT) for comprehensive audio understanding, dual object detectors (YOLO and DETR) for improved accuracy, and sophisticated fusion mechanisms for enhanced cross-modal learning. Experimental evaluation shows the system's effectiveness in general monitoring scenarios as well as specialized industrial safety applications, achieving real-time performance on standard hardware while maintaining high accuracy.

</details>


### [417] [PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation](https://arxiv.org/abs/2511.18833)
*Huadai Liu,Kaicheng Luo,Wen Wang,Qian Chen,Peiwen Sun,Rongjie Huang,Xiangang Li,Jieping Ye,Wei Xue*

Main category: cs.SD

TL;DR: PrismAudio是首个将强化学习集成到视频到音频生成中的框架，通过专门的思维链规划解决现有方法中的目标纠缠问题，并在四个感知维度上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频生成方法存在目标纠缠问题，将竞争性目标混在单一损失函数中，且缺乏人类偏好对齐，需要平衡语义一致性、视听时间同步性、美学质量和空间准确性四个关键感知维度。

Method: 提出PrismAudio框架，将整体推理分解为四个专门的思维链模块（语义、时间、美学和空间CoT），每个模块配备针对性奖励函数，并引入Fast-GRPO算法使用混合ODE-SDE采样大幅降低训练开销。

Result: 在VGGSound测试集和AudioCanvas基准测试中，PrismAudio在所有四个感知维度上都达到了最先进的性能。

Conclusion: PrismAudio通过思维链-奖励对应关系实现了多维强化学习优化，解决了目标纠缠问题，同时保持了可解释性，为视频到音频生成提供了有效的解决方案。

Abstract: Video-to-Audio (V2A) generation requires balancing four critical perceptual dimensions: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy; yet existing methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment. We introduce PrismAudio, the first framework to integrate Reinforcement Learning into V2A generation with specialized Chain-of-Thought (CoT) planning. Our approach decomposes monolithic reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, and Spatial CoT), each paired with targeted reward functions. This CoT-reward correspondence enables multidimensional RL optimization that guides the model to jointly generate better reasoning across all perspectives, solving the objective entanglement problem while preserving interpretability. To make this optimization computationally practical, we propose Fast-GRPO, which employs hybrid ODE-SDE sampling that dramatically reduces the training overhead compared to existing GRPO implementations. We also introduce AudioCanvas, a rigorous benchmark that is more distributionally balanced and covers more realistically diverse and challenging scenarios than existing datasets, with 300 single-event classes and 501 multi-event samples. Experimental results demonstrate that PrismAudio achieves state-of-the-art performance across all four perceptual dimensions on both the in-domain VGGSound test set and out-of-domain AudioCanvas benchmark. The project page is available at https://PrismAudio-Project.github.io.

</details>


### [418] [Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments](https://arxiv.org/abs/2511.19396)
*Jorge Ortigoso-Narro,Jose A. Belloch,Adrian Amor-Martin,Sandra Roger,Maximo Cobos*

Main category: cs.SD

TL;DR: 提出了一种嵌入式系统，集成深度学习跟踪与波束成形技术，实现动态环境中的精确声源定位和定向音频捕获。


<details>
  <summary>Details</summary>
Motivation: 利用目标跟踪和声学波束成形技术的进步，为监控、人机交互和机器人应用开发更精确的声源定位系统。

Method: 结合单摄像头深度估计和立体视觉实现移动物体的3D定位，使用MEMS麦克风构建的平面同心圆麦克风阵列支持2D波束转向，实时跟踪输出持续调整阵列焦点。

Result: 实验评估显示信干比显著提升，系统在多个或移动声源存在时保持鲁棒性能。

Conclusion: 该系统设计适用于视频会议、智能家居设备和辅助技术等应用场景。

Abstract: Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [419] [SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis from a CLIP-Aligned Encoder](https://arxiv.org/abs/2511.17547)
*Jeyoung Lee,Hochul Kang*

Main category: eess.SP

TL;DR: SYNAPSE是一个两阶段框架，通过CLIP对齐的EEG自编码器和轻量级Stable Diffusion适配，实现从脑电信号到高质量图像的生成，在感知保真度和重建效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将扩散生成模型扩展到脑电信号领域，可以加深对人类感知和心智表征的理解，但EEG存在高噪声、低空间分辨率和强个体间变异性的挑战。

Method: 两阶段方法：第一阶段使用CLIP对齐的EEG自编码器学习语义结构化的潜在表示；第二阶段冻结编码器并与轻量级Stable Diffusion适配，实现高效EEG特征条件化。

Result: 在CVPR40数据集上实现了语义一致的潜在空间和最先进的感知保真度，在重建效率和图像质量方面优于现有EEG到图像模型，并能有效跨被试泛化。

Conclusion: 重建大脑感知的内容而非分类的内容，是基于EEG的忠实图像生成的关键。

Abstract: Recent progress in diffusion-based generative models has enabled high-quality image synthesis conditioned on diverse modalities. Extending such models to brain signals could deepen our understanding of human perception and mental representations. However,electroencephalography (EEG) presents major challenges for image generation due to high noise, low spatial resolution, and strong inter-subject variability. Existing approaches,such as DreamDiffusion, BrainVis, and GWIT, primarily adapt EEG features to pre-trained Stable Diffusion models using complex alignment or classification pipelines, often resulting in large parameter counts and limited interpretability. We introduce SYNAPSE, a two-stage framework that bridges EEG signal representation learning and high-fidelity image synthesis. In Stage1, a CLIP-aligned EEG autoencoder learns a semantically structured latent representation by combining signal reconstruction and cross-modal alignment objectives. In Stage2, the pretrained encoder is frozen and integrated with a lightweight adaptation of Stable Diffusion, enabling efficient conditioning on EEG features with minimal trainable parameters. Our method achieves a semantically coherent latent space and state-of-the-art perceptual fidelity on the CVPR40 dataset, outperforming prior EEG-to-image models in both reconstruction efficiency and image quality. Quantitative and qualitative analyses demonstrate that SYNAPSE generalizes effectively across subjects, preserving visual semantics even when class-level agreement is reduced. These results suggest that reconstructing what the brain perceives, rather than what it classifies, is key to faithful EEG-based image generation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [420] [Classification EM-PCA for clustering and embedding](https://arxiv.org/abs/2511.18992)
*Zineddine Tighidet,Lazhar Labiod,Mohamed Nadif*

Main category: stat.ML

TL;DR: 提出一种结合PCA和CEM算法的非顺序方法，同时进行数据嵌入和聚类，以解决高维数据和EM算法收敛慢的问题。


<details>
  <summary>Details</summary>
Motivation: 高斯混合模型在聚类中广泛应用，但面临高维数据和EM算法收敛慢的挑战。CEM算法能加速收敛，但维度缩减仍是问题。

Method: 结合主成分分析(PCA)和分类EM(CEM)算法，同时而非顺序地执行数据嵌入和聚类两个任务。

Result: 该方法在聚类和数据嵌入方面表现出优势，并与其他聚类方法建立了联系。

Conclusion: 提出的PCA-CEM组合方法能有效解决高维聚类问题，同时保持快速收敛特性。

Abstract: The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [421] [Pre-cache: A Microarchitectural Solution to prevent Meltdown and Spectre](https://arxiv.org/abs/2511.17726)
*Subhash Sethumurugan,Hari Cherupalli,Kangjie Lu,John Sartori*

Main category: cs.CR

TL;DR: 提出了针对Meltdown和Spectre攻击的微架构解决方案，通过防止刷新指令向缓存暴露数据来修复漏洞，同时保持较低性能开销。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中的乱序执行和推测执行机制存在安全漏洞，现有的软件补丁方案存在性能开销大且只是临时修复副作用的问题。

Method: 开发微架构层面的解决方案，阻止刷新指令向缓存暴露数据，并可扩展到其他内存结构以防止变种攻击。

Result: 该方案不仅恢复了安全的乱序和推测执行，而且对大多数应用程序的性能影响较小。

Conclusion: 微架构解决方案能有效防御Meltdown和Spectre攻击及其变种，同时维持处理器性能。

Abstract: Recent work has shown that out-of-order and speculative execution mechanisms used to increase performance in the majority of processors expose the processors to critical attacks. These attacks, called Meltdown and Spectre, exploit the side effects of performance-enhancing features in modern microprocessors to expose secret data through side channels in the microarchitecture. The well known implementations of these attacks exploit cache-based side channels since they are the least noisy channels to exfiltrate data. While some software patches attempted to mitigate these attacks, they are ad-hoc and only try to fix the side effects of the vulnerabilites. They may also impose a performance overhead of up to 30%. In this paper, we present a microarchitecture-based solution for Meltdown and Spectre that addresses the vulnerabilities exploited by the attacks. Our solution prevents flushed instructions from exposing data to the cache. Our approach can also be extended to other memory structures in the microarchitecture thereby preventing variants of the attacks which exploit these memory structures. We further identify two new variant attacks based on exploiting the side effects of speculative and out-of-order execution and show how our solution can be used to prevent these attacks. Evaluation results show that our microarchitectural solution not only restores secure out-of-order and speculative execution, but also has relatively low overhead and does not significantly impact performance for most applications.

</details>


### [422] [ioPUF+: A PUF Based on I/O Pull-Up/Down Resistors for Secret Key Generation in IoT Nodes](https://arxiv.org/abs/2511.18412)
*Dilli Babu Porlapothula,Pralay Chakrabarty,Ananya Lakshmi Ravi,Kurian Polachan*

Main category: cs.CR

TL;DR: ioPUF+是一种基于I/O引脚电阻值的物理不可克隆函数(PUF)，利用制造过程中的工艺变化为IC和物联网节点生成唯一指纹，无需定制电路或新IC制造，适合成本敏感的嵌入式系统。


<details>
  <summary>Details</summary>
Motivation: 为物联网节点和嵌入式系统提供低成本、无需定制硬件的安全解决方案，利用现有IC的I/O结构实现设备身份认证和密钥生成。

Method: 通过测量IC I/O引脚的上拉和下拉电阻值生成PUF响应，使用BCH纠错和SHA-256哈希将原始响应转换为加密密钥，并集成AES加密保护设备间通信。

Result: 在30个设备上评估显示：100%可靠性、50.33%独特性、50.54%均匀性，温度和电压变化下误码率分别仅为2.63%和2.10%，完整系统仅需19.8KB Flash、600ms延迟和79mW功耗。

Conclusion: ioPUF+是一种高效、低成本的PUF解决方案，特别适合资源受限的物联网节点，能够在不增加硬件成本的情况下提供强大的安全保护。

Abstract: In this work, we present ioPUF+, which incorporates a novel Physical Unclonable Function (PUF) that generates unique fingerprints for Integrated Circuits (ICs) and the IoT nodes encompassing them. The proposed PUF generates device-specific responses by measuring the pull-up and pull-down resistor values on the I/O pins of the ICs, which naturally vary across chips due to manufacturing-induced process variations. Since these resistors are already integrated into the I/O structures of most ICs, ioPUF+ requires no custom circuitry, and no new IC fabrication. This makes ioPUF+ suitable for cost-sensitive embedded systems built from Commercial Off-The-Shelf (COTS) components. Beyond introducing a new PUF, ioPUF+ includes a complete datapath for converting raw PUF responses into cryptographically usable secret keys using BCH error correction and SHA-256 hashing. Further ioPUF+ also demonstrate a practical use case of PUF derive secret keys in securing device-to-device communication using AES-encryption. We implemented ioPUF+ on the Infineon PSoC-5 microcontroller and evaluated its performance across 30 devices using standard PUF metrics. The results show excellent reliability (intra-device Hamming distance of 100.00%), strong uniqueness (inter-device Hamming distance of 50.33%), near-ideal uniformity (50.54%), and negligible bit aliasing. Stability tests under temperature and supply-voltage variations show worst-case bit-error rates of only 2.63% and 2.10%, respectively. We also profiled the resource and energy usage of the complete ioPUF+ system, including the PUF primitive, BCH decoding, SHA-256 hashing, and AES encryption. The full implementation requires only 19.8 KB of Flash, exhibits a latency of 600 ms, and consumes 79 mW of power, demonstrating the suitabilitiy of ioPUF+ for resource-constrained IoT nodes.

</details>


### [423] [MURMUR: Using cross-user chatter to break collaborative language agents in groups](https://arxiv.org/abs/2511.17671)
*Atharv Singh Patlan,Peiyao Sheng,S. Ashwin Hebbar,Prateek Mittal,Pramod Viswanath*

Main category: cs.CR

TL;DR: 本文提出了一种针对多用户语言代理的新攻击方式——跨用户投毒攻击(CUP)，攻击者通过注入看似普通的消息来污染共享状态，从而让代理在后续任务中执行恶意操作。作者开发了MURMUR框架来系统研究这种攻击，并提出了基于任务聚类的初步防御方案。


<details>
  <summary>Details</summary>
Motivation: 随着语言代理从单用户助手扩展到多用户协作环境，现有的语言模型缺乏隔离用户交互和并发任务的机制，这为跨用户投毒攻击创造了新的攻击向量。

Method: 作者提出了MURMUR框架，使用LLM生成真实的历史感知用户交互，将单用户任务组合成并发的基于群体的场景，系统性地研究CUP攻击。

Result: 实验验证显示CUP攻击在真实的多用户代理系统中成功率很高，且其影响会持续跨越多个任务，对多用户LLM部署构成根本性风险。

Conclusion: 跨用户投毒攻击是多用户语言代理部署中的严重安全威胁，需要开发专门的防御机制来缓解这类新型漏洞。

Abstract: Language agents are rapidly expanding from single-user assistants to multi-user collaborators in shared workspaces and groups. However, today's language models lack a mechanism for isolating user interactions and concurrent tasks, creating a new attack vector inherent to this new setting: cross-user poisoning (CUP). In a CUP attack, an adversary injects ordinary-looking messages that poison the persistent, shared state, which later triggers the agent to execute unintended, attacker-specified actions on behalf of benign users. We validate CUP on real systems, successfully attacking popular multi-user agents. To study the phenomenon systematically, we present MURMUR, a framework that composes single-user tasks into concurrent, group-based scenarios using an LLM to generate realistic, history-aware user interactions. We observe that CUP attacks succeed at high rates and their effects persist across multiple tasks, thus posing fundamental risks to multi-user LLM deployments. Finally, we introduce a first-step defense with task-based clustering to mitigate this new class of vulnerability

</details>


### [424] [Shadows in the Code: Exploring the Risks and Defenses of LLM-based Multi-Agent Software Development Systems](https://arxiv.org/abs/2511.18467)
*Xiaoqing Wang,Keman Huang,Bin Liang,Hongyu Li,Xiaoyong Du*

Main category: cs.CR

TL;DR: 研究发现LLM驱动的多智能体系统存在严重安全风险，提出IMBIA攻击方法能在看似良性的应用中隐藏恶意功能，并开发了防御机制Adv-IMBIA。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的多智能体系统使非技术用户能够开发软件，这些系统通过自然语言需求民主化软件创建的同时，引入了尚未充分探索的重大安全风险。

Method: 识别了两种风险场景：恶意用户与良性智能体（MU-BA）和良性用户与恶意智能体（BU-MA），提出了IMBIA攻击方法，并开发了Adv-IMBIA防御机制。

Result: 在ChatDev、MetaGPT和AgentVerse框架上的评估显示，IMBIA在MU-BA场景中攻击成功率分别为93%、45%和71%，在BU-MA场景中分别为71%、84%和45%。防御机制显著降低了攻击成功率。

Conclusion: 多智能体软件开发系统迫切需要强大的安全措施，研究为实施有针对性的、资源高效的防御策略提供了实用指南。

Abstract: The rapid advancement of Large Language Model (LLM)-driven multi-agent systems has significantly streamlined software developing tasks, enabling users with little technical expertise to develop executable applications. While these systems democratize software creation through natural language requirements, they introduce significant security risks that remain largely unexplored. We identify two risky scenarios: Malicious User with Benign Agents (MU-BA) and Benign User with Malicious Agents (BU-MA). We introduce the Implicit Malicious Behavior Injection Attack (IMBIA), demonstrating how multi-agent systems can be manipulated to generate software with concealed malicious capabilities beneath seemingly benign applications, and propose Adv-IMBIA as a defense mechanism. Evaluations across ChatDev, MetaGPT, and AgentVerse frameworks reveal varying vulnerability patterns, with IMBIA achieving attack success rates of 93%, 45%, and 71% in MU-BA scenarios, and 71%, 84%, and 45% in BU-MA scenarios. Our defense mechanism reduced attack success rates significantly, particularly in the MU-BA scenario. Further analysis reveals that compromised agents in the coding and testing phases pose significantly greater security risks, while also identifying critical agents that require protection against malicious user exploitation. Our findings highlight the urgent need for robust security measures in multi-agent software development systems and provide practical guidelines for implementing targeted, resource-efficient defensive strategies.

</details>


### [425] [Understanding and Mitigating Over-refusal for Large Language Models via Safety Representation](https://arxiv.org/abs/2511.19009)
*Junbo Zhang,Ran Chen,Qianli Zhou,Xinyang Deng,Wen Jiang*

Main category: cs.CR

TL;DR: 提出MOSR方法解决LLM安全防御中的过度拒绝问题，通过表示空间干预在保持安全性的同时减少过度拒绝


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法在提升安全性时往往导致严重的过度拒绝问题，无法在安全性和可用性之间取得良好平衡

Method: MOSR方法包含两个核心组件：重叠感知损失加权（通过量化恶意样本与伪恶意样本在表示空间的相似性确定擦除权重）和上下文感知增强（通过在被拒绝响应前添加有害前缀来补充拒绝决策的必要上下文）

Result: 实验表明MOSR方法在缓解过度拒绝方面优于现有方法，同时很大程度上保持了安全性

Conclusion: 未来的防御方法应该在安全性和过度拒绝之间取得更好的平衡

Abstract: Large language models demonstrate powerful capabilities across various natural language processing tasks, yet they also harbor safety vulnerabilities. To enhance LLM safety, various jailbreak defense methods have been proposed to guard against harmful outputs. However, improvements in model safety often come at the cost of severe over-refusal, failing to strike a good balance between safety and usability. In this paper, we first analyze the causes of over-refusal from a representation perspective, revealing that over-refusal samples reside at the boundary between benign and malicious samples. Based on this, we propose MOSR, designed to mitigate over-refusal by intervening the safety representation of LLMs. MOSR incorporates two novel components: (1) Overlap-Aware Loss Weighting, which determines the erasure weight for malicious samples by quantifying their similarity to pseudo-malicious samples in the representation space, and (2) Context-Aware Augmentation, which supplements the necessary context for rejection decisions by adding harmful prefixes before rejection responses. Experiments demonstrate that our method outperforms existing approaches in mitigating over-refusal while largely maintaining safety. Overall, we advocate that future defense methods should strike a better balance between safety and over-refusal.

</details>


### [426] [FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization](https://arxiv.org/abs/2511.19248)
*Md Akil Raihan Iftee,Syed Md. Ahnaf Hasan,Amin Ahsan Ali,AKM Mahbubur Rahman,Sajib Mistry,Aneesh Krishna*

Main category: cs.CR

TL;DR: FedPoisonTTP是一个针对联邦学习中测试时个性化的灰盒攻击框架，通过数据投毒在本地适应阶段破坏模型性能，并利用协作更新传播攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究忽视了测试时本地适应带来的安全风险，异构域到达、多样化适应算法和有限的跨客户端可见性为恶意参与者创造了攻击漏洞。

Method: 通过对抗查询提取代理模型，使用特征一致性合成分布内毒药，优化攻击目标生成高熵或类置信毒药以规避常见适应过滤器，在本地适应阶段注入毒药并通过协作更新传播。

Result: 在受损视觉基准测试上的广泛实验表明，被攻陷的参与者能够显著降低整体测试时性能。

Conclusion: 联邦学习中的测试时个性化存在严重安全漏洞，需要开发针对性的防御机制来应对此类攻击。

Abstract: Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [427] [N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory](https://arxiv.org/abs/2511.18723)
*Longfei Wang,Junyan Liu,Fan Zhang,Jiangwen Wei,Yuanhua Tang,Jie Sun,Xiaodong Luo*

Main category: cs.AI

TL;DR: 提出了一个名为N2N的可扩展并行框架，用于在分布式内存计算环境中求解大规模MILP问题。该框架支持确定性和非确定性模式，通过节点到节点映射实现分支定界算法的并行化。


<details>
  <summary>Details</summary>
Motivation: 混合整数线性规划（MILP）求解中的并行化是加速求解的有效方法，但由于分支定界框架的复杂性和MILP求解器中众多有效算法组件，实现并行化具有挑战性。

Method: 设计了N2N框架，将分支定界节点映射到分布式计算节点。在确定性模式下，采用基于滑动窗口的算法确保任务按确定顺序生成和求解。集成了CP搜索和通用原始启发式等先进技术，并进行了自适应求解和数据通信优化。

Result: 将N2N与SCIP集成得到N2N-SCIP，在1000个MPI进程下，非确定性N2N-SCIP在Kunpeng和x86计算集群上分别实现了22.52和12.71的加速比，比ParaSCIP快1.98倍和2.08倍。确定性模式也显示出显著性能提升。

Conclusion: N2N框架在分布式并行MILP求解方面表现出优越性能，具有通用性，能够与不同求解器集成，为大规模MILP问题求解提供了有效的并行解决方案。

Abstract: Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (a node-to-node framework that maps the B&B nodes to distributed computing nodes), was proposed to solve large-scale problems in a distributed memory computing environment. Both deterministic and nondeterministic modes are supported, and the framework is designed to be easily integrated with existing solvers. Regarding the deterministic mode, a novel sliding-window-based algorithm was designed and implemented to ensure that tasks are generated and solved in a deterministic order. Moreover, several advanced techniques, such as the utilization of CP search and general primal heuristics, have been developed to fully utilize distributed computing resources and capabilities of base solvers. Adaptive solving and data communication optimization were also investigated. A popular open-source MILP solver, SCIP, was integrated into N2N as the base solver, yielding N2N-SCIP. Extensive computational experiments were conducted to evaluate the performance of N2N-SCIP compared to ParaSCIP, which is a state-of-the-art distributed parallel MILP solver under the UG framework. The nondeterministic N2N-SCIP achieves speedups of 22.52 and 12.71 with 1,000 MPI processes on the Kunpeng and x86 computing clusters, which is 1.98 and 2.08 times faster than ParaSCIP, respectively. In the deterministic mode, N2N-SCIP also shows significant performance improvements over ParaSCIP across different process numbers and computing clusters. To validate the generality of N2N, HiGHS, another open-source solver, was integrated into N2N. The related results are analyzed, and the requirements of N2N on base solvers are also concluded.

</details>


### [428] [Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop](https://arxiv.org/abs/2511.17673)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 提出了结构化认知循环（SCL）架构，通过模块化设计分离推理与执行，解决LLM代理的架构问题，实现零策略违规和完全决策可追溯性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型代理存在的三个基本架构问题：推理与执行纠缠、内存易失性和不可控动作序列。

Method: 引入SCL架构，将代理认知明确分为五个阶段：检索、认知、控制、动作和记忆（R-CCAM），核心是软符号控制机制，将符号约束应用于概率推理。

Result: 在多步条件推理任务中实现零策略违规，消除冗余工具调用，保持完全决策可追溯性。

Conclusion: 通过连接专家系统原则与现代LLM能力，为可靠、可解释和可治理的AI代理提供了实用且理论基础扎实的路径。

Abstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents. Code: https://github.com/enkiluv/scl-core-experiment Demo: https://scl-travel-planner.streamlit.app/

</details>


### [429] [Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific Papers](https://arxiv.org/abs/2511.18036)
*Ziyi Guo,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: 提出了首个用于自动化科学图表生成的标准化基准，包含3000篇论文及其对应的高质量图表，并开发了Paper2SysArch系统作为基准测试的强基线。


<details>
  <summary>Details</summary>
Motivation: 手动创建科学论文系统架构图耗时且主观，现有生成模型缺乏结构控制和语义理解能力，且该领域缺乏标准化基准进行定量评估。

Method: 构建包含3000篇论文及其对应图表的基准数据集，采用三层评估指标（语义准确性、布局连贯性、视觉质量），并提出Paper2SysArch系统，利用多智能体协作将论文转换为结构化可编辑图表。

Result: 在手动整理的更具挑战性的论文子集上，Paper2SysArch系统获得了69.0的综合得分，证明了其有效性。

Conclusion: 主要贡献是建立了大规模基础基准以支持可重复研究和公平比较，提出的系统作为可行的概念验证，为该复杂任务展示了有前景的发展路径。

Abstract: The manual creation of system architecture diagrams for scientific papers is a time-consuming and subjective process, while existing generative models lack the necessary structural control and semantic understanding for this task. A primary obstacle hindering research and development in this domain has been the profound lack of a standardized benchmark to quantitatively evaluate the automated generation of diagrams from text. To address this critical gap, we introduce a novel and comprehensive benchmark, the first of its kind, designed to catalyze progress in automated scientific visualization. It consists of 3,000 research papers paired with their corresponding high-quality ground-truth diagrams and is accompanied by a three-tiered evaluation metric assessing semantic accuracy, layout coherence, and visual quality. Furthermore, to establish a strong baseline on this new benchmark, we propose Paper2SysArch, an end-to-end system that leverages multi-agent collaboration to convert papers into structured, editable diagrams. To validate its performance on complex cases, the system was evaluated on a manually curated and more challenging subset of these papers, where it achieves a composite score of 69.0. This work's principal contribution is the establishment of a large-scale, foundational benchmark to enable reproducible research and fair comparison. Meanwhile, our proposed system serves as a viable proof-of-concept, demonstrating a promising path forward for this complex task.

</details>


### [430] [AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning](https://arxiv.org/abs/2511.19304)
*Jiayi Zhang,Yiran Peng,Fanqi Kong,Yang Cheng,Yifan Wu,Zhaoyang Yu,Jinyu Xiang,Jianhao Ruan,Jinlin Wang,Maojia Song,HongZhang Liu,Xiangru Tang,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: 提出了AutoEnv框架和AutoEnv-36数据集，用于研究智能体在异构环境中的跨环境学习能力，发现固定学习方法难以扩展到多个环境，需要环境自适应的方法选择。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常在单一固定环境中自我进化，而人类能够适应不同动态、观测和奖励结构的多样化环境。跨环境学习能力尚未得到系统测量，缺乏标准化的异构环境集合和统一的学习表示方法。

Method: 1) 提出AutoEnv框架，将环境分解为转移、观测和奖励的分布，低成本生成异构世界；2) 构建AutoEnv-36数据集，包含36个环境和358个验证关卡；3) 将智能体学习形式化为以组件为中心的过程，包含选择、优化和评估三个阶段；4) 设计八种学习方法并在AutoEnv-36上评估。

Result: 1) 七个语言模型在AutoEnv-36上仅获得12-49%的标准化奖励，显示其挑战性；2) 单一学习方法在环境数量增加时收益迅速下降；3) 环境自适应的方法选择显著提升性能，但随着方法空间扩大呈现收益递减。

Conclusion: 固定学习方法无法扩展到异构环境，环境自适应的方法选择是必要的但仍有局限性。AutoEnv和AutoEnv-36为研究跨环境智能体学习提供了测试平台。

Abstract: Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.

</details>


### [431] [PRInTS: Reward Modeling for Long-Horizon Information Seeking](https://arxiv.org/abs/2511.19314)
*Jaewoo Lee,Archiki Prasad,Justin Chih-Yao Chen,Zaid Khan,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.AI

TL;DR: PRInTS是一个生成式过程奖励模型，通过密集评分和轨迹摘要来提升AI代理在长轨迹信息搜索任务中的能力，使较小模型达到或超越前沿模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的过程奖励模型(PRMs)是为短推理设计的，无法处理信息搜索任务中丰富的维度（如工具交互、工具输出推理）以及长时任务中快速增长的上下文。

Method: 提出PRInTS模型，具备双重能力：(1)基于多个步骤质量维度的密集评分；(2)轨迹摘要，压缩增长上下文同时保留步骤评估所需的关键信息。

Result: 在FRAMES、GAIA和WebWalkerQA基准测试中，使用PRInTS的最佳n采样显著提升了开源模型和专用代理的信息搜索能力，匹配或超越了前沿模型性能。

Conclusion: PRInTS通过密集评分和轨迹摘要有效解决了长时信息搜索任务的挑战，使较小模型能够达到前沿模型的性能水平。

Abstract: Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.

</details>


### [432] [Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?](https://arxiv.org/abs/2511.17643)
*Yayan Qiu,Sean Hanna*

Main category: cs.AI

TL;DR: 本研究提出了一种快速检测pix2pix学习拓扑关系能力的方法，通过在GAN前后添加两个基于Grasshopper的检测模块，证明pix2pix能够自动学习空间拓扑关系并应用于建筑设计。


<details>
  <summary>Details</summary>
Motivation: 考虑到空间内在和外在特性的区域特征是建筑设计和城市更新的关键问题，现有方法通过图像和基于图的GAN逐步实现，但模型嵌套和数据转换可能导致信息丢失，需要简化工具以便建筑师和用户参与设计。

Method: 在GAN前后添加两个基于Grasshopper的检测模块，提供定量数据并可视化学习过程，研究不同输入模式（灰度、RGB）对学习效率的影响。

Result: 证明pix2pix能够自动学习空间拓扑关系，填补了从拓扑角度检测基于图像的生成GAN性能的空白，检测方法耗时短、操作简单。

Conclusion: 该方法可为使用GAN保留空间拓扑特征的建筑设计和城市更新应用提供理论基础和数据支持，检测模块可广泛用于定制具有相同拓扑结构的图像数据集和批量检测图像拓扑关系。

Abstract: Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics.

</details>


### [433] [GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction](https://arxiv.org/abs/2511.18874)
*Yuzhi Chen,Yuanchang Xie,Lei Zhao,Pan Liu,Yajie Zou,Chen Wang*

Main category: cs.AI

TL;DR: 提出了GContextFormer，一种无需依赖HD地图的全局上下文感知多模态轨迹预测方法，通过混合注意力和缩放加性聚合实现意图对齐的预测


<details>
  <summary>Details</summary>
Motivation: 现有HD地图依赖模型存在数据获取成本高、更新延迟和易受损坏输入影响的问题，而无地图方法缺乏全局上下文，导致运动-意图不对齐

Method: 采用编码器-解码器架构，运动感知编码器通过有界缩放加性聚合构建场景级意图先验，分层交互解码器通过双路径交叉注意力分解社会推理

Result: 在TOD-VT数据集的八个高速公路匝道场景中优于现有最先进方法，在高速曲率和过渡区域表现出更强的鲁棒性和集中改进

Conclusion: GContextFormer实现了无需地图依赖的意图对齐多模态预测，模块化架构支持向跨域多模态推理任务的扩展

Abstract: Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [434] [TeamPath: Building MultiModal Pathology Experts with Reasoning AI Copilots](https://arxiv.org/abs/2511.17652)
*Tianyu Liu,Weihao Xuan,Hao Wu,Peter Humphrey,Marcello DiStasio,Heli Qi,Rui Yang,Simeng Han,Tinglin Huang,Fang Wu,Nan Liu,Irene Li,Hua Xu,Hongyu Zhao*

Main category: q-bio.QM

TL;DR: TeamPath是一个基于强化学习和路由增强解决方案的AI系统，作为虚拟助手用于专家级疾病诊断、斑块级信息总结和跨模态生成，整合转录组信息用于临床。


<details>
  <summary>Details</summary>
Motivation: 当前病理学专用视觉语言模型在严格推理路径诊断和处理多样化任务方面能力不足，构建真实场景AI助手的挑战依然存在。

Method: 基于大规模组织病理学多模态数据集，采用强化学习和路由增强解决方案构建AI系统。

Result: 与耶鲁医学院病理学家合作证明，TeamPath能够通过识别和修正专家结论及推理路径，帮助病理学家更高效工作。

Conclusion: TeamPath能够根据需求灵活选择最佳设置，作为跨不同模态和专家的信息交流创新可靠系统。

Abstract: Advances in AI have introduced several strong models in computational pathology to usher it into the era of multi-modal diagnosis, analysis, and interpretation. However, the current pathology-specific visual language models still lack capacities in making diagnosis with rigorous reasoning paths as well as handling divergent tasks, and thus challenges of building AI Copilots for real scenarios still exist. Here we introduce TeamPath, an AI system powered by reinforcement learning and router-enhanced solutions based on large-scale histopathology multimodal datasets, to work as a virtual assistant for expert-level disease diagnosis, patch-level information summarization, and cross-modality generation to integrate transcriptomic information for the clinical usage. We also collaborate with pathologists from Yale School of Medicine to demonstrate that TeamPath can assist them in working more efficiently by identifying and correcting expert conclusions and reasoning paths. Overall, TeamPath can flexibly choose the best settings according to the needs, and serve as an innovative and reliable system for information communication across different modalities and experts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [435] [Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs](https://arxiv.org/abs/2511.17566)
*Shuaiyu Xie,Hanbin He,Jian Wang,Bing Li*

Main category: cs.LG

TL;DR: CCLH是一个用于微服务系统根因分析的新框架，通过级联条件学习和异构超图建模来解决传统方法在任务协作和实例群体影响建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统根因分析方法面临两个关键挑战：1）联合学习范式忽视了任务间的因果依赖关系，阻碍了任务间协作；2）主要关注点对点关系，忽略了由部署配置和负载均衡引起的实例群体影响特性。

Method: 提出了CCLH框架，采用级联条件学习来协调诊断任务，提供了实例间群体影响的三级分类，并引入异构超图来建模这些关系以模拟故障传播。

Result: 在三个微服务基准数据集上的广泛实验表明，CCLH在根因定位和故障类型识别两个任务上都优于现有最先进方法。

Conclusion: CCLH通过级联条件学习和异构超图建模，有效解决了传统根因分析方法在任务协作和实例群体影响建模方面的局限性，显著提升了诊断性能。

Abstract: Root cause analysis in microservice systems typically involves two core tasks: root cause localization (RCL) and failure type identification (FTI). Despite substantial research efforts, conventional diagnostic approaches still face two key challenges. First, these methods predominantly adopt a joint learning paradigm for RCL and FTI to exploit shared information and reduce training time. However, this simplistic integration neglects the causal dependencies between tasks, thereby impeding inter-task collaboration and information transfer. Second, these existing methods primarily focus on point-to-point relationships between instances, overlooking the group nature of inter-instance influences induced by deployment configurations and load balancing. To overcome these limitations, we propose CCLH, a novel root cause analysis framework that orchestrates diagnostic tasks based on cascaded conditional learning. CCLH provides a three-level taxonomy for group influences between instances and incorporates a heterogeneous hypergraph to model these relationships, facilitating the simulation of failure propagation. Extensive experiments conducted on datasets from three microservice benchmarks demonstrate that CCLH outperforms state-of-the-art methods in both RCL and FTI.

</details>


### [436] [Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI](https://arxiv.org/abs/2511.17593)
*Saicharan Kolluru*

Main category: cs.LG

TL;DR: vLLM和HuggingFace TGI两大开源LLM服务框架的实证评估：vLLM在高并发下吞吐量最高可达TGI的24倍，而TGI在交互式单用户场景下尾延迟更低。


<details>
  <summary>Details</summary>
Motivation: 生产环境中部署大语言模型需要高效的推理服务系统来平衡吞吐量、延迟和资源利用率，因此需要对现有框架进行系统评估。

Method: 对vLLM和HuggingFace TGI进行多维度基准测试，包括吞吐量性能、端到端延迟、GPU内存利用率和可扩展性特性，使用LLaMA-2模型（7B到70B参数）。

Result: vLLM通过其新颖的PagedAttention机制在高并发工作负载下比TGI实现高达24倍的吞吐量提升，而TGI在交互式单用户场景下表现出更低的尾延迟。

Conclusion: 框架选择应基于具体用例需求：vLLM在高吞吐量批处理场景中表现优异，而TGI更适合具有中等并发性的延迟敏感型交互应用。

Abstract: The deployment of Large Language Models (LLMs) in production environments requires efficient inference serving systems that balance throughput, latency, and resource utilization. This paper presents a comprehensive empirical evaluation of two prominent open-source LLM serving frameworks: vLLM and HuggingFace Text Generation Inference (TGI). We benchmark these systems across multiple dimensions including throughput performance, end-to-end latency, GPU memory utilization, and scalability characteristics using LLaMA-2 models ranging from 7B to 70B parameters. Our experiments reveal that vLLM achieves up to 24x higher throughput than TGI under high-concurrency workloads through its novel PagedAttention mechanism, while TGI demonstrates lower tail latencies for interactive single-user scenarios. We provide detailed performance profiles for different deployment scenarios and offer practical recommendations for system selection based on workload characteristics. Our findings indicate that the choice between these frameworks should be guided by specific use-case requirements: vLLM excels in high-throughput batch processing scenarios, while TGI is better suited for latency-sensitive interactive applications with moderate concurrency.

</details>


### [437] [ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning](https://arxiv.org/abs/2511.18291)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: ADF-LoRA通过同步更新一个低秩矩阵并在去中心化联邦学习中混合两个矩阵，解决了交替低秩更新在去中心化环境中的相位状态不匹配和块级发散问题，实现了更快的收敛和更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 在去中心化联邦学习（DFL）中，交替更新LoRA矩阵会因客户端间的相位状态不匹配和块级发散而面临新的挑战，需要改进机制以保持参数状态的一致性。

Method: 提出ADF-LoRA方法，每轮仅同步更新一个低秩矩阵，并混合两个矩阵以在去中心化传播下维持更一致的参数状态，同时保留交替更新的交叉项抑制效果。

Result: 在多个GLUE任务上的实验表明，ADF-LoRA实现了更快、更平滑的收敛，并在任务中获得了最高的平均准确率，显著优于现有的去中心化联邦学习中的LoRA变体。

Conclusion: ADF-LoRA通过同步更新和矩阵混合机制，有效提升了去中心化联邦学习中交替低秩更新的稳定性和性能，为服务器less拓扑结构提供了更优的解决方案。

Abstract: This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin.

</details>


### [438] [CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning](https://arxiv.org/abs/2511.18611)
*Mengdi Wang,Efe Bozkir,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: CycleSL是一个新颖的免聚合分割学习框架，通过循环更新策略和特征重采样来解决并行分割学习中的可扩展性和性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有分割学习方法存在可扩展性差、服务器资源开销大、模型性能下降等问题，特别是客户端漂移和滞后导致的收敛问题。

Method: 受交替块坐标下降启发，将服务器端训练视为独立的高级机器学习任务，通过重采样客户端提取的特征来缓解异构性，采用先优化服务器模型再更新客户端的循环更新策略。

Result: 在五个公开非IID数据集上的实验表明，CycleSL能有效提升模型性能，并可无缝集成到现有方法中。

Conclusion: CycleSL通过免聚合的循环更新框架，显著提升了分割学习的可扩展性和模型性能，为解决分布式协作训练中的挑战提供了有效方案。

Abstract: Split learning emerges as a promising paradigm for collaborative distributed model training, akin to federated learning, by partitioning neural networks between clients and a server without raw data exchange. However, sequential split learning suffers from poor scalability, while parallel variants like parallel split learning and split federated learning often incur high server resource overhead due to model duplication and aggregation, and generally exhibit reduced model performance and convergence owing to factors like client drift and lag. To address these limitations, we introduce CycleSL, a novel aggregation-free split learning framework that enhances scalability and performance and can be seamlessly integrated with existing methods. Inspired by alternating block coordinate descent, CycleSL treats server-side training as an independent higher-level machine learning task, resampling client-extracted features (smashed data) to mitigate heterogeneity and drift. It then performs cyclical updates, namely optimizing the server model first, followed by client updates using the updated server for gradient computation. We integrate CycleSL into previous algorithms and benchmark them on five publicly available datasets with non-iid data distribution and partial client attendance. Our empirical findings highlight the effectiveness of CycleSL in enhancing model performance. Our source code is available at https://gitlab.lrz.de/hctl/CycleSL.

</details>


### [439] [Federated style aware transformer aggregation of representations](https://arxiv.org/abs/2511.18841)
*Mincheol Jeon,Euinam Huh*

Main category: cs.LG

TL;DR: FedSTAR是一个风格感知的联邦学习框架，通过解耦客户端特定风格因子和共享内容表示来解决个性化联邦学习中的领域异构性、数据不平衡和通信约束问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习缺乏个性化，单一全局模型无法捕捉客户端特定特征，导致预测偏差和泛化能力差，特别是在数据分布高度不同的客户端上。

Method: 使用基于Transformer的注意力机制聚合类原型，解耦客户端特定风格因子和共享内容表示，通过交换紧凑的原型和风格向量而非完整模型参数来减少通信开销。

Result: 实验结果表明，结合内容-风格解耦和注意力驱动的原型聚合在异构环境中提高了个性化程度和鲁棒性，且不增加通信成本。

Conclusion: FedSTAR通过风格感知的联邦学习方法有效解决了个性化联邦学习中的关键挑战，在保持低通信开销的同时提升了模型性能。

Abstract: Personalized Federated Learning (PFL) faces persistent challenges, including domain heterogeneity from diverse client data, data imbalance due to skewed participation, and strict communication constraints. Traditional federated learning often lacks personalization, as a single global model cannot capture client-specific characteristics, leading to biased predictions and poor generalization, especially for clients with highly divergent data distributions.
  To address these issues, we propose FedSTAR, a style-aware federated learning framework that disentangles client-specific style factors from shared content representations. FedSTAR aggregates class-wise prototypes using a Transformer-based attention mechanism, allowing the server to adaptively weight client contributions while preserving personalization.
  Furthermore, by exchanging compact prototypes and style vectors instead of full model parameters, FedSTAR significantly reduces communication overhead. Experimental results demonstrate that combining content-style disentanglement with attention-driven prototype aggregation improves personalization and robustness in heterogeneous environments without increasing communication cost.

</details>


### [440] [Practical Machine Learning for Aphasic Discourse Analysis](https://arxiv.org/abs/2511.17553)
*Jason M. Pittman,Anton Phillips,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.LG

TL;DR: 本研究评估了五种机器学习模型在失语症患者图片描述任务中自动识别正确信息单元(CIU)的能力，发现模型在区分词语方面表现优异，但在识别CIU方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管CIU分析是语言病理学家常用的失语症语言能力评估方法，但由于需要手动编码和分析，临床应用受限。本研究旨在利用机器学习技术自动化CIU识别过程。

Method: 使用五种监督机器学习模型，基于随机选择的人工编码转录本及其词语和CIU数据进行训练，评估模型在图片描述任务中识别CIU的能力。

Result: 词语识别方面所有模型都达到近乎完美的性能(0.995准确率)，但CIU识别表现差异较大，k-NN模型表现最佳(0.824准确率，0.787 AUC)。

Conclusion: 监督机器学习模型能够有效区分词语，但准确识别CIU仍然具有挑战性，表明需要进一步改进模型来可靠地自动化CIU分析。

Abstract: Analyzing spoken discourse is a valid means of quantifying language ability in persons with aphasia. There are many ways to quantify discourse, one common way being to evaluate the informativeness of the discourse. That is, given the total number of words produced, how many of those are context-relevant and accurate. This type of analysis is called Correct Information Unit (CIU) analysis and is one of the most prevalent discourse analyses used by speech-language pathologists (SLPs). Despite this, CIU analysis in the clinic remains limited due to the manual labor needed by SLPs to code and analyze collected speech. Recent advances in machine learning (ML) seek to augment such labor by automating modeling of propositional, macrostructural, pragmatic, and multimodal dimensions of discourse. To that end, this study evaluated five ML models for reliable identification of Correct Information Units (CIUs, Nicholas & Brookshire, 1993), during a picture description task. The five supervised ML models were trained using randomly selected human-coded transcripts and accompanying words and CIUs from persons with aphasia. The baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995) with high AUC range (0.914 min, 0.995 max). In contrast, CIU vs non-CIU showed a greater variability, with the k-nearest neighbor (k-NN) model the highest accuracy (0.824) and second highest AUC (0.787). These findings indicate that while the supervised ML models can distinguish word from not word, identifying CIUs is challenging.

</details>


### [441] [Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation](https://arxiv.org/abs/2511.17577)
*Fengming Yu,Qingyu Meng,Haiwei Pan,Kejia Zhang*

Main category: cs.LG

TL;DR: 提出了一种轻量级优化方法，结合动态注意力头剪枝和知识蒸馏，在保持数学推理能力的同时显著提升大语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理等复杂任务中表现出色，但计算和存储成本高昂，阻碍实际部署。需要找到既能保持推理能力又能提升效率的轻量化方法。

Method: 动态评估多头注意力机制中每个注意力头的重要性（结合权重范数和熵），实时剪枝冗余头以减少计算开销，并通过知识蒸馏将原模型信息迁移到剪枝后的学生模型中。

Result: 在Math23k数据集上，30%剪枝率下：参数减少18.7%，推理速度提升27.5%，FLOPs减少19.3%，准确率仅下降0.7%（从84.4%到83.7%）。

Conclusion: 该方法在保持强大推理性能的同时实现了显著的效率提升，为大语言模型在数学推理任务中的高效部署提供了实用解决方案。

Abstract: With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks.

</details>


### [442] [Llamazip: Leveraging LLaMA for Lossless Text Compression and Training Dataset Detection](https://arxiv.org/abs/2511.17589)
*Sören Dréano,Derek Molloy,Noel Murphy*

Main category: cs.LG

TL;DR: Llamazip是一种基于LLaMA3语言模型预测能力的无损文本压缩算法，通过仅存储模型无法预测的token来实现显著数据压缩，同时具备识别文档是否属于模型训练数据的能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种利用语言模型预测能力的高效无损压缩方法，同时解决语言模型训练数据来源识别的问题，以应对数据来源、知识产权和训练透明度等关键问题。

Method: 基于LLaMA3语言模型的预测能力，仅存储模型无法准确预测的token，分析量化技术和上下文窗口大小对压缩性能的影响。

Result: 实现了显著的数据压缩效果，同时能够识别文档是否属于语言模型的训练数据集，为数据来源追踪提供了新方法。

Conclusion: Llamazip不仅展示了语言模型在无损压缩中的有效性，还开辟了识别训练数据来源的新途径，对确保语言模型训练的透明度和数据治理具有重要意义。

Abstract: This work introduces Llamazip, a novel lossless text compression algorithm based on the predictive capabilities of the LLaMA3 language model. Llamazip achieves significant data reduction by only storing tokens that the model fails to predict, optimizing storage efficiency without compromising data integrity. Key factors affecting its performance, including quantization and context window size, are analyzed, revealing their impact on compression ratios and computational requirements. Beyond compression, Llamazip demonstrates the potential to identify whether a document was part of the training dataset of a language model. This capability addresses critical concerns about data provenance, intellectual property, and transparency in language model training.

</details>


### [443] [PocketLLM: Ultimate Compression of Large Language Models via Meta Networks](https://arxiv.org/abs/2511.17637)
*Ye Tian,Chengcheng Wang,Jing Han,Yehui Tang,Kai Han*

Main category: cs.LG

TL;DR: PocketLLM是一种通过元网络在潜在空间压缩大语言模型的新方法，使用编码器将权重投影到离散潜在向量，通过紧凑码本表示，再用轻量解码器映射回原始权重空间，实现高压缩比。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增长，在边缘设备上存储和传输变得困难，传统量化剪枝方法难以在不损失精度的情况下实现极端压缩。

Method: 提出编码器网络将LLM权重投影到离散潜在向量，用紧凑码本表示，再用轻量解码器映射回原始权重空间，仅需小型解码器、码本和索引。

Result: 实验显示PocketLLM在极高压缩比下仍保持优异性能，例如将Llama 2-7B压缩10倍而精度损失可忽略。

Conclusion: PocketLLM通过潜在空间压缩方法实现了大语言模型的高效压缩，在保持精度的同时显著减小模型体积。

Abstract: As Large Language Models (LLMs) continue to grow in size, storing and transmitting them on edge devices becomes increasingly challenging. Traditional methods like quantization and pruning struggle to achieve extreme compression of LLMs without sacrificing accuracy. In this paper, we introduce PocketLLM, a novel approach to compress LLMs in a latent space via meta-networks. A simple encoder network is proposed to project the weights of LLMs into discrete latent vectors, which are then represented using a compact codebook. A lightweight decoder network is employed to map the codebook's representative vectors back to the original weight space. This method allows for significant compression of the large weights in LLMs, consisting solely of a small decoder, a concise codebook, and an index. Extensive experiments show that PocketLLM achieves superior performance even at significantly high compression ratios, e.g., compressing Llama 2-7B by 10x with a negligible drop in accuracy.

</details>


### [444] [DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams](https://arxiv.org/abs/2511.17693)
*Ginés Carreto Picón,Peng Yuan Zhou,Qi Zhang,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: 提出了DeepCoT（深度持续Transformer），一种无冗余的编码器模型，可在现有深度编码器架构上应用，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: Transformer模型规模不断增大，但在资源受限设备上需要低延迟推理。流数据推理在滑动时间窗口上进行会导致高度冗余计算，现有持续Transformer仅适用于浅层模型，限制了其应用范围。

Method: 开发了DeepCoT，一种无冗余的编码器模型，可最小化改动应用于现有深度编码器架构，实现线性计算成本。

Result: 在音频、视频和文本流上的实验表明，DeepCoT在保持与非持续基线相当性能的同时，为所有Transformer层提供线性计算成本，相比之前的高效模型运行时间减少高达两个数量级。

Conclusion: DeepCoT成功解决了深度Transformer模型在流数据推理中的计算冗余问题，实现了高效且性能相当的持续推理。

Abstract: Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.

</details>


### [445] [Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch](https://arxiv.org/abs/2511.17826)
*Ziyang Zhang,Xinheng Ding,Jiayi Yuan,Rixin Liu,Huizi Mao,Jiarong Xing,Zirui Liu*

Main category: cs.LG

TL;DR: 提出了Tree-Based Invariant Kernels (TBIK)来解决大语言模型推理中的非确定性问题，确保在不同张量并行配置下获得比特级相同的输出结果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务框架在不同系统配置（如张量并行大小、批大小）下会产生非确定性行为，这在LLM评估、多智能体系统和强化学习等应用中造成严重问题，特别是RL训练中训练引擎和推理引擎的并行策略不匹配会导致性能下降甚至崩溃。

Method: 设计并实现了基于树的恒定内核(TBIK)，通过统一的层次化二叉树结构对齐GPU内和GPU间的归约顺序，提供TP不变的矩阵乘法和归约原语。

Result: 实验证实了在不同TP大小下实现了零概率发散和比特级可重现的确定性推理，在RL训练管道中vLLM和FSDP之间获得了比特级相同的结果。

Conclusion: TBIK有效解决了张量并行引起的非确定性问题，为大语言模型应用提供了跨不同并行策略的比特级一致性保证。

Abstract: Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.

</details>


### [446] [Majority of the Bests: Improving Best-of-N via Bootstrapping](https://arxiv.org/abs/2511.18630)
*Amin Rakhsha,Kanika Madan,Tianyu Zhang,Amir-massoud Farahmand,Amir Khasahmadi*

Main category: cs.LG

TL;DR: 提出MoB方法，通过引导采样估计BoN的输出分布并选择众数，在奖励模型不完美时比BoN表现更好


<details>
  <summary>Details</summary>
Motivation: 在奖励模型不完美时，BoN方法无法可靠找到正确答案，性能急剧下降。虽然正确答案在输出分布中概率不高，但通常是众数

Method: MoB通过引导采样估计BoN的输出分布，然后选择该分布的众数作为最终输出

Result: 在5个基准测试、3种基础LLM和2种奖励模型的30个设置中，25个设置表现优于BoN

Conclusion: MoB是BoN和自一致性的简单而强大的替代方案，激励对更细致选择机制的研究

Abstract: Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.

</details>


### [447] [How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining](https://arxiv.org/abs/2511.18903)
*Kairong Luo,Zhenbo Sun,Haodong Wen,Xinyu Shi,Jiarui Cui,Chenyi Dang,Kaifeng Lyu,Wenguang Chen*

Main category: cs.LG

TL;DR: 研究发现课程式预训练效果受限的原因是数据质量升序排列与学习率衰减计划不兼容，提出了两种简单策略来缓解这一问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据稀缺，LLM通常在混合质量数据上训练。课程式预训练（按数据质量升序排列）理论上应能更好利用高质量数据，但先前研究显示其改进有限。

Method: 识别出课程式训练与学习率衰减计划的不兼容性，提出两种策略：(1)使用更温和的学习率衰减计划；(2)用模型平均替代学习率衰减。在1.5B参数模型上验证，训练30B tokens。

Result: 结合两种策略后，在标准基准测试上的平均得分比随机洗牌提高了1.64%，无需额外数据精炼。

Conclusion: 研究呼吁重新评估课程式LLM预训练方法，并强调数据课程与优化方法协同设计的潜力。

Abstract: Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.

</details>


### [448] [SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression](https://arxiv.org/abs/2511.18936)
*Santhosh G S,Saurav Prakash,Balaraman Ravindran*

Main category: cs.LG

TL;DR: SWAN是一种无需微调的KV缓存压缩框架，通过正交矩阵旋转和剪枝来减少内存占用，无需解压缩步骤，在50-60%内存节省下保持接近原始性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自回归推理时面临KV缓存内存占用过大的瓶颈，现有压缩技术存在信息丢失、固定限制或解压缩计算开销等问题。

Method: 使用离线正交矩阵对KV缓存进行旋转和剪枝，压缩后的缓存可直接用于注意力计算，无需重构。结合小密集缓冲区实现运行时可调的压缩级别。

Result: 实验表明SWAN在每token节省50-60%KV缓存内存的情况下，仍能保持接近未压缩基线的性能。

Conclusion: SWAN提供了一种无解压缩设计、高压缩性能下的良好表现以及适应性的实用解决方案，适用于长上下文LLM服务。

Abstract: Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.

</details>


### [449] [RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning](https://arxiv.org/abs/2511.19168)
*Deyi Ji,Yuekui Yang,Liqun Liu,Peng Shu,Haiyang Wu,Shaogang Tang,Xudong Chen,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.LG

TL;DR: RAVEN++是一个改进的视频广告审核框架，通过主动强化学习、细粒度违规理解和渐进式多阶段训练，显著提升了违规检测的精确性、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频广告审核模型（如RAVEN）在细粒度理解、可解释性和泛化能力方面存在不足，需要更精确的违规定位和更好的推理能力。

Method: 1) 主动强化学习动态适应不同难度样本；2) 通过分层奖励函数和推理蒸馏实现细粒度违规理解；3) 渐进式多阶段训练结合知识注入、课程式被动强化学习和主动强化学习。

Result: 在公开和专有数据集上的实验表明，RAVEN++在细粒度违规理解、推理能力和泛化能力方面优于通用大语言模型和专门模型如RAVEN，在线A/B测试也验证了其有效性。

Conclusion: RAVEN++通过创新的训练策略和架构设计，成功解决了视频广告审核中的关键挑战，为复杂广告内容的精确审核提供了有效解决方案。

Abstract: Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.

</details>


### [450] [A Nutrition Multimodal Photoplethysmography Language Model](https://arxiv.org/abs/2511.19260)
*Kyle Verrier,Achille Nazaret,Joseph Futoma,Andrew C. Miller,Guillermo Sapiro*

Main category: cs.LG

TL;DR: 提出了一种结合可穿戴设备PPG信号和饮食描述的NPLM模型，通过将生理信号与饮食上下文结合，显著提升了日常热量摄入预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 饥饿和饱腹感动态影响饮食行为和代谢健康，但在日常环境中难以捕捉。需要开发非侵入性的大规模饮食监测方法。

Method: 开发了NPLM模型，将可穿戴设备的连续PPG信号投影到语言模型可解释的嵌入空间，使模型能够同时对生理信号和饮食上下文进行联合推理。基于19,340名参与者和110万餐食-PPG对进行训练。

Result: 模型将日常热量摄入预测准确率比纯文本基线提高了11%，即使去除80%的餐食文本信息，准确性仍能保持。在独立验证研究(n=140)中重现了这些发现。

Conclusion: 研究证明了将消费级可穿戴设备的生理测量与饮食信息相结合，对于大规模非侵入性饮食监测具有重要价值。

Abstract: Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.

</details>


### [451] [CDLM: Consistency Diffusion Language Models For Faster Sampling](https://arxiv.org/abs/2511.19269)
*Minseo Kim,Chenfeng Xu,Coleman Hooper,Harman Singh,Ben Athiwaratkun,Ce Zhang,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: CDLM通过一致性建模和块级因果注意力掩码，解决了扩散语言模型推理速度慢的问题，实现了3.6-14.5倍的延迟降低，同时保持数学和编程任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然提供了并行生成的优势，但由于需要大量细化步骤且无法使用标准KV缓存，导致推理速度缓慢。

Method: CDLM结合一致性建模来大幅减少采样步骤，实现多令牌最终化；同时在微调中强制使用块级因果注意力掩码，使模型完全兼容KV缓存。

Result: 实验显示CDLM在数学和编程任务上实现了3.6-14.5倍的延迟降低，同时保持了竞争力的准确率。

Conclusion: CDLM成功解决了扩散语言模型的推理瓶颈，为并行生成提供了高效的解决方案。

Abstract: Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.

</details>


### [452] [MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings](https://arxiv.org/abs/2511.19279)
*Victor Rambaud,Salvador Mascarenhas,Yair Lakretz*

Main category: cs.LG

TL;DR: MapFormers是基于Transformer的新架构，能够从观测数据中学习认知地图并并行执行路径整合，通过输入依赖的位置编码实现结构与内容的解耦，在OOD泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏人类和动物所具有的认知地图能力，这种能力能够编码实体间的抽象关系，提供强大的OOD泛化能力。

Method: 开发了两种MapFormers变体，统一了绝对和相对位置编码来分别建模情景记忆和工作记忆，通过输入依赖的矩阵更新Transformer的位置编码来实现结构-内容解耦。

Result: 在包括经典2D导航任务在内的多个任务上测试，MapFormers能够学习底层空间的认知地图，并在OOD泛化（如更长的序列）方面实现近乎完美的性能，优于现有架构。

Conclusion: 结果表明，设计用于学习认知地图的模型具有优越性，输入依赖的位置编码为Transformer引入结构偏置以实现结构-内容解耦具有重要意义，MapFormers在神经科学和AI领域都有广泛应用前景。

Abstract: A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.

</details>


### [453] [Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric](https://arxiv.org/abs/2511.19350)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.LG

TL;DR: 提出了一种可扩展的光谱方法，用于自动估计短文本嵌入的聚类数量，并引入了Cohesion Ratio作为无监督评估指标。


<details>
  <summary>Details</summary>
Motivation: 短文本嵌入聚类是NLP的基础任务，但传统方法需要预先指定聚类数量，这在实际应用中具有挑战性。

Method: 使用余弦相似度构建拉普拉斯特征谱，通过自适应采样策略直接从特征谱结构估计聚类数量。

Result: 在六个短文本数据集和四种嵌入模型上的实验表明，结合该估计器的K-Means和HAC算法显著优于HDBSCAN、OPTICS和Leiden等参数较少的方法。

Conclusion: 该光谱估计器和Cohesion Ratio为短文本数据的无监督组织和评估提供了实用价值。

Abstract: Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.

</details>


### [454] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: BOOD框架通过扩散模型在潜在空间中生成边界外分布数据，显著提升了OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在潜在空间中提取分布外有效特征，因为难以识别类别间的决策边界。

Method: 首先学习文本条件的潜在特征空间，选择靠近决策边界的ID特征，扰动使其跨越边界形成OOD特征，然后用扩散模型解码为像素空间图像。

Result: 在CIFAR-100数据集上，FPR95平均降低29.64%(40.31% vs 10.67%)，AUROC平均提升7.27%(90.15% vs 97.42%)。

Conclusion: BOOD提供了一种更高效的训练策略来合成信息丰富的OOD特征，能更清晰地区分ID和OOD数据。

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training data based on latent space features has proven effective in enhancing out-of-distribution (OOD) detection performance. However, extracting effective features outside the in-distribution (ID) boundary in latent space remains challenging due to the difficulty of identifying decision boundaries between classes. This paper proposes a novel framework called Boundary-based Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD features and generates human-compatible outlier images using diffusion models. BOOD first learns a text-conditioned latent feature space from the ID dataset, selects ID features closest to the decision boundary, and perturbs them to cross the decision boundary to form OOD features. These synthetic OOD features are then decoded into images in pixel space by a diffusion model. Compared to previous works, BOOD provides a more training efficient strategy for synthesizing informative OOD features, facilitating clearer distinctions between ID and OOD data. Extensive experimental results on common benchmarks demonstrate that BOOD surpasses the state-of-the-art method significantly, achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27% improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [455] [Saving Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2511.16520)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: cs.LG

TL;DR: FMPlug是一个插件框架，通过实例引导的时间相关预热启动策略和锐利高斯性正则化，显著提升了基础流匹配模型在逆问题求解中的性能。


<details>
  <summary>Details</summary>
Motivation: 基础流匹配模型在逆问题求解中表现不如领域特定甚至无训练先验，需要解锁其潜力以成为实用的通用先验。

Method: 结合实例引导的时间相关预热启动策略和锐利高斯性正则化，在保持高斯结构的同时添加问题特定指导。

Result: 在图像恢复和科学逆问题中实现了显著的性能提升。

Conclusion: 为将基础流匹配模型转变为实用的、可重用的逆问题求解先验指明了路径。

Abstract: Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors. How can we unlock their potential? We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs. FMPlug combines an instance-guided, time-dependent warm-start strategy with a sharp Gaussianity regularization, adding problem-specific guidance while preserving the Gaussian structures. This leads to a significant performance boost across image restoration and scientific IPs. Our results point to a path for making foundation FM models practical, reusable priors for IP solving.

</details>


### [456] [Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks](https://arxiv.org/abs/2511.17564)
*Guilherme Grancho D. Fernandes,Marco A. Barroca,Mateus dos Santos,Rafael S. Oliveira*

Main category: cs.LG

TL;DR: 使用双向LSTM网络对PLAsTiCC数据集中的瞬变天体光变曲线进行分类，将14个类别重组为5个通用类别以解决类别不平衡问题。模型在S-Like和Periodic类别表现良好，但在Fast和Long类别表现较差，且难以区分Periodic和Non-Periodic对象。


<details>
  <summary>Details</summary>
Motivation: 解决瞬变天体光变曲线分类中的类别不平衡问题，并评估模型在有限时间信息下的性能表现。

Method: 采用双向LSTM神经网络，通过填充、时间重缩放和通量归一化进行预处理，使用掩码层处理变长序列，在19,920个测试对象上训练和评估。

Result: S-Like和Periodic类别的ROC AUC分别为0.95和0.99，PR AUC分别为0.98和0.89；但Fast和Long类别表现较差（Long类别ROC AUC仅0.68）。在部分光变曲线数据（5、10、20天）上性能显著下降。

Conclusion: 类别不平衡和有限时间信息是主要限制因素，建议采用类别平衡策略和关注检测时刻的预处理技术来改进性能。

Abstract: This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance.

</details>


### [457] [EgoCogNav: Cognition-aware Human Egocentric Navigation](https://arxiv.org/abs/2511.17581)
*Zhiwen Qiu,Ziang Liu,Wenqian Niu,Tapomayukh Bhattacharjee,Saleh Kalantari*

Main category: cs.LG

TL;DR: 提出了EgoCogNav多模态自我中心导航框架，通过融合场景特征和感官线索来预测感知路径不确定性，并联合预测轨迹和头部运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注在完全观察场景中的运动预测，往往忽略了捕捉人们对空间感受和反应的人类因素。

Method: 提出EgoCogNav框架，将感知路径不确定性作为潜在状态进行预测，通过融合场景特征与感官线索来联合预测轨迹和头部运动。

Result: EgoCogNav学习到的感知不确定性与人类行为（如扫描、犹豫、回溯）高度相关，并能泛化到未见过的环境。

Conclusion: EgoCogNav能够有效建模人类导航中的认知和体验因素，为理解人-环境交互和实现安全社交导航提供了新方法。

Abstract: Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.

</details>


### [458] [Learning Straight Flows: Variational Flow Matching for Efficient Generation](https://arxiv.org/abs/2511.17583)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Xiao Wang,Yanning Shen*

Main category: cs.LG

TL;DR: 提出S-VFM方法，通过引入变分潜码来强制轨迹直线化，解决Flow Matching中一步生成的局限性问题


<details>
  <summary>Details</summary>
Motivation: Flow Matching依赖学习的弯曲轨迹，难以实现一步生成。现有方法存在离散近似误差、训练不稳定和收敛困难等问题

Method: 将变分潜码（代表"生成概览"）整合到Flow Matching框架中，显式强制轨迹直线化，产生线性生成路径

Result: 在三个挑战性基准测试中取得有竞争力的性能，在训练和推理效率上优于现有方法

Conclusion: S-VFM通过引入变分潜码有效解决了Flow Matching的轨迹弯曲问题，实现了更高效的一步生成

Abstract: Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \textbf{S}traight \textbf{V}ariational \textbf{F}low \textbf{M}atching (\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.

</details>


### [459] [PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis](https://arxiv.org/abs/2511.17585)
*Kang He,Boyu Chen,Yuzhe Ding,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.LG

TL;DR: PaSE框架通过原型对齐校准和Shapley优化均衡来解决多模态情感分析中的模态竞争问题，提升模态间协作性能


<details>
  <summary>Details</summary>
Motivation: 多模态融合中常见模态竞争现象，主导模态会压制弱势模态，导致性能不佳

Method: 采用原型引导校准学习(PCL)精炼单模态表示，通过熵最优传输机制确保语义一致性；引入双阶段优化策略，先使用原型门控融合提取共享表示，再通过Shapley梯度调制(SGM)自适应调整各模态梯度

Result: 在IEMOCAP、MOSI和MOSEI数据集上的广泛实验证实PaSE实现了优越性能并有效缓解了模态竞争

Conclusion: PaSE框架通过原型对齐和Shapley优化有效解决了多模态情感分析中的模态竞争问题

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal performance.In this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition.

</details>


### [460] [CubeletWorld: A New Abstraction for Scalable 3D Modeling](https://arxiv.org/abs/2511.17664)
*Azlaan Mustafa Samad,Hoang H. Nguyen,Lukas Berg,Henrik Müller,Yuan Xue,Daniel Kudenko,Zahra Ahmadi*

Main category: cs.LG

TL;DR: CubeletWorld是一个新颖的城市环境建模框架，通过离散化的3D网格单元（cubelets）来表示和分析城市环境，支持隐私保护的规划、导航和占用预测任务。


<details>
  <summary>Details</summary>
Motivation: 现代城市产生大量异构数据，但将这些数据整合成连贯的空间模型仍然具有挑战性。现有基于智能体感知的方法存在可扩展性限制和隐私问题。

Method: 提出CubeletWorld框架，将城市环境离散化为3D网格单元，将基础设施、移动和环境指标等多样化数据嵌入到局部化的cubelet状态中。

Result: 开发了CubeletWorld状态预测任务，评估了适用于该设置的改进核心模型，分析了空间粒度增加带来的稀疏性表示和基线可扩展性挑战。

Conclusion: CubeletWorld提供了一个灵活可扩展的框架，用于从复杂城市数据中学习，为跨区域的社会人口建模、环境监测和应急响应等领域的可扩展仿真和决策支持开辟了新可能性。

Abstract: Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here.

</details>


### [461] [pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data](https://arxiv.org/abs/2511.18066)
*Md Akil Raihan Iftee,Syed Md. Ahnaf Hasan,Mir Sazzat Hossain,Rakibul Hasan Rajib,Amin Ahsan Ali,AKM Mahbubur Rahman,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 提出了pFedBBN框架，用于解决联邦学习中的测试时适应问题，特别是在类别不平衡场景下，通过平衡批归一化和基于相似度的客户端协作来提升模型鲁棒性和少数类性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的测试时适应面临类别不平衡的挑战，现有方法通常依赖标注数据或客户端协调，无法在无监督条件下处理动态领域变化和分布偏移。

Method: 使用平衡批归一化进行本地客户端适应，通过BBN相似度指导客户端协作，采用类别感知的模型聚合策略，支持完全无监督的本地适应。

Result: 在多个基线方法上的广泛实验表明，pFedBBN在鲁棒性和少数类性能方面持续优于最先进的联邦学习和测试时适应方法。

Conclusion: pFedBBN通过平衡特征归一化和领域感知协作，有效解决了分布偏移和类别不平衡问题，无需客户端提供任何标注或原始数据。

Abstract: Test-time adaptation (TTA) in federated learning (FL) is crucial for handling unseen data distributions across clients, particularly when faced with domain shifts and skewed class distributions. Class Imbalance (CI) remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed CI during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none address class unsupervised adaptation to dynamic domains or distribution shifts at inference time under federated CI constraints. Revealing the failure of state-of-the-art TTA in federated client adaptation in CI scenario, we propose pFedBBN,a personalized federated test-time adaptation framework that employs balanced batch normalization (BBN) during local client adaptation to mitigate prediction bias by treating all classes equally, while also enabling client collaboration guided by BBN similarity, ensuring that clients with similar balanced representations reinforce each other and that adaptation remains aligned with domain-specific characteristics. pFedBBN supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Extensive experiments across diverse baselines show that pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods.

</details>


### [462] [Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj](https://arxiv.org/abs/2511.18248)
*Wei Zhen Teoh*

Main category: cs.LG

TL;DR: 提出了CausalTraj模型，用于联合预测多个交互智能体的轨迹，强调联合评估指标而非单智能体指标，在体育数据分析中生成更真实的多智能体场景。


<details>
  <summary>Details</summary>
Motivation: 现有模型主要基于单智能体精度指标(minADE, minFDE)进行优化，忽视了预测轨迹能否共同形成合理的多智能体未来，导致在联合预测和生成连贯的多智能体场景方面表现不佳。

Method: CausalTraj是一个基于时间因果关系的似然模型，专门设计用于生成联合概率的多智能体轨迹预测。

Result: 在NBA SportVU、Basketball-U和Football-U数据集上，CausalTraj在单智能体精度上表现竞争性，在联合指标(minJADE, minJFDE)上取得最佳记录结果，并能生成定性的连贯和真实的游戏演化。

Conclusion: CausalTraj通过强调联合评估指标，能够更好地评估集体建模能力，生成更连贯和真实的多智能体轨迹预测。

Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.

</details>


### [463] [From Tables to Signals: Revealing Spectral Adaptivity in TabPFN](https://arxiv.org/abs/2511.18278)
*Jianqiao Zheng,Cameron Gordon,Yiping Ji,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 本文通过信号重构视角分析TabPFN，发现其具有比标准ReLU-MLP更宽的频率容量，且频谱能力能根据上下文样本数量自适应调整，无需超参数调优即可完成图像去噪任务。


<details>
  <summary>Details</summary>
Motivation: 理解任务无关的表格基础模型（如TabPFN）的归纳偏置来源，这些模型在表格学习任务中表现出色但其内在机制尚不清楚。

Method: 通过信号重构和频率分析研究TabPFN的上下文学习行为，比较其与标准ReLU-MLP的频率容量差异，分析位置编码对频率响应的影响。

Result: TabPFN具有比标准ReLU-MLP更宽的有效频率容量，其频谱能力能根据上下文样本数量自适应调整（称为频谱适应性），位置编码调节频率响应，且能实现无需训练和超参数调优的图像去噪。

Conclusion: 该分析为表格基础模型的结构和归纳偏置提供了新见解，并突显了其在更广泛信号重构任务中的潜力。

Abstract: Task-agnostic tabular foundation models such as TabPFN have achieved impressive performance on tabular learning tasks, yet the origins of their inductive biases remain poorly understood. In this work, we study TabPFN through the lens of signal reconstruction and provide the first frequency-based analysis of its in-context learning behavior. We show that TabPFN possesses a broader effective frequency capacity than standard ReLU-MLPs, even without hyperparameter tuning. Moreover, unlike MLPs whose spectra evolve primarily over training epochs, we find that TabPFN's spectral capacity adapts directly to the number of samples provided in-context, a phenomenon we term Spectral Adaptivity. We further demonstrate that positional encoding modulates TabPFN's frequency response, mirroring classical results in implicit neural representations. Finally, we show that these properties enable TabPFN to perform training-free and hyperparameter-free image denoising, illustrating its potential as a task-agnostic implicit model. Our analysis provides new insight into the structure and inductive biases of tabular foundation models and highlights their promise for broader signal reconstruction tasks.

</details>


### [464] [TRIDENT: A Trimodal Cascade Generative Framework for Drug and RNA-Conditioned Cellular Morphology Synthesis](https://arxiv.org/abs/2511.18287)
*Rui Peng,Ziru Liu,Lingyuan Ye,Yuxing Lu,Boxin Shi,Jinzhuo Wang*

Main category: cs.LG

TL;DR: TRIDENT是一个级联生成框架，通过同时考虑扰动和相应的基因表达谱来合成真实的细胞形态，显著优于现有方法，在未见化合物上表现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于建模直接关联（如扰动→RNA或扰动→形态），而忽略了RNA到形态的关键因果联系，这限制了构建AI虚拟细胞的能力。

Method: 提出了TRIDENT级联生成框架，构建了MorphoGene数据集（包含98种化合物的L1000基因表达和Cell Painting图像配对），通过RNA条件化来合成细胞形态。

Result: TRIDENT显著优于最先进方法，实现了高达7倍的改进，在未见化合物上表现出强泛化能力。案例研究验证了RNA引导合成能准确产生相应表型。

Conclusion: 通过显式建模转录组-表型组映射，TRIDENT提供了一个强大的计算机模拟工具，使我们更接近预测性虚拟细胞。

Abstract: Accurately modeling the relationship between perturbations, transcriptional responses, and phenotypic changes is essential for building an AI Virtual Cell (AIVC). However, existing methods typically constrained to modeling direct associations, such as Perturbation $\rightarrow$ RNA or Perturbation $\rightarrow$ Morphology, overlook the crucial causal link from RNA to morphology. To bridge this gap, we propose TRIDENT, a cascade generative framework that synthesizes realistic cellular morphology by conditioning on both the perturbation and the corresponding gene expression profile. To train and evaluate this task, we construct MorphoGene, a new dataset pairing L1000 gene expression with Cell Painting images for 98 compounds. TRIDENT significantly outperforms state-of-the-art approaches, achieving up to 7-fold improvement with strong generalization to unseen compounds. In a case study on docetaxel, we validate that RNA-guided synthesis accurately produces the corresponding phenotype. An ablation study further confirms that this RNA conditioning is essential for the model's high fidelity. By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves us closer to a predictive virtual cell.

</details>


### [465] [Auxiliary Gene Learning: Spatial Gene Expression Estimation by Auxiliary Gene Selection](https://arxiv.org/abs/2511.18336)
*Kaito Shiku,Kazuya Nishimura,Shinnosuke Matsuo,Yasuhiro Kojima,Ryoma Bise*

Main category: cs.LG

TL;DR: 提出AGL方法，通过将忽略基因的表达估计重新定义为辅助任务并与主要任务联合训练，利用被忽略基因的潜在价值。为解决辅助基因选择问题，开发了DkGSB方法，通过先验知识和双层优化实现可微的top-k基因选择。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学(ST)技术存在大量观测噪声，传统方法仅使用高变异基因子集进行训练和评估，忽略了其他基因的潜在贡献。由于基因间存在共表达关系，低表达基因可能对目标基因估计仍有帮助。

Method: AGL方法将忽略基因的表达估计重新定义为辅助任务，与主要任务联合训练。为解决辅助基因选择难题，提出DkGSB方法：利用先验知识对基因排序，通过双层优化将组合选择问题松弛为可微的top-k选择问题。

Result: 实验证实了整合辅助基因的有效性，所提方法在性能上优于传统的辅助任务学习方法。

Conclusion: 通过合理选择辅助基因并联合训练，能够充分利用被忽略基因的潜在价值，提升空间转录组学数据分析的性能。

Abstract: Spatial transcriptomics (ST) is a novel technology that enables the observation of gene expression at the resolution of individual spots within pathological tissues. ST quantifies the expression of tens of thousands of genes in a tissue section; however, heavy observational noise is often introduced during measurement. In prior studies, to ensure meaningful assessment, both training and evaluation have been restricted to only a small subset of highly variable genes, and genes outside this subset have also been excluded from the training process. However, since there are likely co-expression relationships between genes, low-expression genes may still contribute to the estimation of the evaluation target. In this paper, we propose $Auxiliary \ Gene \ Learning$ (AGL) that utilizes the benefit of the ignored genes by reformulating their expression estimation as auxiliary tasks and training them jointly with the primary tasks. To effectively leverage auxiliary genes, we must select a subset of auxiliary genes that positively influence the prediction of the target genes. However, this is a challenging optimization problem due to the vast number of possible combinations. To overcome this challenge, we propose Prior-Knowledge-Based Differentiable Top-$k$ Gene Selection via Bi-level Optimization (DkGSB), a method that ranks genes by leveraging prior knowledge and relaxes the combinatorial selection problem into a differentiable top-$k$ selection problem. The experiments confirm the effectiveness of incorporating auxiliary genes and show that the proposed method outperforms conventional auxiliary task learning approaches.

</details>


### [466] [Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems](https://arxiv.org/abs/2511.18417)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 本文提出了类别等变神经网络(CENNs)的统一理论，将群/群胚等变网络、偏序集/格等变网络、图和层神经网络统一起来。通过拓扑类别和Radon测度构建等变性，并证明了等变通用逼近定理。


<details>
  <summary>Details</summary>
Motivation: 扩展等变深度学习的范围，超越群作用，包含几何对称性以及上下文和组合对称性。

Method: 在具有Radon测度的拓扑类别中制定等变性，构建线性和非线性层的类别设置，并证明有限深度CENNs在连续等变变换空间中的稠密性。

Result: 建立了类别等变深度学习的统一框架，并系统地推导了群/群胚、偏序集/格、图和细胞层的通用逼近定理。

Conclusion: 类别等变深度学习能够扩展等变深度学习的视野，不仅包含几何对称性，还包含上下文和组合对称性。

Abstract: We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.

</details>


### [467] [Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels](https://arxiv.org/abs/2511.18457)
*Duncan Stothers,Ben Stothers,Emily Schaeffer,Kishore Mulpuri*

Main category: cs.LG

TL;DR: 开发了一种基于超声优先、辐射保护策略的DDH诊断方法，通过预训练模态特定编码器、测量一致性预测头以及校准延迟规则，实现在保证覆盖率的条件下选择性进行X射线检查。


<details>
  <summary>Details</summary>
Motivation: 减少发育性髋关节发育不良(DDH)诊断中对X射线的依赖，通过超声优先策略在保证诊断准确性的同时降低辐射暴露。

Method: 1) 使用SimSiam在大型无标签数据集上预训练模态特定编码器；2) 冻结骨干网络，训练小型测量一致性预测头；3) 应用单边符合延迟规则校准超声预测，提供有限样本覆盖率保证。

Result: 超声测量误差适中（alpha MAE约9.7度，覆盖率MAE约14.0%），X射线测量误差较小（AI MAE约7.6度，CE MAE约8.9度）。校准后的超声策略在不同规则设置下实现可调的选择性成像。

Conclusion: 该方法提供了一个简单可复现的流程，将有限标签转化为可解释的测量结果和可调的选择性成像曲线，适合临床交接和未来外部验证。

Abstract: We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.
  We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.

</details>


### [468] [SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation](https://arxiv.org/abs/2511.18468)
*Md Akil Raihan Iftee,Mir Sazzat Hossain,Rakibul Hasan Rajib,Tariq Iqbal,Md Mofijul Islam,M Ashraful Amin,Amin Ahsan Ali,AKM Mahbubur Rahman*

Main category: cs.LG

TL;DR: 提出SloMo-Fast框架，这是一个无需源数据的双教师持续测试时适应方法，通过慢教师和快教师的互补机制解决长期遗忘问题，并在循环域偏移场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖源数据或原型，在隐私敏感和资源受限环境中适用性有限，且存在长期遗忘问题，导致在先前遇到域上的性能下降。

Method: SloMo-Fast采用双教师框架：慢教师缓慢遗忘并保留长期知识确保泛化能力，快教师快速适应新域并积累跨域知识。还提出了Cyclic-TTA基准测试循环域偏移。

Result: 在Cyclic-TTA和其他十个CTTA设置中，SloMo-Fast始终优于最先进方法，展现出在演化和重访域上的适应和泛化能力。

Conclusion: SloMo-Fast通过双教师机制有效解决了CTTA中的长期遗忘问题，在无需源数据的情况下实现了优异的适应性和泛化性能。

Abstract: Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains.

</details>


### [469] [TimePre: Bridging Accuracy, Efficiency, and Stability in Probabilistic Time-Series Forecasting](https://arxiv.org/abs/2511.18539)
*Lingyu Jiang,Lingyu Xu,Peiran Li,Qianwen Ge,Dingyi Zhuang,Shuo Xing,Wenjing Chen,Xiangbo Gao,Ting-Hsuan Chen,Xueying Zhan,Xin Zhang,Ziming Zhang,Zhengzhong Tu,Michael Zielewski,Kazunori Yamada,Fangzhou Lin*

Main category: cs.LG

TL;DR: TimePre是一个新的概率时间序列预测框架，通过稳定实例归一化(SIN)解决了MLP骨干网络与多选择学习(MCL)结合时的训练不稳定性和假设崩溃问题，实现了高效、准确且稳定的预测。


<details>
  <summary>Details</summary>
Motivation: 现有的概率时间序列预测方法中，基于扩散的生成模型计算成本高，而非采样的MCL方法虽然高效但存在严重的训练不稳定性和假设崩溃问题，特别是在与高效MLP骨干网络结合时问题更加严重。

Method: 提出TimePre框架，核心是稳定实例归一化(SIN)层，通过修正通道级统计偏移来稳定混合架构，彻底解决灾难性的假设崩溃问题。

Result: 在六个基准数据集上的实验表明，TimePre在关键概率指标上达到新的最先进准确率，推理速度比基于采样的模型快几个数量级，并表现出稳定的性能扩展。

Conclusion: TimePre在概率预测中成功弥合了准确性、效率和稳定性之间的长期差距。

Abstract: Probabilistic Time-Series Forecasting (PTSF) is critical for uncertainty-aware decision making, but existing generative models, such as diffusion-based approaches, are computationally prohibitive due to expensive iterative sampling. Non-sampling frameworks like Multiple Choice Learning (MCL) offer an efficient alternative, but suffer from severe training instability and hypothesis collapse, which has historically hindered their performance. This problem is dramatically exacerbated when attempting to combine them with modern, efficient MLP-based backbones. To resolve this fundamental incompatibility, we propose TimePre, a novel framework that successfully unifies the efficiency of MLP-based models with the distributional flexibility of the MCL paradigm. The core of our solution is Stabilized Instance Normalization (SIN), a novel normalization layer that explicitly remedies this incompatibility. SIN stabilizes the hybrid architecture by correcting channel-wise statistical shifts, definitively resolving the catastrophic hypothesis collapse. Extensive experiments on six benchmark datasets demonstrate that TimePre achieves new state-of-the-art accuracy on key probabilistic metrics. Critically, TimePre achieves inference speeds orders of magnitude faster than sampling-based models and, unlike prior MCL work, demonstrates stable performance scaling. It thus bridges the long-standing gap between accuracy, efficiency, and stability in probabilistic forecasting.

</details>


### [470] [Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers](https://arxiv.org/abs/2511.18670)
*Rowan Bradbury,Aniket Srinivasan Ashok,Sai Ram Kasanagottu,Gunmay Jhingran,Shuai Meng*

Main category: cs.LG

TL;DR: 提出确定性连续替换(DCR)方法，通过确定性退火权重混合教师和学生输出，解决预训练模型中模块替换时的优化稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 替换预训练模型中的模块（特别是将二次自注意力替换为高效注意力）存在优化困难：冷启动重新初始化会破坏冻结骨干网络的稳定性。

Method: 确定性连续替换(DCR)：使用确定性退火权重混合教师和学生输出，消除随机替换中门控引起的梯度方差。

Result: 在单种子研究中，DCR比随机门控和蒸馏基线在受控注意力替换任务上实现了更快的收敛和更强的对齐。

Conclusion: DCR为异构算子交换建立了基础，解决了模块替换中的核心稳定性挑战。

Abstract: Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps.

</details>


### [471] [VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking](https://arxiv.org/abs/2511.18692)
*Kichang Yang,Seonjun Kim,Minjae Kim,Nairan Zhang,Chi Zhang,Youngki Lee*

Main category: cs.LG

TL;DR: Neuron Chunking是一种I/O高效的激活稀疏化方法，通过将神经元重要性估计与存储访问成本相结合，在边缘设备上显著提升大型视觉语言模型的权重卸载性能。


<details>
  <summary>Details</summary>
Motivation: 传统的激活稀疏化方法仅基于激活幅度选择神经元，忽略了访问模式对闪存性能的影响，导致I/O效率低下。

Method: 提出神经元分块策略，在内存中对连续神经元进行分组，通过轻量级抽象建模I/O延迟，选择具有高效用（神经元重要性除以估计延迟）的分块。

Result: 在Jetson Orin Nano和Jetson AGX Orin上分别实现了4.65倍和5.76倍的I/O效率提升。

Conclusion: 通过将稀疏化决策与底层存储行为对齐，Neuron Chunking显著提高了边缘设备上大型视觉语言模型的权重卸载效率。

Abstract: Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.

</details>


### [472] [GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction](https://arxiv.org/abs/2511.18716)
*Zesheng Liu,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: GRIT-LP是一个专门用于极地雷达图像中冰层厚度估计的图变换器，通过分区空间图构建和长程跳跃连接机制，解决了深度图变换器的过平滑和长程依赖建模问题，在均方根误差上比现有方法提升了24.92%。


<details>
  <summary>Details</summary>
Motivation: 准确估计冰层厚度对于理解积雪积累、重建过去气候模式以及减少未来冰盖演化和海平面上升预测的不确定性至关重要。当前图变换器在深度上受到过平滑和弱长程依赖建模的限制。

Method: GRIT-LP结合归纳几何图学习和自注意力机制，采用分区空间图构建策略形成重叠的完全连接局部邻域以保持空间一致性并抑制不相关长程链接的噪声，同时在变换器内部引入长程跳跃连接机制改善信息流并减轻深层注意力层的过平滑。

Result: 实验表明GRIT-LP在均方根误差上比当前最先进方法提升了24.92%，显著优于现有方法。

Conclusion: 图变换器通过捕获局部结构特征和跨内部冰层的长程依赖，在建模时空模式方面表现出有效性，展示了在推进冰冻圈过程数据驱动理解方面的潜力。

Abstract: Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes.

</details>


### [473] [Sampling Control for Imbalanced Calibration in Semi-Supervised Learning](https://arxiv.org/abs/2511.18773)
*Senmao Tian,Xiang Wei,Shunli Zhang*

Main category: cs.LG

TL;DR: SC-SSL是一个解决半监督学习中类别不平衡问题的统一框架，通过解耦采样控制来抑制模型偏差，在训练和推理阶段分别处理特征级和权重级的不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理类别不平衡时通常粗粒度地调整logits，混淆了数据不平衡与不同类别学习难度差异导致的偏差问题，需要更精细的解决方案。

Method: 提出SC-SSL框架：1）训练阶段通过具有显式扩展能力的分类器和自适应调整采样概率来缓解特征级不平衡；2）推理阶段分析线性分类器的权重不平衡，应用后处理采样控制并优化偏置向量来直接校准logits。

Result: 在多个基准数据集和不同分布设置下的广泛实验验证了SC-SSL的一致性和最先进性能。

Conclusion: SC-SSL通过解耦采样控制有效解决了半监督学习中的类别不平衡问题，在训练和推理阶段分别处理不同层面的偏差，取得了优异性能。

Abstract: Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL.

</details>


### [474] [Robust and Generalizable GNN Fine-Tuning via Uncertainty-aware Adapter Learning](https://arxiv.org/abs/2511.18859)
*Bo Jiang,Weijun Zhao,Beibei Wang,Xiao Wang,Jin Tang*

Main category: cs.LG

TL;DR: 提出UAdapterGNN方法，通过集成不确定性学习到GNN适配器中，增强预训练GNN模型在微调过程中对噪声图数据的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AdapterGNN方法容易受到图数据中各种噪声（如噪声边和模糊节点属性）的影响，泛化能力有限，需要增强GNN微调的鲁棒性和泛化能力。

Method: 使用高斯概率适配器来增强预训练GNN模型，当图包含各种噪声时，该方法能自动吸收高斯分布方差变化的影响，从而显著提高模型鲁棒性。

Result: 在多个基准测试上的广泛实验证明了UAdapterGNN方法的有效性、鲁棒性和高泛化能力。

Conclusion: 通过将不确定性学习集成到GNN适配器中，可以有效解决图数据噪声问题，显著提升预训练GNN模型在下游任务中的鲁棒性和泛化性能。

Abstract: Recently, fine-tuning large-scale pre-trained GNNs has yielded remarkable attention in adapting pre-trained GNN models for downstream graph learning tasks. One representative fine-tuning method is to exploit adapter (termed AdapterGNN) which aims to 'augment' the pre-trained model by inserting a lightweight module to make the 'augmented' model better adapt to the downstream tasks. However, graph data may contain various types of noise in downstream tasks, such as noisy edges and ambiguous node attributes. Existing AdapterGNNs are often prone to graph noise and exhibit limited generalizability. How to enhance the robustness and generalization ability of GNNs' fine tuning remains an open problem. In this paper, we show that the above problem can be well addressed by integrating uncertainty learning into the GNN adapter. We propose the Uncertainty-aware Adapter (UAdapterGNN) that fortifies pre-trained GNN models against noisy graph data in the fine-tuning process. Specifically, in contrast to regular AdapterGNN, our UAdapterGNN exploits Gaussian probabilistic adapter to augment the pre-trained GNN model. In this way, when the graph contains various noises,our method can automatically absorb the effects of changes in the variances of the Gaussian distribution, thereby significantly enhancing the model's robustness. Also, UAdapterGNN can further improve the generalization ability of the model on the downstream tasks. Extensive experiments on several benchmarks demonstrate the effectiveness, robustness and high generalization ability of the proposed UAdapterGNN method.

</details>


### [475] [AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention](https://arxiv.org/abs/2511.18960)
*Lei Xiao,Jifeng Li,Juntao Gao,Feiyang Ye,Yan Jin,Jingjing Qian,Jing Zhang,Yong Wu,Xiaoyuan Yu*

Main category: cs.LG

TL;DR: AVA-VLA是一个新的视觉-语言-动作框架，通过引入主动视觉注意力机制，利用历史上下文动态处理视觉信息，在机器人任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在每个时间步独立处理视觉输入，采用MDP建模方式，忽略了历史上下文信息，这在动态顺序决策中不是最优的。

Method: 从POMDP角度重新定义问题，提出AVA模块，利用循环状态（信念状态的神经近似）计算软权重，主动处理任务相关的视觉token。

Result: 在LIBERO和CALVIN等机器人基准测试中达到最先进性能，并在真实双臂机器人平台上验证了实用性及鲁棒的仿真到真实迁移能力。

Conclusion: AVA-VLA通过利用历史上下文动态调制视觉处理，显著提升了VLA模型在具身AI任务中的表现。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.

</details>


### [476] [UniGame: Turning a Unified Multimodal Model Into Its Own Adversary](https://arxiv.org/abs/2511.19413)
*Zhaolong Su,Wang Lu,Hao Chen,Sharon Li,Jindong Wang*

Main category: cs.LG

TL;DR: UniGame是一个自对抗后训练框架，通过轻量级扰动器解决统一多模态模型中理解与生成之间的表示不一致问题，显著提升模型的一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型存在根本性不一致：理解任务偏好紧凑嵌入，而生成任务偏好重构丰富的表示，这种结构权衡导致决策边界错位、跨模态一致性下降以及对分布和对抗性变化的脆弱性。

Method: 在共享token接口应用轻量级扰动器，使生成分支能够主动寻找和挑战脆弱理解，将模型自身转化为对抗者。

Result: UniGame显著提升一致性(+4.6%)、理解能力(+3.6%)、生成质量(+0.02)，在自然基准和对抗性VQA上的分布外和对抗鲁棒性分别提升+4.8%和+6.2%。

Conclusion: 对抗性自博弈是增强未来多模态基础模型一致性、稳定性和统一能力的通用有效原则，该框架架构无关，仅增加不到1%参数，且与现有后训练方法互补。

Abstract: Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame

</details>


### [477] [Flow Map Distillation Without Data](https://arxiv.org/abs/2511.19428)
*Shangyuan Tong,Nanye Ma,Saining Xie,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 提出了一种无需外部数据集的流映射蒸馏方法，仅从先验分布采样，避免了教师-数据不匹配问题，在ImageNet上实现了仅需1步采样的最先进生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有流映射蒸馏方法依赖外部数据集，存在教师-数据不匹配风险，因为静态数据集可能无法完全代表教师的生成能力。

Method: 开发了一个原则性框架，从先验分布采样，学习预测教师的采样路径，并主动纠正自身累积误差以确保高保真度。

Result: 在ImageNet 256x256上FID达到1.45，在ImageNet 512x512上FID达到1.49，仅需1步采样，超越了所有基于数据的对应方法。

Conclusion: 建立了一个更稳健的生成模型加速范式，推动了无需数据的流映射蒸馏的广泛采用。

Abstract: State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [478] [A novel strategy for multi-resource load balancing in agent-based systems](https://arxiv.org/abs/2511.17580)
*Leszek Sliwko,Aleksander Zgrzywa*

Main category: cs.MA

TL;DR: 提出一种基于多资源负载平衡策略的智能体系统，用于优化复杂企业架构的结构设计。


<details>
  <summary>Details</summary>
Motivation: 帮助系统设计者优化复杂企业架构的结构配置，利用智能体的社会行为和适应能力来寻找最优设置。

Method: 采用多资源负载平衡策略，结合智能体的社会行为和自适应能力，开发了允许智能体自评估的方法。

Result: 实现了所提出的智能体系统，并进行了实验验证，展示了实验结果。

Conclusion: 基于多资源负载平衡的智能体系统能够有效优化企业架构配置，智能体的社会行为和自适应能力是实现这一目标的关键。

Abstract: The paper presents a multi-resource load balancing strategy which can be utilised within an agent-based system. This approach can assist system designers in their attempts to optimise the structure for complex enterprise architectures. In this system, the social behaviour of the agent and its adaptation abilities are applied to determine an optimal setup for a given configuration. All the methods have been developed to allow the agent's self-assessment. The proposed agent system has been implemented and the experiment results are presented here.

</details>


### [479] [From Competition to Coordination: Market Making as a Scalable Framework for Safe and Aligned Multi-Agent LLM Systems](https://arxiv.org/abs/2511.17621)
*Brendan Gho,Suman Muppavarapu,Afnan Shaik,Tyson Tsay,James Begin,Kevin Zhu,Archana Vaidheeswaran,Vasu Sharma*

Main category: cs.MA

TL;DR: 提出基于市场机制的多智能体LLM协调框架，通过经济交换方式组织智能体交互，让智能体作为市场参与者交易概率信念，实现自我组织的可验证推理。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型在多智能体系统中部署，其集体行为对可信性、透明度和问责制提出新挑战，传统协调机制难以扩展且决策过程不透明。

Method: 引入市场制造框架，将智能体交互组织为结构化经济交换，每个智能体作为市场参与者更新和交易概率信念，使局部激励与集体认知目标对齐。

Result: 在事实推理、伦理判断和常识推理任务中，基于市场的协调比单次基线准确率提升高达10%，同时保持中间推理步骤的可解释性和透明度。

Conclusion: 经济协调原则能够在多智能体LLM系统中实现问责制和鲁棒性，为自我纠正、社会负责的AI提供可扩展路径，在现实部署中维持信任和监督。

Abstract: As foundation models are increasingly deployed as interacting agents in multi-agent systems, their collective behavior raises new challenges for trustworthiness, transparency, and accountability. Traditional coordination mechanisms, such as centralized oversight or adversarial adjudication, struggle to scale and often obscure how decisions emerge. We introduce a market-making framework for multi-agent large language model (LLM) coordination that organizes agent interactions as structured economic exchanges. In this setup, each agent acts as a market participant, updating and trading probabilistic beliefs, to converge toward shared, truthful outcomes. By aligning local incentives with collective epistemic goals, the framework promotes self-organizing, verifiable reasoning without requiring external enforcement. Empirically, we evaluate this approach across factual reasoning, ethical judgment, and commonsense inference tasks. Market-based coordination yields accuracy gains of up to 10% over single-shot baselines while preserving interpretability and transparency of intermediate reasoning steps. Beyond these improvements, our findings demonstrate that economic coordination principles can operationalize accountability and robustness in multi-agent LLM systems, offering a scalable pathway toward self-correcting, socially responsible AI capable of maintaining trust and oversight in real world deployment scenarios.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [480] [What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models](https://arxiv.org/abs/2511.19324)
*Roksana Goworek,Olivia Macmillan-Scott,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: 本文系统评估了跨语言信息检索中的四种干预方法，发现专门训练的密集检索模型优于词汇匹配方法，对比学习能显著改善语言偏差，重排序效果取决于训练数据质量。


<details>
  <summary>Details</summary>
Motivation: 跨语言信息检索面临资源差异、文字系统不同和嵌入模型跨语言语义对齐弱等挑战，现有方法依赖翻译和单语检索启发式方法，增加了计算开销和噪声。

Method: 系统评估了四种干预类型：文档翻译、使用预训练编码器的多语言密集检索、在词/短语/查询-文档层面的对比学习，以及交叉编码器重排序，在三个基准数据集上进行测试。

Result: 专门为CLIR训练的密集检索模型始终优于词汇匹配方法，从文档翻译中获益甚微。对比学习减轻了语言偏差，对初始对齐弱的编码器带来显著改进。重排序可能有效但取决于交叉编码器训练数据的质量。

Conclusion: 跨语言搜索系统应优先考虑语义多语言嵌入和有针对性的基于学习的对齐，而不是基于翻译的流程，特别是对于跨文字系统和资源不足的语言。

Abstract: Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.

</details>


### [481] [Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval](https://arxiv.org/abs/2511.19325)
*Olivia Macmillan-Scott,Roksana Goworek,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: 评估多语言大语言模型在跨语言查询扩展中的表现，发现查询长度决定提示技术有效性，语言间存在显著差异，微调仅在训练测试数据格式相似时有效。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型将查询扩展从语义增强转向伪文档生成，这对跨语言信息检索特别有益，需要评估不同生成扩展策略的性能驱动因素。

Method: 评估近期多语言大语言模型及其微调变体在多种生成扩展策略上的表现，分析提示技术、查询长度、语言差异和微调效果等因素。

Result: 查询长度决定提示技术有效性，复杂提示通常无额外收益；语言间差异显著，跨语言查询扩展对基线性能最弱的语言改善最大；微调仅在训练测试数据格式相似时有效。

Conclusion: 需要更平衡的多语言和跨语言训练与评估资源来解决语言间的不平衡问题。

Abstract: Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [482] [LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment](https://arxiv.org/abs/2511.17676)
*Xi Wang,Xianyao Ling,Kun Li,Gang Yin,Liang Zhang,Jiang Wu,Annie Wang,Weizhe Wang*

Main category: cs.DB

TL;DR: 本文探讨了生成式AI和智能体技术如何变革企业数据管理与分析，重点关注SQL生成、RAG和向量数据库等技术在企业数据分析中的应用、部署挑战及安全合规问题。


<details>
  <summary>Details</summary>
Motivation: 企业数据管理和分析正经历AI技术的深刻变革，需要解决传统数据库应用与AI驱动工具（如RAG、向量数据库）的融合问题，同时确保数据安全和合规性，降低企业数据访问门槛。

Method: 采用基于大语言模型的SQL生成、多智能体协作框架，结合检索增强生成和向量数据库技术，实现复杂查询理解、安全验证和计算效率优化。

Result: 通过代表性用例展示了AI技术在企业数据分析中的创新应用，有效连接自然语言与结构化数据，提升了分析效率。

Conclusion: 企业数据分析系统部署面临分布式部署、数据安全和SQL生成固有难度的挑战，需要继续探索解决方案以实现更安全高效的AI驱动数据分析。

Abstract: The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics. Traditional database applications and system deployment are fundamentally impacted by AI-driven tools, such as Retrieval-Augmented Generation (RAG) and vector database technologies, which provide new pathways for semantic querying over enterprise knowledge bases. In the meantime, data security and compliance are top priorities for organizations adopting AI technologies. For enterprise data analysis, SQL generations powered by large language models (LLMs) and AI agents, has emerged as a key bridge connecting natural language with structured data, effectively lowering the barrier to enterprise data access and improving analytical efficiency. This paper focuses on enterprise data analysis applications and system deployment, covering a range of innovative frameworks, enabling complex query understanding, multi-agent collaboration, security verification, and computational efficiency. Through representative use cases, key challenges related to distributed deployment, data security, and inherent difficulties in SQL generation tasks are discussed.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [483] [Comparing Labeled Markov Chains: A Cantor-Kantorovich Approach](https://arxiv.org/abs/2511.18103)
*Adrien Banse,Alessandro Abate,Raphaël M. Jungers*

Main category: cs.LO

TL;DR: 本文研究标记马尔可夫链的Cantor-Kantorovich距离，证明其可表示为有限时域总变差距离的折扣和，分析其计算复杂性、连续性性质和近似方法。


<details>
  <summary>Details</summary>
Motivation: 比较两个标记马尔可夫链是评估抽象精度或量化模型扰动的关键挑战，需要研究CK距离的理论基础。

Method: 将CK距离框架化为有限时域总变差距离的折扣和，分析其作为折扣线性距离的性质，并研究计算复杂性和近似方案。

Result: 证明CK距离的精确计算是#P难的，提供上界分析，并开发可计算近似方案（但也是#P难的）。

Conclusion: 为CK距离提供了严格的理论基础，阐明了其与现有距离的关系，建立了有限时域轨迹概率误差的有界性。

Abstract: Labeled Markov Chains (or LMCs for short) are useful mathematical objects to model complex probabilistic languages. A central challenge is to compare two LMCs, for example to assess the accuracy of an abstraction or to quantify the effect of model perturbations. In this work, we study the recently introduced Cantor-Kantorovich (or CK) distance. In particular we show that the latter can be framed as a discounted sum of finite-horizon Total Variation distances, making it an instance of discounted linear distance, but arising from the natural Cantor topology. Building on the latter observation, we analyze the properties of the CK distance along three dimensions: computational complexity, continuity properties and approximation. More precisely, we show that the exact computation of the CK distance is #P-hard. We also provide an upper bound on the CK distance as a function of the approximation relation between the two LMCs, and show that a bounded CK distance implies a bounded error between probabilities of finite-horizon traces. Finally, we provide a computable approximation scheme, and show that the latter is also #P-hard. Altogether, our results provide a rigorous theoretical foundation for the CK distance and clarify its relationship with existing distances.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [484] [Inverse Rendering for High-Genus Surface Meshes from Multi-View Images](https://arxiv.org/abs/2511.18680)
*Xiang Gao,Xinmu Wang,Xiaolong Wu,Jiazhi Li,Jingyu Shi,Yu Guo,Yuanpeng Liu,Xiyun Song,Heather Yu,Zongfang Lin,Xianfeng David Gu*

Main category: cs.GR

TL;DR: 提出了一种基于拓扑信息的逆向渲染方法，用于从多视角图像重建高亏格表面网格，通过自适应V循环重网格化方案和重新参数化的Adam优化器解决现有方法在高亏格表面上的拓扑特征丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有的逆向渲染方法在高亏格表面上经常失败，导致关键拓扑特征丢失，在低亏格表面上则过度平滑导致表面细节丢失。这些问题源于对Adam优化器的过度依赖，可能导致梯度消失和爆炸。

Method: 引入自适应V循环重网格化方案结合重新参数化的Adam优化器，通过周期性粗化和细化变形网格，在优化前告知网格顶点当前的拓扑和几何信息。同时使用Gauss-Bonnet定理构建与真实值亏格数匹配的拓扑基元来保持拓扑一致性。

Result: 实验结果表明该方法优于当前最先进的方法，在Chamfer距离和体积IoU方面取得显著改进，特别是在高亏格表面上，同时也增强了低亏格表面的细节。

Conclusion: 所提出的拓扑感知逆向渲染方法能够有效解决高亏格表面重建中的拓扑特征丢失问题，同时保持几何细节，在多个评估指标上优于现有方法。

Abstract: We present a topology-informed inverse rendering approach for reconstructing high-genus surface meshes from multi-view images. Compared to 3D representations like voxels and point clouds, mesh-based representations are preferred as they enable the application of differential geometry theory and are optimized for modern graphics pipelines. However, existing inverse rendering methods often fail catastrophically on high-genus surfaces, leading to the loss of key topological features, and tend to oversmooth low-genus surfaces, resulting in the loss of surface details. This failure stems from their overreliance on Adam-based optimizers, which can lead to vanishing and exploding gradients. To overcome these challenges, we introduce an adaptive V-cycle remeshing scheme in conjunction with a re-parametrized Adam optimizer to enhance topological and geometric awareness. By periodically coarsening and refining the deforming mesh, our method informs mesh vertices of their current topology and geometry before optimization, mitigating gradient issues while preserving essential topological features. Additionally, we enforce topological consistency by constructing topological primitives with genus numbers that match those of ground truth using Gauss-Bonnet theorem. Experimental results demonstrate that our inverse rendering approach outperforms the current state-of-the-art method, achieving significant improvements in Chamfer Distance and Volume IoU, particularly for high-genus surfaces, while also enhancing surface details for low-genus surfaces.

</details>


### [485] [ChronoGS: Disentangling Invariants and Changes in Multi-Period Scenes](https://arxiv.org/abs/2511.18794)
*Zhongtao Wang,Jiaqi Dai,Qingtian Zhu,Yilong Li,Mai Su,Fei Zhu,Meng Gai,Shaorong Wang,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.GR

TL;DR: ChronoGS是一种时间调制的高斯表示方法，用于重建多时期场景，在统一锚点支架中分离稳定和演化组件，实现时间一致的重建。


<details>
  <summary>Details</summary>
Motivation: 现实应用中常见多时期图像集合（如城市扫描、建筑工地跟踪、环境监测），现有方法无法处理长期不连续变化：静态方法强制单一几何，动态方法假设平滑运动。

Method: 引入时间调制的高斯表示，在统一锚点支架中重建所有时期，设计用于分离稳定和演化组件。

Result: 实验表明ChronoGS在重建质量和时间一致性方面持续优于基线方法。

Conclusion: ChronoGS成功解决了多时期场景重建问题，并发布了ChronoScene数据集以促进相关研究。

Abstract: Multi-period image collections are common in real-world applications. Cities are re-scanned for mapping, construction sites are revisited for progress tracking, and natural regions are monitored for environmental change. Such data form multi-period scenes, where geometry and appearance evolve. Reconstructing such scenes is an important yet underexplored problem. Existing pipelines rely on incompatible assumptions: static and in-the-wild methods enforce a single geometry, while dynamic ones assume smooth motion, both failing under long-term, discontinuous changes. To solve this problem, we introduce ChronoGS, a temporally modulated Gaussian representation that reconstructs all periods within a unified anchor scaffold. It's also designed to disentangle stable and evolving components, achieving temporally consistent reconstruction of multi-period scenes. To catalyze relevant research, we release ChronoScene dataset, a benchmark of real and synthetic multi-period scenes, capturing geometric and appearance variation. Experiments demonstrate that ChronoGS consistently outperforms baselines in reconstruction quality and temporal consistency. Our code and the ChronoScene dataset are publicly available at https://github.com/ZhongtaoWang/ChronoGS.

</details>


### [486] [MatMart: Material Reconstruction of 3D Objects via Diffusion](https://arxiv.org/abs/2511.18900)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.GR

TL;DR: 提出了一种名为ttt的新型材料重建框架，用于3D物体的材料估计和生成，通过两阶段重建和扩散模型实现高保真结果。


<details>
  <summary>Details</summary>
Motivation: 将扩散模型应用于基于物理的材料估计和生成是当前的研究热点，需要解决从有限输入图像重建高质量材料的问题。

Method: 采用两阶段重建：首先从输入图像准确预测材料，然后通过先验引导的材料生成处理未观测视角；使用渐进式推理和视图-材料交叉注意力机制；通过端到端优化单一扩散模型实现材料预测和生成。

Result: 在材料重建方面相比现有方法实现了更优越的性能，具有强可扩展性和灵活性，能够从任意数量的输入图像进行重建。

Conclusion: ttt框架在3D物体材料重建中表现出色，无需依赖额外预训练模型，在各种类型物体上都具有增强的稳定性。

Abstract: Applying diffusion models to physically-based material estimation and generation has recently gained prominence. In this paper, we propose \ttt, a novel material reconstruction framework for 3D objects, offering the following advantages. First, \ttt\ adopts a two-stage reconstruction, starting with accurate material prediction from inputs and followed by prior-guided material generation for unobserved views, yielding high-fidelity results. Second, by utilizing progressive inference alongside the proposed view-material cross-attention (VMCA), \ttt\ enables reconstruction from an arbitrary number of input images, demonstrating strong scalability and flexibility. Finally, \ttt\ achieves both material prediction and generation capabilities through end-to-end optimization of a single diffusion model, without relying on additional pre-trained models, thereby exhibiting enhanced stability across various types of objects. Extensive experiments demonstrate that \ttt\ achieves superior performance in material reconstruction compared to existing methods.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [487] [Self-Empowering VLMs: Achieving Hierarchical Consistency via Self-Elicited Knowledge Distillation](https://arxiv.org/abs/2511.18415)
*Wei Yang,Yiran Zhu,Zilin Li,Xunjia Zhang,Hongtao Wang*

Main category: cs.MM

TL;DR: 提出SEKD方法，通过自蒸馏让视觉语言模型学习层次推理能力，无需人工标注即可提升模型在层次理解任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在层次理解任务中表现不佳，主要问题在于无法维持跨层级状态，而非缺乏分类学知识。

Method: SEKD方法：让同一个VLM通过逐步推理作为教师模型，暴露其硬标签、软分布和解码器隐藏状态，然后让单次推理的学生模型蒸馏这些信号。

Result: 学生模型在领域内路径一致性提升29.5个百分点，在未见分类学上的零样本HCA从4.15%提升至42.26%，在数学基准测试中也有提升。

Conclusion: SEKD提供了一种无需标注成本的实用方法，为紧凑型VLMs注入依赖感知的多步推理能力。

Abstract: Vision-language models (VLMs) possess rich knowledge but often fail on hierarchical understanding tasks, where the goal is to predict a coarse-to-fine taxonomy path that remains consistent across all levels. We compare three inference paradigms for hierarchical VQA and find that stepwise reasoning, when conditioned on prior answers, significantly outperforms single-pass prompting. Further analysis indicates that the main limitation of current VLMs is their inability to maintain cross-level state, rather than a lack of taxonomic knowledge. Motivated by this diagnosis, we propose Self-Elicited Knowledge Distillation (SEKD), which requires no human labels or external tools: the same VLM is prompted to reason step by step and act as a teacher by exposing its hard labels, soft distributions, and decoder hidden states, while a single-pass student distills these signals. The student VLM remains efficient while approaching the accuracy of its multi-step teacher. It improves in-domain path consistency (HCA) by up to +29.50 percentage points, raises zero-shot HCA on an unseen taxonomy from 4.15% to 42.26%, and yields gains on challenging mathematical benchmarks. Because all supervision is self-elicited, SEKD scales to new taxonomies and datasets without annotation cost, providing a practical route to imbue compact VLMs with dependency-aware multi-step reasoning.

</details>


### [488] [Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach](https://arxiv.org/abs/2511.19080)
*Fan Nie,Jiangqun Ni,Jian Zhang,Bin Zhang,Weizhe Zhang,Bin Li*

Main category: cs.MM

TL;DR: 本文提出了一种基于变分贝叶斯的多模态深度伪造检测框架FoVB，通过建模音频-视觉相关性作为高斯分布潜变量来检测跨模态不一致性。


<details>
  <summary>Details</summary>
Motivation: AIGC内容的广泛应用带来了安全风险，特别是音视频深度伪造。需要开发有效且泛化性强的多模态深度伪造检测方法，利用音视频相关性学习来暴露跨模态不一致性。

Method: 采用变分贝叶斯估计重构相关性学习：1) 使用差分卷积和高通滤波器提取局部和全局伪造痕迹；2) 通过变分贝叶斯估计音视频相关性的高斯潜变量；3) 通过正交约束将变量分解为模态特定和相关性特定变量。

Result: 在多个基准测试中，FoVB方法优于其他最先进的方法。

Conclusion: 所提出的FoVB框架通过变分贝叶斯方法有效建模音视频相关性，在多模态深度伪造检测中表现出色。

Abstract: The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes. Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection. Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection. In this paper, we reformulate the correlation learning with variational Bayesian estimation, where audio-visual correlation is approximated as a Gaussian distributed latent variable, and thus develop a novel framework for deepfake detection, i.e., Forgery-aware Audio-Visual Adaptation with Variational Bayes (FoVB). Specifically, given the prior knowledge of pre-trained backbones, we adopt two core designs to estimate audio-visual correlations effectively. First, we exploit various difference convolutions and a high-pass filter to discern local and global forgery traces from both modalities. Second, with the extracted forgery-aware features, we estimate the latent Gaussian variable of audio-visual correlation via variational Bayes. Then, we factorize the variable into modality-specific and correlation-specific ones with orthogonality constraint, allowing them to better learn intra-modal and cross-modal forgery traces with less entanglement. Extensive experiments demonstrate that our FoVB outperforms other state-of-the-art methods in various benchmarks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [489] [From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence](https://arxiv.org/abs/2511.18538)
*Jian Yang,Wei Zhang,Shark Liu,Jiajun Wu,Shawn Guo,Yizhi Li*

Main category: cs.SE

TL;DR: 本文全面分析了代码大语言模型的完整生命周期，从数据准备到后训练阶段，涵盖预训练、监督微调、强化学习和自主编码代理，比较了通用LLM和专用代码LLM的性能，并探讨了学术研究与实际部署之间的差距。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在自动软件开发中的广泛应用，需要系统性地分析代码LLMs的完整技术栈，弥合学术研究与实践部署之间的差距，为开发者提供实用的技术指导。

Method: 通过一系列分析和探测实验，系统研究代码LLMs的完整生命周期，包括数据管理、预训练、监督微调、强化学习、提示工程等关键技术，并比较通用LLM和专用代码LLM的性能。

Result: 分析了GPT-4、Claude、LLaMA等通用LLM以及StarCoder、Code LLaMA等专用代码LLM的技术特点和性能表现，识别了代码正确性、安全性、上下文感知等实际部署中的关键挑战。

Conclusion: 代码LLMs已显著提升软件开发效率，但仍需解决代码质量、安全性、大规模代码库集成等问题，未来研究应更注重实际部署需求与学术基准的对接。

Abstract: Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [490] [Deep Learning-based Lightweight RGB Object Tracking for Augmented Reality Devices](https://arxiv.org/abs/2511.17508)
*Alice Smith,Bob Johnson,Xiaoyu Zhu,Carol Lee*

Main category: cs.HC

TL;DR: 提出了一种轻量级RGB目标跟踪算法，专门针对资源受限的AR平台，在保持高精度的同时大幅降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习目标跟踪器虽然精度高，但计算和内存开销过大，不适合可穿戴AR设备。

Method: 采用紧凑的Siamese神经网络架构，结合模型剪枝、量化和知识蒸馏等优化技术，离线训练后在设备上实时部署。

Result: 在标准跟踪基准测试中达到与最先进跟踪器相当的精度，在移动AR头显上以约30FPS实时运行，比现有高性能跟踪器快一个数量级。

Conclusion: 该工作为AR应用实现了实用、鲁棒的目标跟踪，为轻量级设备上更交互和动态的AR体验打开了大门。

Abstract: Augmented Reality (AR) applications often require robust real-time tracking of objects in the user's environment to correctly overlay virtual content. Recent advances in computer vision have produced highly accurate deep learning-based object trackers, but these models are typically too heavy in computation and memory for wearable AR devices. In this paper, we present a lightweight RGB object tracking algorithm designed specifically for resource-constrained AR platforms. The proposed tracker employs a compact Siamese neural network architecture and incorporates optimization techniques such as model pruning, quantization, and knowledge distillation to drastically reduce model size and inference cost while maintaining high tracking accuracy. We train the tracker offline on large video datasets using deep convolutional neural networks and then deploy it on-device for real-time tracking. Experimental results on standard tracking benchmarks show that our approach achieves comparable accuracy to state-of-the-art trackers, yet runs in real-time on a mobile AR headset at around 30 FPS -- more than an order of magnitude faster than prior high-performance trackers on the same hardware. This work enables practical, robust object tracking for AR use-cases, opening the door to more interactive and dynamic AR experiences on lightweight devices.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [491] [Beyond the Rubric: Cultural Misalignment in LLM Benchmarks for Sexual and Reproductive Health](https://arxiv.org/abs/2511.17554)
*Sumon Kanti Dey,Manvi S,Zeel Mehta,Meet Shah,Unnati Agrawal,Suhani Jalota,Azra Ismail*

Main category: cs.CY

TL;DR: 研究发现当前基于西方规范的LLM健康评估基准存在文化偏见，无法准确评估针对全球南方国家设计的健康聊天机器人效果


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在性生殖健康领域对全球南方国家服务的有效性，揭示现有西方中心评估基准的局限性

Method: 使用OpenAI的HealthBench基准评估印度性生殖健康聊天机器人，对比自动评分与人工定性分析结果

Result: 自动评分系统一致给出低分，但人工分析显示许多回答在文化和医学上都是恰当的，主要问题在于西方偏见（法律框架、饮食假设、成本模型等）

Conclusion: 需要开发文化适应性评估框架，在保证质量标准的同时满足不同人群需求

Abstract: Large Language Models (LLMs) have been positioned as having the potential to expand access to health information in the Global South, yet their evaluation remains heavily dependent on benchmarks designed around Western norms. We present insights from a preliminary benchmarking exercise with a chatbot for sexual and reproductive health (SRH) for an underserved community in India. We evaluated using HealthBench, a benchmark for conversational health models by OpenAI. We extracted 637 SRH queries from the dataset and evaluated on the 330 single-turn conversations. Responses were evaluated using HealthBench's rubric-based automated grader, which rated responses consistently low. However, qualitative analysis by trained annotators and public health experts revealed that many responses were actually culturally appropriate and medically accurate. We highlight recurring issues, particularly a Western bias, such as for legal framing and norms (e.g., breastfeeding in public), diet assumptions (e.g., fish safe to eat during pregnancy), and costs (e.g., insurance models). Our findings demonstrate the limitations of current benchmarks in capturing the effectiveness of systems built for different cultural and healthcare contexts. We argue for the development of culturally adaptive evaluation frameworks that meet quality standards while recognizing needs of diverse populations.

</details>


### [492] [A Cross-Cultural Assessment of Human Ability to Detect LLM-Generated Fake News about South Africa](https://arxiv.org/abs/2511.17682)
*Tim Schlippe,Matthias Wölfel,Koena Ronny Mabokela*

Main category: cs.CY

TL;DR: 文化亲近性影响AI生成假新闻检测能力：南非参与者能更好识别本国真实新闻，但在识别假新闻方面表现更差，这可能源于对新闻源的更高信任度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能生成复杂假新闻，理解人类在不同文化背景下的检测能力变得至关重要，特别是在内容跨越文化边界时。

Method: 对89名参与者（56名南非人，33名其他国籍）进行调查，让他们评估10篇真实南非新闻和10篇AI生成的假新闻。

Result: 南非人在识别本国真实新闻方面表现更好（偏离理想评分40% vs 52%），但在识别假新闻方面表现更差（62% vs 55%）。两组总体偏离程度相似（51% vs 53%）。

Conclusion: 文化熟悉度有助于验证真实信息，但在评估伪造内容时可能引入偏见，这对理解跨文化误信息检测和应对全球化信息生态系统中的AI假新闻具有重要意义。

Abstract: This study investigates how cultural proximity affects the ability to detect AI-generated fake news by comparing South African participants with those from other nationalities. As large language models increasingly enable the creation of sophisticated fake news, understanding human detection capabilities becomes crucial, particularly across different cultural contexts. We conducted a survey where 89 participants (56 South Africans, 33 from other nationalities) evaluated 10 true South African news articles and 10 AI-generated fake versions. Results reveal an asymmetric pattern: South Africans demonstrated superior performance in detecting true news about their country (40% deviation from ideal rating) compared to other participants (52%), but performed worse at identifying fake news (62% vs. 55%). This difference may reflect South Africans' higher overall trust in news sources. Our analysis further shows that South Africans relied more on content knowledge and contextual understanding when judging credibility, while participants from other countries emphasised formal linguistic features such as grammar and structure. Overall, the deviation from ideal rating was similar between groups (51% vs. 53%), suggesting that cultural familiarity appears to aid verification of authentic information but may also introduce bias when evaluating fabricated content. These insights contribute to understanding cross-cultural dimensions of misinformation detection and inform strategies for combating AI-generated fake news in increasingly globalised information ecosystems where content crosses cultural and geographical boundaries.

</details>


### [493] [Animated Territorial Data Extractor (ATDE): A Computer-Vision Method for Extracting Territorial Data from Animated Historical Maps](https://arxiv.org/abs/2511.17920)
*Hamza Alshamy,Isaiah Woram,Advay Mishra,Zihan Xia,Pascal Wallisch*

Main category: cs.CY

TL;DR: ATDE是一个计算机视觉工具，可从动画历史地图视频中提取定量领土数据，通过颜色分割和过滤技术识别代表领土控制的像素，并将其转换为结构化时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 开发一个无需预定义形状文件就能从动画历史地图视频中自动提取领土数据的工具，用于教育演示、初步数据探索和领土动态比较分析。

Method: 使用HSV颜色分割、RGB通道过滤和直接邻居过滤来识别领土控制像素，结合时间对齐和跨视频缩放的预处理，将动画视频转换为时间序列数据。

Result: 在十个中国朝代（公元前200年至公元1912年）上测试，生成的逐年像素计数与预期历史模式一致。

Conclusion: ATDE虽然不是权威历史数据集的替代品，但适用于教育演示、初步数据探索和领土动态比较分析，且可应用于任何给定种子颜色和基本配置的动画地图视频。

Abstract: We present Animated Territorial Data Extractor (ATDE), a computer vision tool that extracts quantitative territorial data from animated historical map videos. ATDE employs HSV-based color segmentation, RGB channel filtering, and Direct-Neighbor Filtering to identify and count pixels representing territorial control. Combined with preprocessing for temporal alignment and cross-video scaling, the pipeline converts animated videos into structured time-series data. We demonstrate the tool on ten Chinese dynasties (200 BCE - 1912 CE), producing year-by-year pixel counts that align with expected historical patterns. While not a substitute for authoritative historical datasets, ATDE is well-suited for educational demonstrations, preliminary data exploration, and comparative analysis of territorial dynamics. The tool requires no pre-existing shapefiles and can be applied to any animated map video given seed colors and basic configuration. Code and examples are available on GitHub.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [494] [Evaluation of Hardware-based Video Encoders on Modern GPUs for UHD Live-Streaming](https://arxiv.org/abs/2511.18686)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: eess.IV

TL;DR: 评估多代NVIDIA、Intel GPU和Qualcomm Snapdragon移动SoC中硬件视频编码器的率失真性能、编码速度和功耗，并与软件编码器进行比较。


<details>
  <summary>Details</summary>
Motivation: 随着VTuber、游戏直播和现场活动等实时视频内容的兴起，对GPU中高效硬件编码器的需求增加，特别是针对4K/8K超高清分辨率的实时视频编码任务。

Method: 使用PSNR、SSIM和基于机器学习的VMAF等指标，评估多代硬件编码器的率失真性能、编码速度和功耗，并与软件编码器（包括最新的H.266/VVC编解码器）进行比较。

Result: 现代GPU硬件编码器在实时编码场景中能够匹配软件编码器的率失真性能；虽然新硬件的编码速度有所提高，但硬件代际间的率失真性能改进大多可以忽略不计；还计算了各硬件编码器匹配YouTube转码质量所需的比特率。

Conclusion: 硬件编码器在实时视频编码场景中表现出色，能够提供与软件编码器相当的率失真性能，但在硬件代际间的率失真性能提升有限。

Abstract: Many GPUs have incorporated hardware-accelerated video encoders, which allow video encoding tasks to be offloaded from the main CPU and provide higher power efficiency. Over the years, many new video codecs such as H.265/HEVC, VP9, and AV1 were added to the latest GPU boards. Recently, the rise of live video content such as VTuber, game live-streaming, and live event broadcasts, drives the demand for high-efficiency hardware encoders in the GPUs to tackle these real-time video encoding tasks, especially at higher resolutions such as 4K/8K UHD. In this paper, RD performance, encoding speed, as well as power consumption of hardware encoders in several generations of NVIDIA, Intel GPUs as well as Qualcomm Snapdragon Mobile SoCs were evaluated and compared to the software counterparts, including the latest H.266/VVC codec, using several metrics including PSNR, SSIM, and machine-learning based VMAF. The results show that modern GPU hardware encoders can match the RD performance of software encoders in real-time encoding scenarios, and while encoding speed increased in newer hardware, there is mostly negligible RD performance improvement between hardware generations. Finally, the bitrate required for each hardware encoder to match YouTube transcoding quality was also calculated.

</details>


### [495] [Robust Detection of Retinal Neovascularization in Widefield Optical Coherence Tomography](https://arxiv.org/abs/2511.17744)
*Jinyi Hao,Jie Wang,Kotaro Tsuboi,Liqin Gao,Tristan T. Hormel,Yukun Guo,An-Lun Wu,Min Gao,Christina J. Flaxel,Steven T. Bailey,Thomas S. Hwang,Yali Jia*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的方法，用于在宽场OCT/OCTA图像上进行视网膜新生血管的诊断和分期，将RNV识别重新定义为直接的二元定位任务。


<details>
  <summary>Details</summary>
Motivation: 视网膜新生血管(RNV)是糖尿病视网膜病变中威胁视力的发展，及时干预可预防视力丧失。宽场OCTA技术有望改善RNV的早期检测，但现有算法主要针对窄视野图像优化。

Method: 采用完全自动化的方法，将RNV识别重新定义为二元定位任务，不依赖多层视网膜分割。在589个宽场扫描图像上训练和验证，扫描范围17x17-mm至26x21-mm。

Result: RNV诊断的设备相关AUC范围为0.96-0.99，分割的平均IOU范围为0.76-0.88。方法还展示了纵向监测病变生长的能力。

Conclusion: 基于深度学习的宽场OCTA图像分析可为改善RNV筛查和管理提供有价值的手段。

Abstract: Retinal neovascularization (RNV) is a vision threatening development in diabetic retinopathy (DR). Vision loss associated with RNV is preventable with timely intervention, making RNV clinical screening and monitoring a priority. Optical coherence tomography (OCT) angiography (OCTA) provides high-resolution imaging and high-sensitivity detection of RNV lesions. With recent commercial devices introducing widefield OCTA imaging to the clinic, the technology stands to improve early detection of RNV pathology. However, to meet clinical requirements these imaging capabilities must be combined with effective RNV detection and quantification, but existing algorithms for OCTA images are optimized for conventional, i.e. narrow, fields of view. Here, we present a novel approach for RNV diagnosis and staging on widefield OCT/OCTA. Unlike conventional methods dependent on multi-layer retinal segmentation, our model reframes RNV identification as a direct binary localization task. Our fully automated approach was trained and validated on 589 widefield scans (17x17-mm to 26x21-mm) collected from multiple devices at multiple clinics. Our method achieved a device-dependent area under curve (AUC) ranging from 0.96 to 0.99 for RNV diagnosis, and mean intersection over union (IOU) ranging from 0.76 to 0.88 for segmentation. We also demonstrate our method's ability to monitor lesion growth longitudinally. Our results indicate that deep learning-based analysis for widefield OCTA images could offer a valuable means for improving RNV screening and management.

</details>


### [496] [Spectral Super-Resolution Neural Operator with Atmospheric Radiative Transfer Prior](https://arxiv.org/abs/2511.17895)
*Ziye Zhang,Bin Pan,Zhenwei Shi*

Main category: eess.IV

TL;DR: 提出SSRNO框架，将大气辐射传输先验融入光谱超分辨率任务，通过三阶段处理实现物理一致的光谱重建和零样本外推


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法忽略物理原理，导致大气影响波段的光谱重建不真实，需要结合物理先验提升重建质量

Method: 三阶段框架：上采样（使用先验信息扩展多光谱图像）、重建（神经算子学习连续映射）、精炼（硬约束消除色彩失真），采用GMP方法和SAC层

Result: 实验验证了方法的有效性和泛化能力，实现了连续光谱重建和零样本外推

Conclusion: SSRNO成功将物理先验融入数据驱动方法，实现了更物理一致的光谱超分辨率重建

Abstract: Spectral super-resolution (SSR) aims to reconstruct hyperspectral images (HSIs) from multispectral observations, with broad applications in remote sensing. Data-driven methods are widely used, but they often overlook physical principles, leading to unrealistic spectra, particularly in atmosphere-affected bands. To address this challenge, we propose the Spectral Super-Resolution Neural Operator (SSRNO), which incorporates atmospheric radiative transfer (ART) prior into the data-driven procedure, yielding more physically consistent predictions. The proposed SSRNO framework consists of three stages: upsampling, reconstruction, and refinement. In the upsampling stage, we leverage prior information to expand the input multispectral image, producing a physically plausible hyperspectral estimate. Subsequently, we utilize a neural operator in the reconstruction stage to learn a continuous mapping across the spectral domain. Finally, the refinement stage imposes a hard constraint on the output HSI to eliminate color distortion. The upsampling and refinement stages are implemented via the proposed guidance matrix projection (GMP) method, and the reconstruction neural operator adopts U-shaped spectral-aware convolution (SAC) layers to capture multi-scale features. Moreover, we theoretically demonstrate the optimality of the GMP method. With the neural operator and ART priors, SSRNO also achieves continuous spectral reconstruction and zero-shot extrapolation. Various experiments validate the effectiveness and generalization ability of the proposed approach.

</details>


### [497] [Linear Algebraic Approaches to Neuroimaging Data Compression: A Comparative Analysis of Matrix and Tensor Decomposition Methods for High-Dimensional Medical Images](https://arxiv.org/abs/2511.18197)
*Jaeho Kim,Daniel David,Ana Vizitiv*

Main category: eess.IV

TL;DR: 比较Tucker分解和SVD在神经影像数据压缩中的表现，Tucker分解在保持多维关系和重建保真度方面更优，SVD在极端压缩时表现更好但牺牲保真度


<details>
  <summary>Details</summary>
Motivation: 评估不同分解方法在神经影像数据压缩中的适用性，特别是对保持结构和时间关系的需求

Method: 使用Tucker分解和奇异值分解(SVD)对神经影像数据进行压缩，比较两种方法的性能

Result: Tucker分解在保持多维关系方面表现更好，具有优越的重建保真度和感知相似性；SVD在极端压缩场景下表现优异但牺牲了保真度

Conclusion: Tucker分解更适合需要保持结构和时间关系的应用场景

Abstract: This paper evaluates Tucker decomposition and Singular Value Decomposition (SVD) for compressing neuroimaging data. Tucker decomposition preserves multi-dimensional relationships, achieving superior reconstruction fidelity and perceptual similarity. SVD excels in extreme compression but sacrifices fidelity. The results highlight Tucker decomposition's suitability for applications requiring the preservation of structural and temporal relationships.

</details>


### [498] [Shape-Adapting Gated Experts: Dynamic Expert Routing for Colonoscopic Lesion Segmentation](https://arxiv.org/abs/2511.18493)
*Gia Huy Thai,Hoang-Nguyen Vu,Anh-Minh Phan,Quang-Thinh Ly,Tram Dinh,Thi-Ngoc-Truc Nguyen,Nhat Ho*

Main category: eess.IV

TL;DR: 提出了SAGE框架，通过动态专家路由解决WSI中细胞异质性带来的挑战，在三个医学基准测试中达到最先进的分割性能。


<details>
  <summary>Details</summary>
Motivation: 细胞尺度和形态的多样性是WSI癌症检测的主要挑战，现有CNN-Transformer混合模型使用静态计算图导致冗余计算和适应性不足。

Method: SAGE框架将静态骨干网络重构为动态路由专家架构，采用双路径设计：骨干流保持表示，专家路径通过分层门控选择性激活，SA-Hub协调CNN和Transformer模块的结构和语义表示。

Result: 在EBHI、DigestPath和GlaS三个医学基准测试中分别达到95.57%、95.16%和94.17%的Dice分数，实现最先进的分割性能，并具有良好的跨域泛化能力。

Conclusion: SAGE为动态专家路由提供了可扩展基础，能够灵活地进行视觉推理，通过自适应平衡局部细化和全局上下文来提升模型性能。

Abstract: The substantial diversity in cell scale and form remains a primary challenge in computer-aided cancer detection on gigapixel Whole Slide Images (WSIs), attributable to cellular heterogeneity. Existing CNN-Transformer hybrids rely on static computation graphs with fixed routing, which consequently causes redundant computation and limits their adaptability to input variability. We propose Shape-Adapting Gated Experts (SAGE), an input-adaptive framework that enables dynamic expert routing in heterogeneous visual networks. SAGE reconfigures static backbones into dynamically routed expert architectures. SAGE's dual-path design features a backbone stream that preserves representation and selectively activates an expert path through hierarchical gating. This gating mechanism operates at multiple hierarchical levels, performing a two-level, hierarchical selection between shared and specialized experts to modulate model logits for Top-K activation. Our Shape-Adapting Hub (SA-Hub) harmonizes structural and semantic representations across the CNN and the Transformer module, effectively bridging diverse modules. Embodied as SAGE-UNet, our model achieves superior segmentation on three medical benchmarks: EBHI, DigestPath, and GlaS, yielding state-of-the-art Dice Scores of 95.57%, 95.16%, and 94.17%, respectively, and robustly generalizes across domains by adaptively balancing local refinement and global context. SAGE provides a scalable foundation for dynamic expert routing, enabling flexible visual reasoning.

</details>


### [499] [Neural B-Frame Coding: Tackling Domain Shift Issues with Lightweight Online Motion Resolution Adaptation](https://arxiv.org/abs/2511.18724)
*Sang NguyenQuang,Xiem HoangVan,Wen-Hsiao Peng*

Main category: eess.IV

TL;DR: 提出了轻量级分类器来预测下采样因子，解决B帧编码器中由于训练和测试GOP大小不匹配导致的运动估计不准确问题，在保持编码性能的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 分层时间预测的B帧编码器存在域偏移问题，训练和测试时的GOP大小不匹配导致大运动估计不准确。传统方法通过下采样视频帧将大运动转化为小运动，但确定最优下采样因子需要昂贵的率失真优化。

Method: 提出了三种轻量级分类器变体：1）使用Focal Loss训练的二分类器（Bi-Class）选择高/低分辨率；2）基于率失真成本软标签训练的多分类器（Mu-Class）；3）结合多分类器预测能力和二分类器选择性搜索的协同分类器（Co-Class）。

Result: 实验结果表明，这些分类器方法能够达到与穷举搜索方法相当的编码性能，同时显著降低计算复杂度。

Conclusion: 提出的轻量级分类器方法能够有效解决B帧编码器的域偏移问题，在不需重新训练编码器的情况下实现计算效率与编码性能的良好平衡。

Abstract: Learned B-frame codecs with hierarchical temporal prediction often encounter the domain-shift issue due to mismatches between the Group-of-Pictures (GOP) sizes for training and testing, leading to inaccurate motion estimates, particularly for large motion. A common solution is to turn large motion into small motion by downsampling video frames during motion estimation. However, determining the optimal downsampling factor typically requires costly rate-distortion optimization. This work introduces lightweight classifiers to predict downsampling factors. These classifiers leverage simple state signals from current and reference frames to balance rate-distortion performance with computational cost. Three variants are proposed: (1) a binary classifier (Bi-Class) trained with Focal Loss to choose between high and low resolutions, (2) a multi-class classifier (Mu-Class) trained with novel soft labels based on rate-distortion costs, and (3) a co-class approach (Co-Class) that combines the predictive capability of the multi-class classifier with the selective search of the binary classifier. All classifier methods can work seamlessly with existing B-frame codecs without requiring codec retraining. Experimental results show that they achieve coding performance comparable to exhaustive search methods while significantly reducing computational complexity. The code is available at: https://github.com/NYCU-MAPL/Fast-OMRA.git.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [500] [Temporal-adaptive Weight Quantization for Spiking Neural Networks](https://arxiv.org/abs/2511.17567)
*Han Zhang,Qingyan Meng,Jiaqi Wang,Baiyu Chen,Zhengyu Ma,Xiaopeng Fan*

Main category: cs.NE

TL;DR: 提出Temporal-adaptive Weight Quantization (TaWQ)方法，通过结合权重量化和时间动态，在时间维度上自适应分配超低位权重，在保持高能效的同时仅产生0.22%的量化损失。


<details>
  <summary>Details</summary>
Motivation: 受生物神经系统中星形胶质细胞介导的突触调节启发，解决SNN中权重量化在减少能耗的同时不牺牲准确性的挑战。

Method: 提出TaWQ方法，将权重量化与时间动态相结合，沿时间维度自适应分配超低位权重。

Result: 在静态(ImageNet)和神经形态(CIFAR10-DVS)数据集上的实验表明，TaWQ保持高能效(4.12M, 0.63mJ)，在ImageNet上仅产生0.22%的量化损失。

Conclusion: TaWQ方法成功实现了在脉冲神经网络中进行权重量化，在显著提高能效的同时保持了高精度。

Abstract: Weight quantization in spiking neural networks (SNNs) could further reduce energy consumption. However, quantizing weights without sacrificing accuracy remains challenging. In this study, inspired by astrocyte-mediated synaptic modulation in the biological nervous systems, we propose Temporal-adaptive Weight Quantization (TaWQ), which incorporates weight quantization with temporal dynamics to adaptively allocate ultra-low-bit weights along the temporal dimension. Extensive experiments on static (e.g., ImageNet) and neuromorphic (e.g., CIFAR10-DVS) datasets demonstrate that our TaWQ maintains high energy efficiency (4.12M, 0.63mJ) while incurring a negligible quantization loss of only 0.22% on ImageNet.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [501] [Speech Recognition Model Improves Text-to-Speech Synthesis using Fine-Grained Reward](https://arxiv.org/abs/2511.17555)
*Guansu Wang,Peijie Sun*

Main category: eess.AS

TL;DR: 提出了W3AR方法，利用预训练ASR模型的注意力机制为TTS系统提供细粒度的词级对齐奖励信号，提升TTS质量和零样本鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前TTS评估方法落后，MOS评分对整段语音进行回归评估，而失败通常发生在少数问题词上，需要更细粒度的评估方法。

Method: 利用编码器-解码器ASR模型（如Whisper）的交叉注意力机制，通过词级对齐的注意力驱动奖励信号来优化TTS模型。

Result: W3AR提升了现有TTS系统的质量，并增强了在未见说话人上的零样本鲁棒性。

Conclusion: 理解模型可以作为评估器，为生成模型提供信息丰富、细粒度的反馈进行优化。

Abstract: Recent advances in text-to-speech (TTS) have enabled models to clone arbitrary unseen speakers and synthesize high-quality, natural-sounding speech. However, evaluation methods lag behind: typical mean opinion score (MOS) estimators perform regression over entire utterances, while failures usually occur in a few problematic words. We observe that encoder-decoder ASR models (e.g., Whisper) surface word-level mismatches between speech and text via cross-attention, providing a fine-grained reward signal. Building on this, we introduce Word-level TTS Alignment by ASR-driven Attentive Reward (W3AR). Without explicit reward annotations, W3AR uses attention from a pre-trained ASR model to drive finer-grained alignment and optimization of sequences predicted by a TTS model. Experiments show that W3AR improves the quality of existing TTS systems and strengthens zero-shot robustness on unseen speakers. More broadly, our results suggest a simple recipe for generative modeling: understanding models can act as evaluators, delivering informative, fine-grained feedback for optimization.

</details>


### [502] [InstructAudio: Unified speech and music generation with natural language instruction](https://arxiv.org/abs/2511.18487)
*Chunyu Qiang,Kang Yin,Xiaopeng Wang,Yuzhe Liang,Jiahui Zhao,Ruibo Fu,Tianrui Wang,Cheng Gong,Chen Zhang,Longbiao Wang,Jianwu Dang*

Main category: eess.AS

TL;DR: InstructAudio是一个统一的指令控制框架，通过自然语言描述实现对音频属性的控制，支持语音、音乐和对话生成，在英语和中文中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的TTS和TTM系统在基于指令的控制方面存在显著限制，TTS系统通常依赖参考音频控制音色，TTM系统则受限于需要专家知识标注的输入条件。这两种任务虽然具有共同的声学建模特性，但一直独立发展，难以实现通过自然语言指令的统一建模。

Method: 采用联合和单扩散变换器层，使用标准化的指令-音素输入格式，在5万小时语音和2万小时音乐数据上进行训练，实现多任务学习和跨模态对齐。

Result: 与主流TTS和TTM模型相比，InstructAudio在大多数指标上达到最优结果，是首个统一语音和音乐生成的指令控制框架。

Conclusion: InstructAudio成功解决了TTS和TTM系统在指令控制方面的局限性，实现了通过自然语言指令统一控制音频属性的目标，为语音和音乐生成提供了统一的解决方案。

Abstract: Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation. TTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis. Despite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions. We introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese. The model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment. Fig. 1 visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics. To our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: https://qiangchunyu.github.io/InstructAudio/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [503] [MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots](https://arxiv.org/abs/2511.17889)
*Ting Huang,Dongjian Li,Rui Yang,Zeyu Zhang,Zida Yang,Hao Tang*

Main category: cs.RO

TL;DR: MobileVLA-R1是一个统一的视觉-语言-动作框架，通过构建大规模思维链数据集和两阶段训练范式，实现了四足机器人的显式推理和连续控制，在VLN和VLA任务上性能提升约5%。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将高级语义推理与低级驱动连接起来，导致在现实世界中存在不稳定的接地和弱泛化问题。

Method: 构建MobileVLA-CoT大规模思维链数据集，采用监督CoT对齐与GRPO强化学习的两阶段训练范式。

Result: 在VLN和VLA任务上表现优于强基线，性能提升约5%，在复杂环境中的四足机器人上验证了鲁棒性能。

Conclusion: MobileVLA-R1通过显式推理和连续控制的统一框架，有效解决了四足机器人的自然语言指令接地问题。

Abstract: Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.

</details>


### [504] [Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game](https://arxiv.org/abs/2511.17925)
*Jeonghwan Kim,Wontaek Kim,Yidan Lu,Jin Cheng,Fatemeh Zargarbashi,Zicheng Zeng,Zekun Qi,Zhiyang Dou,Nitish Sontakke,Donghoon Baek,Sehoon Ha,Tianyu Li*

Main category: cs.RO

TL;DR: Switch-JustDance是一个基于任天堂Switch游戏《Just Dance》的低成本、可复现的机器人全身控制基准测试框架，通过游戏内置评分系统评估机器人控制器性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人全身控制评估方法依赖预收集的人类运动数据或仿真实验，缺乏标准化基准测试，难以进行真实环境下的公平人机比较。

Method: 利用Switch游戏《Just Dance》作为平台，通过流媒体、运动重建和运动重定向模块将游戏编舞转换为机器人可执行动作，使用游戏内置评分系统评估控制器性能。

Result: 验证了Just Dance平台在可靠性、有效性、敏感性和潜在偏差方面的评估特性，证明其能提供一致且可解释的性能度量。基于此框架评估了三种最先进的人形机器人全身控制器。

Conclusion: Switch-JustDance为机器人全身控制提供了一个低成本、可复现的基准测试工具，能够有效评估和比较不同控制器的性能表现。

Abstract: Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.

</details>


### [505] [Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting](https://arxiv.org/abs/2511.18140)
*Yilong Wang,Cheng Qian,Ruomeng Fan,Edward Johns*

Main category: cs.RO

TL;DR: ObAct是一个主动视觉模仿学习框架，通过动态分配观察者和执行者角色，让观察者移动到最佳视觉位置为执行者提供清晰视野，从而提升策略的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决静态摄像头在机器人操作任务中因遮挡和视角不佳导致的观察质量下降问题，提升模仿学习策略的鲁棒性。

Method: 使用双臂机器人系统，动态分配观察者和执行者角色：观察者臂构建3D高斯泼溅表示，虚拟探索找到最佳相机位姿后移动到该位置；执行者臂使用观察者的观察执行策略。

Result: 相比静态摄像头设置，轨迹转移方法在无遮挡和有遮挡情况下分别提升145%和233%，行为克隆分别提升75%和143%。

Conclusion: ObAct框架通过主动视觉观察显著提升了模仿学习策略的性能和鲁棒性，使策略能够在更接近无遮挡训练分布的条件下执行任务。

Abstract: We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.

</details>


### [506] [Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video](https://arxiv.org/abs/2511.18322)
*Henrik Krauss,Johann Licher,Naoya Takeishi,Annika Raatz,Takehisa Yairi*

Main category: cs.RO

TL;DR: 提出了Attention Broadcast Decoder (ABCD)模块，用于软体连续机器人的数据驱动动力学学习，能够生成像素级注意力图来定位潜在维度贡献，同时过滤静态背景。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的软体连续机器人动力学学习缺乏物理可解释性，而基于模型的方法需要先验知识且计算成本高。需要弥合这一差距。

Method: 引入ABCD模块作为自动编码器潜在动力学学习的即插即用组件，通过将注意力图与2D振荡器网络耦合，实现学习动力学的直接可视化。

Result: 在单段和双段软体连续机器人上验证，ABCD模型显著提高多步预测精度：双段机器人的Koopman算子误差减少5.7倍，振荡器网络误差减少3.5倍。

Conclusion: 这种完全数据驱动的方法产生了紧凑、物理可解释的模型，适用于控制应用，能够实现超出训练数据的平滑潜在空间外推。

Abstract: Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.

</details>


### [507] [Enhancing UAV Search under Occlusion using Next Best View Planning](https://arxiv.org/abs/2511.18353)
*Sigrid Helene Strand,Thomas Wiedemann,Bram Burczek,Dmitriy Shutin*

Main category: cs.RO

TL;DR: 提出了一种用于遮挡环境中无人机搜索救援的优化规划策略，通过几何启发式和可见性启发式两种新方法来解决最佳视角问题，可见性启发式在模拟和真实环境中都表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 在密集森林等遮挡环境中进行搜救任务时，无人机需要优化相机位置和视角来捕捉清晰的地面视图，但现有方法在遮挡环境中的搜索效果有限。

Method: 提出了两种新颖的优化启发式方法：几何启发式和可见性启发式，用于选择最佳相机视点，并开发了高效的算法来解决遮挡环境中的最佳视角问题。

Result: 可见性启发式在模拟森林中识别了超过90%的隐藏物体，比几何启发式的检测率高10%。真实世界实验表明可见性启发式在树冠下提供更好的覆盖范围。

Conclusion: 可见性启发式在遮挡环境中具有更好的搜救性能，能够显著提高无人机在密集森林等复杂环境中的搜索效率。

Abstract: Search and rescue missions are often critical following sudden natural disasters or in high-risk environmental situations. The most challenging search and rescue missions involve difficult-to-access terrains, such as dense forests with high occlusion. Deploying unmanned aerial vehicles for exploration can significantly enhance search effectiveness, facilitate access to challenging environments, and reduce search time. However, in dense forests, the effectiveness of unmanned aerial vehicles depends on their ability to capture clear views of the ground, necessitating a robust search strategy to optimize camera positioning and perspective. This work presents an optimized planning strategy and an efficient algorithm for the next best view problem in occluded environments. Two novel optimization heuristics, a geometry heuristic, and a visibility heuristic, are proposed to enhance search performance by selecting optimal camera viewpoints. Comparative evaluations in both simulated and real-world settings reveal that the visibility heuristic achieves greater performance, identifying over 90% of hidden objects in simulated forests and offering 10% better detection rates than the geometry heuristic. Additionally, real-world experiments demonstrate that the visibility heuristic provides better coverage under the canopy, highlighting its potential for improving search and rescue missions in occluded environments.

</details>


### [508] [AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations](https://arxiv.org/abs/2511.18617)
*Litian Gong,Fatemeh Bahrani,Yutai Zhou,Amin Banayeeanzade,Jiachen Li,Erdem Biyik*

Main category: cs.RO

TL;DR: AutoFocus-IL是一种通过视觉语言模型自动生成时间显著性图来引导策略关注任务相关特征的视觉模仿学习方法，无需昂贵的人工监督，在数据效率和泛化性方面优于标准行为克隆和最先进的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有显著性正则化方法需要昂贵的人工监督（如人类注视数据或手动显著性标注），限制了实际应用。AutoFocus-IL旨在开发一种无需人工监督的方法，通过自动识别关键对象来改善视觉模仿学习的数据效率和泛化能力。

Method: 利用视觉语言模型自动识别和跟踪演示中的关键对象，生成时间显著性图来突出因果视觉信号并抑制干扰因素。这些图用于正则化行为克隆策略，增强视觉注意力与任务相关线索的对齐。

Result: 在CARLA模拟器和真实机器人操作任务中的实验表明，AutoFocus-IL不仅优于标准行为克隆，还超过了需要人类监督（如注视数据）的最先进基线方法。

Conclusion: AutoFocus-IL提供了一种简单有效的视觉模仿学习方法，通过自动生成的显著性图引导策略关注任务相关特征，无需昂贵的人工监督，在数据效率和泛化性方面表现出色。

Abstract: AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.

</details>


### [509] [Stable Multi-Drone GNSS Tracking System for Marine Robots](https://arxiv.org/abs/2511.18694)
*Shuo Wen,Edwin Meriaux,Mariana Sosa Guzmán,Zhizun Wang,Junming Shi,Gregory Dudek*

Main category: cs.RO

TL;DR: 提出了一种基于多无人机GNSS的海洋机器人跟踪系统，通过视觉检测、多目标跟踪和GNSS三角定位，结合置信度加权的扩展卡尔曼滤波器，实现实时稳定的GNSS估计。


<details>
  <summary>Details</summary>
Motivation: 水下环境中GNSS信号不可靠，传统定位方法存在误差累积、计算量大或依赖基础设施等问题，需要一种可靠的水面及近水面海洋机器人定位方案。

Method: 结合高效视觉检测、轻量级多目标跟踪、GNSS三角定位和置信度加权的扩展卡尔曼滤波器，并引入跨无人机跟踪ID对齐算法确保全局一致性。

Result: 在多样化复杂场景中验证了系统的可扩展性和鲁棒性，能够实现稳定的多机器人跟踪。

Conclusion: 该系统为水面及近水面海洋机器人提供了一种可靠、实时的定位解决方案，解决了传统方法的局限性。

Abstract: Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.

</details>


### [510] [CNN-Based Camera Pose Estimation and Localisation of Scan Images for Aircraft Visual Inspection](https://arxiv.org/abs/2511.18702)
*Xueyan Oh,Leonard Loh,Shaohui Foong,Zhong Bao Andy Koh,Kow Leong Ng,Poh Kang Tan,Pei Lin Pearlin Toh,U-Xuan Tan*

Main category: cs.RO

TL;DR: 提出一种无需基础设施的现场方法，用于估计PTZ相机姿态并定位扫描图像，通过深度卷积神经网络在合成图像上进行微调来预测相机姿态，在真实飞机上实现小于0.24米和2度的姿态估计误差。


<details>
  <summary>Details</summary>
Motivation: 商用飞机外部目视检查通常需要人工进行，但存在自动化需求以减少对人力的依赖。现有定位方法大多需要基础设施，这在不受控制的室外环境和有限的周转时间内具有挑战性，且许多航空公司不允许接触飞机表面或使用无人机进行检查。

Method: 使用用于检查任务的同一PTZ相机进行初始化，通过仅在合成图像上微调的深度卷积神经网络预测自身姿态。应用领域随机化生成数据集，并利用飞机几何改进损失函数。提出初始化、扫描路径规划和图像精确定位的工作流程。

Result: 在真实飞机上进行实验验证，所有真实场景的相机姿态估计均方根误差均小于0.24米和2度。

Conclusion: 该方法无需基础设施且易于部署，能够有效解决飞机外部检查中的相机定位问题，满足机场停机坪有限周转时间的需求。

Abstract: General Visual Inspection is a manual inspection process regularly used to detect and localise obvious damage on the exterior of commercial aircraft. There has been increasing demand to perform this process at the boarding gate to minimise the downtime of the aircraft and automating this process is desired to reduce the reliance on human labour. Automating this typically requires estimating a camera's pose with respect to the aircraft for initialisation but most existing localisation methods require infrastructure, which is very challenging in uncontrolled outdoor environments and within the limited turnover time (approximately 2 hours) on an airport tarmac. Additionally, many airlines and airports do not allow contact with the aircraft's surface or using UAVs for inspection between flights, and restrict access to commercial aircraft. Hence, this paper proposes an on-site method that is infrastructure-free and easy to deploy for estimating a pan-tilt-zoom camera's pose and localising scan images. This method initialises using the same pan-tilt-zoom camera used for the inspection task by utilising a Deep Convolutional Neural Network fine-tuned on only synthetic images to predict its own pose. We apply domain randomisation to generate the dataset for fine-tuning the network and modify its loss function by leveraging aircraft geometry to improve accuracy. We also propose a workflow for initialisation, scan path planning, and precise localisation of images captured from a pan-tilt-zoom camera. We evaluate and demonstrate our approach through experiments with real aircraft, achieving root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes.

</details>


### [511] [Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation](https://arxiv.org/abs/2511.18950)
*Juntao Gao,Feiyang Ye,Jing Zhang,Wenjing Qian*

Main category: cs.RO

TL;DR: 提出Compressor-VLA框架，通过语义任务压缩器和空间细化压缩器实现指令引导的视觉token压缩，在保持任务性能的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型在具身AI中面临计算开销大的问题，现有任务无关的token剪枝方法难以保留任务关键视觉信息。

Method: 提出混合指令条件token压缩框架，包含语义任务压缩器（提取整体任务相关上下文）和空间细化压缩器（保留细粒度空间细节），通过自然语言指令动态调节压缩过程。

Result: 在LIBERO基准测试中达到竞争性成功率，FLOPs减少59%，视觉token数量减少超过3倍，真实机器人部署验证了模拟到现实的迁移性和实用性。

Conclusion: Compressor-VLA有效实现了任务导向的视觉信息压缩，指令引导能动态调整模型感知焦点到任务相关对象。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.

</details>


### [512] [Mixture of Horizons in Action Chunking](https://arxiv.org/abs/2511.19433)
*Dong Jing,Gang Wang,Jiaqi Liu,Weiliang Tang,Zelong Sun,Yunchao Yao,Zhenyu Wei,Yunhui Liu,Zhiwu Lu,Mingyu Ding*

Main category: cs.RO

TL;DR: 提出了混合视野（MoH）策略来解决VLA模型中动作块长度选择的权衡问题，通过并行处理不同视野的动作段并融合输出，同时获得长期预见性和短期精确性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在机器人操作中性能对动作块长度（视野）敏感，存在长期视野提供全局预见性但降低细粒度精度，短期视野提升局部控制但难以处理长期任务的固有权衡。

Method: MoH将动作块重新排列为多个不同视野的段，用共享动作变换器并行处理，通过轻量线性门融合输出，支持动态推理和自适应视野选择。

Result: 在π₀、π₀.₅和π_reg策略上的实验表明，MoH在仿真和真实任务中均带来显著提升，在混合任务设置下π₀.₅+MoH在LIBERO上达到99%平均成功率的新SOTA。

Conclusion: MoH策略有效缓解了视野选择的权衡问题，在保持高性能的同时实现了2.5倍吞吐量提升，是即插即用的高效解决方案。

Abstract: Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\textbf{action chunk length}$ used during training, termed $\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons

</details>
