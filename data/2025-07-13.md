<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.CV](#cs.CV) [Total: 22]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 论文提出了一种两步因果实验方法，研究大型语言模型（LLMs）中认知偏见的来源，发现偏见主要源于预训练而非微调或随机噪声。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs中认知偏见的来源，明确预训练、微调和训练随机性对偏见的影响。

Method: 采用两步实验：1）多次微调模型以研究训练随机性对偏见的影响；2）引入跨调换（cross-tuning）方法，交换指令数据集以隔离偏见来源。

Result: 训练随机性引入一定变异性，但偏见主要由预训练决定；相同预训练骨干的模型偏见模式更相似。

Conclusion: 理解微调模型的偏见需考虑预训练来源，为未来评估和减少LLMs偏见提供指导。

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [2] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在社会科学调查中的可靠性，发现其对问题表述和答案选项的扰动敏感，并表现出明显的“近因偏差”。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs作为人类受试者代理的可靠性及其对已知响应偏差的易感性。

Method: 测试了九种不同的LLMs，应用11种扰动（包括问题表述和答案选项结构的变化），模拟了超过167,000次访谈。

Result: 所有模型均表现出不同程度的近因偏差，且对语义变化（如改写）和组合扰动敏感。较大模型更稳健，但仍存在局限性。

Conclusion: 提示设计和鲁棒性测试在利用LLMs生成合成调查数据时至关重要。

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [3] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

TL;DR: SynthTextEval是一个用于全面评估合成文本的工具包，支持多维度评估，包括实用性、公平性、隐私风险等，适用于高风险领域如医疗和法律。


<details>
  <summary>Details</summary>
Motivation: 合成文本在AI开发中具有潜力，但需要系统评估以确保其可行性和隐私保护效果。

Method: 提供多维度评估工具，支持用户上传或生成合成文本，并标准化评估指标。

Result: 工具包在医疗和法律领域的数据集上展示了功能和有效性。

Conclusion: 通过标准化评估指标，SynthTextEval旨在提升合成文本的可行性和AI开发中的隐私保护。

Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [4] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文提出了一种针对医疗领域大语言模型（LLMs）的安全评估协议，填补了现有研究中缺乏医疗领域特定安全评估的空白。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在医疗领域的广泛应用，其安全性问题日益突出，尤其是对患者和临床医生的潜在影响。现有评估多关注通用领域，缺乏医疗领域的针对性。

Method: 设计了针对患者和临床医生视角的安全评估协议，构建了包含466个样本的PatientSafetyBench，并在MediPhi模型上进行了红队测试。

Result: 首次从患者、临床医生和普通用户三个视角定义了医疗LLMs的安全评估标准，为医疗领域的安全部署奠定了基础。

Conclusion: 该研究为医疗LLMs的安全性提供了系统评估方法，有助于未来更安全的医疗应用部署。

Abstract: As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [5] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

TL;DR: 论文研究了协作学习中打断行为的重要性，并提出了一种在多组对话环境中识别打断的先进方法，适用于教室场景。


<details>
  <summary>Details</summary>
Motivation: 协作学习中的打断行为对知识构建至关重要，但现有研究多基于单对话环境，而教室中多组对话的重叠语音问题需要解决。

Method: 分析了单对话和多组对话环境中的打断检测，开发了一种对重叠语音鲁棒的打断识别方法。

Result: 提出了一种适用于教室环境的打断识别方法，并揭示了打断在协作互动中的语言和韵律特征。

Conclusion: 研究为未来在多组对话中跟踪打断行为提供了基础，强调了重叠语音的影响。

Abstract: Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [6] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

TL;DR: 提出了一种多智能体检索增强框架，用于生成针对健康错误信息的反驳言论，通过结合静态和动态证据优化生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖有限证据且对最终输出控制不足，需改进反驳言论的生成质量。

Method: 采用多智能体框架，结合多个LLM优化知识检索、证据增强和响应精炼，整合静态与动态证据。

Result: 方法在礼貌性、相关性、信息量和事实准确性上优于基线，并通过消融研究和人工评估验证了其有效性。

Conclusion: 该框架能生成高质量反驳言论，且精炼过程显著提升质量并获人类偏好。

Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [7] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: 提出了一种结合GNN和CNN的新模型架构，用于高效处理长文本，解决了Transformer的二次计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在处理长文本时的时间和计算资源消耗问题，提高效率。

Method: 结合GNN和CNN，利用实时图生成机制处理字符级输入，集成LLM信息，通过CNN捕捉局部模式，图结构扩展感受野。

Result: 模型在文本分类任务中表现高效且竞争力强，生成的图具有语义组织特性。

Conclusion: 新模型在效率和性能上均优于现有技术，适用于长文本处理。

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [8] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: MedReadCtrl是一个可读性控制的指令调优框架，帮助LLMs在不损失意义的情况下调整输出复杂度，显著优于GPT-4，并在临床任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决医疗领域中生成AI在人类-AI沟通中的挑战，确保内容既个性化又易于理解。

Method: 引入MedReadCtrl框架，通过指令调优控制输出复杂度，评估了九个数据集和三个任务。

Result: MedReadCtrl在可读性指令跟随错误率上显著低于GPT-4，临床任务表现优异，专家更偏好其输出。

Conclusion: MedReadCtrl能够将临床内容转化为易于理解的语言，同时保留医学意图，为患者教育和AI医疗公平提供了可扩展的解决方案。

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [9] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

TL;DR: SynthEHR-Eviction是一个结合LLMs、人工标注和自动提示优化的管道，用于从临床记录中提取驱逐状态，创建了最大的公共驱逐相关SDoH数据集，性能优于GPT-4o和BioBERT。


<details>
  <summary>Details</summary>
Motivation: 驱逐是健康的社会决定因素（SDoH）之一，但研究不足，且电子健康记录（EHRs）中缺乏结构化数据，限制了应用。

Method: 结合LLMs、人工标注和自动提示优化（APO）构建管道，提取临床记录中的驱逐状态，并创建细粒度数据集。

Result: 微调后的LLMs在驱逐和其他SDoH任务上分别达到88.8%和90.3%的Macro-F1分数，优于其他模型，同时降低标注成本80%。

Conclusion: 该管道高效、可扩展，适用于其他信息提取任务，为驱逐相关研究提供了新工具。

Abstract: Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [10] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

TL;DR: 研究将时间序列推理能力蒸馏到小型指令调优语言模型中，构建可解释的时间序列基础模型。


<details>
  <summary>Details</summary>
Motivation: 开发轻量级、可解释的模型，以自然语言解释时间序列模式，适用于设备端或隐私敏感场景。

Method: 使用合成数据集生成自然语言注释，监督微调紧凑Qwen模型，并引入评估指标。

Result: 微调后的模型获得有意义的解释能力，验证了压缩时间序列理解的可行性。

Conclusion: 为开发小型、可解释的时间序列模型提供了具体基础。

Abstract: In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [11] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

TL;DR: 本文提出了一种名为SAND的框架，通过显式地让LLM代理在行动前进行候选行动审议，解决了现有方法因行动空间探索不足而导致的次优行动问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如监督微调或偏好优化）可能导致LLM代理过度依赖看似合理但次优的行动，缺乏对替代行动的探索和比较。

Method: SAND框架通过自洽行动采样和执行引导的行动评价，合成逐步行动审议思路，并利用这些审议轨迹迭代微调LLM代理。

Result: 在两个代表性交互代理任务中，SAND比初始监督微调平均提升20%，并优于现有最先进的代理调优方法。

Conclusion: SAND框架通过行动审议显著提升了LLM代理的性能，证明了其在解决行动空间探索不足问题上的有效性。

Abstract: Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [12] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

TL;DR: RLEP框架通过重放已验证的高质量轨迹，提升大语言模型在强化学习中的稳定性和性能，显著减少训练时间并提高最终准确率。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习训练大语言模型时的不稳定性和策略漂移问题。

Method: 采用两阶段框架：先收集已验证轨迹，再在训练中重放这些轨迹，混合新生成的rollouts进行优化。

Result: 在多个基准测试中显著提升准确率（如AIME-2024从38.2%提升至39.9%）。

Conclusion: RLEP通过重放高质量经验，有效提升训练效率和模型性能。

Abstract: Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


### [13] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

TL;DR: 论文提出‘机器废话’作为大语言模型（LLM）中真实性缺失的框架，并引入‘废话指数’和分类法，评估了RLHF和CoT提示对废话的影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLM中真实性缺失的普遍现象，提出‘机器废话’作为统一框架，以揭示其机制。

Method: 引入废话指数和四种废话分类法，在多个数据集（包括新设计的BullshitEval）上进行实证评估。

Result: RLHF显著加剧废话，CoT提示放大特定形式；政治语境中废话普遍，以‘模糊措辞’为主。

Conclusion: 研究揭示了AI对齐的系统性挑战，为提升LLM真实性提供了新见解。

Abstract: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


### [14] [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
*Mihir Parmar,Palash Goyal,Xin Liu,Yiwen Song,Mingyang Ling,Chitta Baral,Hamid Palangi,Tomas Pfister*

Main category: cs.CL

TL;DR: PLAN-TUNING是一种后训练框架，通过从大语言模型提取任务分解（规划轨迹）并微调小模型，提升复杂推理能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 利用任务分解（类似人类规划）提升小语言模型的性能，目前研究较少。

Method: 从大语言模型提取规划轨迹，通过监督和强化学习微调小模型。

Result: 在GSM8k和MATH基准上平均提升7%，在OlympiadBench和AIME 2024上分别提升10%和12%。

Conclusion: PLAN-TUNING有效提升小语言模型的复杂推理能力，具有泛化优势。

Abstract: Recently, decomposing complex problems into simple subtasks--a crucial part
of human-like natural planning--to solve the given problem has significantly
boosted the performance of large language models (LLMs). However, leveraging
such planning structures during post-training to boost the performance of
smaller open-source LLMs remains underexplored. Motivated by this, we introduce
PLAN-TUNING, a unified post-training framework that (i) distills synthetic task
decompositions (termed "planning trajectories") from large-scale LLMs and (ii)
fine-tunes smaller models via supervised and reinforcement-learning objectives
designed to mimic these planning processes to improve complex reasoning. On
GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by
an average $\sim7\%$. Furthermore, plan-tuned models show better generalization
capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$
performance improvements on OlympiadBench and AIME 2024, respectively. Our
detailed analysis demonstrates how planning trajectories improves complex
reasoning capabilities, showing that PLAN-TUNING is an effective strategy for
improving task-specific performance of smaller LLMs.

</details>


### [15] [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
*Keqin Bao,Nuo Chen,Xiaoyuan Li,Binyuan Hui,Bowen Yu,Fuli Feng,Junyang Lin,Xiangnan He,Dayiheng Liu*

Main category: cs.CL

TL;DR: 论文提出TeaR方法，通过数据筛选和强化学习提升LLM的推理能力，实验显示显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 增强LLM的推理能力是研究重点，但现有方法依赖复杂代码结构，导致过拟合。

Method: TeaR结合数据筛选和强化学习，优化模型在代码任务中的推理路径。

Result: 在多个基准测试中，TeaR显著提升性能，如Qwen2.5-7B提升35.9%。

Conclusion: TeaR有效提升LLM的通用推理能力，为未来研究提供新方向。

Abstract: Enhancing reasoning capabilities remains a central focus in the LLM reasearch
community. A promising direction involves requiring models to simulate code
execution step-by-step to derive outputs for given inputs. However, as code is
often designed for large-scale systems, direct application leads to
over-reliance on complex data structures and algorithms, even for simple cases,
resulting in overfitting to algorithmic patterns rather than core reasoning
structures. To address this, we propose TeaR, which aims at teaching LLMs to
reason better. TeaR leverages careful data curation and reinforcement learning
to guide models in discovering optimal reasoning paths through code-related
tasks, thereby improving general reasoning abilities. We conduct extensive
experiments using two base models and three long-CoT distillation models, with
model sizes ranging from 1.5 billion to 32 billion parameters, and across 17
benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results
consistently show significant performance improvements. Notably, TeaR achieves
a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

</details>


### [16] [Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature](https://arxiv.org/abs/2507.07499)
*Hein Htet,Amgad Ahmed Ali Ibrahim,Yutaka Sasaki,Ryoji Asahi*

Main category: cs.CL

TL;DR: 提出了一种基于DyGIE++和多种BERT变体的NER与RE方法，用于从科学文献中提取ORR催化剂信息，构建了FC-CoMIcs数据集，并验证了领域特定BERT模型的优越性。


<details>
  <summary>Details</summary>
Motivation: ORR催化剂对提升燃料电池效率至关重要，但从复杂多样的文献中提取结构化信息仍具挑战性。

Method: 采用DyGIE++结合MatSciBERT和PubMedBERT等预训练模型，进行数据标注、整合和微调，以提升信息提取准确性。

Result: 微调的PubMedBERT在NER任务中F1-score达82.19%，MatSciBERT在RE任务中F1-score为66.10%，领域特定模型优于通用模型。

Conclusion: 该方法展示了自动化文献分析的潜力，领域特定BERT模型在ORR催化剂提取中表现更优。

Abstract: The oxygen reduction reaction (ORR) catalyst plays a critical role in
enhancing fuel cell efficiency, making it a key focus in material science
research. However, extracting structured information about ORR catalysts from
vast scientific literature remains a significant challenge due to the
complexity and diversity of textual data. In this study, we propose a named
entity recognition (NER) and relation extraction (RE) approach using DyGIE++
with multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,
to extract ORR catalyst-related information from the scientific literature,
which is compiled into a fuel cell corpus for materials informatics
(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12
critical entities and two relationship types between pairs of the entities. Our
methodology involves data annotation, integration, and fine-tuning of
transformer-based models to enhance information extraction accuracy. We assess
the impact of different BERT variants on extraction performance and investigate
the effects of annotation consistency. Experimental evaluations demonstrate
that the fine-tuned PubMedBERT model achieves the highest NER F1-score of
82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.
Furthermore, the comparison with human annotators highlights the reliability of
fine-tuned models for ORR catalyst extraction, demonstrating their potential
for scalable and automated literature analysis. The results indicate that
domain-specific BERT models outperform general scientific models like BlueBERT
for ORR catalyst extraction.

</details>


### [17] [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505)
*Varin Sikka,Vishal Sikka*

Main category: cs.CL

TL;DR: LLMs在复杂计算和代理任务中存在能力限制，无法验证超出其复杂性范围的任务准确性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在AI中的广泛应用，研究其能力限制（如幻觉问题）和代理任务中的表现变得重要。

Method: 从计算复杂性的角度分析LLM推理能力，并通过示例展示其局限性。

Result: LLMs无法执行或验证超出特定复杂性的计算和代理任务。

Conclusion: 研究揭示了LLMs的能力边界，对实际应用中的任务设计具有指导意义。

Abstract: With widespread adoption of transformer-based language models in AI, there is
significant interest in the limits of LLMs capabilities, specifically so-called
hallucinations, occurrences in which LLMs provide spurious, factually incorrect
or nonsensical information when prompted on certain subjects. Furthermore,
there is growing interest in agentic uses of LLMs - that is, using LLMs to
create agents that act autonomously or semi-autonomously to carry out various
tasks, including tasks with applications in the real world. This makes it
important to understand the types of tasks LLMs can and cannot perform. We
explore this topic from the perspective of the computational complexity of LLM
inference. We show that LLMs are incapable of carrying out computational and
agentic tasks beyond a certain complexity, and further that LLMs are incapable
of verifying the accuracy of tasks beyond a certain complexity. We present
examples of both, then discuss some consequences of this work.

</details>


### [18] [Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System](https://arxiv.org/abs/2507.07509)
*Yuanchen Shi,Longyin Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 提出了一种利用有限真实数据和专家知识微调语言模型的框架，构建了中文心理支持对话数据集（CPsDD），并开发了综合代理对话支持系统（CADSS），在策略预测和情感支持对话任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决非英语语言中心理支持数据集稀缺的问题，满足日益增长的心理支持需求。

Method: 通过Dialog Generator和Dialog Modifier生成和优化心理辅导对话，构建CPsDD数据集，并开发CADSS系统，包含Profiler、Summarizer、Planner和Supporter模块。

Result: CPsDD包含68K对话，覆盖13组、16种心理问题、13种原因和12种支持重点；CADSS在CPsDD和ESConv数据集上达到最优性能。

Conclusion: 提出的框架和系统有效解决了心理支持数据稀缺问题，并在实际任务中表现出色。

Abstract: The growing need for psychological support due to increasing pressures has
exposed the scarcity of relevant datasets, particularly in non-English
languages. To address this, we propose a framework that leverages limited
real-world data and expert knowledge to fine-tune two large language models:
Dialog Generator and Dialog Modifier. The Generator creates large-scale
psychological counseling dialogues based on predefined paths, which guide
system response strategies and user interactions, forming the basis for
effective support. The Modifier refines these dialogues to align with
real-world data quality. Through both automated and manual review, we construct
the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K
dialogues across 13 groups, 16 psychological problems, 13 causes, and 12
support focuses. Additionally, we introduce the Comprehensive Agent Dialogue
Support System (CADSS), where a Profiler analyzes user characteristics, a
Summarizer condenses dialogue history, a Planner selects strategies, and a
Supporter generates empathetic responses. The experimental results of the
Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate
that CADSS achieves state-of-the-art performance on both CPsDD and ESConv
datasets.

</details>


### [19] [Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems](https://arxiv.org/abs/2507.07518)
*Mikey Elmers,Koji Inoue,Divesh Lala,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 研究将语音活动预测（VAP）应用于三元对话中的轮流发言预测，发现三元对话训练的VAP模型优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统研究多关注二元对话，而三元对话中的轮流发言预测尚未充分探索。

Method: 在日语三元对话数据集上训练多个VAP模型，仅使用声学数据预测未来语音活动。

Result: 三元对话训练的VAP模型在所有模型中表现最佳，但对话类型影响准确性。

Conclusion: VAP可用于三元对话中的轮流发言预测，未来将整合到语音对话系统中。

Abstract: Turn-taking is a fundamental component of spoken dialogue, however
conventional studies mostly involve dyadic settings. This work focuses on
applying voice activity projection (VAP) to predict upcoming turn-taking in
triadic multi-party scenarios. The goal of VAP models is to predict the future
voice activity for each speaker utilizing only acoustic data. This is the first
study to extend VAP into triadic conversation. We trained multiple models on a
Japanese triadic dataset where participants discussed a variety of topics. We
found that the VAP trained on triadic conversation outperformed the baseline
for all models but that the type of conversation affected the accuracy. This
study establishes that VAP can be used for turn-taking in triadic dialogue
scenarios. Future work will incorporate this triadic VAP turn-taking model into
spoken dialogue systems.

</details>


### [20] [CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text](https://arxiv.org/abs/2507.07539)
*Akram Elbouanani,Evan Dufraisse,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLMs）和少样本提示的多语言主观性检测方法，在CheckThat! 2025评测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在少样本提示下的表现，尤其是在噪声或低质量数据环境中，是否能够超越传统微调的小语言模型（SLMs）。

Method: 使用精心设计的少样本提示，结合LLMs，并尝试了辩论式提示和多种示例选择策略。

Result: 在CheckThat! 2025评测中，系统在多个语言（如阿拉伯语和波兰语）中排名第一，其他语言也进入前四。尤其在阿拉伯语数据上表现稳健。

Conclusion: LLM-based少样本学习在多语言情感任务中高效且适应性强，为传统微调提供了有力替代方案，尤其在标注数据稀缺或不一致时。

Abstract: This paper presents a competitive approach to multilingual subjectivity
detection using large language models (LLMs) with few-shot prompting. We
participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation
campaign. We show that LLMs, when paired with carefully designed prompts, can
match or outperform fine-tuned smaller language models (SLMs), particularly in
noisy or low-quality data settings. Despite experimenting with advanced prompt
engineering techniques, such as debating LLMs and various example selection
strategies, we found limited benefit beyond well-crafted standard few-shot
prompts. Our system achieved top rankings across multiple languages in the
CheckThat! 2025 subjectivity detection task, including first place in Arabic
and Polish, and top-four finishes in Italian, English, German, and multilingual
tracks. Notably, our method proved especially robust on the Arabic dataset,
likely due to its resilience to annotation inconsistencies. These findings
highlight the effectiveness and adaptability of LLM-based few-shot learning for
multilingual sentiment tasks, offering a strong alternative to traditional
fine-tuning, particularly when labeled data is scarce or inconsistent.

</details>


### [21] [The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora](https://arxiv.org/abs/2507.07543)
*Chen Amiraz,Yaroslav Fyodorov,Elad Haramaty,Zohar Karnin,Liane Lewin-Eytan*

Main category: cs.CL

TL;DR: 研究跨语言检索增强生成（RAG）在特定领域中的表现，发现检索是瓶颈，并提出改进策略。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言检索在特定领域中的隐藏问题，如语言不平衡和预训练数据重叠。

Method: 使用阿拉伯语-英语的真实企业数据集，分析不同语言组合的检索行为。

Result: 检索是跨语言场景的主要瓶颈，提出策略显著提升性能。

Conclusion: 改进跨语言检索在现实RAG应用中有重要意义。

Abstract: Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with significant performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose a simple retrieval strategy
that addresses this source of failure by enforcing equal retrieval from both
languages, resulting in substantial improvements in cross-lingual and overall
performance. These results highlight meaningful opportunities for improving
multilingual retrieval, particularly in practical, real-world RAG applications.

</details>


### [22] [The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs](https://arxiv.org/abs/2507.07562)
*Jierun Chen,Tiezheng Yu,Haoli Bai,Lewei Yao,Jiannan Wu,Kaican Li,Fei Mi,Chaofan Tao,Lei Zhu,Manyi Zhang,Xiaohui Li,Lu Hou,Lifeng Shang,Qun Liu*

Main category: cs.CL

TL;DR: 研究了长链思维监督微调（SFT）和强化学习（RL）在多模态推理模型中的协同效果，发现两者结合未能产生叠加优势，反而存在权衡。


<details>
  <summary>Details</summary>
Motivation: 探索长链思维SFT和RL在多模态推理模型中的联合有效性及其相互作用。

Method: 通过多模态推理基准测试，分析SFT和RL的独立及联合效果。

Result: SFT提升复杂问题性能但降低简单问题表现；RL提高泛化性但复杂问题改进有限；结合方法未能实现叠加优势。

Conclusion: 需开发更自适应的方法以充分发挥联合后训练技术的潜力。

Abstract: Large vision-language models (VLMs) increasingly adopt post-training
techniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and
reinforcement learning (RL) to elicit sophisticated reasoning. While these
methods exhibit synergy in language-only models, their joint effectiveness in
VLMs remains uncertain. We present a systematic investigation into the distinct
roles and interplay of long-CoT SFT and RL across multiple multimodal reasoning
benchmarks. We find that SFT improves performance on difficult questions by
in-depth, structured reasoning, but introduces verbosity and degrades
performance on simpler ones. In contrast, RL promotes generalization and
brevity, yielding consistent improvements across all difficulty levels, though
the improvements on the hardest questions are less prominent compared to SFT.
Surprisingly, combining them through two-staged, interleaved, or progressive
training strategies, as well as data mixing and model merging, all fails to
produce additive benefits, instead leading to trade-offs in accuracy, reasoning
style, and response length. This ``synergy dilemma'' highlights the need for
more seamless and adaptive approaches to unlock the full potential of combined
post-training techniques for reasoning VLMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [23] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

TL;DR: 论文提出了一种多模态实体链接（MEL）方法MMoE，通过多级专家混合机制解决现有方法中的提及歧义和模态内容动态选择问题。


<details>
  <summary>Details</summary>
Motivation: 现有MEL方法未能解决提及歧义和模态内容动态选择问题，限制了性能。

Method: MMoE包含描述感知提及增强、多模态特征提取、以及两级专家混合模块，动态选择信息。

Result: 实验表明MMoE优于现有方法。

Conclusion: MMoE通过动态选择多模态信息，显著提升了MEL任务性能。

Abstract: Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [24] [CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings](https://arxiv.org/abs/2507.07125)
*Cristina Mata,Kanchana Ranasinghe,Michael S. Ryoo*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoPT的新方法，利用文本嵌入的无监督域适应（UDA）技术，提升语义分割的性能。


<details>
  <summary>Details</summary>
Motivation: 语义分割的标注成本高，而现有UDA方法未充分利用文本的领域无关特性。

Method: 通过LLM生成领域描述，结合CLIP模型生成文本嵌入，提出Covariance-based Pixel-Text loss（CoPT）学习领域不变特征。

Result: 在四个基准测试中，CoPT实现了UDA分割的最新性能。

Conclusion: CoPT通过文本嵌入显著提升了UDA分割的效果，代码已开源。

Abstract: Unsupervised domain adaptation (UDA) involves learning class semantics from
labeled data within a source domain that generalize to an unseen target domain.
UDA methods are particularly impactful for semantic segmentation, where
annotations are more difficult to collect than in image classification. Despite
recent advances in large-scale vision-language representation learning, UDA
methods for segmentation have not taken advantage of the domain-agnostic
properties of text. To address this, we present a novel Covariance-based
Pixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn
domain-invariant features in an image segmentation encoder. The text embeddings
are generated through our LLM Domain Template process, where an LLM is used to
generate source and target domain descriptions that are fed to a frozen CLIP
model and combined. In experiments on four benchmarks we show that a model
trained using CoPT achieves the new state of the art performance on UDA for
segmentation. The code can be found at https://github.com/cfmata/CoPT.

</details>


### [25] [Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning](https://arxiv.org/abs/2507.07139)
*Renyang Liu,Guanlin Li,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

TL;DR: 论文提出了一种名为Recall的对抗性框架，旨在测试图像生成模型（IGMs）在去学习技术中的鲁棒性，揭示了现有方法的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管图像生成模型（如Stable Diffusion）在生成高质量多样化内容方面取得了进展，但其可能产生有害或侵权内容的问题引发了伦理和法律担忧。去学习技术（MU）被提出作为解决方案，但其鲁棒性和有效性尚未充分研究。

Method: 论文提出Recall框架，通过优化对抗性图像提示（而非仅依赖文本提示）来测试去学习模型的鲁棒性，利用扩散模型的多模态条件能力。

Result: 实验表明，Recall在对抗效果、计算效率和语义保真度上均优于现有基线，揭示了当前去学习机制的漏洞。

Conclusion: 研究强调了开发更鲁棒的去学习解决方案的必要性，以确保生成模型的安全性和可靠性。

Abstract: Recent advances in image generation models (IGMs), particularly
diffusion-based architectures such as Stable Diffusion (SD), have markedly
enhanced the quality and diversity of AI-generated visual content. However,
their generative capability has also raised significant ethical, legal, and
societal concerns, including the potential to produce harmful, misleading, or
copyright-infringing content. To mitigate these concerns, machine unlearning
(MU) emerges as a promising solution by selectively removing undesirable
concepts from pretrained models. Nevertheless, the robustness and effectiveness
of existing unlearning techniques remain largely unexplored, particularly in
the presence of multi-modal adversarial inputs.
  To bridge this gap, we propose Recall, a novel adversarial framework
explicitly designed to compromise the robustness of unlearned IGMs. Unlike
existing approaches that predominantly rely on adversarial text prompts, Recall
exploits the intrinsic multi-modal conditioning capabilities of diffusion
models by efficiently optimizing adversarial image prompts with guidance from a
single semantically relevant reference image. Extensive experiments across ten
state-of-the-art unlearning methods and diverse tasks show that Recall
consistently outperforms existing baselines in terms of adversarial
effectiveness, computational efficiency, and semantic fidelity with the
original textual prompt. These findings reveal critical vulnerabilities in
current unlearning mechanisms and underscore the need for more robust solutions
to ensure the safety and reliability of generative models. Code and data are
publicly available at \textcolor{blue}{https://github.com/ryliu68/RECALL}.

</details>


### [26] [Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey](https://arxiv.org/abs/2507.07148)
*Getamesay Haile Dagnaw,Yanming Zhu,Muhammad Hassan Maqsood,Wencheng Yang,Xingshuai Dong,Xuefei Yin,Alan Wee-Chung Liew*

Main category: cs.CV

TL;DR: 该论文综述了可解释人工智能（XAI）在生物医学图像分析中的应用，填补了现有综述的不足，提出了模态感知的分类方法，并探讨了多模态和视觉语言模型的作用。


<details>
  <summary>Details</summary>
Motivation: 现有XAI综述缺乏对生物医学图像分析特定需求的关注，且忽略了多模态和视觉语言模型的最新进展。

Method: 通过系统分类XAI方法，分析其原理、优势和局限性，并提出模态中心的分类法。

Result: 提出了针对生物医学图像分析的XAI方法分类，总结了评估指标和开源框架，并讨论了未来方向。

Conclusion: 该综述为生物医学图像分析中可解释深度学习的发展提供了及时且深入的基础。

Abstract: Explainable artificial intelligence (XAI) has become increasingly important
in biomedical image analysis to promote transparency, trust, and clinical
adoption of DL models. While several surveys have reviewed XAI techniques, they
often lack a modality-aware perspective, overlook recent advances in multimodal
and vision-language paradigms, and provide limited practical guidance. This
survey addresses this gap through a comprehensive and structured synthesis of
XAI methods tailored to biomedical image analysis.We systematically categorize
XAI methods, analyzing their underlying principles, strengths, and limitations
within biomedical contexts. A modality-centered taxonomy is proposed to align
XAI methods with specific imaging types, highlighting the distinct
interpretability challenges across modalities. We further examine the emerging
role of multimodal learning and vision-language models in explainable
biomedical AI, a topic largely underexplored in previous work. Our
contributions also include a summary of widely used evaluation metrics and
open-source frameworks, along with a critical discussion of persistent
challenges and future directions. This survey offers a timely and in-depth
foundation for advancing interpretable DL in biomedical image analysis.

</details>


### [27] [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
*Zongmeng Zhang,Wengang Zhou,Jie Zhao,Houqiang Li*

Main category: cs.CV

TL;DR: 该论文研究了多模态大语言模型（MLLMs）中的幻觉现象，提出模态冲突是导致幻觉的原因，并构建了MMMC数据集。通过三种方法（提示工程、监督微调和强化学习）缓解幻觉，实验表明强化学习效果最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在多模态任务中表现优异，但在真实场景中易产生幻觉。论文从模态冲突的角度研究这一现象，填补了现有研究的空白。

Method: 定义了模态冲突并构建MMMC数据集，提出基于提示工程、监督微调和强化学习的三种方法。

Result: 强化学习方法在缓解模态冲突导致的幻觉中表现最佳，监督微调方法稳定且有潜力。

Conclusion: 论文揭示了模态冲突对MLLMs幻觉的影响，为提升模型鲁棒性提供了新见解。

Abstract: Despite the impressive capabilities of multimodal large language models
(MLLMs) in vision-language tasks, they are prone to hallucinations in
real-world scenarios. This paper investigates the hallucination phenomenon in
MLLMs from the perspective of modality conflict. Unlike existing works focusing
on the conflicts between model responses and inputs, we study the inherent
conflicts in inputs from different modalities that place MLLMs in a dilemma and
directly lead to hallucinations. We formally define the modality conflict and
construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this
phenomenon in vision-language tasks. Three methods based on prompt engineering,
supervised fine-tuning, and reinforcement learning are proposed to alleviate
the hallucination caused by modality conflict. Extensive experiments are
conducted on the MMMC dataset to analyze the merits and demerits of these
methods. Our results show that the reinforcement learning method achieves the
best performance in mitigating the hallucination under modality conflict, while
the supervised fine-tuning method shows promising and stable performance. Our
work sheds light on the unnoticed modality conflict that leads to
hallucinations and provides more insights into the robustness of MLLMs.

</details>


### [28] [Aerial Maritime Vessel Detection and Identification](https://arxiv.org/abs/2507.07153)
*Antonella Barisic Kulas,Frano Petric,Stjepan Bogdan*

Main category: cs.CV

TL;DR: 论文提出了一种在GNSS不可用环境下通过无人机视觉自主识别目标船只的方法，结合YOLOv8检测、特征匹配和色调直方图分析。


<details>
  <summary>Details</summary>
Motivation: 在GNSS不可用环境中，自主识别目标船只对于搜救和威胁检测至关重要。

Method: 使用YOLOv8检测船只，结合特征匹配和色调直方图距离分析识别目标，并通过几何原理定位。

Result: 在MBZIRC2023竞赛中验证了方法的有效性，评估了视角对检测和定位精度的影响。

Conclusion: 提出的方法在GNSS不可用环境下能有效识别和定位目标船只。

Abstract: Autonomous maritime surveillance and target vessel identification in
environments where Global Navigation Satellite Systems (GNSS) are not available
is critical for a number of applications such as search and rescue and threat
detection. When the target vessel is only described by visual cues and its last
known position is not available, unmanned aerial vehicles (UAVs) must rely
solely on on-board vision to scan a large search area under strict
computational constraints. To address this challenge, we leverage the YOLOv8
object detection model to detect all vessels in the field of view. We then
apply feature matching and hue histogram distance analysis to determine whether
any detected vessel corresponds to the target. When found, we localize the
target using simple geometric principles. We demonstrate the proposed method in
real-world experiments during the MBZIRC2023 competition, integrated into a
fully autonomous system with GNSS-denied navigation. We also evaluate the
impact of perspective on detection accuracy and localization precision and
compare it with the oracle approach.

</details>


### [29] [CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation](https://arxiv.org/abs/2507.07154)
*Desheng Li,Chaoliang Liu,Zhiyong Xiao*

Main category: cs.CV

TL;DR: CL-Polyp是一种基于对比学习的息肉分割网络，通过自监督策略提升特征提取能力，无需额外标注数据，并在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有息肉分割方法依赖额外标注数据和任务相似性，限制了泛化能力，因此提出CL-Polyp以解决这些问题。

Method: 采用对比学习增强编码器特征提取能力，结合MASPP模块和CA模块优化多尺度特征融合和边界重建。

Result: 在五个基准数据集上表现优异，IoU指标在Kvasir-SEG和CVC-ClinicDB上分别提升0.011和0.020。

Conclusion: CL-Polyp在临床息肉分割任务中表现出色，验证了其有效性。

Abstract: Accurate segmentation of polyps from colonoscopy images is crucial for the
early diagnosis and treatment of colorectal cancer. Most existing deep
learning-based polyp segmentation methods adopt an Encoder-Decoder
architecture, and some utilize multi-task frameworks that incorporate auxiliary
tasks such as classification to enhance segmentation performance. However,
these approaches often require additional labeled data and rely on task
similarity, which can limit their generalizability. To address these
challenges, we propose CL-Polyp, a contrastive learning-enhanced polyp
segmentation network. Our method leverages contrastive learning to improve the
encoder's ability to extract discriminative features by contrasting positive
and negative sample pairs derived from polyp images. This self-supervised
strategy enhances visual representation without requiring additional
annotations. In addition, we introduce two lightweight and effective modules:
the Modified Atrous Spatial Pyramid Pooling (MASPP) module for better
multi-scale feature fusion, and the Channel Concatenate and Element Add (CA)
module to fuse low-level and upsampled features for improved boundary
reconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,
CVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp
consistently outperforms state-of-the-art methods. Specifically, it improves
the IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,
respectively, validating its effectiveness in clinical polyp segmentation
tasks.

</details>


### [30] [Interpretable EEG-to-Image Generation with Semantic Prompts](https://arxiv.org/abs/2507.07157)
*Arshak Rezvani,Ali Akbari,Kosar Sanjar Arani,Maryam Mirian,Emad Arasteh,Martin J. McKeown*

Main category: cs.CV

TL;DR: 该论文提出了一种通过EEG信号与多级语义描述对齐的方法，绕过直接图像生成，实现了从脑信号解码视觉体验的先进技术。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用EEG信号解码视觉体验，尽管EEG在空间细节上有限，但其时间精确性为研究提供了可能性。

Method: 通过对比学习将EEG信号映射到多级语义描述，利用预训练的潜在扩散模型生成图像。

Result: 在EEGCVPR数据集上实现了最先进的视觉解码效果，揭示了EEG信号与语义描述的神经认知路径关联。

Conclusion: 通过语义中介，实现了与认知对齐的EEG视觉解码，展示了语义层次在解码中的重要性。

Abstract: Decoding visual experience from brain signals offers exciting possibilities
for neuroscience and interpretable AI. While EEG is accessible and temporally
precise, its limitations in spatial detail hinder image reconstruction. Our
model bypasses direct EEG-to-image generation by aligning EEG signals with
multilevel semantic captions -- ranging from object-level to abstract themes --
generated by a large language model. A transformer-based EEG encoder maps brain
activity to these captions through contrastive learning. During inference,
caption embeddings retrieved via projection heads condition a pretrained latent
diffusion model for image generation. This text-mediated framework yields
state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable
alignment to known neurocognitive pathways. Dominant EEG-caption associations
reflected the importance of different semantic levels extracted from perceived
images. Saliency maps and t-SNE projections reveal semantic topography across
the scalp. Our model demonstrates how structured semantic mediation enables
cognitively aligned visual decoding from EEG.

</details>


### [31] [A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality](https://arxiv.org/abs/2507.07202)
*Mohamed Elmoghany,Ryan Rossi,Seunghyun Yoon,Subhojyoti Mukherjee,Eslam Bakr,Puneet Mathur,Gang Wu,Viet Dac Lai,Nedim Lipka,Ruiyi Zhang,Varun Manjunatha,Chien Nguyen,Daksh Dangi,Abel Salinas,Mohammad Taesiri,Hongjie Chen,Xiaolei Huang,Joe Barrow,Nesreen Ahmed,Hoda Eldardiry,Namyong Park,Yu Wang,Jaemin Cho,Anh Totti Nguyen,Zhengzhong Tu,Thien Nguyen,Dinesh Manocha,Mohamed Elhoseiny,Franck Dernoncourt*

Main category: cs.CV

TL;DR: 论文分析了当前视频生成模型的局限性，提出了长视频生成的关键架构和训练策略，并构建了新的分类法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型只能生成5-16秒的短视频，且长视频难以保持角色一致性和场景连贯性，多主题长视频问题更突出。

Method: 综合研究了32篇视频生成论文，识别关键架构和训练策略，构建分类法并比较不同方法的性能。

Result: 提出了长视频生成的关键组件和策略，并展示了分类表和性能比较。

Conclusion: 研究为长视频生成提供了系统化的分析和改进方向。

Abstract: Despite the significant progress that has been made in video generative
models, existing state-of-the-art methods can only produce videos lasting 5-16
seconds, often labeled "long-form videos". Furthermore, videos exceeding 16
seconds struggle to maintain consistent character appearances and scene layouts
throughout the narrative. In particular, multi-subject long videos still fail
to preserve character consistency and motion coherence. While some methods can
generate videos up to 150 seconds long, they often suffer from frame redundancy
and low temporal diversity. Recent work has attempted to produce long-form
videos featuring multiple characters, narrative coherence, and high-fidelity
detail. We comprehensively studied 32 papers on video generation to identify
key architectural components and training strategies that consistently yield
these qualities. We also construct a comprehensive novel taxonomy of existing
methods and present comparative tables that categorize papers by their
architectural designs and performance characteristics.

</details>


### [32] [Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement](https://arxiv.org/abs/2507.07230)
*Priyank Pathak,Yogesh S. Rawat*

Main category: cs.CV

TL;DR: 提出了一种基于颜色的轻量级方法CSCI，用于解决衣物更换重识别问题，无需额外标注或模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖额外模型或标注，资源消耗大，探索颜色作为轻量级替代方案。

Method: CSCI利用RGB信息，通过S2A自注意力机制分离颜色与身份特征。

Result: 在多个数据集上显著提升性能，图像ReID提升2.9%-5.0%，视频ReID提升1.0%-2.5%。

Conclusion: 颜色是解决衣物更换重识别中外观偏见的有效且经济的方案。

Abstract: Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals
across different locations and times, irrespective of clothing. Existing
methods often rely on additional models or annotations to learn robust,
clothing-invariant features, making them resource-intensive. In contrast, we
explore the use of color - specifically foreground and background colors - as a
lightweight, annotation-free proxy for mitigating appearance bias in ReID
models. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that
leverages color information directly from raw images or video frames. CSCI
efficiently captures color-related appearance bias ('Color See') while
disentangling it from identity-relevant ReID features ('Color Ignore'). To
achieve this, we introduce S2A self-attention, a novel self-attention to
prevent information leak between color and identity cues within the feature
space. Our analysis shows a strong correspondence between learned color
embeddings and clothing attributes, validating color as an effective proxy when
explicit clothing labels are unavailable. We demonstrate the effectiveness of
CSCI on both image and video ReID with extensive experiments on four CC-ReID
datasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for
image-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID
without relying on additional supervision. Our results highlight the potential
of color as a cost-effective solution for addressing appearance bias in
CC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.

</details>


### [33] [Automated Video Segmentation Machine Learning Pipeline](https://arxiv.org/abs/2507.07242)
*Johannes Merz,Lucien Fostier*

Main category: cs.CV

TL;DR: 提出了一种自动化视频分割流程，通过机器学习生成时间一致的实例掩码，显著提升VFX生产效率。


<details>
  <summary>Details</summary>
Motivation: 解决VFX制作中掩码生成慢且资源密集的问题。

Method: 结合文本提示的灵活对象检测、逐帧图像分割和鲁棒视频跟踪，确保时间一致性。

Result: 减少了人工工作量，加快了初步合成的创建速度，提供了全面的分割数据。

Conclusion: 该流程显著提升了VFX制作的效率，并快速被艺术家采用。

Abstract: Visual effects (VFX) production often struggles with slow, resource-intensive
mask generation. This paper presents an automated video segmentation pipeline
that creates temporally consistent instance masks. It employs machine learning
for: (1) flexible object detection via text prompts, (2) refined per-frame
image segmentation and (3) robust video tracking to ensure temporal stability.
Deployed using containerization and leveraging a structured output format, the
pipeline was quickly adopted by our artists. It significantly reduces manual
effort, speeds up the creation of preliminary composites, and provides
comprehensive segmentation data, thereby enhancing overall VFX production
efficiency.

</details>


### [34] [DisenQ: Disentangling Q-Former for Activity-Biometrics](https://arxiv.org/abs/2507.07262)
*Shehreen Azad,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 提出了一种多模态语言引导框架DisenQ，通过结构化文本监督解耦生物特征、运动和非生物特征，解决了活动生物特征识别中的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统身份识别在多样化活动中面临身份线索与运动动态和外观变化交织的挑战，现有视觉数据方法因提取不准确而受限。

Method: 引入DisenQ（解耦Q-Former），利用结构化语言指导解耦特征，避免身份线索受外观和运动变化影响。

Result: 在三个活动视频基准测试中取得最优性能，并在传统视频识别基准上表现出强泛化能力。

Conclusion: 该框架通过语言引导有效解决了活动生物特征识别的复杂性，展示了广泛适用性。

Abstract: In this work, we address activity-biometrics, which involves identifying
individuals across diverse set of activities. Unlike traditional person
identification, this setting introduces additional challenges as identity cues
become entangled with motion dynamics and appearance variations, making
biometrics feature learning more complex. While additional visual data like
pose and/or silhouette help, they often struggle from extraction inaccuracies.
To overcome this, we propose a multimodal language-guided framework that
replaces reliance on additional visual data with structured textual
supervision. At its core, we introduce \textbf{DisenQ} (\textbf{Disen}tangling
\textbf{Q}-Former), a unified querying transformer that disentangles
biometrics, motion, and non-biometrics features by leveraging structured
language guidance. This ensures identity cues remain independent of appearance
and motion variations, preventing misidentifications. We evaluate our approach
on three activity-based video benchmarks, achieving state-of-the-art
performance. Additionally, we demonstrate strong generalization to complex
real-world scenario with competitive performance on a traditional video-based
identification benchmark, showing the effectiveness of our framework.

</details>


### [35] [LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation](https://arxiv.org/abs/2507.07274)
*Ananya Raval,Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

TL;DR: LinguaMark是一个用于评估多语言视觉问答任务的大规模多模态模型（LMMs）的基准测试，覆盖11种语言和5种社会属性，重点关注偏见、答案相关性和忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有LMMs在语言覆盖上存在局限性，导致输出偏见和不公平，但多语言能力的评估较少被关注。

Method: 提出LinguaMark基准测试，包含6,875个图像-文本对，评估模型在偏见、答案相关性和忠实性三个指标上的表现。

Result: 闭源模型（如GPT-4o和Gemini2.5）整体表现最佳，开源模型（如Gemma3和Qwen2.5）在某些方面表现竞争性，Qwen2.5在多语言泛化上表现突出。

Conclusion: LinguaMark为多语言LMMs评估提供了工具，促进可重复性和进一步研究。

Abstract: Large Multimodal Models (LMMs) are typically trained on vast corpora of
image-text data but are often limited in linguistic coverage, leading to biased
and unfair outputs across languages. While prior work has explored multimodal
evaluation, less emphasis has been placed on assessing multilingual
capabilities. In this work, we introduce LinguaMark, a benchmark designed to
evaluate state-of-the-art LMMs on a multilingual Visual Question Answering
(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages
and five social attributes. We evaluate models using three key metrics: Bias,
Answer Relevancy, and Faithfulness. Our findings reveal that closed-source
models generally achieve the highest overall performance. Both closed-source
(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform
competitively across social attributes, and Qwen2.5 demonstrates strong
generalization across multiple languages. We release our benchmark and
evaluation code to encourage reproducibility and further research.

</details>


### [36] [MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning](https://arxiv.org/abs/2507.07297)
*Chengfei Wu,Ronald Seoh,Bingxuan Li,Liqiang Zhang,Fengrong Han,Dan Goldwasser*

Main category: cs.CV

TL;DR: MagiC是一个评估视觉语言模型多模态认知能力的基准，关注答案准确性、推理质量与视觉证据的对齐，并引入新指标分析模型局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型是否真正进行视觉推理尚不明确，需评估其是否依赖表面模式或数据集偏差。

Method: 构建包含5,500个弱监督QA和900个人工标注示例的基准，评估15个模型在四个维度的表现，并引入诊断设置和新指标。

Result: MagiC揭示了当前视觉推理方法的关键局限性，并提供了改进机会。

Conclusion: MagiC为评估和改进视觉语言模型的推理能力提供了全面工具。

Abstract: Recent advances in large vision-language models have led to impressive
performance in visual question answering and multimodal reasoning. However, it
remains unclear whether these models genuinely perform grounded visual
reasoning or rely on superficial patterns and dataset biases. In this work, we
introduce MagiC, a comprehensive benchmark designed to evaluate grounded
multimodal cognition, assessing not only answer accuracy but also the quality
of step-by-step reasoning and its alignment with relevant visual evidence. Our
benchmark includes approximately 5,500 weakly supervised QA examples generated
from strong model outputs and 900 human-curated examples with fine-grained
annotations, including answers, rationales, and bounding box groundings. We
evaluate 15 vision-language models ranging from 7B to 70B parameters across
four dimensions: final answer correctness, reasoning validity, grounding
fidelity, and self-correction ability. MagiC further includes diagnostic
settings to probe model robustness under adversarial visual cues and assess
their capacity for introspective error correction. We introduce new metrics
such as MagiScore and StepSense, and provide comprehensive analyses that reveal
key limitations and opportunities in current approaches to grounded visual
reasoning.

</details>


### [37] [ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation](https://arxiv.org/abs/2507.07317)
*Sherry X. Chen,Yi Wei,Luowei Zhou,Suren Kumar*

Main category: cs.CV

TL;DR: ADIEE提出了一种自动化数据集创建方法，用于训练评分模型以评估指令引导的图像编辑效果，显著提升了开源和专有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏有效的自动化评估方法，开源模型对齐困难，专有模型不透明且成本高，且缺乏公开训练数据集。

Method: 通过ADIEE生成大规模数据集（超10万样本），并微调LLaVA-NeXT-8B模型以解码自定义令牌的数值分数。

Result: 评分模型在所有基准测试中优于开源和Gemini-Pro 1.5模型，显著提升了与人类评分的相关性及比较准确性。

Conclusion: ADIEE方法有效解决了指令引导图像编辑的自动化评估问题，并可作为奖励模型提升编辑效果。

Abstract: Recent advances in instruction-guided image editing underscore the need for
effective automated evaluation. While Vision-Language Models (VLMs) have been
explored as judges, open-source models struggle with alignment, and proprietary
models lack transparency and cost efficiency. Additionally, no public training
datasets exist to fine-tune open-source VLMs, only small benchmarks with
diverse evaluation schemes. To address this, we introduce ADIEE, an automated
dataset creation approach which is then used to train a scoring model for
instruction-guided image editing evaluation. We generate a large-scale dataset
with over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified
to decode a numeric score from a custom token. The resulting scorer outperforms
all open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a
0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,
and improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench
and 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the
state-of-the-art. The scorer can act as a reward model, enabling automated best
edit selection and model fine-tuning. Notably, the proposed scorer can boost
MagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43
(+8.98%).

</details>


### [38] [Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory](https://arxiv.org/abs/2507.07333)
*Hui Pang,Sunil Hadap,Violetta Shevchenko,Rahul Suresh,Amin Banitalebi-Dehkordi*

Main category: cs.CV

TL;DR: 提出了一种基于Kubelka-Munk理论的快速图像合成方法，用于虚拟试妆，提升肤色与粉底融合的真实感，并构建了可扩展的端到端框架。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟试妆中肤色与粉底颜色融合的真实性和方法可扩展性问题。

Method: 近似Kubelka-Munk理论以加速图像合成，构建依赖电商产品信息的端到端框架。

Result: 在真实化妆图像上验证，框架表现优于其他技术。

Conclusion: 该方法在虚拟试妆中实现了高效且真实的肤色与粉底融合，具有实际应用潜力。

Abstract: Augmented reality is revolutionizing beauty industry with virtual try-on
(VTO) applications, which empowers users to try a wide variety of products
using their phones without the hassle of physically putting on real products. A
critical technical challenge in foundation VTO applications is the accurate
synthesis of foundation-skin tone color blending while maintaining the
scalability of the method across diverse product ranges. In this work, we
propose a novel method to approximate well-established Kubelka-Munk (KM) theory
for faster image synthesis while preserving foundation-skin tone color blending
realism. Additionally, we build a scalable end-to-end framework for realistic
foundation makeup VTO solely depending on the product information available on
e-commerce sites. We validate our method using real-world makeup images,
demonstrating that our framework outperforms other techniques.

</details>


### [39] [Entity Re-identification in Visual Storytelling via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.07340)
*Daniel A. P. Oliveira,David Martins de Matos*

Main category: cs.CV

TL;DR: 论文提出了一种对比强化学习方法，通过合成负例训练模型区分连贯图像序列与无关图像，提升视觉故事系统中角色和对象的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉故事系统在跨帧识别同一角色或对象时表现不佳，导致引用不一致和幻觉问题，原因是缺乏明确的跨帧实体连接训练。

Method: 采用对比强化学习框架，扩展Story Reasoning数据集，使用双组分奖励函数优化Direct Preference Optimization，微调Qwen Storyteller模型。

Result: 实验显示，实体识别和连贯性显著提升，如grounding mAP提高14.8%，F1提高17.1%，跨帧实体持续性增加13.7%。

Conclusion: 对比强化学习方法有效提升了视觉故事系统中实体一致性和故事连贯性。

Abstract: Visual storytelling systems, particularly large vision-language models,
struggle to maintain character and object identity across frames,
  often failing to recognize when entities in different images represent the
same individuals or objects,
  leading to inconsistent references and referential hallucinations.
  This occurs because models lack explicit training on when to establish entity
connections across frames.
  We propose a contrastive reinforcement learning approach that trains models
to discriminate between coherent image sequences
  and stories from unrelated images.
  We extend the Story Reasoning dataset with synthetic negative examples to
teach appropriate entity connection behavior.
  We employ Direct Preference Optimization with a dual-component reward
function that promotes grounding and re-identification of entities
  in real stories while penalizing incorrect entity connections in synthetic
contexts.
  Using this contrastive framework, we fine-tune Qwen Storyteller (based on
Qwen2.5-VL 7B).
  Evaluation shows improvements in grounding mAP from 0.27 to 0.31 (+14.8%), F1
from 0.35 to 0.41 (+17.1%).
  Pronoun grounding accuracy improved across all pronoun types except ``its'',
  and cross-frame character and object persistence increased
  across all frame counts, with entities appearing in 5 or more frames
advancing from 29.3% to 33.3% (+13.7%).
  Well-structured stories, containing the chain-of-thought and grounded story,
increased from 79.1% to 97.5% (+23.3%).

</details>


### [40] [PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency](https://arxiv.org/abs/2507.07374)
*Haotian Wang,Aoran Xiao,Xiaoqin Zhang,Meng Yang,Shijian Lu*

Main category: cs.CV

TL;DR: PacGDC是一种标签高效的技术，通过合成伪几何数据增强深度补全的泛化能力，减少对大规模标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决深度补全模型训练需要大量标注数据的问题，提出一种标签高效的方法。

Method: 利用2D到3D投影中的模糊性和一致性，合成多种伪几何数据，结合深度基础模型和插值/重定位策略。

Result: 在多个基准测试中表现出卓越的泛化能力，适应不同场景和深度稀疏性。

Conclusion: PacGDC提供了一种高效的数据增强方法，显著提升了深度补全的泛化性能。

Abstract: Generalizable depth completion enables the acquisition of dense metric depth
maps for unseen environments, offering robust perception capabilities for
various downstream tasks. However, training such models typically requires
large-scale datasets with metric depth labels, which are often labor-intensive
to collect. This paper presents PacGDC, a label-efficient technique that
enhances data diversity with minimal annotation effort for generalizable depth
completion. PacGDC builds on novel insights into inherent ambiguities and
consistencies in object shapes and positions during 2D-to-3D projection,
allowing the synthesis of numerous pseudo geometries for the same visual scene.
This process greatly broadens available geometries by manipulating scene scales
of the corresponding depth maps. To leverage this property, we propose a new
data synthesis pipeline that uses multiple depth foundation models as scale
manipulators. These models robustly provide pseudo depth labels with varied
scene scales, affecting both local objects and global layouts, while ensuring
projection consistency that supports generalization. To further diversify
geometries, we incorporate interpolation and relocation strategies, as well as
unlabeled images, extending the data coverage beyond the individual use of
foundation models. Extensive experiments show that PacGDC achieves remarkable
generalizability across multiple benchmarks, excelling in diverse scene
semantics/scales and depth sparsity/patterns under both zero-shot and few-shot
settings. Code: https://github.com/Wang-xjtu/PacGDC.

</details>


### [41] [Adaptive Particle-Based Shape Modeling for Anatomical Surface Correspondence](https://arxiv.org/abs/2507.07379)
*Hong Xu,Shireen Y. Elhabian*

Main category: cs.CV

TL;DR: PSM方法通过粒子配置量化形状变异性，但缺乏自适应性。本文提出两种机制提升适应性并保持一致性：邻域对应损失和测地线对应算法。


<details>
  <summary>Details</summary>
Motivation: 现有PSM方法无法自动调整粒子配置以适应局部几何特征，限制了复杂解剖变异的准确表示。

Method: 引入邻域对应损失和测地线对应算法，提升适应性并保持粒子配置一致性。

Result: 在挑战性数据集上验证了方法的有效性和可扩展性，分析了适应性与对应性的权衡。

Conclusion: 新机制显著提升了PSM的适应性，同时保持了粒子配置的一致性，优于现有方法。

Abstract: Particle-based shape modeling (PSM) is a family of approaches that
automatically quantifies shape variability across anatomical cohorts by
positioning particles (pseudo landmarks) on shape surfaces in a consistent
configuration. Recent advances incorporate implicit radial basis function
representations as self-supervised signals to better capture the complex
geometric properties of anatomical structures. However, these methods still
lack self-adaptivity -- that is, the ability to automatically adjust particle
configurations to local geometric features of each surface, which is essential
for accurately representing complex anatomical variability. This paper
introduces two mechanisms to increase surface adaptivity while maintaining
consistent particle configurations: (1) a novel neighborhood correspondence
loss to enable high adaptivity and (2) a geodesic correspondence algorithm that
regularizes optimization to enforce geodesic neighborhood consistency. We
evaluate the efficacy and scalability of our approach on challenging datasets,
providing a detailed analysis of the adaptivity-correspondence trade-off and
benchmarking against existing methods on surface representation accuracy and
correspondence metrics.

</details>


### [42] [Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting in Videos](https://arxiv.org/abs/2507.07381)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.CV

TL;DR: 提出了一种多尺度注意力门移位模块（MSAGSM），用于增强体育视频中的精确事件检测，并通过新数据集TTA验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有PES模型的时空模块在时间感受野和空间适应性上存在局限，需改进以提升性能。

Method: 提出MSAGSM，结合多尺度时间扩张和多头空间注意力，增强时空建模能力。

Result: 在五个PES基准测试中，MSAGSM均显著提升性能，达到新SOTA。

Conclusion: MSAGSM是一种轻量级即插即用模块，能有效提升体育视频事件检测性能。

Abstract: Precise Event Spotting (PES) in sports videos requires frame-level
recognition of fine-grained actions from single-camera footage. Existing PES
models typically incorporate lightweight temporal modules such as Gate Shift
Module (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with
temporal context. However, these modules are limited in both temporal receptive
field and spatial adaptability. We propose a Multi-Scale Attention Gate Shift
Module (MSAGSM) that enhances GSM with multi-scale temporal dilations and
multi-head spatial attention, enabling efficient modeling of both short- and
long-term dependencies while focusing on salient regions. MSAGSM is a
lightweight plug-and-play module that can be easily integrated with various 2D
backbones. To further advance the field, we introduce the Table Tennis
Australia (TTA) dataset-the first PES benchmark for table tennis-containing
over 4800 precisely annotated events. Extensive experiments across five PES
benchmarks demonstrate that MSAGSM consistently improves performance with
minimal overhead, setting new state-of-the-art results.

</details>


### [43] [KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware Representation in Videos](https://arxiv.org/abs/2507.07393)
*Jinseong Kim,Junghoon Song,Gyeongseon Baek,Byeongjoon Noh*

Main category: cs.CV

TL;DR: KeyRe-ID是一个基于关键点的视频行人重识别框架，通过全局和局部分支增强时空表示学习，在MARS和iLIDS-VID基准上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 利用人体关键点提升行人重识别的时空表示学习能力。

Method: 结合全局分支（Transformer时序聚合）和局部分支（基于关键点的动态身体区域分割），生成细粒度特征。

Result: 在MARS上达到91.73% mAP和97.32% Rank-1，在iLIDS-VID上达到96.00% Rank-1和100.0% Rank-5。

Conclusion: KeyRe-ID通过关键点引导的全局和局部特征学习，显著提升了行人重识别性能。

Abstract: We propose \textbf{KeyRe-ID}, a keypoint-guided video-based person
re-identification framework consisting of global and local branches that
leverage human keypoints for enhanced spatiotemporal representation learning.
The global branch captures holistic identity semantics through
Transformer-based temporal aggregation, while the local branch dynamically
segments body regions based on keypoints to generate fine-grained, part-aware
features. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate
state-of-the-art performance, achieving 91.73\% mAP and 97.32\% Rank-1 accuracy
on MARS, and 96.00\% Rank-1 and 100.0\% Rank-5 accuracy on iLIDS-VID. The code
for this work will be publicly available on GitHub upon publication.

</details>


### [44] [Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer](https://arxiv.org/abs/2507.07394)
*Zhimin Zhang,Bi'an Du,Caoyuan Ma,Zheng Wang,Wei Hu*

Main category: cs.CV

TL;DR: 提出了一种新的跨类别动物运动迁移框架，专注于保留物种特有的习惯行为，并结合大语言模型（LLM）处理未观察到的物种。


<details>
  <summary>Details</summary>
Motivation: 现有运动迁移方法主要针对人类运动，忽视了动物特有的习惯行为，因此需要一种能保留这些行为的跨类别迁移方法。

Method: 基于生成框架，引入习惯保留模块和类别特定习惯编码器，结合LLM处理新物种。

Result: 在DeformingThings4D-skl数据集上验证了模型的有效性，实验和定量分析证明了其优越性。

Conclusion: 提出的框架成功实现了跨类别动物运动迁移，并保留了习惯行为，为动画和虚拟现实应用提供了新方法。

Abstract: Animal motion embodies species-specific behavioral habits, making the
transfer of motion across categories a critical yet complex task for
applications in animation and virtual reality. Existing motion transfer
methods, primarily focused on human motion, emphasize skeletal alignment
(motion retargeting) or stylistic consistency (motion style transfer), often
neglecting the preservation of distinct habitual behaviors in animals. To
bridge this gap, we propose a novel habit-preserved motion transfer framework
for cross-category animal motion. Built upon a generative framework, our model
introduces a habit-preservation module with category-specific habit encoder,
allowing it to learn motion priors that capture distinctive habitual
characteristics. Furthermore, we integrate a large language model (LLM) to
facilitate the motion transfer to previously unobserved species. To evaluate
the effectiveness of our approach, we introduce the DeformingThings4D-skl
dataset, a quadruped dataset with skeletal bindings, and conduct extensive
experiments and quantitative analyses, which validate the superiority of our
proposed model.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [45] [Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure](https://arxiv.org/abs/2507.07223)
*Myoungsoo Jung*

Main category: cs.DC

TL;DR: 论文提出了一种基于CXL的模块化数据中心架构，结合XLink和分层内存模型，以解决AI工作负载在内存和通信带宽上的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载（如LLMs和RAG）对内存、通信带宽和资源灵活性提出了极高要求，传统GPU架构难以扩展。

Method: 提出模块化数据中心架构，结合CXL和XLink技术，设计分层内存模型，并评估轻量级CXL实现、HBM和硅光子技术。

Result: 评估显示，该架构显著提升了AI基础设施的可扩展性、吞吐量和灵活性。

Conclusion: 通过CXL和XLink的结合，解决了AI工作负载的扩展瓶颈，为未来数据中心设计提供了新方向。

Abstract: Modern AI workloads such as large language models (LLMs) and
retrieval-augmented generation (RAG) impose severe demands on memory,
communication bandwidth, and resource flexibility. Traditional GPU-centric
architectures struggle to scale due to growing inter-GPU communication
overheads. This report introduces key AI concepts and explains how Transformers
revolutionized data representation in LLMs. We analyze large-scale AI hardware
and data center designs, identifying scalability bottlenecks in hierarchical
systems. To address these, we propose a modular data center architecture based
on Compute Express Link (CXL) that enables disaggregated scaling of memory,
compute, and accelerators. We further explore accelerator-optimized
interconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink
Fusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance
data transfers while preserving memory coherence. We also propose a
hierarchical memory model that combines local and pooled memory, and evaluate
lightweight CXL implementations, HBM, and silicon photonics for efficient
scaling. Our evaluations demonstrate improved scalability, throughput, and
flexibility in AI infrastructure.

</details>


### [46] [Distributed Training under Packet Loss](https://arxiv.org/abs/2507.07114)
*Erez Weintraub,Ron Banner,Ariel Orda*

Main category: cs.DC

TL;DR: 提出了一种新型分布式训练框架，可在不可靠连接下保持模型准确性和收敛性，通过无偏梯度聚合和有界参数漂移实现。


<details>
  <summary>Details</summary>
Motivation: 现有分布式框架依赖可靠连接，导致尾延迟和可扩展性问题，而不可靠连接可能牺牲模型准确性和收敛性。

Method: 采用两阶段防御机制：无偏梯度聚合和有界参数广播，确保梯度估计正确性和模型参数一致性。

Result: 在LLAMA2 7B模型上实验显示，容忍10%丢包率仅导致0.8%困惑度变化。

Conclusion: 该框架填补了高效通信协议与现代大模型训练需求之间的鸿沟，支持在普通或广域网络上进行鲁棒学习。

Abstract: State-of-the-art language and vision models are routinely trained across
thousands of GPUs, often spanning multiple data-centers, yet today's
distributed frameworks still assume reliable connections (e.g., InfiniBand or
RoCE). The resulting acknowledgment traffic and retransmissions inflate tail
latencies and limit scalability. Leveraging unreliable connections will reduce
latency but may sacrifice model accuracy and convergence once packets are
dropped. A principled, end-to-end solution that preserves accuracy and
convergence guarantees under genuine packet loss has previously been missing.
We address this critical gap by introducing a novel distributed training
framework capable of operating over unreliable connections, offering unbiased
gradient aggregation and bounded parameter drift without modifying model code
or optimizers. The key insight is a two-stage defense against missing messages:
(i) Unbiased gradient aggregation: each worker reconstructs a consistent
gradient estimate from whatever packets arrive, guaranteeing expectation-level
correctness; and (ii) Bounded-drift parameter broadcasts: we prove the
inter-worker model discrepancy remains O(1) even after arbitrarily many
iterations, preventing the unbounded divergence typical of asynchronous setups.
Analytical bounds are matched by experiments on the LLAMA2 7B model with 64
GPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.
This work bridges the gap between communication-efficient datacenter protocols
and the accuracy and generalization guarantees demanded by modern large-model
training, enabling robust, high-throughput learning on commodity or wide-area
networks.

</details>


### [47] [Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces](https://arxiv.org/abs/2507.07116)
*Juan Cano-Benito,Andrea Cimmino,Sven Hertling,Heiko Paulheim,Raúl García-Castro*

Main category: cs.DC

TL;DR: 本文系统评估了不同类型DLT（公有、私有、混合）在语义数据存储中的表现，发现私有DLT效率最高，混合DLT在公开审计与操作效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 解决数据空间中语义数据在DLT上高效存储的缺口，支持去中心化数据生态系统的语义互操作性。

Method: 使用真实知识图谱作为实验基础，比较不同DLT的性能、存储效率、资源消耗及语义数据更新与查询能力。

Result: 私有DLT在语义内容存储和管理上最有效，混合DLT在公开审计与操作效率间提供平衡。

Conclusion: 研究为去中心化数据生态系统基于数据主权需求选择合适DLT基础设施提供了依据。

Abstract: Data spaces are emerging as decentralised infrastructures that enable
sovereign, secure, and trustworthy data exchange among multiple participants.
To achieve semantic interoperability within these environments, the use of
semantic web technologies and knowledge graphs has been proposed. Although
distributed ledger technologies (DLT) fit as the underlying infrastructure for
data spaces, there remains a significant gap in terms of the efficient storage
of semantic data on these platforms. This paper presents a systematic
evaluation of semantic data storage across different types of DLT (public,
private, and hybrid), using a real-world knowledge graph as an experimental
basis. The study compares performance, storage efficiency, resource
consumption, and the capabilities to update and query semantic data. The
results show that private DLTs are the most efficient for storing and managing
semantic content, while hybrid DLTs offer a balanced trade-off between public
auditability and operational efficiency. This research leads to a discussion on
the selection of the most appropriate DLT infrastructure based on the data
sovereignty requirements of decentralised data ecosystems.

</details>


### [48] [Collective Communication Profiling of Modern-day Machine Learning Workloads](https://arxiv.org/abs/2507.07117)
*Jit Gupta,Andrew Li,Tarun Banka,Ariel Cohen,T. Sridhar,Raj Yavatkar*

Main category: cs.DC

TL;DR: 分析机器学习分布式系统中集体通信行为对网络性能的影响，并提出优化框架和拓扑的建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习任务中的集体通信操作可能导致网络拥塞和丢包，影响性能，需分析这些模式以优化网络资源配置。

Method: 通过Nvidia Collective Communication Library记录和分析多种模型（如DeepSeek、GPT、Llama）的通信行为，调整并行度、节点数和模型类型等参数。

Result: 研究发现当前集体通信框架和网络拓扑需重新设计，以适应网络异常对机器学习工作负载的影响。

Conclusion: 建议重新思考集体通信框架和网络拓扑，以优化机器学习任务的网络性能。

Abstract: Machine Learning jobs, carried out on large number of distributed high
performance systems, involve periodic communication using operations like
AllReduce, AllGather, and Broadcast. These operations may create high bandwidth
and bursty traffic patterns, leading to network congestion and packet loss,
thus impacting the performance of these jobs. Hence it is imperative to analyze
these patterns, which can be helpful in provisioning network resources
depending on the type of machine learning workloads. In this poster we carry
out extensive analysis of the collective communication behavior seen in a wide
variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we
instrument Nvidia Collective Communication Library logging functionality for
richer context about the collectives and workloads. We adjust configuration
parameters that influence collective communication behavior, such as
parallelism, number of nodes, and model type. This overview presents and
discusses some of the results on the collective communication behavior for the
open source DeepSeek V3 inferencing model, which includes operation type and
count, transfer sizes per operation, and request size distribution. Our
analysis shows that it makes sense to rethink current collective communication
frameworks and network topologies so as to accommodate the effect of network
anomalies on the mentioned workloads.

</details>


### [49] [Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding](https://arxiv.org/abs/2507.07120)
*Nidhi Bhatia,Ankit More,Ritika Borkar,Tiyasa Mitra,Ramon Matas,Ritchie Zhao,Maximilian Golub,Dheevatsa Mudigere,Brian Pharris,Bita Darvish Rouhani*

Main category: cs.DC

TL;DR: Helix Parallelism是一种混合执行策略，通过KV并行和TP/EP并行优化LLM的推理效率，减少延迟并支持更大批次。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的KV历史增长，实时自回归解码面临FFN权重访问和长KV缓存读取的瓶颈，传统TP方法效率有限。

Method: 提出Helix Parallelism，结合KV并行和TP/EP并行，引入轻量通信步骤和Helix HOP-B技术以减少通信开销。

Result: Helix在固定批次下减少TTL达1.5倍，支持32倍更大批次，提升DeepSeek-R1的吞吐-延迟Pareto效率。

Conclusion: Helix使超长序列实时推理成为可能，显著优化了GPU效率和延迟表现。

Abstract: As LLMs scale to multi-million-token KV histories, real-time autoregressive
decoding under tight Token-to-Token Latency (TTL) constraints faces growing
pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)
weights and reading long KV caches. While Tensor Parallelism (TP) helps
mitigate the cost of FFN weight reads, it does not scale well for attention.
When TP width exceeds the number of KV heads, it leads to inefficient KV
duplication, limits parallelism, and constrains batch size. Simultaneously,
DRAM reads for long KV histories scale linearly with batch size, further
capping efficiency.
  We introduce Helix Parallelism, a hybrid execution strategy that applies KV
parallelism during attention to shard KV caches across GPUs, then reuses the
same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN
computation. To preserve exact attention behavior, Helix includes a lightweight
communication step. To minimize the exposed communication cost, we introduce
Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through
batchwise overlap, preserving low TTL while improving GPU efficiency. Compared
to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at
fixed batch sizes and supports up to 32x larger batches under the same latency
budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on
Blackwell and making real-time inference with ultra-long-sequence practical.

</details>


### [50] [Ampere: Communication-Efficient and High-Accuracy Split Federated Learning](https://arxiv.org/abs/2507.07130)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: Ampere是一种新型的联邦学习系统，通过减少设备计算和通信开销，同时提高模型准确性，解决了Split Federated Learning (SFL)的问题。


<details>
  <summary>Details</summary>
Motivation: SFL系统虽然减少了设备计算成本，但引入了大量通信开销并降低了非独立同分布（non-IID）数据的模型准确性。Ampere旨在同时解决这些问题。

Method: Ampere采用单向块间训练和轻量级辅助网络生成方法，减少梯度传输和中间交换，并通过整合激活数据来应对数据异质性。

Result: 实验表明，Ampere在模型准确性、训练时间、通信开销和设备计算方面显著优于SFL基线系统。

Conclusion: Ampere在联邦学习中表现出色，尤其在处理非IID数据时具有显著优势。

Abstract: A Federated Learning (FL) system collaboratively trains neural networks
across devices and a server but is limited by significant on-device computation
costs. Split Federated Learning (SFL) systems mitigate this by offloading a
block of layers of the network from the device to a server. However, in doing
so, it introduces large communication overheads due to frequent exchanges of
intermediate activations and gradients between devices and the server and
reduces model accuracy for non-IID data. We propose Ampere, a novel
collaborative training system that simultaneously minimizes on-device
computation and device-server communication while improving model accuracy.
Unlike SFL, which uses a global loss by iterative end-to-end training, Ampere
develops unidirectional inter-block training to sequentially train the device
and server block with a local loss, eliminating the transfer of gradients. A
lightweight auxiliary network generation method decouples training between the
device and server, reducing frequent intermediate exchanges to a single
transfer, which significantly reduces the communication overhead. Ampere
mitigates the impact of data heterogeneity by consolidating activations
generated by the trained device block to train the server block, in contrast to
SFL, which trains on device-specific, non-IID activations. Extensive
experiments on multiple CNNs and transformers show that, compared to
state-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up
to 13.26% while reducing training time by up to 94.6%, (ii) reduces
device-server communication overhead by up to 99.1% and on-device computation
by up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for
various non-IID degrees highlighting superior performance when faced with
heterogeneous data.

</details>


### [51] [M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure](https://arxiv.org/abs/2507.07144)
*Hongyi Xie,Min Zhou,Qiao Yu,Jialiang Yu,Zhenli Sheng,Hong Xie,Defu Lian*

Main category: cs.DC

TL;DR: M$^2$-MFP是一种多尺度分层内存故障预测框架，通过二进制空间特征提取器和双路径时间建模架构，显著提升了内存故障预测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 内存故障对云基础设施的稳定性构成威胁，现有预测方法存在泛化性差和性能不足的问题，因此需要更高效的方法。

Method: 将可纠正错误转换为多级二进制矩阵表示，引入二进制空间特征提取器（BSFE），并开发双路径时间建模架构（时间块模块和时间点模块）。

Result: 在基准数据集和实际部署中，M$^2$-MFP显著优于现有最先进方法。

Conclusion: M$^2$-MFP通过多尺度特征提取和时间建模，有效提升了内存故障预测的性能，增强了云基础设施的可靠性。

Abstract: As cloud services become increasingly integral to modern IT infrastructure,
ensuring hardware reliability is essential to sustain high-quality service.
Memory failures pose a significant threat to overall system stability, making
accurate failure prediction through the analysis of memory error logs (i.e.,
Correctable Errors) imperative. Existing memory failure prediction approaches
have notable limitations: rule-based expert models suffer from limited
generalizability and low recall rates, while automated feature extraction
methods exhibit suboptimal performance. To address these limitations, we
propose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction
framework designed to enhance the reliability and availability of cloud
infrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level
binary matrix representations and introduces a Binary Spatial Feature Extractor
(BSFE) to automatically extract high-order features at both DIMM-level and
bit-level. Building upon the BSFE outputs, we develop a dual-path temporal
modeling architecture: 1) a time-patch module that aggregates multi-level
features within observation windows, and 2) a time-point module that employs
interpretable rule-generation trees trained on bit-level patterns. Experiments
on both benchmark datasets and real-world deployment show the superiority of
M$^2$-MFP as it outperforms existing state-of-the-art methods by significant
margins. Code and data are available at this repository:
https://github.com/hwcloud-RAS/M2-MFP.

</details>


### [52] [Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience](https://arxiv.org/abs/2507.07352)
*Loïc Pottier,Konstantia Georgouli,Timothy S. Carpenter,Fikret Aydin,Jeremy O. B. Tempkin,Dwight V. Nissley,Frederick H. Streitz,Thomas R. W. Scogland,Peer-Timo Bremer,Felice C. Lightstone,Helgi I. Ingólfsson*

Main category: cs.DC

TL;DR: 论文介绍了多尺度机器学习建模基础设施MuMMI及其轻量版mini-MuMMI，用于管理大规模并行多尺度模拟，并展示了其在RAS-RAF膜相互作用中的应用。


<details>
  <summary>Details</summary>
Motivation: 多尺度模型在复杂现象建模中广泛应用，但跨尺度模拟的计算管理和资源需求一直是挑战。机器学习方法为解决这一问题提供了新思路。

Method: 提出MuMMI及其轻量版mini-MuMMI，用于管理和协调大规模并行多尺度分子动力学模拟，支持从毫秒到纳秒的不同时间尺度。

Result: mini-MuMMI在RAS-RAF膜相互作用研究中展示了实用性，并具备扩展到其他应用的潜力。

Conclusion: MuMMI和mini-MuMMI为多尺度模拟提供了高效的工作流管理工具，未来可拓展至更广泛的应用领域。

Abstract: Computational models have become one of the prevalent methods to model
complex phenomena. To accurately model complex interactions, such as detailed
biomolecular interactions, scientists often rely on multiscale models comprised
of several internal models operating at difference scales, ranging from
microscopic to macroscopic length and time scales. Bridging the gap between
different time and length scales has historically been challenging but the
advent of newer machine learning (ML) approaches has shown promise for tackling
that task. Multiscale models require massive amounts of computational power and
a powerful workflow management system. Orchestrating ML-driven multiscale
studies on parallel systems with thousands of nodes is challenging, the
workflow must schedule, allocate and control thousands of simulations operating
at different scales. Here, we discuss the massively parallel Multiscale
Machine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow
management infrastructure, that can orchestrate thousands of molecular dynamics
(MD) simulations operating at different timescales, spanning from millisecond
to nanosecond. More specifically, we introduce a novel version of MuMMI called
"mini-MuMMI". Mini-MuMMI is a curated version of MuMMI designed to run on
modest HPC systems or even laptops whereas MuMMI requires larger HPC systems.
We demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions
and discuss the different challenges behind the generalization of multiscale
workflows and how mini-MuMMI can be leveraged to target a broader range of
applications outside of MD and RAS-RAF interactions.

</details>


### [53] [KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows](https://arxiv.org/abs/2507.07400)
*Zaifeng Pan,Ajjkumar Patel,Zhengding Hu,Yipeng Shen,Yue Guan,Wan-Lu Li,Lianhui Qin,Yida Wang,Yufei Ding*

Main category: cs.DC

TL;DR: KVFlow是一种针对基于LLM的代理工作流程优化的KV缓存管理框架，通过预测代理的未来使用情况，减少缓存未命中和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有系统使用LRU策略管理KV缓存，无法预测代理的未来使用，导致频繁缓存未命中和计算开销。

Method: KVFlow通过Agent Step Graph抽象代理执行计划，并使用步骤到执行值指导细粒度缓存驱逐策略，同时引入重叠KV预取机制。

Result: KVFlow在单工作流程和并发工作流程中分别实现了1.83倍和2.19倍的加速。

Conclusion: KVFlow显著提升了代理工作流程的效率，减少了缓存未命中和计算开销。

Abstract: Large language model (LLM) based agentic workflows have become a popular
paradigm for coordinating multiple specialized agents to solve complex tasks.
To improve serving efficiency, existing LLM systems employ prefix caching to
reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby
avoiding redundant computation across repeated invocations. However, current
systems typically evict KV caches using a Least Recently Used (LRU) policy,
which fails to anticipate future agent usage and often discards KV caches
shortly before their reuse. This leads to frequent cache misses and substantial
recomputation or swapping overhead. We present KVFlow, a workflow-aware KV
cache management framework tailored for agentic workloads. KVFlow abstracts the
agent execution schedule as an Agent Step Graph and assigns each agent a
steps-to-execution value that estimates its temporal proximity to future
activation. These values guide a fine-grained eviction policy at the KV node
level, allowing KVFlow to preserve entries likely to be reused and efficiently
manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a
fully overlapped KV prefetching mechanism, which proactively loads required
tensors from CPU to GPU in background threads for agents scheduled in the next
step, thereby avoiding cache miss stalls during generation. Compared to SGLang
with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for
single workflows with large prompts, and up to 2.19$\times$ speedup for
scenarios with many concurrent workflows.

</details>


### [54] [Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems](https://arxiv.org/abs/2507.07671)
*Jovan Prodanov,Blaž Bertalanič,Carolina Fortuna,Shih-Kai Chou,Matjaž Branko Jurič,Ramon Sanchez-Iborra,Jernej Hribar*

Main category: cs.DC

TL;DR: 论文提出了一种基于多智能体强化学习的动态资源扩展引擎（MARLISE），用于解决边缘云系统中资源扩展的挑战，通过DQN和PPO算法实现高效资源管理。


<details>
  <summary>Details</summary>
Motivation: 传统静态阈值和预定义规则的资源扩展方法在动态环境中效率低下，无法满足边缘云基础设施的性能需求。

Method: 使用深度强化学习算法（DQN和PPO）开发MARLISE引擎，实现动态、反应式的资源扩展。

Result: 实验表明，MARLISE在动态负载下能显著降低微服务响应时间并提高资源效率，优于启发式方法。

Conclusion: MARLISE为边缘云系统提供了一种高效的动态资源扩展解决方案，提升了性能和资源利用率。

Abstract: Modern edge-cloud systems face challenges in efficiently scaling resources to
handle dynamic and unpredictable workloads. Traditional scaling approaches
typically rely on static thresholds and predefined rules, which are often
inadequate for optimizing resource utilization and maintaining performance in
distributed and dynamic environments. This inefficiency hinders the
adaptability and performance required in edge-cloud infrastructures, which can
only be achieved through the newly proposed in-place scaling. To address this
problem, we propose the Multi-Agent Reinforcement Learning-based In-place
Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with
in-place resource scaling. We develop our solution using two Deep Reinforcement
Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization
(PPO). We analyze each version of the proposed MARLISE solution using dynamic
workloads, demonstrating their ability to ensure low response times of
microservices and scalability. Our results show that MARLISE-based approaches
outperform heuristic method in managing resource elasticity while maintaining
microservice response times and achieving higher resource efficiency.

</details>


### [55] [KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling](https://arxiv.org/abs/2507.07932)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Qiang Guan,Hailong Jiang*

Main category: cs.DC

TL;DR: KIS-S框架结合GPU感知的Kubernetes推理模拟器和基于PPO的自动扩展器，显著提升GPU推理工作负载的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决Kubernetes中GPU推理工作负载的自动扩展问题，特别是在动态和突发流量下，默认机制（如HPA）表现不佳且缺乏GPU级指标集成。

Method: 提出KIS-S框架，结合KISim（GPU感知模拟器）和KIScaler（基于PPO的自动扩展器），在模拟中学习延迟感知和资源高效的扩展策略。

Result: 实验显示，KIScaler平均奖励提升75.2%，P95延迟降低6.7倍，且无需重新训练即可泛化。

Conclusion: KIS-S填补了反应式自动扩展与智能编排之间的差距，适用于可扩展的GPU加速环境。

Abstract: Autoscaling GPU inference workloads in Kubernetes remains challenging due to
the reactive and threshold-based nature of default mechanisms such as the
Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty
traffic patterns and lack integration with GPU-level metrics. We present KIS-S,
a unified framework that combines KISim, a GPU-aware Kubernetes Inference
Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based
autoscaler. KIScaler learns latency-aware and resource-efficient scaling
policies entirely in simulation, and is directly deployed without retraining.
Experiments across four traffic patterns show that KIScaler improves average
reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and
generalizes without retraining. Our work bridges the gap between reactive
autoscaling and intelligent orchestration for scalable GPU-accelerated
environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [56] [Accelerating Transposed Convolutions on FPGA-based Edge Devices](https://arxiv.org/abs/2507.07683)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 论文提出MM2IM加速器，通过结合矩阵乘法和col2IM方法优化TCONV层在边缘设备上的性能，显著提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: 现有TCONV实现方法（IOM）存在输出映射复杂、计算重叠和无效计算等问题，导致边缘设备性能瓶颈。

Method: 提出MM2IM加速器，结合矩阵乘法和col2IM方法，利用SECDA-TFLite工具包实现并评估。

Result: 在261种TCONV配置中平均加速1.9倍，在知名生成模型中最高加速4.2倍，能效提升显著。

Conclusion: MM2IM在边缘设备上高效优化TCONV，显著优于现有方法。

Abstract: Transposed Convolutions (TCONV) enable the up-scaling mechanism within
generative Artificial Intelligence (AI) models. However, the predominant
Input-Oriented Mapping (IOM) method for implementing TCONV has complex output
mapping, overlapping sums, and ineffectual computations. These inefficiencies
further exacerbate the performance bottleneck of TCONV and generative models on
resource-constrained edge devices. To address this problem, in this paper we
propose MM2IM, a hardware-software co-designed accelerator that combines Matrix
Multiplication (MatMul) with col2IM to process TCONV layers on
resource-constrained edge devices efficiently. Using the SECDA-TFLite design
toolkit, we implement MM2IM and evaluate its performance across 261 TCONV
problem configurations, achieving an average speedup of 1.9x against a
dual-thread ARM Neon optimized CPU baseline. We then evaluate the performance
of MM2IM on a range of TCONV layers from well-known generative models achieving
up to 4.2x speedup, and compare it against similar resource-constrained TCONV
accelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate
MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x
energy reduction against the CPU baseline.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data](https://arxiv.org/abs/2507.07589)
*Arpana Sinhal,Anay Sinhal,Amit Sinhal*

Main category: cs.LG

TL;DR: 本研究通过多模态数据集和先进机器学习模型，解决了医护人员压力监测中的数据集不足和方法局限问题，提出了可部署的压力监测系统。


<details>
  <summary>Details</summary>
Motivation: 医护人员（尤其是护士）面临高职业压力，COVID-19加剧了这一问题。现有研究缺乏全面数据集和鲁棒分析方法。

Method: 使用SMOTE预处理数据集以平衡压力状态，评估并组合随机森林、XGBoost和MLP模型为Stacking Classifier。

Result: 提出了一个可复现的分析流程和公开数据集，为压力监测系统的开发提供了实用方案。

Conclusion: 研究为医护人员心理健康保护提供了技术支持，未来可扩展人口多样性和探索边缘计算实现低延迟警报。

Abstract: Healthcare professionals, particularly nurses, face elevated occupational
stress, a concern amplified during the COVID-19 pandemic. While wearable
sensors offer promising avenues for real-time stress monitoring, existing
studies often lack comprehensive datasets and robust analytical frameworks.
This study addresses these gaps by introducing a multimodal dataset comprising
physiological signals, electrodermal activity, heart rate and skin temperature.
A systematic literature review identified limitations in prior stress-detection
methodologies, particularly in handling class imbalance and optimizing model
generalizability. To overcome these challenges, the dataset underwent
preprocessing with the Synthetic Minority Over sampling Technique (SMOTE),
ensuring balanced representation of stress states. Advanced machine learning
models including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were
evaluated and combined into a Stacking Classifier to leverage their collective
predictive strengths. By using a publicly accessible dataset and a reproducible
analytical pipeline, this work advances the development of deployable
stress-monitoring systems, offering practical implications for safeguarding
healthcare workers' mental health. Future research directions include expanding
demographic diversity and exploring edge-computing implementations for low
latency stress alerts.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [58] [A Graph Isomorphism-based Decentralized Algorithm for Modular Robot Configuration Formation](https://arxiv.org/abs/1602.03104)
*Ayan Dutta,Prithviraj Dasgupta,Carl Nelson*

Main category: cs.RO

TL;DR: 提出了一种基于图同构的算法，用于模块化机器人系统中的配置形成问题，旨在减少时间和能量消耗。


<details>
  <summary>Details</summary>
Motivation: 解决模块化机器人系统中模块从不同初始配置和位置移动到目标配置的问题。

Method: 采用基于效用的框架和图同构算法，模块选择目标配置中的位置，同时尽量保留原始配置。

Result: 算法具有完整性和帕累托最优性，实验显示规划时间短（100模块仅需毫秒级），优于市场分配算法。

Conclusion: 提出的算法在时间和通信效率上优于现有方法，适用于模块化机器人系统的配置形成。

Abstract: We consider the problem of configuration formation in modular robot systems
where a set of modules that are initially in different configurations and
located at different locations are required to assume appropriate positions so
that they can get into a new, user-specified, target configuration. We propose
a novel algorithm based on graph isomorphism, where the modules select
locations or spots in the target configuration using a utility-based framework,
while retaining their original configuration to the greatest extent possible,
to reduce the time and energy required by the modules to assume the target
configuration. We have shown analytically that our proposed algorithm is
complete and guarantees a Pareto-optimal allocation. Experimental simulations
of our algorithm with different number of modules in different initial
configurations and located initially at different locations, show that the
planning time of our algorithm is nominal (order of msec. for 100 modules). We
have also compared our algorithm against a market-based allocation algorithm
and shown that our proposed algorithm performs better in terms of time and
number of messages exchanged.

</details>
