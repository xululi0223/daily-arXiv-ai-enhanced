<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 86]
- [cs.CV](#cs.CV) [Total: 295]
- [cs.DC](#cs.DC) [Total: 40]
- [cs.AR](#cs.AR) [Total: 14]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.GL](#cs.GL) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.AI](#cs.AI) [Total: 18]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.LG](#cs.LG) [Total: 42]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: 提出了TimeStampEval基准，用于从长文本记录中检索非逐字引用的精确时间戳。开发的两阶段方法显著提高了检索准确性，同时将推理成本降低了90%以上。


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊匹配在语义相同但语法不同的引用检索中的失败问题，特别是在对齐官方书面记录和语音转文字记录时。应用场景是自动化的长篇播客，将国会记录片段汇编成AI主持的叙述。

Method: 采用两阶段方法：首先使用RapidFuzz进行预过滤，然后使用LLM在短片段上进行验证。提示设计将查询放在文本记录之前，并使用紧凑格式。

Result: 该方法将模糊匹配准确率提高了50个百分点，同时将延迟减半，并将每个正确结果的成本降低了96%。在10个文本记录（50k-900k tokens，1989-2025）上的扩展测试证实了对文本长度、词汇漂移和领域变化的鲁棒性。

Conclusion: 提示设计比模型选择更重要，适度的推理预算可以显著提高准确性。"辅助模糊"方法在保持高拒绝准确率的同时，大幅提高了检索性能并降低了成本。

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [2] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0是一个开源研究代理，通过交互式扩展作为模型规模和上下文长度之外的第三个性能改进维度，能够在256K上下文窗口下执行多达600次工具调用，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅扩展模型规模或上下文长度，而MiroThinker探索在模型层面进行交互扩展，通过更深入和频繁的代理-环境交互来提升性能。

Method: 使用强化学习训练模型实现高效的交互扩展，让模型能够处理更深层次的代理-环境交互，利用环境反馈和外部信息获取来纠正错误和优化轨迹。

Result: 在GAIA、HLE、BrowseComp和BrowseComp-ZH四个基准测试中，72B变体分别达到81.9%、37.7%、47.1%和55.6%的准确率，超越先前开源代理并接近商业对应物。

Conclusion: 交互扩展展现出与模型规模和上下文长度类似的扩展行为，确立了交互扩展作为构建下一代开源研究代理的第三个关键维度。

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [3] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: 该论文质疑语言模型是否真正具备推理能力，认为LM只是实现隐式有限阶马尔可夫核的统计模式匹配器，而非真正的推理者。


<details>
  <summary>Details</summary>
Motivation: 评估NLP领域中关于语言模型推理能力的定义是否与LM的实际训练、信息处理和生成过程一致，澄清LM产生类似推理输出的本质。

Method: 假设基于Transformer的语言模型实现了一个隐式有限阶马尔可夫核，将上下文映射到条件标记分布，分析推理类输出与统计规律性的关系。

Result: LM的推理类输出对应学习核中的统计规律性和近似统计不变性，而非显式逻辑机制的实现，这解释了为什么LM会产生推理类输出但缺乏逻辑一致性保证。

Conclusion: 语言模型是统计模式匹配器而非真正的推理者，这一区分对于评估LM中的认知不确定性至关重要，需要更准确地描述NLP系统的计算过程。

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [4] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 评估了7个开源大语言模型(0.6B-70B参数)在水电许可文档信息提取中的性能-计算资源权衡，发现14B参数是有效性阈值，小模型存在系统性幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决监管文档信息提取中性能与计算资源之间的关键权衡问题，为实际部署提供实证指导。

Method: 在水电许可文档上评估七个开源权重模型(0.6B-70B参数)，分析验证方法的有效性。

Result: 识别出14B参数阈值，验证方法从无效(F1<0.15)转变为可行(F1=0.64)；消费级模型通过适当验证达到64% F1，小模型停滞在51%，大规模模型接近77%但需企业基础设施。

Conclusion: 建立了首个监管背景下开源权重信息提取的资源-性能综合映射，为基于证据的模型选择提供指导，结果对水电合规具有直接价值，且参数缩放效应的见解可推广到其他信息提取任务。

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [5] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 探索使用基于LLM的自动形式化方法来验证LLM生成的输出与自然语言需求的一致性，通过两个实验展示了该方法在一致性检查和逻辑验证方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在从自然语言生成结构化输出方面表现出潜力，但缺乏正式方法来验证这些输出的准确性。本文旨在填补这一空白。

Method: 使用简单的基于LLM的自动形式化器，将非正式语句转换为形式逻辑，然后验证LLM生成的输出与自然语言需求的一致性。进行了两个实验：一致性检查和逻辑不一致性识别。

Result: 第一个实验成功识别了两个不同表述的自然语言需求在逻辑上是等价的；第二个实验识别出了给定自然语言需求与LLM生成输出之间的逻辑不一致性。

Conclusion: 虽然研究有限，但自动形式化在确保LLM生成输出的保真度和逻辑一致性方面具有显著潜力，为未来更广泛的研究奠定了基础。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [6] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 提出一个分析电影剧本情感弧线并基于角色上下文进行扩展分析的框架，使用基于NRC-VAD数据集的定制词典进行情感分析，并通过层次聚类技术对相似情感情节进行聚类。


<details>
  <summary>Details</summary>
Motivation: 故事理解和分析是自然语言理解中的挑战领域，需要深度计算语义表示和句法处理。大量叙事数据需要自动化语义分析和计算学习而非人工分析方法。

Method: 使用基于NRC-VAD数据集Valence、Arousal和Dominance分数的定制词典进行词典式情感分析，应用LabMTsimple storylab模块，并通过Wards层次聚类技术对相似情感情节进行聚类。

Result: 在电影数据集上的实验评估表明，该分析结果对消费者和读者在选择叙事或故事时很有帮助。

Conclusion: 该框架能够提取叙事中传达的高层次和低层次概念，为电影剧本的情感分析提供了有效的方法。

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [7] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: 本文介绍了包含6,393份放射学报告的标注语料库，用于评估大语言模型在随访依从性检测任务中的表现。GPT-4o在优化提示设置下表现最佳，接近人类标注者水平。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对放射学任务的领域特定数据集来系统评估大语言模型的性能，特别是随访依从性检测任务。

Method: 构建了包含6,393份放射学报告的标注语料库，系统比较了传统机器学习分类器（逻辑回归、支持向量机、Longformer）与生成式大语言模型（GPT-4o、GPT-OSS-20B）在基础设置和任务优化设置下的性能。

Result: GPT-4o在高级设置下表现最佳（F1=0.832），接近人类标注者间一致性（F1=0.846）。传统机器学习模型如逻辑回归和支持向量机也表现良好（F1分别为0.776和0.775）。

Conclusion: 虽然通过提示优化的大语言模型能够接近人类水平，但可解释且资源效率高的传统模型仍然是重要的基准方法。

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [8] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: MedPT是首个针对巴西葡萄牙语的大规模真实医疗问答语料库，包含38.4万对医患互动问答，通过多阶段筛选和LLM标注增强数据质量，支持医疗专科路由等任务，旨在解决低资源语言医疗AI发展不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在医疗领域的发展主要集中于高资源语言，而简单翻译无法捕捉特定临床和文化差异（如地方性疾病），这为其他语言社区创建了重要障碍。

Method: 构建了38.4万对真实医患问答语料库，采用混合定量-定性分析的多阶段筛选协议去除噪声，通过LLM驱动的标注将问题分类为7种语义类型，并进行上下文增强处理。

Result: 语料库涵盖3,200个主题，展现了医患沟通的自然不对称性。在医疗专科路由任务中，微调1.7B参数模型在20类设置下达到94%的F1分数，错误分析显示误分类反映了真实的临床模糊性。

Conclusion: MedPT数据集公开释放，旨在促进葡萄牙语世界更公平、准确和具有文化意识的医疗技术发展，证明了其在捕捉深度语义丰富性方面的价值。

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [9] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: ClinStructor是一个利用大语言模型将临床自由文本转换为结构化问答对的管道，旨在解决临床笔记中的偏见、泛化性和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含丰富信息但格式非结构化，存在无意偏见、跨临床环境泛化性差和可解释性差的问题。

Method: 使用大语言模型将临床自由文本转换为结构化的任务特定问答对，然后再进行预测建模。

Result: 在ICU死亡率预测任务中，与直接微调相比，该方法仅导致预测性能适度下降（AUC下降2-3%），同时显著提高了透明度和可控性。

Conclusion: ClinStructor为在临床环境中构建可靠、可解释和可泛化的机器学习模型奠定了坚实基础。

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [10] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: 该研究通过监督微调和强化学习技术改进GPT-2的心理治疗对话生成能力，在多个评估指标上取得显著提升，特别是情感准确率达到99.34%。


<details>
  <summary>Details</summary>
Motivation: COVID-19加剧了心理健康服务的可及性挑战，虽然大语言模型提供24/7服务，但预训练模型缺乏情境和情感意识，需要改进以提供适当的治疗性回应。

Method: 重构输入格式以同时处理情境信息和情感状态，采用多组件奖励函数使模型输出与专业治疗师回应和标注情感对齐，结合监督微调和强化学习技术。

Result: 强化学习在多个指标上优于基线GPT-2：BLEU(0.0111)、ROUGE-1(0.1397)、ROUGE-2(0.0213)、ROUGE-L(0.1317)、METEOR(0.0581)，情感准确率达到99.34%（基线为66.96%）。

Conclusion: 强化学习在开发治疗对话系统方面有效，可作为治疗师的有价值辅助工具，同时保持必要的人类临床监督。

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [11] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: CALM是一个可解释的临床文本分类框架，通过将预测分解为各个语义组件的加和贡献，提供透明的风险解释。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在临床文本分类中预测不透明的问题，满足研究人员和医生需要理解患者记录中哪些部分驱动风险信号的需求。

Method: 采用加性大语言模型框架，将半结构化文本输入分解为语义组件，预测结果是各组件贡献的加和，实现忠实解释。

Result: CALM在保持与传统LLM分类器相当性能的同时，提高了可信度，支持质量保证检查，并在模型开发和审计中揭示临床有意义的模式。

Conclusion: CALM框架为临床文本分类提供了可解释的解决方案，既保持了高性能，又增强了模型的透明度和可信度，适合临床实践和研究应用。

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [12] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 提出了InData数据集，用于评估LLM在多步骤工具推理方面的能力，发现当前LLM在复杂数据分析任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 解决LLM直接生成代码访问敏感数据的安全风险，通过预定义的安全工具限制数据访问。

Method: 引入Indirect Data Engagement (InData)数据集，包含三个难度级别的数据分析问题，评估15个开源LLM的多步骤工具推理能力。

Result: 大型模型在简单任务上准确率高(97.3%)，但在困难任务上表现显著下降(69.6%)，显示当前LLM缺乏稳健的多步骤工具推理能力。

Conclusion: InData数据集有助于开发和评估具有更强多步骤工具使用能力的LLM，将公开数据集和代码。

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [13] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: 提出了LLM-KAT评估方法和实体匿名化技术，以增强大语言模型在知识图谱对话生成任务中对外部知识的利用能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识图谱对话生成任务中倾向于依赖内部知识而非外部知识图谱，导致生成内容与提供的外部知识脱节。

Method: 引入LLM-KAT评估知识附着度，并提出实体匿名化技术来鼓励模型更好地利用外部知识。

Result: 在OpenDialKG数据集上的实验表明，该方法有效提升了LLM对外部知识的附着能力。

Conclusion: 通过实体匿名化和专门评估方法，可以显著改善大语言模型在知识图谱对话生成中的外部知识利用效果。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [14] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 该论文研究语言模型的熵校准问题，发现模型存在校准错误，且随着模型规模增大，校准错误的改善非常缓慢。理论分析表明，对于接近1的幂律指数，校准错误的缩放指数接近0。实证研究验证了这一发现，并证明在理论上可以通过预测未来熵来减少熵同时保持对数损失。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型的熵校准问题，探索模型生成过程中的错误累积现象，以及校准错误是否随模型规模改善，是否能在不牺牲对数损失的情况下实现校准。

Method: 首先在简化理论设置中分析校准错误的缩放行为与数据分布幂律指数的关系，然后在0.5B到70B参数的语言模型上进行实证测量，最后从理论上证明通过预测未来熵可以减少熵同时保持对数损失的可能性。

Result: 发现校准错误的缩放指数接近0，意味着更大模型与小模型以相似速率累积错误；理论证明在假设可以获得预测未来熵的黑盒情况下，可以减少熵同时保持对数损失。

Conclusion: 模型校准错误随规模改善非常缓慢，需要类似截断的解决方案；理论上存在不牺牲对数损失而减少熵的可能性，但这需要预测未来熵的能力。

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [15] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: 提出了一个名为ReasoningNER的推理框架，将NER从隐式模式匹配转向显式推理，在零样本设置下性能显著优于GPT-4。


<details>
  <summary>Details</summary>
Motivation: 生成式LLM在NER中通常通过指令微调提升性能，但缺乏可验证的推理机制，导致在零样本和低资源场景下泛化能力差。

Method: 三阶段框架：1)生成NER导向的思维链数据集；2)使用CoT微调模型生成推理链；3)推理增强阶段使用奖励信号优化推理过程。

Result: 在零样本设置下达到SOTA性能，F1分数比GPT-4高出12.3个百分点，展示了在推理导向信息提取方面的巨大潜力。

Conclusion: ReasoningNER框架显著提升了NER任务的认知能力，证明了显式推理机制在信息提取任务中的重要性。

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [16] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: 研究表明CoT解释在道德场景中具有双重作用：既能提高透明度，也能因确认偏见而误导用户，特别是当推理链被扰动或采用自信语调时，会抑制错误检测并维持用户依赖。


<details>
  <summary>Details</summary>
Motivation: 探讨CoT解释在透明性和误导性之间的平衡，分析其在多模态道德场景中如何影响用户信任和错误检测能力，揭示解释可能强化确认偏见的问题。

Method: 通过系统扰动推理链和操纵传递语调，分析视觉语言模型中的推理错误及其对用户信任和错误检测能力的影响。

Result: 发现两个关键效应：(1)用户常将信任等同于结果一致性，即使推理有缺陷也维持依赖；(2)自信语调会抑制错误检测但维持依赖，表明传递风格可凌驾于正确性之上。

Conclusion: CoT解释既能澄清也能误导，强调NLP系统需要提供鼓励审查和批判性思维而非盲目信任的解释。

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [17] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 提出了一个评估大语言模型文化能力的新基准，通过真实情境上下文测试文化推理能力，并引入四个补充指标来全面评估响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型文化能力评估方法过于关注脱离语境的正确性或强制选择判断，忽视了文化理解和推理的需求，无法准确评估模型在多元文化环境中的表现。

Method: 引入基于真实情境上下文的评估基准，使用标准精确匹配指标和四个补充指标（覆盖率、特异性、内涵和连贯性）来评估模型的文化推理能力。

Result: 实证分析显示，传统评估方法会系统性高估文化能力且评估结果不稳定，而提出的厚评估方法能暴露推理深度差异、减少方差，提供更稳定可解释的文化理解信号。

Conclusion: 厚评估方法比传统薄评估能更准确地评估大语言模型的文化能力，为模型在多元文化环境中的部署提供了更可靠的评估框架。

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [18] [Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task](https://arxiv.org/abs/2511.12109)
*Felipe Fujita,Hideyuki Takada*

Main category: cs.CL

TL;DR: 结合回译和微调在小规模日语语料上显著提升英日神经机器翻译质量，COMET分数从0.460提升至0.597。


<details>
  <summary>Details</summary>
Motivation: 探索在有限训练数据情况下，如何通过回译和微调的组合策略提升低资源语言对的翻译质量。

Method: 首先使用回译生成合成数据，然后在小规模真实平行数据集上进行微调，最后将两种方法结合使用。

Result: 基线模型COMET=0.460，单独回译提升至0.468，单独微调提升至0.589，两者结合达到最佳效果0.597。

Conclusion: 回译和微调的协同使用能够显著提升低资源语言对的翻译质量，为小规模语料库提供了轻量级但有效的改进策略。

Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.

</details>


### [19] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: LLMLagBench是一个用于评估大语言模型训练数据时间边界的基准测试，通过检测模型对近期事件的了解程度来识别其知识的新鲜度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定时间点之前的数据上进行预训练，这造成了严格的知识边界。当这个限制未知或被忽视时，模型可能会在推理任务中无意间混合过时的时效性信息与通用知识，从而影响回答的准确性。

Method: 引入LLMLagBench基准测试，通过评估模型对近期事件的知识来系统性地识别LLM训练数据的最早可能时间边界。对大量LLM进行评估，包括有明确声明和未声明训练截止时间的模型。

Result: 通过人工验证和与公开的LLM预训练信息比较，评估了基准测试的可靠性。

Conclusion: LLMLagBench提供了一个系统方法来识别LLM的知识时间边界，有助于理解模型的知识新鲜度和潜在的信息混合问题。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [20] [PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection](https://arxiv.org/abs/2511.12130)
*Bingbing Wang,Zhixin Bai,Zhengda Jin,Zihan Wang,Xintong Song,Jingjie Lin,Sixuan Li,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出了U-MStance数据集和PRISM模型，解决了多模态对话立场检测中的伪多模态和用户同质性问题，通过用户画像和多模态对齐显著提升了立场检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究存在伪多模态（源帖子有视觉线索但评论仅文本）和用户同质化（忽略个人特质对立场表达的影响）两大局限，无法反映真实世界的多模态交互。

Method: 1) 从历史帖子和评论中提取纵向用户画像；2) 通过思维链对齐对话上下文中的文本和视觉线索；3) 采用相互任务增强机制联合优化立场检测和立场感知响应生成。

Result: 在U-MStance数据集上的实验表明，PRISM相比强基线模型取得了显著性能提升，验证了用户中心和上下文基础的多模态推理的有效性。

Conclusion: 用户中心的多模态推理对于现实立场理解至关重要，PRISM通过用户画像和多模态对齐有效解决了现有方法的局限性。

Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.

</details>


### [21] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: 提出了AI-Salesman框架，通过双阶段架构解决目标驱动说服对话的挑战，包括训练阶段的贝叶斯监督强化学习和推理阶段的动态大纲引导代理，在真实销售数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 目标驱动说服对话（如电话销售）需要复杂的多轮规划和严格的事实忠实性，这对现有大语言模型构成挑战。缺乏任务特定数据和直接应用LLM存在的策略脆弱性及事实幻觉问题限制了该领域发展。

Method: 构建TeleSalesCorpus真实销售对话数据集；提出AI-Salesman双阶段框架：训练阶段使用贝叶斯监督强化学习从噪声对话中学习稳健销售策略；推理阶段采用动态大纲引导代理，利用预构建脚本库提供动态策略指导。

Result: 实验结果表明，AI-Salesman在自动指标和综合人工评估中均显著优于基线模型，在复杂说服场景中展现出有效性。

Conclusion: AI-Salesman框架通过创新的双阶段架构和动态大纲引导机制，成功解决了目标驱动说服对话中的策略规划和事实忠实性问题，为相关应用提供了有效解决方案。

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [22] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: VBackChecker是一个无需参考的幻觉检测框架，通过像素级Grounding LLM验证MLLM生成响应与视觉输入的一致性，在R^2-HalBench基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在严重的幻觉问题，需要准确检测以确保实际应用的可靠性。

Method: 基于"眼见为实"原则，使用具备推理和分割能力的像素级Grounding LLM来验证响应与视觉输入的一致性，并设计了R-Instruct指令调优数据生成管道。

Result: 在R^2-HalBench基准上超越先前复杂框架，性能与GPT-4o相当，在像素级定位任务中提升超过10%。

Conclusion: VBackChecker为MLLM幻觉检测提供了有效的参考无关解决方案，具有强泛化能力和可解释性。

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [23] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: CriticSearch是一个细粒度信用分配框架，通过回顾性批评机制提供密集的逐轮反馈，解决了搜索代理中基于强化学习的优化存在的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理管道依赖强化学习优化，但存在稀疏结果奖励问题，导致探索效率低下和训练不稳定。

Method: 引入回顾性批评机制，使用冻结的非对称批评LLM基于完整轨迹和黄金答案对每个轮次进行回顾性评估，将评估转换为稳定的密集奖励来指导策略改进。

Result: 在多样化多跳推理基准测试中，CriticSearch始终优于现有基线方法，实现了更快的收敛、更好的训练稳定性和更高的性能。

Conclusion: CriticSearch通过密集的逐轮反馈有效解决了搜索代理训练中的稀疏奖励问题，提升了多跳推理任务的性能。

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [24] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: MME-RAG是一个多管理器-专家检索增强生成框架，将实体识别分解为类型级判断和跨度级提取两个协调阶段，通过轻量级管理器和专业专家实现精确的领域自适应提取。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在任务导向对话中的领域适应性和检索可控性方面仍面临挑战，特别是在细粒度实体识别任务中。

Method: 提出MME-RAG框架，包含轻量级管理器进行类型级判断和专业专家进行跨度级提取，每个专家配备KeyInfo检索器注入语义对齐的少样本示例。

Result: 在CrossNER、MIT-Movie、MIT-Restaurant和新构建的多领域客服数据集上的实验表明，MME-RAG在大多数领域表现优于现有基线方法。

Conclusion: 层次分解和KeyInfo引导的检索是鲁棒性和跨领域泛化的关键驱动因素，MME-RAG为自适应对话理解提供了可扩展和可解释的解决方案。

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [25] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: CONFACTCHECK是一种高效的幻觉检测方法，通过检查生成文本中事实探针的一致性来检测幻觉，无需外部知识库，在资源受限环境下优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生事实错误的幻觉文本，在医疗、金融等关键领域存在严重风险。现有方法在模型访问受限时需要多次API调用，增加延迟和成本。

Method: 基于直觉：生成文本中对事实探针的响应应该在单个LLM内部和不同LLM之间保持一致。通过检查这种一致性来检测幻觉，无需外部知识库。

Result: 在多个数据集上的严格评估显示，CONFACTCHECK能以更少资源高效检测幻觉事实，在相似条件下比现有基线方法获得更高的准确率分数。

Conclusion: CONFACTCHECK提供了一种资源高效的幻觉检测方法，在模型访问受限的实际应用场景中具有重要价值。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [26] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: 提出了ViConBERT框架，通过对比学习和基于词义解释的蒸馏来学习越南语上下文嵌入，并创建了ViConWSD数据集用于评估语义理解。


<details>
  <summary>Details</summary>
Motivation: 越南语缺乏用于细粒度语义理解的强大模型和评估资源，而现有的上下文词嵌入进展主要局限于英语等高资源语言。

Method: 结合对比学习(SimCLR)和基于词义解释的蒸馏来学习越南语上下文嵌入，并构建了大规模合成数据集ViConWSD用于评估。

Result: ViConBERT在WSD上达到F1=0.87，在ViCon上AP=0.88，在ViSim-400上Spearman's rho=0.60，优于强基线模型。

Conclusion: ViConBERT在建模离散词义和分级语义关系方面表现出色，为越南语语义理解提供了有效解决方案。

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [27] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 提出使用小型LLM压缩输入给大型LLM的新范式，开发了Cmprsr模型，在压缩率控制和下游任务性能方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 降低使用黑盒大语言模型的高成本，通过压缩输入来减少计算开销。

Method: 使用Textgrad优化压缩元提示，对Qwen3-4B进行SFT和GRPO联合训练，实现压缩率控制和任务性能最大化。

Result: Cmprsr在MeetingBank、LongBench和GSM8k数据集上优于提取式和普通抽象压缩方法，能精确控制压缩率。

Conclusion: Cmprsr模型在压缩能力和下游任务性能方面表现优异，为成本控制提供了有效解决方案。

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [28] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: 本文提出了一种利用现有抽象摘要自动生成相应抽取式摘要的轻量级方法，旨在为法律案例摘要研究社区创建包含两种摘要类型的增强数据集。


<details>
  <summary>Details</summary>
Motivation: 法律判决摘要对法律从业者构成沉重认知负担，而深度神经网络生成的抽象摘要容易误传法律术语或忽略关键细节，因此需要更可靠的抽取式摘要方法。但人工标注抽取式摘要成本高昂。

Method: 设计了一个轻量透明的流程，利用现有的抽象黄金标准摘要自动生成相应的抽取式黄金标准版本，确保专家意见从抽象摘要传递到抽取式摘要中。

Result: 计划增强七个现有案例摘要数据集，通过添加相应的抽取式摘要来丰富数据资源，并进行结构、词汇和语义维度的广泛比较评估。

Conclusion: 承诺公开发布增强的数据集，相信该资源将推动法律文档自动摘要领域的发展。

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [29] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: 研究比较了LLMs和人类在抢答式问答中的表现差异，发现LLMs在维基百科未覆盖的问题和需要数值答案的问题上表现更差。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多NLP任务中超越了人类表现，但尚不清楚对人类困难的问题是否对LLMs同样困难，因此研究在抢答设置下比较LLMs和人类在问答中的表现差异。

Method: 收集包含问题、答案和人类正确率的日语问答数据，在多种设置下提示LLMs回答这些问题，并从两个分析视角比较LLMs与人类的正确率。

Result: 实验结果显示，与人类相比，LLMs在正确答案未被维基百科条目覆盖的问题上表现更差，且在需要数值答案的问题上也有困难。

Conclusion: LLMs和人类在问答难度上存在差异，LLMs更依赖维基百科等知识源，且在数值推理方面存在短板。

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [30] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型中的"讽刺反弹"现象，即否定指令反而会增加被禁止概念的可及性。通过两个实验发现：否定后立即出现反弹，语义干扰会加剧反弹，而重复有助于抑制；更强的极性分离与更持久的反弹相关。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解否定指令在LLMs中如何引发讽刺反弹现象，这与人类认知中的"讽刺反弹效应"类似，即试图抑制某个概念反而会增强其在思维中的可及性。

Method: 采用两个实验：(1) 负载与内容实验：在否定指令后变化干扰文本类型（语义、句法、重复）并测量反弹强度；(2) 极性分离实验：测试模型是否能区分概念的中性和负面框架，以及这种分离是否预测反弹持续性。还进行了电路追踪分析。

Result: 结果显示：否定后立即出现反弹；较长或语义干扰会加剧反弹，而重复有助于抑制；更强的极性分离与更持久的反弹相关；电路分析发现中间层注意力头会放大被禁止的token，而早期层则进行抑制。

Conclusion: 该研究将认知预测的讽刺反弹与长上下文干扰的机制性见解联系起来，并发布了ReboundBench数据集以支持未来研究。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [31] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: ILAKKANAM是首个泰米尔语特定语言评估基准，基于820个斯里兰卡学校泰米尔语考试问题构建，评估显示LLMs在低年级表现良好但随语言复杂度增加而下降。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准依赖英语翻译数据集，无法捕捉泰米尔语等低资源、形态丰富语言的语言和文化细微差别。

Method: 使用820个斯里兰卡学校泰米尔语考试问题构建基准，由训练有素的语言学家在五个语言类别和一个事实知识类别下标注，涵盖1-13年级。

Result: Gemini 2.5表现最佳，开源模型落后；所有模型在低年级问题表现良好，但随着语言复杂度增加明显下降；模型整体表现与识别语言类别能力无强相关性。

Conclusion: LLMs在泰米尔语的语言能力可能由暴露驱动而非真正理解，需要更多语言基础工作来改善低资源语言的表现。

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [32] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 提出了MRMBench基准和推理时探测方法，用于评估奖励模型在多维度偏好上的表现，发现该方法与LLM对齐性能强相关，揭示了奖励模型在多维度偏好捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估方法通常在固定成对排名测试集上进行，但无法提供各偏好维度的性能信息，需要更细粒度的评估方法来衡量奖励模型在不同偏好维度上的表现。

Method: 构建了MRMBench基准，包含6个不同偏好维度的探测任务；提出了推理时探测方法，识别奖励预测中使用的维度并增强可解释性。

Result: MRMBench与大型语言模型对齐性能强相关；奖励模型在多维度偏好捕捉上表现不佳；推理时探测方法能可靠评估奖励预测置信度。

Conclusion: MRMBench是开发先进奖励模型的可靠参考，揭示了多目标优化在奖励建模中的潜力，推理时探测方法能提升LLM对齐效果。

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [33] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 提出了SerenQA框架来评估LLM在科学知识图谱问答中发现意外洞察的能力，重点关注药物重定位任务中的惊喜发现。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA系统通常返回高度相关但可预测的答案，缺乏发现惊喜和新颖答案的能力。

Method: 提出了SerenQA框架，包含基于相关性、新颖性和惊喜度的严谨惊喜度指标，以及从临床知识图谱提取的专家标注基准。评估流程包括知识检索、子图推理和惊喜探索三个子任务。

Result: 实验表明，最先进的LLM在检索方面表现良好，但在识别真正令人惊喜和有价值的发现方面仍有困难。

Conclusion: LLM在发现惊喜洞察方面仍有显著改进空间，SerenQA为未来研究提供了评估框架和资源。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [34] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: SGuard-v1是一个轻量级的大语言模型安全护栏系统，包含ContentFilter和JailbreakFilter两个组件，分别用于检测有害内容和对抗性提示，基于2B参数的Granite模型构建，支持12种语言。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在人类-AI对话场景中的安全风险，包括有害内容生成和对抗性提示攻击，需要开发轻量级但高效的安全防护系统。

Method: 构建两个专门化模型：ContentFilter基于MLCommons危害分类学检测安全风险，JailbreakFilter通过精心设计的课程学习覆盖60种主要攻击类型。使用约140万训练实例对基础模型进行指令调优。

Result: 在公开和专有安全基准测试中达到最先进的安全性能，同时保持轻量级部署，提供多类安全预测和二元置信度分数以提高可解释性。

Conclusion: SGuard-v1作为轻量级安全护栏系统，在减少部署开销的同时实现了优秀的安全性能，并以Apache-2.0许可证发布以促进AI安全研究和实际部署。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [35] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: QA-Noun是一个基于问答的框架，用于捕捉名词为中心的语义关系，通过9个问题模板覆盖名词的显式句法和隐式上下文角色，与QA-SRL结合实现句子意义的细粒度分解。


<details>
  <summary>Details</summary>
Motivation: 现有的基于QA的语义方法主要关注谓词-论元关系，但忽略了名词为中心的语义表示，需要补充名词语义关系来完善细粒度语义分解。

Method: 定义9个问题模板来捕捉名词的句法和上下文角色，创建包含2000多个标注名词的数据集，训练模型并与QA-SRL集成实现统一语义分解。

Result: QA-Noun几乎完全覆盖了AMR的名词论元，同时揭示了额外的上下文隐含关系，与QA-SRL结合比FactScore和DecompScore等方法的粒度提高了130%以上。

Conclusion: QA-Noun补充了基于QA的语义框架，形成了全面且可扩展的细粒度语义分解方法，适用于跨文本对齐任务。

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [36] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: TAdaRAG是一个新颖的检索增强生成框架，通过动态构建任务自适应的知识图谱来解决传统RAG中信息截断和无关细节的问题，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法由于输入上下文窗口限制需要截断外部知识，导致信息丢失、响应幻觉和推理链断裂，且检索的非结构化知识包含无关细节影响准确推理。

Method: 提出意图驱动的路由机制到领域特定提取模板，结合监督微调和基于强化学习的隐式提取机制，确保知识整合的简洁性、连贯性和非冗余性。

Result: 在六个公共基准测试和一个真实商业基准测试(NowNewsQA)上，使用三个骨干模型评估，TAdaRAG在多个领域和长文本任务中优于现有方法。

Conclusion: TAdaRAG展示了强大的泛化能力和实际有效性，能够通过动态知识图谱构建显著提升RAG系统的性能。

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [37] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 提出一种因果框架来分析和缓解RLHF奖励模型中的长度偏差问题，通过反事实数据增强方法训练奖励模型，使其能够独立于冗长度评估内容质量。


<details>
  <summary>Details</summary>
Motivation: RLHF训练的奖励模型存在长度偏差，系统性地倾向于偏爱更长的回答，将冗长度与质量混为一谈。

Method: 使用反事实数据增强方法生成响应对：1）内容相似但长度不同的对；2）长度相似但内容不同的对，用于训练奖励模型。

Result: 实证评估表明该方法减少了奖励分配中的长度偏差，并导致策略模型产生更简洁、内容聚焦的输出。

Conclusion: 所提出的方法有效减少了长度偏差，提高了RLHF流程中奖励建模的鲁棒性和内容敏感性。

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [38] [MMWOZ: Building Multimodal Agent for Task-oriented Dialogue](https://arxiv.org/abs/2511.12586)
*Pu-Hai Yang,Heyan Huang,Heng-Da Xu,Fanshu Sun,Xian-Ling Mao,Chaoxu Mu*

Main category: cs.CL

TL;DR: MMWOZ是一个新的多模态对话数据集，扩展自MultiWOZ 2.3，包含GUI前端界面和操作指令，旨在弥合传统任务导向对话系统与实际应用之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统任务导向对话系统依赖定制后端API，而现实世界中广泛存在前端GUI界面且缺乏定制API，这造成了实际应用中的显著差距。

Method: 开发web风格GUI作为前端，设计自动化脚本将对话状态和系统动作转换为GUI操作指令，收集网页快照和对应操作指令，并提出了MATE多模态模型作为基线。

Result: 创建了MMWOZ多模态对话数据集，并建立了MATE基线模型，为构建实用的多模态任务导向对话代理提供了实验基础。

Conclusion: MMWOZ数据集和MATE模型为研究实用的多模态任务导向对话系统提供了重要基础，有助于弥合传统对话系统与实际应用之间的差距。

Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.

</details>


### [39] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: GAPO是一种基于GRPO的扩展方法，通过计算群体层面的奖励来解决LLM的模式崩溃问题，提高生成响应的多样性而不损害准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常出现模式崩溃问题，即使存在多个有效答案，也反复生成相同的少数完成结果，这限制了它们在各种任务中的多样性。

Method: 引入Group-Aware Policy Optimization (GAPO)，作为GRPO的简单扩展，计算群体层面的奖励，使用频率感知的奖励函数鼓励在有效LLM完成结果上进行均匀采样。

Result: GAPO训练的模型产生有效且更多样化的响应，在开放提示下也能提高响应多样性，同时在标准LLM基准测试（GSM8K、MATH、HumanEval、MMLU-Pro）上不损害准确性。

Conclusion: GAPO能够有效解决LLM的模式崩溃问题，提高生成响应的多样性，同时保持模型性能，代码将公开提供。

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [40] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Uni-MoE 2.0是一个全开源的全模态大模型，基于Qwen2.5-7B架构构建，通过动态容量MoE设计、渐进式训练策略和精心策划的多模态数据匹配技术，在语言中心的多模态理解、推理和生成方面取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 推进Lychee Uni-MoE系列在语言中心多模态理解、推理和生成方面的能力，构建一个能够理解全模态并生成图像、文本和语音的开放模型。

Method: 采用动态容量MoE设计，使用共享、路由和空专家平衡计算效率与能力；Omni-Modality 3D RoPE确保自注意力层的时空跨模态对齐；渐进式监督微调策略激活模态特定专家，并通过平衡数据组合和迭代GSPO-DPO方法稳定RL训练。

Result: 在85个基准测试中达到SOTA或极具竞争力的性能，在76个基准中超过50个优于Qwen2.5-Omni（使用1.2T token训练）；视频理解平均提升7%，全模态理解平均提升7%，视听推理提升4%；长语音处理WER降低4.2%，在低级图像处理和可控生成方面领先。

Conclusion: Uni-MoE 2.0在计算效率与能力之间取得了良好平衡，通过创新的MoE架构和训练策略，在多种多模态任务上表现出色，证明了其在全模态理解和生成方面的强大能力。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [41] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 提出了NOTAM语义解析任务，构建了Knots数据集，通过多智能体协作框架实现全面字段发现，并系统评估了多种提示工程策略和模型适应技术，显著提升了航空文本理解能力。


<details>
  <summary>Details</summary>
Motivation: NOTAMs作为关键飞行安全信息渠道，其复杂语言结构和隐含推理给自动化解析带来挑战。现有研究主要关注分类和命名实体识别等表层任务，缺乏深度语义理解。

Method: 提出NOTAM语义解析任务，构建包含12,347个专家标注NOTAMs的Knots数据集，覆盖194个飞行情报区，采用多智能体协作框架进行字段发现，系统评估提示工程策略和模型适应技术。

Result: 实验结果表明所提方法有效，在航空文本理解和处理方面取得显著改进，为自动化NOTAM分析系统提供了有价值的见解。

Conclusion: NOTAM语义解析任务结合航空领域知识能够产生结构化的、富含推理的输出，为航空安全信息的自动化处理提供了有效解决方案。

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [42] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: Reason-KE++是一个SFT+RL框架，通过过程级忠实度对齐解决LLM在复杂多跳推理任务中的事实幻觉问题，在MQUAKE-CF-3k上达到95.48%的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂多跳推理任务中对新知识的忠实度问题，现有SFT方法存在'忠实度差距'，导致参数先验覆盖上下文事实，产生关键事实幻觉。

Method: 提出Reason-KE++框架，结合SFT和RL，核心是阶段感知奖励机制，为中间推理步骤提供密集监督，包括分解和子答案正确性等过程级评估。

Result: 在MQUAKE-CF-3k数据集上达到95.48%的准确率，相比之前方法提升5.28%，同时发现仅基于结果的RL会损害推理完整性。

Conclusion: 对于复杂任务，对齐推理过程对于构建可信赖的LLM至关重要，过程级忠实度是解决LLM对齐问题的关键。

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [43] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: 提出了一种用于波斯语到英语语音翻译的直接语音到语音翻译系统，通过合成平行语音数据和自监督预训练来解决低资源语言的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 直接语音到语音翻译系统需要大量平行语音数据，但对于波斯语等低资源语言来说，这种数据非常稀缺，因此需要开发有效的数据增强方法。

Method: 使用三个组件：基于conformer的编码器、因果transformer解码器和基于单元的神经声码器。通过大语言模型翻译波斯语转录文本，再用零样本文本到语音系统合成英语语音，构建合成平行语料库。

Result: 在CVSS语料库的波斯语-英语部分，使用合成数据比直接基线提高了4.6 ASR BLEU分数，可用平行语音数据量增加了约6倍。

Conclusion: 结合自监督预训练、离散语音单元和合成平行数据对于改善波斯语-英语等低资源语言对的直接语音到语音翻译是有效的。

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [44] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: EvoSynth是一个自主的进化合成框架，通过多智能体系统自主设计、进化和执行基于代码的新型攻击算法，突破了现有自动化红队框架只能选择、组合或改进现有攻击策略的限制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自动化红队框架的越狱逻辑局限于选择、组合或改进现有攻击策略，限制了其创造力，无法自主发明全新的攻击机制。

Method: 采用多智能体系统自主设计、进化和执行基于代码的新型攻击算法，包含代码级自校正循环，能够根据失败情况迭代重写自身攻击逻辑。

Result: 在高度鲁棒的模型如Claude-Sonnet-4.5上实现了85.5%的攻击成功率，生成的攻击比现有方法更加多样化。

Conclusion: EvoSynth通过进化合成越狱方法的新方向，为未来研究提供了新的框架和思路。

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [45] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: AFM是一种动态上下文管理器，通过语义相似度、半衰期权重和重要性分类为历史消息分配三种保真度级别，在严格token预算下显著减少推理成本同时保持对话安全性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在多轮对话中固定上下文窗口和朴素内存策略的瓶颈问题，避免完整对话重放的高成本或静态摘要导致的安全关键信息丢失。

Method: AFM根据当前查询的语义相似度、半衰期权重和重要性分类，为每条历史消息分配FULL、COMPRESSED或PLACEHOLDER保真度级别，并按时间顺序打包消息。

Result: 在涉及严重花生过敏用户的旅行规划安全基准测试中，AFM在短中长度对话中保持过敏信息，安全性能与朴素重放相当，平均token使用量减少66%。

Conclusion: AFM提供模块化Python实现，可在不牺牲安全性或事实连续性的前提下显著降低推理成本。

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [46] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: LLMs在复杂推理任务中表现超人类，但在简单集合成员查询任务中却频繁失败，显示出其理解的脆弱性和不可预测性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在简单推理任务中的失败模式，以揭示其可靠性和可解释性方面的潜在问题。

Method: 通过大规模系统性实验，测试LLMs在集合成员查询任务中的表现，考察提示措辞、语义结构、元素排序和模型选择等多个维度。

Result: LLMs在这个基础任务上的表现始终脆弱且不可预测，表明模型对集合概念的理解是碎片化和复杂的。

Conclusion: 通过简单问题的大规模实验可以全面映射和分析LLMs的失败模式，这为LLM评估提供了一种有价值的方法论。

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [47] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: 研究发现语言模型训练中存在相变重组现象，即使在小型模型中也能观察到，这些相变在线性训练空间中可直接检测到，且发生在训练早期阶段。


<details>
  <summary>Details</summary>
Motivation: 探索相变是否仅存在于大型语言模型，还是也能在小模型中观察到；能否在线性训练空间中直接检测相变；这些相变是否在训练早期就出现。

Method: 训练小型GPT风格transformer模型，分析词汇使用演变，跟踪平均词长、正确与错误词汇数量、词汇多样性变化，应用泊松和亚泊松统计量化词汇连接和重组。

Result: 在训练过程中发现了一个明显的相变点，这些相变在标准损失或验证曲线中不明显，但通过词汇和统计探针变得可见。

Conclusion: 相变重组是语言模型训练的普遍特征，可在小型模型中观察到，在线性训练空间中可直接检测，且发生在训练早期阶段，为理解语言模型训练的非线性动态提供了新视角。

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [48] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 提出使用中断机制来增强大语言模型的对齐性，通过在用户输入中定期插入控制语句来防止越狱行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐研究主要关注通过训练和提示来提高模型对抗攻击的鲁棒性，但缺乏随着用户输入长度增加而扩展的对齐方法。研究表明LLM越狱概率随输入长度增加而上升。

Method: 提出中断机制，在用户输入中每约x个token插入控制语句，这种方法可推广到思维链过程以防止策略性行为。

Result: 论文提出了中断机制的概念框架，但未提供具体的实验结果数据。

Conclusion: 中断机制是增强LLM对齐性的潜在解决方案，能够随着用户输入长度扩展而保持对齐效果。

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [49] [Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing](https://arxiv.org/abs/2511.12784)
*Hayden Moore,Asfahan Shah*

Main category: cs.CL

TL;DR: 评估大型语言模型在自动形式化任务中对语义相似但措辞不同的自然语言输入的鲁棒性，发现即使语义保持高度一致，模型输出仍会因措辞变化而出现显著波动。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自动形式化方面表现出色，但近期研究表明它们在文本到SQL任务中对自然语言输入的改写很敏感。本文旨在验证这种敏感性是否也存在于自动形式化领域。

Method: 使用MiniF2F和ProofNet的Lean 4版本作为形式化基准，生成语义相似但措辞不同的自然语言陈述，并在两个现代LLMs上进行交叉评估，测量语义有效性和编译有效性。

Result: 结果显示模型在改写输入上的性能存在变异性，自然语言陈述的微小变化会显著影响模型输出。

Conclusion: LLMs在自动形式化任务中对自然语言输入的改写具有敏感性，这突显了提高模型鲁棒性的必要性。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.

</details>


### [50] [BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals](https://arxiv.org/abs/2511.12821)
*Ruiyu Wang,Yuzhang Xie,Xiao Hu,Carl Yang,Jiaying Lu*

Main category: cs.CL

TL;DR: BioMedJImpact是一个大规模生物医学数据集，整合了文献计量指标、合作特征和LLM提取的AI参与度指标，用于分析合作强度和AI参与度如何共同影响期刊影响力。


<details>
  <summary>Details</summary>
Motivation: 现有开放资源很少捕捉合作结构和AI研究如何共同塑造生物医学期刊声望，需要开发综合数据集来推进期刊层面的科学影响力和AI参与度分析。

Method: 从PubMed Central的174万篇文章构建数据集，提出可复现的三阶段LLM流程提取AI参与度特征，分析合作强度和AI参与度在疫情前后对期刊影响力的共同影响。

Result: 发现两个一致趋势：合作强度更高的期刊（特别是作者团队更大更多样化的）获得更高引用影响力；AI参与度与期刊声望的相关性日益增强，尤其在四分位排名中。

Conclusion: BioMedJImpact既是捕捉生物医学与AI交叉的综合数据集，也是经过验证的方法框架，支持可扩展的、内容感知的科学计量分析。

Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

</details>


### [51] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 通过目标激活工程引导LLaMA 3.1-8B展现更人性化的情感表达，使用归因修补识别关键干预位点，从对比文本对的激活差异推导情感表达向量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型对话流畅度不断提升，但为其注入细腻、人性化的情感表达仍具挑战性。现有对齐技术往往只处理表层输出或需要大量微调。

Method: 采用归因修补识别因果影响组件，通过对比文本对（正面vs负面情感示例）的激活差异推导情感表达向量，并将这些向量应用于新对话提示。

Result: 引导后的响应显示出增强的积极情感（如喜悦、信任）和更频繁的第一人称代词使用，表明更强的个人参与度。

Conclusion: 研究提供了一个精确且可解释的框架，为对话AI研究开辟了新方向。

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [52] [Quantifying consistency and accuracy of Latent Dirichlet Allocation](https://arxiv.org/abs/2511.12850)
*Saranzaya Magsarjav,Melissa Humphries,Jonathan Tuke,Lewis Mitchell*

Main category: cs.CL

TL;DR: 论文提出了一种新的稳定性度量方法，用于评估LDA主题模型的一致性，发现LDA能够正确识别文档中的主题数量但生成的主题并非真实主题。


<details>
  <summary>Details</summary>
Motivation: 概率主题模型由于其随机性在重复运行时会产生不同的结果，这种不稳定性影响了模型的可重复性、可靠性和解释性，引发了对主题模型是否真正捕捉到有意义主题的质疑。

Method: 定义了一种结合准确性和一致性的新稳定性度量方法，利用LDA的生成特性生成带有真实标签的新语料库，并对这些语料库运行LDA 50次以评估输出变异性。

Result: LDA能够正确确定文档中的基础主题数量，并且在多次重复运行时返回相似主题，表现出较高的内部一致性，但这些主题并非真实主题。

Conclusion: LDA主题模型在识别主题数量方面表现准确且具有内部一致性，但生成的主题内容与真实主题存在差异，需要进一步改进以确保模型捕捉到真正有意义的主题。

Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

</details>


### [53] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: NeuroLex是一个专门针对脑电图报告的轻量级领域自适应语言模型，通过EEG报告文本训练，在EEG语言理解和生成任务上优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型无法捕捉临床脑电图报告中的领域特定语言惯例，需要专门针对EEG报告语言特点的模型。

Method: 使用哈佛脑电图数据库的EEG报告文本，通过span-corruption预训练和指令微调（报告润色、段落摘要、术语问答）来训练模型。

Result: 与同等规模的通用模型相比，NeuroLex实现了更低的困惑度、更高的提取和摘要准确率、更好的标签效率，以及对否定和事实幻觉的更强鲁棒性。

Conclusion: NeuroLex为生物医学文本建模和脑机接口应用提供了可解释且语言驱动的神经解码基础。

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [54] [From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models](https://arxiv.org/abs/2511.12861)
*Wenxin Zhu,Andong Chen,Yuchen Song,Kehai Chen,Conghui Zhu,Ziyan Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文系统综述了多模态思维链(MCoT)技术，分析了其背景动机、主流方法、评估基准、应用场景、当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在感知任务中的显著成功，提升其复杂推理能力成为关键研究方向。现有模型存在推理路径不透明和泛化能力不足等问题，而思维链推理在语言模型中已证明能增强推理透明度和输出可解释性，有望在扩展到多模态领域时改善模型推理能力。

Method: 从三个方面介绍主流MCoT方法：思维链范式、后训练阶段和推理阶段，并分析其底层机制。

Result: 总结了现有的评估基准和指标，讨论了MCoT的应用场景。

Conclusion: 分析了MCoT当前面临的挑战，并对其未来研究方向进行了展望。

Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

</details>


### [55] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: 本文比较了三种基于Transformer的架构（BERT、GPT-2、DeBERTa）在文本希望表达分类任务中的表现。BERT在准确率和计算效率方面表现最佳，而GPT-2在检测讽刺性希望表达方面有优势。


<details>
  <summary>Details</summary>
Motivation: 开发一个计算框架来分析文本中的希望表达，应用于心理健康和社交媒体分析领域，同时探索不同Transformer架构在专门情感检测任务中的适用性。

Method: 使用三种Transformer架构（BERT、GPT-2、DeBERTa）进行二元分类（希望vs非希望）和多类别分类（五个希望相关类别），比较它们的性能和计算效率。

Result: BERT表现最佳，二元分类准确率84.49%，多类别分类72.03%，且训练时间最短（443秒）。GPT-2准确率最低（79.34%二元，71.29%多类别），但在讽刺检测方面表现出色（92.46%召回率）。DeBERTa表现中等但计算成本最高。

Conclusion: 对于专门的情感检测任务，架构的适用性可能比模型大小更重要，BERT在希望表达分类任务中提供了最佳的性能与效率平衡。

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [56] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: 对Google搜索AI生成内容（AI Overviews和Featured Snippets）在母婴健康领域的质量审计显示存在信息不一致、医疗安全措施缺失等问题


<details>
  <summary>Details</summary>
Motivation: 评估AI驱动的搜索功能在健康信息领域的质量，因为这些功能用户依赖度高但缺乏控制权，可能影响公共健康

Method: 通过系统算法审计1,508个真实母婴护理和孕期相关查询，使用多维度质量评估框架

Result: 33%的搜索结果页面内AIO和FS信息不一致，医疗安全措施严重缺失（AIO仅11%，FS仅7%），健康网站主导信息来源但FS常链接商业来源

Conclusion: AI介导的健康信息需要更强的质量控制，研究方法为高风险领域AI系统审计提供了可转移框架

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [57] [Visual Room 2.0: Seeing is Not Understanding for MLLMs](https://arxiv.org/abs/2511.12928)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 该论文提出了Visual Room 2.0基准，用于评估多模态大语言模型(MLLMs)的感知-认知对齐，发现MLLMs在感知能力上强于认知能力，且认知不依赖于感知推理。


<details>
  <summary>Details</summary>
Motivation: 基于Searle的中文房间思想扩展到多模态领域，质疑MLLMs是否能真正理解所看到的内容，即"看见不等于理解"。

Method: 构建了一个包含3个层次(低、中、高)和17个代表性任务的分层基准，涵盖从属性识别到场景理解的感知组件，以及从文本蕴含到因果和社会推理的认知组件。数据集包含350个多模态样本，每个样本有6个渐进问题。

Result: 评估10个最先进的MLLMs发现：(1) MLLMs的感知能力比认知能力更强(提升8.0%)；(2) 认知似乎不因果依赖于基于感知的推理；(3) 认知随模型规模扩展，但感知不随更大变体一致改进。

Conclusion: 该工作将"看见≠理解"操作化为可测试的假设，为MLLMs从感知处理到认知推理提供了新范式。

Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.

</details>


### [58] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: 提出HCNR方法，通过识别和恢复关键表达神经元来修复SFT对LLM诚实性的损害，相比基线方法实现33.25%诚实性恢复和2.23倍加速。


<details>
  <summary>Details</summary>
Motivation: 监督微调严重损害LLM的诚实性，这对高风险领域的安全部署至关重要。现有恢复方法假设SFT深度破坏了模型识别知识边界的能力，但研究发现模型仍保留此能力，受损的是忠实表达这种意识的能力。

Method: 提出HCNR方法：1）识别并恢复关键表达控制神经元到预训练状态；2）通过Hessian引导的补偿机制协调任务导向神经元。

Result: 在4个QA任务和5个LLM家族上的实验表明，HCNR有效恢复33.25%受损诚实性，相比基线方法实现至少2.23倍加速和超过10倍数据减少。

Conclusion: HCNR为可信LLM部署提供了实用解决方案，通过精准修复表达抑制能力而非全局参数调整，实现高效诚实性恢复。

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [59] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: AA-Omniscience基准测试评估语言模型的事实回忆能力和知识校准能力，涵盖42个经济相关主题的6000个问题。Claude 4.1 Opus获得最高分4.8，但大多数前沿模型在事实性和校准方面仍存在弱点。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要关注通用能力，但可靠使用需要事实准确性和识别知识差距的能力。

Method: 创建AA-Omniscience基准，包含来自权威学术和行业来源的6000个问题，涵盖6个领域的42个经济相关主题，测量全知指数（-100到100）。

Result: Claude 4.1 Opus获得最高分4.8，是仅有的三个得分超过零的模型之一。不同领域表现各异，来自三个不同研究实验室的模型在六个领域中领先。

Conclusion: 模型在事实性和校准方面存在持续弱点，性能因领域而异，应根据具体用例需求而非通用性能选择模型。

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [60] [How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm](https://arxiv.org/abs/2511.13040)
*Kasun Wickramasinghe,Nisansa de Silva*

Main category: cs.CL

TL;DR: 本文探讨了多语言嵌入模型与对齐单语模型在双语词典归纳任务中的表现差异，分析了BLI作为嵌入空间对齐度评估指标的局限性，并提出基于词干的新BLI方法和词汇剪枝技术来改进评估。


<details>
  <summary>Details</summary>
Motivation: 虽然多语言嵌入已成为主流选择，但需要验证其在所有方面是否都优于对齐单语模型，以及其更高的计算成本是否总是合理。需要探索两种极端方案之间的折衷。

Method: 使用双语词典归纳作为评估指标，比较传统嵌入对齐技术、新型多语言模型和组合对齐技术在高资源和低资源语言环境中的表现，并研究语言家族的影响。

Result: 发现BLI在某些情况下不能真实反映对齐程度，组合嵌入对齐技术通常表现更好，但在某些情况下多语言嵌入表现更佳（主要是低资源语言）。

Conclusion: 提出了基于词干的BLI方法和词汇剪枝技术来更准确地评估嵌入空间对齐度，特别是在多语言嵌入模型中。

Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).

</details>


### [61] [Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training](https://arxiv.org/abs/2511.13043)
*Xinyuan Zhou,Yi Lei,Xiaoyu Zhou,Jingyi Sun,Yu Zhu,Zhongyi Ye,Weitai Zhang,Quan Liu,Si Wei,Cong Liu*

Main category: cs.CL

TL;DR: Spark-Prover-X1-7B是一个7B参数的定理证明模型，通过三阶段训练框架提升轻量级LLM的形式推理能力，在多个基准测试中达到同类开源模型的最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动定理证明中高质量形式语言数据稀缺的问题，探索如何释放中等规模LLM的推理潜力。

Method: 三阶段训练框架：1）在广泛数学语料上进行持续预训练，引入新颖数据任务（包括"CoT增强状态预测"）；2）在专家迭代循环中进行监督微调；3）应用组相对策略优化（GRPO）处理最具挑战性的问题。

Result: Spark-Prover-X1-7B在同类开源模型中达到最优性能，平均通过率37.0%（pass@32），在PutnamBench上解决27个问题，在CombiBench上达到24.0%。

Conclusion: 多样化的训练数据和渐进式精炼的训练管道为增强轻量级LLM的形式推理能力提供了有效路径。

Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

</details>


### [62] [BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models](https://arxiv.org/abs/2511.13095)
*Chuyuan Li,Giuseppe Carenini*

Main category: cs.CL

TL;DR: BeDiscovER是一个评估现代LLM语篇理解能力的综合基准套件，包含5个公开语篇任务共52个数据集，涵盖词汇、句子和文档级别。评估发现先进模型在时间推理算术方面表现强，但在完整文档推理和语义语篇现象方面仍有困难。


<details>
  <summary>Details</summary>
Motivation: 为了全面评估现代推理语言模型在语篇层面的知识理解能力，需要建立一个更新、全面的评估基准。

Method: 整合5个公开语篇任务共52个数据集，涵盖语篇词汇、句子和文档级别，包括语篇解析、时间关系提取、语篇粒子消歧等任务，并评估开源LLM和前沿模型。

Result: 最先进模型在时间推理的算术方面表现强劲，但在完整文档推理和某些细微语义语篇现象（如修辞关系识别）方面存在困难。

Conclusion: 现代LLM在语篇理解方面取得了进展，但在复杂文档推理和语义语篇现象理解方面仍需改进，BeDiscovER为系统评估提供了有效工具。

Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.

</details>


### [63] [Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study](https://arxiv.org/abs/2511.13107)
*Zhichao He,Mouxiao Bian,Jianhong Zhu,Jiayuan Chen,Yunqiu Wang,Wenxia Zhao,Tianbin Li,Bing Han,Jie Xu,Junyan Wu*

Main category: cs.CL

TL;DR: 本研究评估了当代大语言模型在零样本设置下识别随机对照试验对CONSORT 2010声明依从性的准确性，发现模型整体表现一般，能较好识别合规项目但难以检测不合规和不适用的项目。


<details>
  <summary>Details</summary>
Motivation: 手动验证CONSORT依从性是一个耗时费力的过程，成为同行评审和证据合成的重要瓶颈，因此需要评估LLMs在此任务中的潜力。

Method: 构建了包含150篇已发表RCT的金标准数据集，在零样本设置下评估LLMs的三分类任务性能，主要指标为宏平均F1分数，辅以项目级性能指标和定性错误分析。

Result: 表现最佳的Gemini-2.5-Flash和DeepSeek-R1模型宏F1分数分别为0.634，Cohen's Kappa系数分别为0.280和0.282，仅与专家共识达成一般一致性。模型在识别合规项目时准确率高（F1>0.850），但在识别不合规和不适用的项目时表现很差（F1<0.400）。

Conclusion: LLMs作为CONSORT检查的初步筛选助手具有潜力，能够有效识别报告良好的项目，但目前无法可靠检测报告遗漏或方法学缺陷，因此不能替代人类专家在试验质量关键评估中的作用。

Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

</details>


### [64] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: Agent-Event-Coder (AEC) 是一个多智能体框架，将零样本事件抽取视为类似软件工程的代码生成过程，通过检索、规划、编码和验证四个专门智能体的协作，显著提升了大型语言模型在零样本事件抽取中的性能。


<details>
  <summary>Details</summary>
Motivation: 零样本事件抽取对大型语言模型具有挑战性，直接提示通常会产生不完整或结构无效的输出，如错误分类的触发器、缺失参数和模式违规。

Method: AEC将事件抽取分解为四个专门子任务：检索、规划、编码和验证，每个任务由专门的LLM智能体处理。事件模式表示为可执行的类定义，通过验证智能体实现确定性验证和精确反馈。

Result: 在五个不同领域和六个LLM上的实验表明，AEC始终优于先前的零样本基线方法，展示了将事件抽取视为代码生成的有效性。

Conclusion: 通过将事件抽取视为代码生成过程，AEC框架使LLM能够在零样本设置中产生精确、完整且模式一致的事件抽取结果。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [65] [A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.13126)
*Nigar Alishzade,Gulchin Abdullayeva*

Main category: cs.CL

TL;DR: 比较循环神经网络和注意力机制在孤立手语识别中的表现，发现Vanilla Transformer在准确率上优于ConvLSTM，但ConvLSTM计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 系统比较循环神经网络和注意力机制在手语识别中的性能差异，为不同应用场景下的架构选择提供指导。

Method: 在阿塞拜疆手语数据集和单词级美国手语数据集上实现并评估ConvLSTM和Vanilla Transformer两种代表性模型。

Result: Vanilla Transformer在两个数据集上都优于ConvLSTM，在AzSLD上达到76.8% Top-1准确率，在WLASL上达到88.3%。ConvLSTM计算效率更高但准确率较低。

Conclusion: Transformer在准确率和手语者独立性方面表现更好，ConvLSTM在计算效率和时序建模方面有优势，两者各有互补优势，应根据应用需求和资源限制选择架构。

Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.

</details>


### [66] [Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels](https://arxiv.org/abs/2511.13152)
*Sourya Dipta Das,Shubham Kumar,Kuldeep Yadav*

Main category: cs.CL

TL;DR: 提出了一种零样本语法能力评估框架，利用未标记数据和大型语言模型生成伪标签，无需人工标注即可训练基于Transformer的语法评分模型


<details>
  <summary>Details</summary>
Motivation: 口语语法评估面临自发性和非结构化挑战，且需要大量专家标注，大规模数据创建不现实

Method: 使用基于语法能力量表的提示词让LLM在未标记数据上生成预测作为伪标签，通过处理标签噪声的新训练框架训练Transformer模型

Result: 实验结果显示该方法能高精度估计语法能力分数，LLM选择和干净-噪声样本比例对性能有重要影响

Conclusion: 该方法为可扩展、低资源的语法评估系统铺平了道路，具有鲁棒性和可解释性

Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

</details>


### [67] [Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159)
*Zaara Zabeen Arpa,Sadnam Sakib Apurbo,Nazia Karim Khan Oishee,Ajwad Abrar*

Main category: cs.CL

TL;DR: 提出了首个公开的孟加拉语语料库，用于区分ASR转录中的重复性不流利和形态学重叠现象，并通过LLM和微调方法建立了基准性能。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语ASR转录中词重复的歧义问题：区分无意的不流利重复和故意的语法重叠，避免标准不流利校正方法误删有效语言信息。

Method: 构建了20,000条手动标注的孟加拉语语料库，使用两种方法进行基准测试：多语言大语言模型的少样本提示和任务特定的编码器模型微调。

Result: LLM在少样本提示下达到82.68%的准确率，但微调方法表现更优，BanglaBERT模型达到84.78%的准确率和0.677的F1分数。

Conclusion: 为孟加拉语建立了强大的语言感知基线，为开发语义保持的文本规范化系统提供了关键数据资源。

Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.

</details>


### [68] [TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine](https://arxiv.org/abs/2511.13169)
*Tianai Huang,Jiayuan Chen,Lu Lu,Pengcheng Chen,Tianbin Li,Bing Han,Wenchao Tang,Jie Xu,Ming Li*

Main category: cs.CL

TL;DR: TCM-5CEval是一个更细粒度的中医领域大语言模型评估基准，从五个关键维度评估模型能力，发现模型在经典文本解释和推理稳定性方面存在显著弱点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在中医等专业文化领域的应用需要更严谨的评估，基于前期TCM-3CEval工作发现的知识差距和文化背景对齐问题，需要更全面的评估框架。

Method: 构建TCM-5CEval基准，包含五个评估维度：核心知识、经典文献、临床决策、中药学和临床非药物治疗，并对15个主流LLM进行全面评估和排列一致性测试。

Result: 评估显示模型在基础知识回忆方面表现良好，但在经典文本解释上存在困难。排列一致性测试揭示所有模型都存在推理脆弱性，对选项顺序敏感，表现显著下降。

Conclusion: TCM-5CEval不仅提供了更详细的中医LLM能力诊断工具，还暴露了模型推理稳定性的根本弱点，该基准已上传至Medbench平台供进一步研究。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

</details>


### [69] [Translation Entropy: A Statistical Framework for Evaluating Translation Systems](https://arxiv.org/abs/2511.13180)
*Ronit D. Gross,Yanir Harel,Ido Kanter*

Main category: cs.CL

TL;DR: 本文提出了一种量化评估翻译器性能的方法，通过分析翻译过程中词汇替换对翻译结果的影响来估计翻译熵，为人工翻译器的客观基准测试提供了新标准。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏量化评估翻译器性能的客观方法，因为即使是单一语言的熵也未知。需要开发一种能够定量衡量翻译器性能的方法。

Method: 通过选取基准句子，仅改变其中一个特定标记，观察翻译结果是否相同。分析这种现象的统计规律，计算特定标记被替换而保持翻译不变的概率，从而得到翻译熵。

Result: 该方法能够量化估计翻译器的翻译熵，可用于对多个公开翻译器进行排名，并揭示互译熵是否对称。双标记替换实验显示翻译简并性具有乘积效应。

Conclusion: 翻译熵是一个可测量的属性，为人工翻译器的客观基准测试奠定了基础。该方法在MarianMT、T5-Base和NLLB-200等翻译器上得到了验证。

Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

</details>


### [70] [Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study](https://arxiv.org/abs/2511.13182)
*Mihai Dan Nadas,Laura Diosan*

Main category: cs.CL

TL;DR: 评估多种大型语言模型在罗马尼亚语变音符号恢复任务中的表现，发现GPT-4o等模型表现优异，而Llama系列模型表现波动较大。


<details>
  <summary>Details</summary>
Motivation: 自动变音符号恢复对于处理罗马尼亚语等富含变音符号的语言至关重要，需要评估不同LLM在此任务上的性能。

Method: 使用综合语料库测试多种LLM（包括GPT系列、Gemini、Llama系列、Mixtral等），采用从零样本到复杂多样本的多重提示模板。

Result: GPT-4o等模型实现高精度变音符号恢复，持续超越中性回显基线，而Meta的Llama系列模型表现出更大的变异性。

Conclusion: 模型架构、训练数据和提示设计对变音符号恢复性能有显著影响，为改进富含变音符号语言的NLP工具指明了方向。

Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.

</details>


### [71] [Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225)
*Tyler Loakman,Joseph James,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文评估了视觉语言模型在语音识别任务中的表现，发现即使经过微调，模型也难以准确理解语音谱图和波形图，表明需要特定的参数知识而不仅仅是配对样本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的发展，需要评估它们在融合视觉和语言模态任务中的能力，特别是在语音识别领域作为专业语音学家的表现。

Method: 创建包含4000+英语单词的新数据集，通过多选任务测试模型从语音谱图和波形图中预测正确音素或字素转录的能力，使用基于音素编辑距离的干扰项。

Result: 零样本和微调模型的表现很少超过随机水平，表明模型缺乏理解此类语音表示所需的特定参数知识。

Conclusion: 视觉语言模型在语音识别任务中表现不佳，需要专门的参数知识来正确解释语音谱图和波形图，而不仅仅是配对样本训练。

Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

</details>


### [72] [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254)
*Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach*

Main category: cs.CL

TL;DR: SoCE是一种模型融合方法，通过识别不同类别专家模型并使用非均匀加权平均来提升性能，在多个领域实现SOTA结果。


<details>
  <summary>Details</summary>
Motivation: LLM训练资源密集且耗时，模型融合（souping）能在不重新训练的情况下提升性能，但现有均匀平均方法未充分利用类别间性能差异。

Method: 利用基准测试组合识别最优模型候选，对弱相关类别集群识别专家模型，采用优化加权平均而非均匀权重进行融合。

Result: 在多种领域（多语言、工具调用、数学）提升性能和鲁棒性，在伯克利函数调用排行榜上达到最先进水平。

Conclusion: SoCE通过类别专家识别和非均匀加权平均，提供了一种有效的模型融合方法，显著优于传统均匀平均方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

</details>


### [73] [RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2511.13329)
*Shufan Yang,Zifeng Cheng,Zhiwei Jiang,Yafeng Yin,Cong Wang,Shiping Ge,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: RegionMarker是一种区域触发语义水印框架，通过在低维空间中定义触发区域并向相关文本嵌入注入水印，为EaaS提供全面的版权保护。


<details>
  <summary>Details</summary>
Motivation: 现有的EaaS水印方法只能抵抗部分攻击，无法提供全面保护，存在模型提取攻击导致经济损失的风险。

Method: 使用秘密降维矩阵投影到子空间，随机选择触发区域，在整个触发区域嵌入水印，并将文本嵌入本身作为水印。

Result: 在多个数据集上的实验表明，RegionMarker能有效抵抗不同攻击方法，保护EaaS版权。

Conclusion: RegionMarker通过区域触发机制实现了对EaaS的全面版权保护，能够抵抗多种攻击手段。

Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

</details>


### [74] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: 该论文介绍了阿拉伯语方言情感分析共享任务，使用包含沙特和摩洛哥方言的酒店评论数据集，旨在开发面向酒店业的阿拉伯语方言情感分析工具。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯世界酒店业越来越依赖客户反馈来改进服务，因此需要先进的阿拉伯语情感分析工具，特别是针对方言的分析能力。

Method: 构建了多方言数据集，包含538条情感平衡的酒店评论，从现代标准阿拉伯语翻译为沙特和摩洛哥方言，并由母语者验证翻译准确性和情感保持。

Result: 超过40个团队注册参与，12个团队提交系统，最佳系统F1分数达到0.81，证明了跨阿拉伯语方言情感分析的可行性。

Conclusion: 该共享任务成功展示了阿拉伯语方言情感分析的潜力，同时揭示了跨方言情感分析的持续挑战，为酒店业客户体验分析提供了重要资源。

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [75] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 本文通过控制性PEFT/LoRA研究，发现LLM在任务和语言间的迁移存在不对称性：同任务跨语言迁移通常为正，而跨任务迁移常导致性能下降，揭示了稳定的捐赠者-接收者结构。


<details>
  <summary>Details</summary>
Motivation: 理解LLM在任务和语言间的迁移特性，探究改进一个任务或语言如何影响其他任务和语言及其组合，为风险感知微调和模型专业化提供指导。

Method: 采用PEFT/LoRA方法，在不同开源LLM家族和规模上进行控制性研究，将任务和语言作为迁移轴，在单一任务-语言源上微调模型，并测量在所有其他任务-语言目标对上的迁移效果。

Result: 发现两个一致模式：1）任务内vs任务间不对称性 - 同任务跨语言迁移可靠为正，跨任务迁移常导致性能下降；2）跨语言和任务的稳定捐赠者-接收者结构。

Conclusion: 研究结果对风险感知微调和模型专业化具有重要意义，揭示了LLM迁移的特定模式，为有效利用有限计算资源进行模型优化提供指导。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [76] [Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts](https://arxiv.org/abs/2511.13381)
*Siyu Zhu,Mouxiao Bian,Yue Xie,Yongyu Tang,Zhikang Yu,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Xiaoyan Dong*

Main category: cs.CL

TL;DR: PEDIASBench评估框架显示，当前大语言模型在儿科医疗中表现出基础知识掌握良好，但在复杂推理、动态诊疗决策和人文关怀方面存在局限，尚不能独立承担儿科诊疗工作，但有望在决策支持、医学教育和患者沟通方面发挥作用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医学领域的快速发展，需要评估它们是否能在真实临床环境中胜任儿科医生的工作，了解其在儿科医疗中的实际能力和局限性。

Method: 开发了PEDIASBench系统评估框架，从基础知识应用、动态诊疗能力、儿科医疗安全和医学伦理三个维度，评估了12个代表性模型在19个儿科亚专业和211种典型疾病上的表现。

Result: 最先进模型在基础知识上表现良好（如Qwen3-235B-A22B在执照级问题上准确率超90%），但随着任务复杂度增加性能下降约15%；在动态诊疗场景中，DeepSeek-R1在病例推理中得分最高（平均0.58），但多数模型难以适应实时患者变化；在医学伦理和安全任务上，Qwen2.5-72B表现最佳（准确率92.05%），但人文敏感性仍有限。

Conclusion: 当前儿科大语言模型受限于有限的动态决策能力和未充分发展的人文关怀，未来应关注多模态整合和临床反馈-模型迭代循环，以提升安全性、可解释性和人机协作。虽然不能独立进行儿科诊疗，但在决策支持、医学教育和患者沟通方面具有潜力。

Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.

</details>


### [77] [Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction](https://arxiv.org/abs/2511.13410)
*Zhaopei Huang,Qifeng Dai,Guozheng Wu,Xiaopeng Wu,Kehan Chen,Chuan Yu,Xubin Li,Tiezheng Ge,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 提出了PAL-Bench基准和H²Memory内存框架，用于评估和改进面向服务的个性化对话助手在长期交互中的能力。


<details>
  <summary>Details</summary>
Motivation: 随着智能个人设备的普及，需要能够理解用户特定特征的个性化对话助手，但现有方法往往忽视长期交互的复杂性，无法捕捉用户的主观特征。

Method: 开发了多步骤基于LLM的合成流程创建PAL-Set数据集，并提出H²Memory分层异构内存框架，结合检索增强生成来改进个性化响应生成。

Result: 在PAL-Bench和外部数据集上的综合实验证明了所提出内存框架的有效性。

Conclusion: PAL-Bench为评估面向服务的个性化助手提供了新基准，H²Memory框架显著提升了长期用户-代理交互中的个性化能力。

Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.

</details>


### [78] [Non-Linear Scoring Model for Translation Quality Evaluation](https://arxiv.org/abs/2511.13467)
*Serge Gladkoff,Lifeng Han,Katerina Gasova*

Main category: cs.CL

TL;DR: 本文提出了一种基于对数函数的非线性翻译质量评估模型，解决了传统线性评估方法在不同文本长度下产生偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的线性翻译质量评估方法在不同文本长度下会产生偏差：对短文本过度惩罚，对长文本惩罚不足，与专家直觉不一致。

Method: 基于多范围框架，提出了一个双参数对数模型 E(x) = a * ln(1 + b * x)，通过心理物理学和认知负荷理论支持，使用一维根查找步骤从两个容忍点进行校准。

Result: 实证数据显示可接受的错误数量随样本大小呈对数增长而非线性增长。该模型在相对误差±20%范围内提供明确的线性近似区间，提高了可解释性和评估者间信度。

Conclusion: 该模型通过操作感知有效的评分范式，推进了翻译质量评估向更准确和可扩展的方向发展，为基于AI的文档级评估提供了更强的基础。

Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.

</details>


### [79] [Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns](https://arxiv.org/abs/2511.13481)
*Attapol T. Rutherford,Sirisak Chueykamhang,Thachaparn Bunditlurdruk,Nanthicha Angsuwichitkul*

Main category: cs.CL

TL;DR: 使用方面情感分析(ABSA)解码泰语财务年报中的模糊情感，开发标注指南并标注100多份报告，通过事件研究验证对股价的实际影响。


<details>
  <summary>Details</summary>
Motivation: 财务文件常使用模糊语言来呈现积极或中性展望，即使实际状况不佳，理解这些情感对洞察市场行为至关重要。

Method: 开发模糊情感标注指南，标注100多份泰语财务年报，使用多种文本分类模型进行基准测试，并进行事件研究评估对股价的影响。

Result: 在情感分类任务中表现出色，市场反应受报告中特定方面选择性影响。

Conclusion: 财务文本情感分析具有复杂性，解决模糊语言对准确评估市场情绪至关重要。

Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.

</details>


### [80] [Applying Large Language Models to Characterize Public Narratives](https://arxiv.org/abs/2511.13505)
*Elinor Poole-Dayan,Daniel T Kessler,Hannah Chiou,Margaret Hughes,Emily S Lin,Marshall Ganz,Deb Roy*

Main category: cs.CL

TL;DR: 提出基于LLM的公共叙事自动标注框架，在8个叙事14个代码上达到接近专家水平的性能(F1=0.80)，并扩展到22个故事和政治演讲分析。


<details>
  <summary>Details</summary>
Motivation: 公共叙事是领导力发展和公民动员的重要工具，但系统分析面临主观解释和专家标注成本高的挑战。

Method: 开发与领域专家合作制定的代码本，评估LLM在公共叙事定性标注中的表现，并与专家标注进行对比。

Result: LLM达到接近专家水平的性能，平均F1分数0.80，成功扩展到更大数据集和政治演讲分析。

Conclusion: LLM辅助标注为可扩展的叙事分析提供了潜力，同时指出了计算公民叙事研究的局限性和未来方向。

Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.

</details>


### [81] [Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529)
*Máté Gedeon,Piroska Zsófia Barta,Péter Mihajlik,Tekla Etelka Gráczi,Anna Kohári,Katalin Mády*

Main category: cs.CL

TL;DR: 该论文介绍了两个新的匈牙利语语音数据集BEA-Large和BEA-Dialogue，用于解决匈牙利语在语音识别研究中代表性不足的问题，并建立了可复现的基线模型。


<details>
  <summary>Details</summary>
Motivation: 高资源语言在自动语音识别方面取得了显著进展，但匈牙利语等语言由于缺乏自发言语和对话语料库而代表性不足。

Method: 从匈牙利语音语料库BEA中构建了两个新数据集：BEA-Large（255小时自发言语）和BEA-Dialogue（85小时自然对话），并使用公开可用的ASR模型建立可复现基线。

Result: 微调的Fast Conformer模型在自发言语上词错误率为14.18%，在重复语音上为4.8%；说话人日志错误率在13.05%到18.26%之间。

Conclusion: 对话ASR仍然具有挑战性，特别是由于不流利、重叠和非正式语音模式。通过发布这些数据集和基线，旨在推进匈牙利语音技术并为其他语言开发自发言语和对话基准提供方法框架。

Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

</details>


### [82] [Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation](https://arxiv.org/abs/2511.13590)
*Hao Wang,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: 提出了一个基于核心意图、语句类型、语法结构和关键操作的文本到SQL分类新分类法，并基于此构建了SQL-Synth数据集，该数据集比现有基准具有更好的多样性和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到SQL数据集覆盖范围有限，无法捕捉真实世界应用的多样性，需要更全面的分类方法和数据集。

Method: 提出了新的文本到SQL分类法，结合大型语言模型开发了分类法指导的数据集合成流程，构建了SQL-Synth数据集。

Result: SQL-Synth数据集在多样性和覆盖范围上优于现有基准，现有LLMs在该数据集上表现有限，但微调可以显著提升性能。

Conclusion: 提出的分类法具有重要影响，不仅能全面分析数据集和不同LLMs的性能，还能指导LLMs训练数据的构建。

Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

</details>


### [83] [Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents](https://arxiv.org/abs/2511.13593)
*Piaohong Wang,Motong Tian,Jiaxian Li,Yuan Liang,Yuqing Wang,Qianben Chen,Tiannan Wang,Zhicong Lu,Jiawei Ma,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: O-Mem是一个基于主动用户画像的新型记忆框架，通过动态提取和更新用户特征与事件记录，实现分层检索和个性化响应，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在复杂环境中维持长期交互时面临上下文一致性和动态个性化挑战，传统记忆系统依赖语义分组检索，容易忽略语义无关但关键的用户信息并引入检索噪声。

Method: 提出O-Mem框架，基于主动用户画像动态提取和更新用户特征与事件记录，支持人物属性和主题相关上下文的分层检索。

Result: 在LoCoMo基准上达到51.76%（比之前最优方法LangMem提升近3%），在PERSONAMEM上达到62.99%（比A-Mem提升3.5%），同时提升了token和交互响应时间效率。

Conclusion: O-Mem为开发高效且类人的个性化AI助手开辟了有前景的方向。

Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.

</details>


### [84] [Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues](https://arxiv.org/abs/2511.13658)
*Jiaming Qu,Mengtian Guo,Yue Wang*

Main category: cs.CL

TL;DR: 使用大型语言模型将机器学习检测到的欺骗性评论线索转化为人类可理解的语言现象，帮助人们识别虚假评论。


<details>
  <summary>Details</summary>
Motivation: 欺骗性评论误导消费者、损害企业并破坏在线市场信任。虽然机器学习分类器能有效区分欺骗性评论，但其学习到的特征往往难以被人类理解。

Method: 利用大型语言模型将机器学习学到的词汇线索翻译成人类可理解的语言现象，这些现象基于数据实证，具有跨领域泛化能力。

Result: 通过此方法获得的语言现象比LLM先验知识或上下文学习获得的现象更具预测性，且能帮助人们在缺乏检测分类器的环境中评估评论可信度。

Conclusion: 该方法能够产生基于数据、可泛化且具有预测能力的人类可理解语言现象，有助于提高在线评论的可信度评估。

Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

</details>


### [85] [Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation](https://arxiv.org/abs/2511.13689)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,Joseph K J*

Main category: cs.CL

TL;DR: 提出了TAI框架，结合LLM和潜在扩散模型，通过翻译和图像生成增强印度诗歌的可访问性，支持联合国可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 印度诗歌语言复杂、文化内涵深厚，但现有研究忽视了印度语言诗歌，非母语者难以理解其层次含义和文化典故。

Method: 使用TAI框架，包括：(1)翻译模块采用几率比偏好对齐算法准确翻译形态丰富的诗歌；(2)图像生成模块使用语义图捕捉隐喻和含义的关系，创建视觉化表示。

Result: 综合实验评估显示TAI Diffusion在诗歌图像生成任务中优于强基线模型，并构建了包含21种低资源印度语言的1570首诗歌数据集。

Conclusion: 该工作填补了诗歌翻译和视觉理解的空白，旨在扩大可访问性并丰富读者体验。

Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.

</details>


### [86] [Generalist Foundation Models Are Not Clinical Enough for Hospital Operations](https://arxiv.org/abs/2511.13703)
*Lavender Y. Jiang,Angelica Chen,Xu Han,Xujin Chris Liu,Radhika Dua,Kevin Eaton,Frederick Wolff,Robert Steele,Jeff Zhang,Anton Alyakin,Qingkai Pan,Yanbing Chen,Karl L. Sangwon,Daniel A. Alber,Jaden Stryker,Jin Vivian Lee,Yindalon Aphinyanaphongs,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: Lang1模型家族通过结合医疗EHR数据和互联网文本进行预训练，在医疗操作决策任务上显著优于通用模型，证明了领域特定预训练和监督微调在医疗AI系统中的重要性。


<details>
  <summary>Details</summary>
Motivation: 通用基础模型在医疗知识方面表现良好，但缺乏医疗操作决策所需的专业知识，如患者流动、成本和护理质量等关键决策。

Method: 开发Lang1模型家族（100M-7B参数），使用NYU Langone Health的800亿临床token和6270亿互联网token进行预训练，并创建ReMedE基准评估5个关键医疗任务。

Result: 微调后的Lang1-1B在AUROC上比微调通用模型提高3.64%-6.75%，比零样本模型提高1.66%-23.66%，且能有效迁移到其他临床任务和外部医疗系统。

Conclusion: 医疗系统AI需要领域特定预训练、监督微调和真实世界评估的结合，专业LLM在专业任务上可与通用模型竞争。

Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [87] [Psychological stress during Examination and its estimation by handwriting in answer script](https://arxiv.org/abs/2511.11633)
*Abhijeet Kumar,Chetan Agarwal,Pronoy B. Neogi,Mayank Goswami*

Main category: cs.CV

TL;DR: 该研究结合笔迹学和人工智能，通过分析学生手写考试卷来量化心理压力水平，使用OCR和基于Transformer的情感分析模型，超越了传统评分系统。


<details>
  <summary>Details</summary>
Motivation: 传统评分系统无法深入了解学生在考试期间的认知和情绪状态，需要一种数据驱动的方法来量化心理压力。

Method: 集成高分辨率图像处理、TrOCR和使用RoBERTa模型的情感熵融合，通过五模型投票机制和无监督异常检测生成压力指数。

Result: 开发了一个创新的学术取证框架，能够稳健地量化学生的心理压力水平。

Conclusion: 该方法为学术评估提供了新的视角，能够通过笔迹分析深入了解学生的心理状态。

Abstract: This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.

</details>


### [88] [Real-time pothole detection with onboard sensors and camera on vehicles](https://arxiv.org/abs/2511.11643)
*Aswath Muthuselvam,Jeevak Raj S,Mohanaprasad K*

Main category: cs.CV

TL;DR: 使用车载传感器和SVM分类器实时检测道路坑洼，在2公里路段上达到98.1%的准确率


<details>
  <summary>Details</summary>
Motivation: 道路状况对日常通勤很重要，车辆数量增加需要频繁评估路况以确保交通顺畅，小裂缝可能发展成大坑洼

Method: 使用车载传感器收集数据，采用SVM分类器进行坑洼检测

Result: 在2公里路段（包含26个坑洼）上实现98.1%的检测准确率

Conclusion: 该方法能有效实时检测坑洼，为大规模坑洼管理和分析提供有用数据

Abstract: Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes

</details>


### [89] [A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model](https://arxiv.org/abs/2511.11659)
*Kesong Zheng,Zhi Song,Peizhou Li,Shuyi Yao,Zhenxing Bian*

Main category: cs.CV

TL;DR: 为解决农田生态系统缺乏标准化生境分类系统、生境类型覆盖不全以及现有模型无法有效整合语义和纹理特征的问题，本研究构建了包含15类农田系统生境的超高分辨率遥感图像数据集，并提出动态加权特征融合网络(DWFF-Net)，在构建的数据集上取得了0.6979的mIoU和0.8049的F1分数。


<details>
  <summary>Details</summary>
Motivation: 当前农田生态系统缺乏标准化的生境分类系统，生境类型覆盖不完整，现有模型无法有效整合语义和纹理特征，导致多尺度生境分割精度不足、边界模糊。

Method: 构建包含15类农田系统生境的超高分辨率遥感图像数据集；提出动态加权特征融合网络(DWFF-Net)，编码器使用冻结参数的DINOv3提取基础特征，引入数据级自适应动态加权策略进行特征融合，解码器包含动态权重计算网络实现多层特征深度融合，采用混合损失函数优化训练。

Result: 在构建的数据集上，提出的模型达到0.6979的mIoU和0.8049的F1分数，分别比基线网络提高0.021和0.0161。消融研究证实多层特征融合的互补性，有效提高了田埂等微生境类别的IoU。

Conclusion: 本研究建立了基于自适应多层特征融合的农田系统生境识别框架，能够以低成本实现亚米级精度的生境制图，为农田景观的细粒度生境监测提供有力技术支持。

Abstract: Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.

</details>


### [90] [AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2511.11662)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: AGENet是一个用于医学图像分割的少样本学习框架，通过结合边缘感知的测地距离学习和自适应原型提取，在有限标注数据下实现精确的边界分割。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要大量标注数据，这在临床应用中形成瓶颈。现有少样本分割方法在医学图像边界分割上表现不佳，特别是当解剖结构相似且缺乏空间上下文时。

Method: 结合三个主要组件：(1)边缘感知测地距离学习模块，通过迭代快速行进细化来尊重解剖边界；(2)自适应原型提取，通过空间加权聚合捕获全局结构和局部边界细节；(3)自适应参数学习，自动适应不同器官特征。

Result: 在多个医学影像数据集上的广泛实验表明，相比最先进方法有所改进，显著减少了边界误差，同时保持计算效率。

Conclusion: 该方法非常适合需要精确分割但标注数据有限的临床应用，通过轻量级几何建模实现了更好的边界分割性能。

Abstract: Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.

</details>


### [91] [EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance](https://arxiv.org/abs/2511.11700)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: 提出了EPSegFZ，一种无需预训练的点云语义分割网络，通过ProERA模块、DRPE交叉注意力和LGPE模块，在少样本和零样本场景下实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练阶段，限制了模型灵活性；同时未能充分利用支持集中的文本信息，影响了少样本性能和零样本能力。

Method: 使用Prototype-Enhanced Registers Attention模块和Dual Relative Positional Encoding交叉注意力机制进行特征提取，结合Language-Guided Prototype Embedding模块利用文本信息。

Result: 在S3DIS和ScanNet基准测试中分别比最先进方法提升了5.68%和3.82%。

Conclusion: EPSegFZ无需预训练即可实现优异的少样本和零样本点云语义分割性能，通过充分利用多模态信息提升了模型能力。

Abstract: Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.

</details>


### [92] [Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement](https://arxiv.org/abs/2511.11702)
*Lian He,Meng Liu,Qilang Ye,Yu Zhou,Xiang Deng,Gangyi Ding*

Main category: cs.CV

TL;DR: TASA是一个用于3D场景级可操作性分割的新框架，通过结合2D语义线索和3D几何推理，在粗到细的方式下实现高效准确的可操作性检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注对象级可操作性或将2D预测简单提升到3D，忽略了点云中的丰富几何结构信息且计算成本高。需要解决语义推理和空间定位的挑战。

Method: TASA框架包含任务感知的2D可操作性检测模块来识别可操作点，指导任务相关视图的选择；以及3D可操作性细化模块，将2D语义先验与局部3D几何信息结合。

Result: 在SceneFun3D数据集上的实验表明，TASA在场景级可操作性分割的准确性和效率上都显著优于基线方法。

Conclusion: TASA通过几何优化的方法有效解决了3D场景级可操作性分割问题，在保持准确性的同时提高了计算效率。

Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

</details>


### [93] [LE-CapsNet: A Light and Enhanced Capsule Network](https://arxiv.org/abs/2511.11708)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 提出LE-CapsNet作为CapsNet的轻量、增强且更准确的变体，在CIFAR-10数据集上达到76.73%准确率，推理速度比CapsNet快4倍，在AffNIST数据集上达到94.3%准确率。


<details>
  <summary>Details</summary>
Motivation: CapsNet虽然具有检测重叠类别图像和处理变换图像的优势，但存在速度慢、资源消耗大、参数多和准确率不如CNN的问题。

Method: 开发了LE-CapsNet，这是一个轻量级、增强的CapsNet变体，仅使用380万权重参数。

Result: 在CIFAR-10数据集上达到76.73%准确率，推理速度比CapsNet快4倍；在AffNIST数据集上达到94.3%准确率，优于CapsNet的90.52%。

Conclusion: LE-CapsNet在保持CapsNet优势的同时，显著提升了性能、速度和效率，对仿射变换图像具有更强的鲁棒性。

Abstract: Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).

</details>


### [94] [Target-Balanced Score Distillation](https://arxiv.org/abs/2511.11710)
*Zhou Xu,Qi Wang,Yuxiao Yang,Luyuan Zhang,Zhang Liang,Yang Li*

Main category: cs.CV

TL;DR: 本文提出了Target-Balanced Score Distillation (TBSD)方法，解决了Score Distillation Sampling (SDS)在3D资产生成中纹理优化与形状保真度的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统的SDS方法在3D资产生成中存在过饱和和过平滑问题，而引入负提示的变体方法面临纹理优化有限或纹理增益但形状失真的关键权衡。

Method: 提出目标平衡分数蒸馏(TBSD)，将生成建模为多目标优化问题，并引入自适应策略来有效解决纹理与形状的权衡。

Result: 大量实验表明，TBSD显著优于现有最先进方法，生成具有高保真纹理和几何精确形状的3D资产。

Conclusion: TBSD通过系统分析和自适应多目标优化策略，成功解决了3D生成中纹理与形状的权衡问题，实现了高质量的3D资产生成。

Abstract: Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.

</details>


### [95] [CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition](https://arxiv.org/abs/2511.11716)
*Sudhakar Sah,Nikhil Chabbra,Matthieu Durnerin*

Main category: cs.CV

TL;DR: CompressNAS是一个受MicroNAS启发的框架，将秩选择作为全局搜索问题，通过快速精度估计器评估候选分解，在内存和精度约束下实现高效的秩探索。


<details>
  <summary>Details</summary>
Motivation: 深度卷积神经网络在微控制器和轻量级NPU上部署困难，现有低秩张量分解方法局部选择秩，忽略了压缩与精度之间的全局权衡。

Method: 使用CompressNAS框架，将秩选择作为全局搜索问题，采用快速精度估计器评估候选分解，实现高效的秩探索。

Result: 在ImageNet上，CompressNAS将ResNet-18压缩8倍，精度下降小于4%；在COCO上，YOLOv5s实现2倍压缩无精度损失，YOLOv5n实现2倍压缩精度下降2.5%。

Conclusion: 提出了新的压缩模型家族STResNet，与其他高效模型相比具有竞争力。

Abstract: Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.

</details>


### [96] [AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks](https://arxiv.org/abs/2511.11720)
*Jiao Chen,Haoyi Wang,Jianhua Tang,Junyi Wang*

Main category: cs.CV

TL;DR: AdaptFly是一个无需权重更新的测试时自适应框架，通过提示引导调整分割模型，支持资源受限和资源丰富的无人机两种互补适应模式，实现无人机群协作感知。


<details>
  <summary>Details</summary>
Motivation: 无人机网络需要鲁棒的语义分割，但基础模型在天气、光照和视角变化下性能下降。资源受限的无人机无法运行基于梯度的测试时自适应，而资源丰富的无人机独立适应会浪费共享经验。

Method: AdaptFly采用两种适应模式：对资源受限无人机使用轻量级token-prompt检索，对资源丰富无人机使用无梯度稀疏视觉提示优化。通过激活统计检测器触发适应，跨无人机知识池整合提示知识。

Result: 在UAVid和VDD基准测试以及真实无人机部署中，AdaptFly显著提高了分割精度和鲁棒性，优于静态模型和最先进的TTA基线方法。

Conclusion: AdaptFly为新兴低空经济中的弹性、通信高效感知提供了一条实用路径，通过无权重更新的提示引导适应实现无人机群协作。

Abstract: Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.

</details>


### [97] [Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video](https://arxiv.org/abs/2511.11725)
*Zekai Shi,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: 提出一种基于人类视觉盲点的生物合理性掩码策略，用于从儿童视角数据中学习视觉表示和词汇-指称映射


<details>
  <summary>Details</summary>
Motivation: 解决婴儿首次学习词汇时面临的指称不确定性挑战，即一个词可能指代环境中的任何物体、组件或属性

Method: 使用基于掩码自编码器的视觉骨干网络，结合人类眼睛盲点知识设计新型掩码策略，然后用于对比学习视频-文本模型

Result: 提出的生物合理性掩码策略在从跨情境和时间扩展片段中学习词汇-指称映射方面至少与随机掩码同样有效

Conclusion: 生物启发的掩码策略为从生态有效数据中学习词汇-指称映射提供了可行的替代方案

Abstract: Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

</details>


### [98] [GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion](https://arxiv.org/abs/2511.11730)
*Yongjun Xiao,Dian Meng,Xinlei Huang,Yanran Liu,Shiwei Ruan,Ziyue Qiao,Xubin Zheng*

Main category: cs.CV

TL;DR: GROVER是一个用于空间多组学数据自适应融合的新框架，通过图卷积网络和对比学习解决多模态数据整合的挑战。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学、蛋白质组学和表观基因组学缺乏病理形态学背景，需要与组织病理学图像整合以进行全面的疾病组织分析，但多模态数据的异质性和分辨率不匹配带来了重大挑战。

Method: 使用基于Kolmogorov-Arnold网络的图卷积网络编码器捕获模态间非线性依赖关系，引入spot-feature-pair对比学习策略优化跨模态对应关系，设计动态专家路由机制自适应选择信息丰富的模态。

Result: 在真实世界空间组学数据集上的实验表明，GROVER优于现有最优基线方法。

Conclusion: GROVER为多模态整合提供了一个鲁棒可靠的解决方案。

Abstract: Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.

</details>


### [99] [Exposing DeepFakes via Hyperspectral Domain Mapping](https://arxiv.org/abs/2511.11732)
*Aditya Mehta,Swarnim Chaudhary,Pratik Narang,Jagat Sesh Challa*

Main category: cs.CV

TL;DR: HSI-Detect通过将RGB图像重建为31通道高光谱图像，在光谱域进行Deepfake检测，相比RGB基线有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现代生成和扩散模型产生的图像高度逼真，可能误导人类感知和自动化检测系统。大多数检测方法仅在RGB空间操作，只能分析三个光谱通道。

Method: 提出两阶段流程：首先从标准RGB输入重建31通道高光谱图像，然后在光谱域进行检测。扩展输入表示到更密集的光谱带可以放大RGB域中微弱或不可见的操作伪影。

Result: 在FaceForensics++数据集上评估，相比仅使用RGB的基线方法，HSI-Detect显示出持续改进。

Conclusion: 光谱域映射在Deepfake检测中具有前景。

Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.

</details>


### [100] [Toward bilipshiz geometric models](https://arxiv.org/abs/2511.11735)
*Yonatan Sverdlov,Eitan Rosen,Nadav Dym*

Main category: cs.CV

TL;DR: 本文研究了点云神经网络是否保持对称感知距离的双Lipschitz等价性，发现标准不变网络不满足此性质，提出了改进方法并验证其优势。


<details>
  <summary>Details</summary>
Motivation: 受等变学习文献启发，研究点云网络是否保持对称感知距离的双Lipschitz等价性，这对网络性能有重要影响。

Method: 分析两种对称感知度量（Procrustes匹配和Hard Gromov Wasserstein距离），证明它们不等价，然后修改标准不变网络以获得双Lipschitz保证。

Result: 实验表明，提出的双Lipschitz模型在3D点云对应任务上优于标准不变模型。

Conclusion: 点云网络的双Lipschitz性质很重要，通过适当修改可以实现这一性质并提升性能。

Abstract: Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.
  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.

</details>


### [101] [Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models](https://arxiv.org/abs/2511.11751)
*Sanchit Sinha,Guangzhi Xiong,Zhenghao He,Aidong Zhang*

Main category: cs.CV

TL;DR: Concept-RuleNet是一个多代理神经符号系统，通过视觉概念挖掘和符号推理，在保持透明推理的同时增强视觉基础，提高预测准确性并减少幻觉符号。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型虽然预测准确，但缺乏决策解释性，特别是在处理分布外数据时容易产生事实幻觉。现有神经符号方法仅从任务标签中提取符号，缺乏对底层视觉数据的充分基础。

Method: 1) 多模态概念生成器从训练图像子集中挖掘判别性视觉概念；2) 利用视觉概念进行符号发现，锚定在真实图像统计中；3) 大型语言模型推理代理将符号组合成可执行的一阶规则；4) 推理时视觉验证代理量化符号存在程度并触发规则执行。

Result: 在五个基准测试（包括两个医学成像任务和三个代表性不足的自然图像数据集）上，系统将最先进的神经符号基线平均提升了5%，同时将规则中幻觉符号的出现减少了高达50%。

Conclusion: Concept-RuleNet通过视觉基础的概念挖掘和透明符号推理，有效提升了神经符号系统的性能，同时显著减少了幻觉问题，为可解释AI提供了可行路径。

Abstract: Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.

</details>


### [102] [Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition](https://arxiv.org/abs/2511.11754)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: 提出了一种隐式稀疏风格的Transformer变体架构——Batch Transformers，通过关注"重要"维度来减少编码器-解码器架构中的瓶颈大小，并在人脸识别任务中测试了合成图像生成效果。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer关注整个维度的序列或批量实体，而本文旨在通过关注主要成分维度来显著减少编码器-解码器神经网络架构中的瓶颈大小。

Method: 提出Batch Transformers架构，采用隐式稀疏风格，对"重要"维度（主要成分）实施注意力机制，实现特征选择。在化妆和遮挡数据集上测试人脸识别任务的合成图像生成。

Result: 该架构能够增加有限原始数据集的变异性，在合成图像生成方面表现出良好效果。

Conclusion: Batch Transformers通过关注重要维度实现了编码器-解码器架构的瓶颈大小显著减少，在人脸识别任务的合成图像生成中验证了其有效性。

Abstract: A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.

</details>


### [103] [Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing](https://arxiv.org/abs/2511.11780)
*Hossein Mohebbi,Mohammed Abdulrahman,Yanting Miao,Pascal Poupart,Suraj Kothawade*

Main category: cs.CV

TL;DR: Image-POSER是一个基于强化学习的反射式框架，通过协调预训练的文本到图像和图像到图像专家模型，处理长组合提示，并使用视觉语言模型批评器提供结构化反馈来监督对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在处理长组合提示时表现不佳，无法可靠地执行创意工作流程中的复杂任务。

Method: 将图像合成和编辑建模为马尔可夫决策过程，通过动态任务分解处理长提示，使用强化学习学习非平凡的专家管道，自适应地结合不同模型的优势。

Result: 在行业标准和自定义基准测试中，Image-POSER在对齐度、保真度和美学质量方面均优于基线模型（包括前沿模型），并在人类评估中持续获得偏好。

Conclusion: 强化学习可以赋予AI系统自主分解、重新排序和组合视觉模型的能力，朝着通用视觉助手的方向发展。

Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

</details>


### [104] [SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction](https://arxiv.org/abs/2511.11824)
*Zhongping Dong,Pengyang Yu,Shuangjian Li,Liming Chen,Mohand Tahar Kechadi*

Main category: cs.CV

TL;DR: SOTFormer是一个统一目标检测、跟踪和短期轨迹预测的端到端框架，使用恒定内存的时序transformer，在遮挡、尺度变化和快速运动场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决遮挡、尺度变化和时间漂移对单目标跟踪和短期运动预测的挑战，这些因素破坏了实时感知所需的时间一致性。

Method: 采用最小恒定内存时序transformer，通过真实值启动的内存和burn-in锚点损失稳定初始化，单个轻量级时序注意力层跨帧优化嵌入。

Result: 在Mini-LaSOT基准测试中达到76.3 AUC和53.7 FPS，在快速运动、尺度变化和遮挡情况下优于TrackFormer和MOTRv2等transformer基线。

Conclusion: SOTFormer通过统一框架实现了稳定的身份传播和实时推理，在复杂场景下表现出优越性能。

Abstract: Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.

</details>


### [105] [MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning](https://arxiv.org/abs/2511.11837)
*Fatemeh Elhambakhsh,Gaurav Ameta,Aditi Roy,Hyunwoong Ko*

Main category: cs.CV

TL;DR: 提出了MP-GFormer，一种3D几何感知的动态图变换器，通过将演化的3D几何表示集成到动态图学习中，用于预测加工操作序列，相比现有方法在主要和子操作预测准确率上分别提升了24%和36%。


<details>
  <summary>Details</summary>
Motivation: 现有动态图学习方法在加工工艺规划中虽然能捕捉时空依赖关系，但无法融入零件的三维几何信息，缺乏领域感知能力，导致在预测加工操作序列时存在局限性。

Method: MP-GFormer方法利用StereoLithography表面网格表示每次加工操作后零件的3D几何形状，通过注意力机制将演化的3D几何表示集成到动态图学习中，使用边界表示方法处理初始3D设计。

Result: 在合成数据集上的评估显示，该方法在主要操作预测准确率上提升了24%，在子操作预测准确率上提升了36%，优于现有最先进方法。

Conclusion: MP-GFormer通过集成3D几何信息显著提升了加工操作序列预测的性能，证明了3D几何感知在加工工艺规划中的重要性。

Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.

</details>


### [106] [Defending Unauthorized Model Merging via Dual-Stage Weight Protection](https://arxiv.org/abs/2511.11851)
*Wei-Jia Chen,Min-Yen Tsai,Cheng-Yi Lee,Chia-Mu Yu*

Main category: cs.CV

TL;DR: MergeGuard是一个双阶段权重保护框架，通过重新分布任务相关信息并注入结构化扰动，破坏模型合并兼容性，同时保持原始模型性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型和开放存储库的快速扩散使得模型合并成为一种方便但有风险的做法，未经授权的模型合并不仅侵犯知识产权，还破坏模型所有权和问责制。

Method: 第一阶段通过L2正则化优化重新分布任务相关信息，确保重要梯度均匀分散；第二阶段注入结构化扰动以错配任务子空间，破坏损失景观中的曲率兼容性。

Result: 在视觉（ViT-L-14）和语言（Llama2、Gemma2、Mistral）模型上的广泛实验表明，MergeGuard将合并模型准确率降低高达90%，而受保护模型的性能损失小于1.5%。

Conclusion: MergeGuard通过重塑模型参数几何形状，使合并模型崩溃为破坏性干扰，同时保持受保护模型完全功能，有效防止未经授权的模型合并。

Abstract: The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.

</details>


### [107] [FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision](https://arxiv.org/abs/2511.11864)
*Muzammal Shafique,Nasir Rahim,Jamil Ahmad,Mohammad Siadat,Khalid Malik,Ghaus Malik*

Main category: cs.CV

TL;DR: 提出了一种基于符号距离函数(SDF)的新损失函数FocusSDF，通过自适应地为靠近边界的像素分配更高权重，使网络专注于边界区域，从而解决医学图像分割中边界保持的挑战。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在临床实践中至关重要，但现有模型大多未显式编码边界信息，导致边界保持成为持续挑战。

Method: 引入基于符号距离函数的FocusSDF损失函数，自适应地为靠近病灶或器官边界的像素分配更高权重，使网络具有边界感知能力。

Result: 在涵盖脑动脉瘤、中风、肝脏和乳腺肿瘤分割任务的多个数据集上，与包括MedSAM在内的五种最先进模型和四种基于距离的损失函数进行比较，FocusSDF始终表现出优越性能。

Conclusion: FocusSDF在医学图像分割中优于现有的基于距离变换的损失函数，有效提升了边界区域的关注度。

Abstract: Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.

</details>


### [108] [Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)](https://arxiv.org/abs/2511.11882)
*Simon Durand,Samuel Foucher,Alexandre Delplanque,Joëlle Taillon,Jérôme Théau*

Main category: cs.CV

TL;DR: 本研究探讨了使用合成图像补充有限训练数据，以提高在零样本和少样本设置下对北极麝牛的检测性能。结果显示添加合成图像能提升检测效果，但存在收益递减现象。


<details>
  <summary>Details</summary>
Motivation: 传统野生动物调查方法资源密集且受限于后勤挑战，而深度学习目标检测模型在稀疏分布物种（如麝牛）上因训练数据有限而效果不佳。

Method: 比较了仅使用真实图像的基线模型与5个零样本和5个少样本模型，这些模型在训练集中逐步添加更多合成图像。

Result: 零样本模型中添加合成图像提高了检测性能；少样本模型中结合真实和合成图像获得了更好的召回率和略高的准确率，但改进不显著。

Conclusion: 合成图像在数据稀缺时能有效训练准确的目标检测模型，为监测稀有或难以接近的物种提供了重要视角，可提高监测频率。

Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.

</details>


### [109] [Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation](https://arxiv.org/abs/2511.11890)
*Camila Machado de Araujo,Egon P. B. S. Borges,Ricardo Marcelo Canteiro Grangeiro,Allan Pinto*

Main category: cs.CV

TL;DR: 开发了Harpia CUDA处理库，为Annotat3D提供可扩展的交互式3D分割功能，支持超过单GPU内存容量的大型数据集处理


<details>
  <summary>Details</summary>
Motivation: 高分辨率体积成像技术生成的大型数据集对现有处理工具的效率、分割和交互探索能力提出了挑战

Method: 基于CUDA开发Harpia处理库，具有严格内存控制、原生分块执行和GPU加速的过滤、标注和量化工具套件

Result: 相比NVIDIA cuCIM和scikit-image等框架，在处理速度、内存效率和可扩展性方面有显著提升

Conclusion: 结合交互式人机界面和高效GPU资源管理，该系统特别适合共享HPC基础设施中的协作科学成像工作流

Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

</details>


### [110] [Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks](https://arxiv.org/abs/2511.11898)
*Arnav Singhvi,Vasiliki Bikia,Asad Aali,Akshay Chaudhari,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文探索了在医学视觉-语言系统中应用自动化提示优化技术，通过DSPy框架在放射学、胃肠病学和皮肤病学等五个医学成像任务中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言基础模型在医学基准测试中表现不佳，而微调需要大量领域特定数据和计算资源，手动提示工程又难以泛化。这促使研究者寻求能够利用模型嵌入知识同时减少对人类设计提示依赖的方法。

Method: 采用DSPy框架进行结构化自动提示优化，在五个医学成像任务中实现了提示管道，评估了10个开源视觉-语言模型和四种提示优化技术。

Result: 优化后的管道相比零样本提示基线实现了53%的中位数相对改进，在零样本性能较低的任务中最大改进范围从300%到3,400%。

Conclusion: 自动化提示优化在医学AI系统中具有巨大潜力，能够显著提升需要准确临床图像解释的视觉应用性能，同时减少对提示设计的依赖，让临床医生专注于患者护理和临床决策。

Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.

</details>


### [111] [PI-NAIM: Path-Integrated Neural Adaptive Imputation Model](https://arxiv.org/abs/2511.11908)
*Afifa Khaled,Ebrahim Hamid Sumiea*

Main category: cs.CV

TL;DR: PI-NAIM是一个双路径架构，根据缺失模式的复杂性动态路由样本到优化的插补方法，结合统计插补和神经网络，在医疗多模态数据上实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像和多模态临床环境中经常面临模态缺失的挑战，现有插补方法要么缺乏表示能力，要么计算成本高昂。

Method: 提出双路径架构：智能路径路由将低缺失样本导向高效统计插补（MICE），复杂模式导向神经网络（GAIN）；跨路径注意力融合利用缺失感知嵌入智能结合两个分支；端到端联合优化插补精度和下游任务性能。

Result: 在MIMIC-III和多模态基准测试中实现最先进性能：RMSE为0.108（基线为0.119-0.152），死亡率预测的AUROC达到0.812。

Conclusion: PI-NAIM的模块化设计能够无缝集成到处理不完整传感器测量、缺失模态或损坏输入的视觉管道中，为现实场景提供统一解决方案。

Abstract: Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model

</details>


### [112] [Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models](https://arxiv.org/abs/2511.11910)
*Siyou Li,Huanan Wu,Juexi Shao,Yinghao Ma,Yujian Gan,Yihao Luo,Yuwei Wang,Dong Nie,Lu Wang,Wengqing Wu,Le Zhang,Massimo Poesio,Juntao Yu*

Main category: cs.CV

TL;DR: QTSplus是一个轻量级的视觉token选择模块，通过动态选择与文本查询最相关的视觉证据来压缩长视频处理的视觉流，显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在长视频理解中视觉token数量线性增长导致的注意力成本、内存和延迟爆炸问题。

Method: 使用交叉注意力对视觉token评分，基于查询复杂度预测保留预算，通过可微分直通估计器选择Top-n token，并使用小型重编码器保持时间顺序。

Result: 在Qwen2.5-VL中集成后，视觉流压缩达89%，端到端延迟降低28%，在8个长视频理解基准测试中保持接近原始模型的精度，在TempCompass方向准确率提升20.5分。

Conclusion: QTSplus是扩展MLLMs到真实世界长视频场景的有效通用机制，能在保留任务相关证据的同时显著提升效率。

Abstract: Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.
  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \textbf{89\%} and reduces end-to-end latency by \textbf{28\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \textbf{+20.5} and \textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.
  We will make all code, data, and trained models' weights publicly available.

</details>


### [113] [From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing](https://arxiv.org/abs/2511.11944)
*Ling Wang,Yunfan Lu,Wenzong Ma,Huizai Yao,Pengteng Li,Hui Xiong*

Main category: cs.CV

TL;DR: 首次使用事件相机进行图像去雾，通过事件引导的扩散模型将事件的高动态范围信息转移到RGB图像中，在重度雾霾条件下实现清晰成像。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB的去雾方法受限于动态范围，容易丢失结构和光照细节。事件相机具有更高的动态范围和微秒级延迟，适合雾霾场景，但缺乏真实配对数据。

Method: 提出事件引导的扩散模型，设计事件引导模块将稀疏的HDR事件特征映射到扩散潜在空间，在生成过程中提供精确的结构指导。

Result: 在两个基准测试和自建的重度雾霾无人机数据集上取得了最先进的结果。

Conclusion: 事件相机为图像去雾提供了新的解决方案，通过扩散模型有效利用事件的高动态范围信息，显著提升去雾效果。

Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.

</details>


### [114] [Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs](https://arxiv.org/abs/2511.11959)
*Leonardi Melo,Luís Gustavo,Dimmy Magalhães,Lucciani Vieira,Mauro Araújo*

Main category: cs.CV

TL;DR: 比较三种基于U-Net的架构在巴西岩画岩刻语义分割中的性能，其中Attention-Residual BEGL-UNet表现最佳，Dice得分为0.710。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过改进的U-Net架构提高岩画岩刻的语义分割精度，以支持考古遗产的数字保护。

Method: 比较了三种U-Net变体：BEGL-UNet、Attention-Residual BEGL-UNet（含残差块和门控注意力机制）和Spatial Channel Attention BEGL-UNet（使用空间通道注意力模块）。所有方法都采用结合二元交叉熵和高斯边缘增强的BEGL损失函数，并在巴西考古遗址图像上使用5折交叉验证进行实验。

Result: Attention-Residual BEGL-UNet表现最佳，Dice得分为0.710，验证损失为0.067，召回率为0.854。Spatial Channel Attention BEGL-UNet的Dice得分为0.707，召回率为0.857。基线BEGL-UNet的Dice得分为0.690。注意力机制相比基线提升了2.5-2.9%的Dice得分。

Conclusion: 注意力机制在考古遗产数字保护中具有显著效果，能有效提升岩画岩刻语义分割的精度。

Abstract: This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Poço da Bebidinha Archaeological Complex, Piauí, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.

</details>


### [115] [From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology](https://arxiv.org/abs/2511.11984)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Junchao Zhu,Haibo Wang,Daniel Reisenbüchler,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Steven Salvatoree,Surya Seshane,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: 本研究系统评估了视觉语言模型在细粒度肾小球亚型分类中的表现，发现在仅有4-8个标注样本的情况下，病理学专用视觉语言骨干网络结合标准微调是最有效的起点。


<details>
  <summary>Details</summary>
Motivation: 细粒度肾小球亚型分类对肾脏活检诊断至关重要，但临床上有价值的标注数据稀缺且难以获取。现有计算方法主要在全监督下进行粗粒度疾病分类，不清楚视觉语言模型如何在数据受限情况下适应临床有意义的亚型分类。

Method: 将细粒度肾小球亚型分类建模为临床现实的少样本问题，系统评估病理学专用和通用视觉语言模型，分析分类性能（准确率、AUC、F1）和学习表示的几何结构，包括图像-文本嵌入对齐和亚型可分性。

Result: 病理学专用视觉语言骨干网络结合标准微调是最有效的起点，即使每个亚型只有4-8个标注样本，这些模型也能开始捕捉差异，在区分度和校准方面取得显著提升。正负样本区分与图像-文本对齐同等重要。

Conclusion: 监督水平和适应策略共同影响诊断性能和多模态结构，为模型选择、适应策略和标注投资提供了指导。病理学专用视觉语言模型在少样本设置下表现出色，但额外监督仍能带来增量改进。

Abstract: Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.

</details>


### [116] [BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups](https://arxiv.org/abs/2511.11989)
*Songsong Zhang,Chuanqi Tang,Hongguang Zhang,Guijian Tang,Minglong Li,Xueqiong Li,Shaowu Yang,Yuanxi Peng,Wenjing Yang,Jing Zhao*

Main category: cs.CV

TL;DR: 提出了一种突破面部特写限制的身份保持个性化生成方法，通过身份-语义分离的双线推理管道、身份自适应融合策略和身份聚合前置模块，解决了现有方法过度关注面部区域、视觉叙事性弱和语义一致性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有身份保持个性化生成方法过度强调面部区域，导致输出被面部特写主导，视觉叙事性弱且在复杂文本提示下语义一致性差，核心限制在于身份特征嵌入削弱了生成模型的语义表达能力。

Method: 设计了身份-语义分离的双线推理管道解决传统单路径架构中的表示冲突；提出身份自适应融合策略，将身份-语义融合推迟到噪声预测阶段，避免身份嵌入对语义的干扰；引入身份聚合前置模块聚合身份信息并替换随机初始化。

Result: 实验验证该方法在超越面部特写的IPPG任务中实现了稳定有效的性能，无需手动遮罩或微调即可高效生成，可作为即插即用组件快速部署到现有IPPG框架中。

Conclusion: 该方法解决了对面部特写的过度依赖，促进了电影级角色-场景创作，为相关领域提供了更丰富的个性化生成能力。

Abstract: Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.

</details>


### [117] [Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks](https://arxiv.org/abs/2511.11993)
*Jiaming Liang,Chi-Man Pun*

Main category: cs.CV

TL;DR: 本文针对变换攻击的参数优化盲点问题，提出了动态参数优化方法DPO，基于同心衰减模型CDM，将复杂度从O(m^n)降低到O(nlogm)，显著提升了迁移攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有变换攻击存在三个主要问题：(1)仅基于低迭代次数评估性能具有误导性；(2)对不同代理模型、迭代次数和任务使用统一参数；(3)传统网格搜索复杂度高，限制了参数优化。

Method: 首先通过实证研究揭示迁移性与参数强度的三种动态模式，提出同心衰减模型CDM解释这些模式，然后基于上升-下降模式设计高效的动态参数优化DPO算法。

Result: 在不同代理模型、迭代次数和任务上的综合实验表明，DPO能显著提升变换攻击的迁移性。

Conclusion: 提出的动态参数优化方法有效解决了变换攻击的参数优化问题，显著提升了迁移攻击性能，为对抗攻击研究提供了新的优化思路。

Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.

</details>


### [118] [LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation](https://arxiv.org/abs/2511.12005)
*Xinyu He,Botong Zhao,Bingbing Li,Shujing Lyu,Jiwei Shen,Yue Lu*

Main category: cs.CV

TL;DR: 提出了LithoSeg，一种用于光刻SEM图像分割的粗到细网络，通过人机交互引导SAM获得鲁棒性，并将2D分割转化为1D回归问题，在分割精度和测量精度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 光刻SEM图像的精确分割和测量对于半导体制造工艺控制和良率提升至关重要，但现有方法缺乏足够的精度和鲁棒性，限制了实际应用。

Method: 采用粗到细两阶段方法：粗阶段使用人机交互引导的SAM获得鲁棒分割；细阶段将2D分割转化为1D回归问题，使用粗掩码采样沟槽法向剖面，通过轻量级MLP进行逐点细化。

Result: LithoSeg在分割精度和测量精度上均优于先前方法，同时需要更少的监督，为实际应用提供了良好前景。

Conclusion: LithoSeg通过创新的粗到细网络设计，有效解决了光刻SEM图像分割的精度和鲁棒性问题，在减少监督需求的同时提升了性能。

Abstract: Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.

</details>


### [119] [Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy](https://arxiv.org/abs/2511.12006)
*Kai-Wen K. Yang,Andrew Bai,Alexandra Bermudez,Yunqi Hong,Zoe Latham,Iris Sloan,Michael Liu,Vishrut Goyal,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.CV

TL;DR: 提出SIT-ADDA-Auto框架，仅适应早期卷积层而非整个网络，实现显微镜图像的无标签跨域适应，在多种场景下优于全网络适应方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在显微镜应用中面临域偏移问题，传统对抗域适应方法会破坏已学习的语义表示，需要更有效的适应策略。

Method: SIT-ADDA-Auto框架：仅适应早期卷积层并冻结深层，结合浅层对抗对齐和预测不确定性自动选择适应深度，无需目标域标签。

Result: 在曝光、光照变化、跨仪器迁移和多种染色条件下，SIT-ADDA在重建和下游分割任务上优于全编码器适应和非对抗基线方法，减少语义特征漂移。

Conclusion: 为显微镜无标签域适应提供了设计原则，并给出了实际应用方案，代码已公开。

Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.

</details>


### [120] [Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis](https://arxiv.org/abs/2511.12018)
*Shounak Ray Chaudhuri,Arash Jahangiri,Christopher Paolini*

Main category: cs.CV

TL;DR: 提出了一种基于多摄像头计算机视觉的实时交通安全评估框架，通过计算后侵占时间(PET)来分析信号交叉口的安全状况，在边缘设备上实现高分辨率实时处理。


<details>
  <summary>Details</summary>
Motivation: 传统基于事故的交通安全分析受限于数据稀疏性和延迟问题，需要开发实时、高分辨率的评估方法来改善交叉口安全。

Method: 使用四个同步摄像头提供连续视觉覆盖，在NVIDIA Jetson AGX Xavier设备上使用YOLOv11分割进行车辆检测，通过单应矩阵将车辆多边形转换为统一的鸟瞰图，并开发了像素级PET算法进行精细危险可视化。

Result: 框架能够在边缘设备上以平均2.68 FPS的速度识别高风险区域，生成800×800像素的对数热图，精度达到3.3平方厘米，实现亚秒级精度和实时吞吐量。

Conclusion: 验证了去中心化基于视觉的PET分析在智能交通系统中的可行性，为高分辨率、实时和可扩展的交叉口安全评估提供了可复制的方法论。

Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.

</details>


### [121] [LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension](https://arxiv.org/abs/2511.12020)
*Xianglong Shi,Silin Cheng,Sirui Zhao,Yunhan Jiang,Enhong Chen,Yang Liu,Sebastien Ourselin*

Main category: cs.CV

TL;DR: 提出了LIHE框架解决弱监督广义指代表达理解任务，通过两阶段方法处理零或多个目标的表达，结合双曲和欧几里得几何防止语义崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督指代表达理解方法受限于一对一映射假设，无法处理现实场景中对应零个或多个目标的表达，需要更实用的广义任务范式。

Method: LIHE框架包含两个阶段：指称解耦阶段预测目标数量并分解复杂表达为子表达；指称定位阶段使用HEMix混合相似度模块，结合欧几里得几何的精确对齐和双曲几何的层次建模能力。

Result: 在gRefCOCO和Ref-ZOM上建立了首个有效的弱监督WGREC基线，HEMix在标准REC基准上持续改进，IoU@0.5提升高达2.5%。

Conclusion: LIHE框架成功解决了弱监督广义指代表达理解任务，通过混合几何方法有效防止语义崩溃，为更实用的指代表达理解系统奠定了基础。

Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.

</details>


### [122] [Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging](https://arxiv.org/abs/2511.12024)
*Jose Reinaldo Cunha Santos A V Silva Neto,Hodaka Kawachi,Yasushi Yagi,Tomoya Nakamura*

Main category: cs.CV

TL;DR: 提出NSDD方法，通过单次前向传播蒸馏迭代DDNM+求解器的零空间组件，在无配对监督下实现快速、逼真的无透镜成像重建


<details>
  <summary>Details</summary>
Motivation: 现有无透镜相机重建方法存在配对监督导致的领域偏差问题，而通用扩散先验在噪声、高度复用和病态的无透镜反卷积设置中不稳定

Method: 分离范围空间约束和零空间扩散先验更新，引入NSDD方法蒸馏DDNM+求解器的零空间组件，基于无透镜测量和范围空间锚点

Result: 在Lensless-FFHQ和PhlatCam数据集上，NSDD是第二快的方法，接近教师模型的感知质量，优于DPS和经典凸优化基线

Conclusion: NSDD为实现快速、无需真实值、逼真的无透镜成像提供了一条实用路径

Abstract: State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.

</details>


### [123] [Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark](https://arxiv.org/abs/2511.12026)
*Rulin Zhou,Wenlong He,An Wang,Jianhang Zhang,Xuanhui Zeng,Xi Zhang,Chaowei Zhu,Haijun Hu,Hongliang Ren*

Main category: cs.CV

TL;DR: VL-SurgPT是首个大规模多模态手术点跟踪数据集，结合视觉跟踪与文本描述，包含908个体内视频片段，涵盖组织和器械跟踪，并提出TG-SurgPT文本引导跟踪方法提升在恶劣视觉条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 手术环境中的点跟踪面临烟雾遮挡、镜面反射和组织变形等复杂视觉条件挑战，现有数据集缺乏理解跟踪失败机制所需的语义上下文。

Method: 构建VL-SurgPT数据集（908个视频片段，17,171个标注点），建立8种最先进跟踪方法的基准，并提出TG-SurgPT文本引导跟踪方法，利用语义描述提升鲁棒性。

Result: 实验结果表明，融入点状态信息显著提高了跟踪精度和可靠性，特别是在传统纯视觉方法难以应对的恶劣视觉场景中。

Conclusion: 通过桥接视觉和语言模态，VL-SurgPT推动了上下文感知跟踪系统的发展，这对于在挑战性术中条件下仍能保持性能的计算机辅助手术应用至关重要。

Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.

</details>


### [124] [GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory](https://arxiv.org/abs/2511.12027)
*Jeong Hun Yeo,Sangyun Chung,Sungjune Park,Dae Hoe Kim,Jinyoung Moon,Yong Man Ro*

Main category: cs.CV

TL;DR: GCAgent是一个全局上下文感知代理框架，通过创新的图示和叙事情景记忆来解决长视频理解中的长期依赖问题，在Video-MME基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在长视频理解中的令牌限制和长期时间依赖捕获困难的问题，现有方法难以捕捉全局上下文和复杂事件关系。

Method: 提出GCAgent框架，核心创新是图示和叙事情景记忆，将事件及其因果时间关系结构化建模为简洁有序的上下文；采用多阶段的感知-行动-反思循环，使用记忆管理器检索相关情景上下文进行推理。

Result: 在Video-MME Long split上相比强基线MLLM准确率提升达23.5%；在7B规模MLLM中达到最先进性能，Long split准确率73.4%，Video-MME基准测试总体平均71.9%。

Conclusion: 基于代理的推理范式和结构化记忆为认知启发的长视频理解提供了有效解决方案，验证了该框架的优越性。

Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.

</details>


### [125] [VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation](https://arxiv.org/abs/2511.12030)
*Jun Zhou,Chi Xu,Kaifeng Tang,Yuting Ge,Tingrui Guo,Li Cheng*

Main category: cs.CV

TL;DR: 提出一种联合视觉和物理线索的手-物体3D姿态估计框架，通过视觉-物理线索联合学习和候选姿态聚合，实现视觉一致且物理合理的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖视觉线索，常违反物理约束；而引入物理推理的方法通常依赖后优化或不可微物理引擎，损害视觉一致性和端到端可训练性。

Method: 1) 视觉-物理线索联合学习：提取2D视觉线索和3D物理线索；2) 候选姿态聚合：通过扩散生成多个候选姿态，结合视觉和物理预测进行精炼。

Result: 在姿态精度和物理合理性方面显著优于现有最先进方法。

Conclusion: 该框架成功整合视觉和物理线索，实现了视觉一致且物理合理的手-物体姿态估计。

Abstract: Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.

</details>


### [126] [Improved Masked Image Generation with Knowledge-Augmented Token Representations](https://arxiv.org/abs/2511.12032)
*Guotao Liang,Baoquan Zhang,Zhiyuan Wen,Zihao Han,Yunming Ye*

Main category: cs.CV

TL;DR: KA-MIG框架通过引入三种先验知识图（共现图、语义相似图、位置-标记不兼容图）来增强掩码图像生成的语义依赖建模，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有掩码图像生成方法仅依赖模型自身学习视觉标记序列的语义依赖关系，但由于单个标记缺乏明确语义且序列较长，直接学习这些依赖关系具有挑战性。

Method: 提出KA-MIG框架，构建三种先验知识图，设计图感知编码器学习标记和位置感知表示，并通过轻量级融合机制将这些增强表示集成到现有MIG方法中。

Result: 在ImageNet上的类条件图像生成实验中，该方法相比现有MIG方法取得了改进。

Conclusion: 通过引入先验知识图，KA-MIG有效增强了模型捕捉语义依赖关系的能力，从而提高了生成质量。

Abstract: Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.

</details>


### [127] [Calibrated Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2511.12034)
*Xiaohao Liu,Xiaobo Xia,Jiaheng Wei,Shuo Yang,Xiu Su,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出CalMRL方法，通过表示级插补解决多模态表示学习中模态缺失导致的锚点偏移问题，使现有方法能够处理模态不完整数据。


<details>
  <summary>Details</summary>
Motivation: 现有多模态表示学习方法需要所有模态同时存在，无法有效利用普遍存在的模态缺失数据集，这限制了实际应用。

Method: 利用模态先验和内在联系，在表示层面对缺失模态进行建模插补，采用双步学习方法，基于共享潜在变量的后验分布闭式解解决优化困境。

Result: 理论验证了锚点偏移的缓解和收敛性，实验证明CalMRL的优越性，为现有方法提供了处理模态缺失数据的新灵活性。

Conclusion: CalMRL有效解决了多模态表示学习中模态缺失带来的对齐问题，扩展了方法的应用范围。

Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.

</details>


### [128] [SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2511.12040)
*Xinyuan Hu,Changyue Shi,Chuxiao Yang,Minghao Chen,Jiajun Ding,Tao Wei,Chen Wei,Zhou Yu,Min Tan*

Main category: cs.CV

TL;DR: SRSplat是一个前馈式3D重建框架，能从少量低分辨率图像重建高分辨率3D场景，通过结合外部参考图像和内部纹理线索来补偿纹理信息不足。


<details>
  <summary>Details</summary>
Motivation: 现有的3D重建方法在从稀疏低分辨率图像重建时往往无法恢复精细纹理细节，这源于低分辨率输入中高频信息的固有缺乏。

Method: 构建场景特定参考图库，使用多模态大语言模型和扩散模型生成；引入参考引导特征增强模块对齐和融合低分辨率输入与参考图像的特征；训练解码器预测高斯基元；使用纹理感知密度控制自适应调整高斯密度。

Result: 在RealEstate10K、ACID和DTU等多个数据集上的实验表明，SRSplat优于现有方法，并展现出强大的跨数据集和跨分辨率泛化能力。

Conclusion: SRSplat通过有效整合外部参考信息和内部纹理线索，成功解决了从稀疏低分辨率图像重建高分辨率3D场景的挑战。

Abstract: Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.

</details>


### [129] [FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification](https://arxiv.org/abs/2511.12044)
*Cheng-Chang Tsai,Kai-Wen Cheng,Chun-Shien Lu*

Main category: cs.CV

TL;DR: 提出FedSDA方法，通过扩散模型和染色分离技术对齐联邦学习中各客户端的染色分布，解决组织病理学图像的非IID数据问题，同时避免隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据上面临挑战，特别是在组织病理学图像中，特征分布偏移问题严重阻碍了联邦学习的应用。

Method: 基于扩散模型拟合数据分布的能力，利用染色分离提取关键特征，提出FedSDA方法在联邦学习框架中对齐各客户端的染色分布。

Result: 实验结果表明FedSDA不仅能有效改进关注客户端模型更新差异的基线方法，还优于从数据分布角度处理非IID问题的基线方法。

Conclusion: FedSDA为计算病理学社区提供了有价值和实用的见解，能够有效缓解客户端间的分布偏移问题。

Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.

</details>


### [130] [DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging](https://arxiv.org/abs/2511.12047)
*Huimin Cheng,Xiaowei Yu,Shushan Wu,Luyang Fang,Chao Cao,Jing Zhang,Tianming Liu,Dajiang Zhu,Wenxuan Zhong,Ping Ma*

Main category: cs.CV

TL;DR: DCMM-Transformer是一种用于医学图像分析的Vision Transformer架构，通过引入度校正混合成员模型作为自注意力中的加性偏置，解决了标准ViT无法利用解剖结构的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像存在潜在的解剖分组（如器官、组织和病理区域），但标准Vision Transformers无法有效利用这些结构。现有的方法如SBM-Transformer存在不可微分性、训练不稳定性和无法建模复杂社区结构的问题。

Method: 提出DCMM-Transformer，将度校正混合成员模型作为加性偏置引入自注意力机制中，以完全可微分和可解释的方式引入社区结构和度异质性，避免了乘性掩码和二值采样。

Result: 在包括脑部、胸部、乳腺和眼部等多种医学成像数据集上的综合实验表明，该方法具有优越的性能和泛化能力。

Conclusion: 所学习的组结构和结构化注意力调制显著增强了可解释性，产生了具有解剖意义和语义一致性的注意力图。

Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.

</details>


### [131] [DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training](https://arxiv.org/abs/2511.12048)
*Saksham Kumar,Ashish Singh,Srinivasarao Thota,Sunil Kumar Singh,Chandan Kumar*

Main category: cs.CV

TL;DR: 提出DeiTFake方法，基于DeiT变换器和两阶段渐进训练策略，用于深度伪造检测，在OpenForensics数据集上达到99.22%的准确率。


<details>
  <summary>Details</summary>
Motivation: 深度伪造对数字媒体完整性构成重大威胁，需要开发有效的检测方法来应对这一挑战。

Method: 使用DeiT变换器架构和两阶段渐进训练策略：第一阶段使用标准增强进行迁移学习，第二阶段使用高级仿射和深度伪造特定增强进行微调。

Result: 在OpenForensics数据集（190,335张图像）上，第一阶段达到98.71%准确率，第二阶段达到99.22%准确率和0.9997的AUROC，优于最新基线。

Conclusion: DeiTFake方法通过知识蒸馏捕获细微操作伪影，提高了检测模型的鲁棒性，为面部深度伪造检测提供了实用基准。

Abstract: Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\% accuracy after stage one and 99.22\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.

</details>


### [132] [UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization](https://arxiv.org/abs/2511.12054)
*Cuiqun Chen,Qi Chen,Bin Yang,Xingyi Zhang*

Main category: cs.CV

TL;DR: 提出UniABG框架，通过对抗视图桥接和图过滤校准实现无监督跨视图地理定位，在University-1652和SUES-200数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决跨视图地理定位中监督方法依赖大量标注数据、无监督方法因跨视图域差距导致伪标签噪声的问题。

Method: 采用双阶段框架：1) 视图感知对抗桥接(VAAB)建模视图不变特征；2) 异构图过滤校准(HGFC)构建双视图结构图精化跨视图关联。

Result: 在University-1652数据集上卫星→无人机AP提升10.63%，在SUES-200上提升16.73%，甚至超越监督基线。

Conclusion: UniABG通过结合对抗学习和图结构建模，有效解决了无监督跨视图地理定位中的伪标签噪声问题，实现了优异的性能。

Abstract: Cross-view geo-localization (CVGL) matches query images ($\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\rightarrow$ Drone AP by +10.63\% on University-1652 and +16.73\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG

</details>


### [133] [PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling](https://arxiv.org/abs/2511.12056)
*Sijie Wang,Qiang Wang,Shaohuai Shi*

Main category: cs.CV

TL;DR: PipeDiT是一个用于加速视频生成的流水线框架，通过序列并行、模块解耦和注意力协同处理等技术，显著提升了扩散变换器模型的推理速度并降低了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散变换器(DiT)的视频生成模型虽然性能出色，但存在推理速度慢和内存消耗高的问题，限制了实际部署应用。

Method: 1. 设计序列并行流水线算法(PipeSP)，实现潜在生成和GPU间通信的流水线化；2. 提出DeDiVAE方法将扩散模块和VAE模块解耦到不同GPU组，实现执行流水线化；3. 提出注意力协同处理(Aco)方法优化VAE组的GPU资源利用。

Result: 在8-GPU系统上的实验表明，PipeDiT在多种常见分辨率和时间步配置下，相比OpenSoraPlan和HunyuanVideo实现了1.06倍到4.02倍的加速。

Conclusion: PipeDiT框架有效解决了DiT模型在视频生成中的推理延迟和内存瓶颈问题，为实际部署提供了可行的解决方案。

Abstract: Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.

</details>


### [134] [MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity](https://arxiv.org/abs/2511.12061)
*Zhichen Lai,Hua Lu,Huan Li,Jialiang Li,Christian S. Jensen*

Main category: cs.CV

TL;DR: MovSemCL是一个用于轨迹相似度计算的运动语义对比学习框架，通过运动语义特征转换、分层注意力编码和曲率引导的数据增强，解决了现有方法在语义建模、计算效率和物理合理性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的轨迹相似度计算方法存在三个关键问题：对轨迹语义和层次结构建模不足；点级编码导致计算成本高；使用物理上不合理的增强方法扭曲轨迹语义。

Method: 将原始GPS轨迹转换为运动语义特征并分割为补丁，使用内外补丁注意力编码局部和全局轨迹模式，采用曲率引导的增强策略保留信息段（如转弯和交叉口）并掩码冗余段。

Result: 在真实数据集上的实验表明，MovSemCL在相似度搜索任务中平均排名接近理想值1，在启发式近似任务中提升高达20.3%，同时推理延迟降低达43.4%。

Conclusion: MovSemCL通过有效的运动语义建模、分层表示学习和物理合理的增强策略，显著提升了轨迹相似度计算的性能和效率。

Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

</details>


### [135] [DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal](https://arxiv.org/abs/2511.12066)
*Jialang Lu,Shuning Sun,Pu Wang,Chen Wu,Feng Gao,Lina Gong,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: DCA-LUT是一个基于深度学习的紫色边缘去除框架，通过色度感知坐标变换模块和5D查找表实现高效的非线性色彩映射，在合成和真实数据集上都达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 紫色边缘是相机镜头纵向色差引起的持久伪影，传统解决方案依赖复杂昂贵的复消色差镜头硬件和手工特征提取，忽略了数据驱动的方法。

Method: 提出色度感知坐标变换模块学习图像自适应色彩空间，将边缘分离到专用维度；使用5D查找表进行最终色彩校正；构建了大规模合成紫色边缘数据集用于训练和评估。

Result: 在合成和真实世界数据集上的广泛实验表明，该方法在紫色边缘去除方面实现了最先进的性能。

Conclusion: DCA-LUT是首个用于紫色边缘去除的深度学习框架，通过物理启发的分离策略和高效的色彩映射，有效解决了这一长期存在的成像问题。

Abstract: Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.

</details>


### [136] [Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound](https://arxiv.org/abs/2511.12077)
*Dengming Zhang,Weitao You,Jingxiong Li,Weishen Lin,Wenda Shi,Xue Zhao,Heda Zuo,Junxian Wu,Lingyun Sun*

Main category: cs.CV

TL;DR: VAEmotionLLM是一个两阶段框架，通过有限音频预训练让视觉语言模型获得听觉能力，并增强跨模态情感理解。第一阶段通过视觉引导音频对齐实现"看而听"，第二阶段通过轻量级跨模态情感适配器增强情感理解。


<details>
  <summary>Details</summary>
Motivation: 当前音频-视觉语言模型通常需要大规模音频预训练才能赋予视觉语言模型听觉能力，这限制了可扩展性。同时，大多数现有工作是人类中心或单模态的，忽视了艺术作品有意表达的情感。

Method: 两阶段框架：1) VG-Align通过将冻结的视觉通路蒸馏到新的音频通路，在同步音视频片段上对齐共享LLM的下一个token分布；2) EmoAdapter通过情感增强器和情感监督器注入情感敏感残差并应用情感监督。

Result: 在ArtEmoBenchmark上达到最先进结果，优于音频、视觉和音频-视觉基线方法。消融研究表明所提出的组件是互补的。

Conclusion: VAEmotionLLM证明了通过有限音频预训练实现视觉语言模型听觉能力的可行性，并显著提升了跨模态情感理解能力。

Abstract: Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.

</details>


### [137] [Point Cloud Quantization through Multimodal Prompting for 3D Understanding](https://arxiv.org/abs/2511.12079)
*Hongxuan Li,Wencheng Zhu,Huiying Xu,Xinzhong Zhu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于多模态提示驱动的点云量化框架，利用文本嵌入作为原型先验，通过双约束量化空间和Gumbel-Softmax松弛实现几何与语义信息的联合编码。


<details>
  <summary>Details</summary>
Motivation: 现有基于可训练向量或聚类质心的原型方法在代表性和可解释性方面存在不足，而多模态对齐在视觉语言模型中已显示出潜力。

Method: 使用预训练模型的文本嵌入作为原型先验，通过多模态提示自适应细化原型，构建双约束量化空间（紧凑性和分离性正则化），并采用Gumbel-Softmax松弛实现可微分离散化。

Result: 在ModelNet40和ScanObjectNN数据集上的广泛实验证明了该方法的优越有效性。

Conclusion: 所提出的多模态提示驱动量化框架能够有效解决原型代表性和可解释性问题，实现几何与语义信息的联合编码。

Abstract: Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.

</details>


### [138] [Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning](https://arxiv.org/abs/2511.12082)
*Lokender Singh,Saksham Kumar,Chandan Kumar*

Main category: cs.CV

TL;DR: 提出了一种基于改进ResNet-101架构和概率推理的多标签图像分类方法，在COCO-2014数据集上实现了0.794 mAP的优异性能，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 多标签图像分类在计算机视觉应用中具有广泛需求，但现有方法在处理标签依赖性和不确定性方面存在不足，需要更有效的方法来提升分类准确性。

Method: 使用改进的ResNet-101架构，结合概率推理来建模标签依赖关系和不确定性，从而提升多标签分类的预测精度。

Result: 在COCO-2014数据集上取得了0.794 mAP的优异结果，超越了ResNet-SRN（0.771）和Vision Transformer基线（0.785），达到了接近最先进的性能水平。

Conclusion: 将概率推理集成到深度学习模型中能够有效解决多标签场景中的挑战，该方法在多标签图像分类任务中表现出色，为相关应用提供了新的技术路径。

Abstract: Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.

</details>


### [139] [SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving](https://arxiv.org/abs/2511.12084)
*Ji-Ping Jin,Chen-Bin Feng,Rui Fan,Chi-Man Vong*

Main category: cs.CV

TL;DR: 提出了SemanticStitch框架，通过深度学习整合语义先验信息来改善图像拼接质量，特别关注前景对象的完整性保护。


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接方法因拍摄角度、位置差异和物体移动导致错位和视觉不一致，且传统接缝裁剪方法忽略语义信息，破坏前景连续性。

Method: 开发了基于深度学习的SemanticStitch框架，包含强调显著对象语义完整性的新型损失函数，并构建了两个专门的真实世界数据集进行评估。

Result: 实验结果显示相比传统技术有显著改进，为实际应用提供了有力支持。

Conclusion: SemanticStitch通过整合语义先验有效提升了图像拼接质量，特别是在保护前景对象完整性方面表现出色。

Abstract: Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.

</details>


### [140] [Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning](https://arxiv.org/abs/2511.12090)
*Shengqin Jiang,Tianqi Kong,Yuankai Qi,Haokui Zhang,Lina Yao,Quan Z. Sheng,Qingshan Liu,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种分层分组提示调优方法，通过层分组共享提示和使用根提示生成子提示，减少灾难性遗忘，在持续学习中实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法在每个层独立添加任务特定提示，虽然灵活性高但容易导致某些层不必要更新，可能覆盖先前任务的关键特征表示，增加灾难性遗忘风险。

Method: 分层分组提示调优：1) 同组层共享大致相同的提示，通过位置编码调整；2) 使用单一任务特定根提示学习为每个层组生成子提示，增强子提示间的协同性。

Result: 在四个基准测试上的广泛实验表明，该方法相比多个最先进方法实现了更优的性能。

Conclusion: 所提出的分层分组提示调优方法通过增强模型稳定性和减少提示独立性，有效缓解了灾难性遗忘问题，在持续学习中表现出色。

Abstract: Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.

</details>


### [141] [Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio](https://arxiv.org/abs/2511.12095)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: PACE是首个针对SNN和事件视觉的数据集蒸馏框架，通过压缩大型训练数据集为紧凑的合成数据集，实现快速SNN训练，在多个数据集上显著减少训练时间和存储成本。


<details>
  <summary>Details</summary>
Motivation: SNN由于时间编码导致训练成本高昂，限制了实际部署。需要降低SNN训练成本的方法。

Method: PACE包含两个核心模块：ST-DSM（使用残差膜电位来密集化基于脉冲的特征并执行精细的时空匹配）和PEQ-N（提供即插即用的概率整数量化器）。

Result: 在DVS-Gesture、CIFAR10-DVS和N-MNIST数据集上，PACE优于现有方法，在N-MNIST上达到84.4%准确率，训练时间减少50倍以上，存储成本减少6000倍。

Conclusion: PACE能够生成紧凑的替代数据集，实现分钟级的SNN训练和高效的边缘部署。

Abstract: Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \textbf{ST-DSM} and \textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \(84.4\%\) accuracy, about \(85\%\) of the full training set performance, while reducing training time by more than \(50\times\) and storage cost by \(6000\times\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.

</details>


### [142] [Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks](https://arxiv.org/abs/2511.12097)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Xudong Jiang,Dacheng Tao*

Main category: cs.CV

TL;DR: SpikeNM是首个面向SNN的半结构化N:M剪枝框架，通过M路基对数参数化和可微分top-k采样器，线性化计算复杂度，结合神经科学启发的资格蒸馏方法，在保持精度的同时实现硬件友好的稀疏模式。


<details>
  <summary>Details</summary>
Motivation: 现有SNN剪枝方法存在两难：非结构化剪枝难以硬件加速，结构化剪枝缺乏灵活性且精度损失严重。需要一种既能硬件友好又能保持精度的半结构化剪枝方案。

Method: 采用N:M半结构化剪枝，使用M路基对数参数化和可微分top-k采样器线性化复杂度，提出资格蒸馏(EID)将时间累积信用转为块级软目标，稳定高稀疏度下的搜索过程。

Result: 在2:4稀疏度下，SpikeNM在主流数据集上保持甚至提升精度，同时产生硬件友好的稀疏模式，与SNN固有的脉冲稀疏性互补。

Conclusion: SpikeNM成功解决了SNN深度架构参数膨胀问题，通过半结构化剪枝在保持精度的同时实现硬件友好的稀疏化，为边缘部署提供了可行方案。

Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \textbf{SpikeNM}, the first SNN-oriented \emph{semi-structured} \(N{:}M\) pruning framework that learns sparse SNNs \emph{from scratch}, enforcing \emph{at most \(N\)} non-zeros per \(M\)-weight block. To avoid the combinatorial space complexity \(\sum_{k=1}^{N}\binom{M}{k}\) growing exponentially with \(M\), SpikeNM adopts an \(M\)-way basis-logit parameterization with a differentiable top-\(k\) sampler, \emph{linearizing} per-block complexity to \(\mathcal O(M)\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \(2{:}4\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.

</details>


### [143] [DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT](https://arxiv.org/abs/2511.12098)
*Xianhao Zhou,Jianghao Wu,Ku Zhao,Jinlong He,Huangxuan Zhao,Lei Chen,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: 提出DGCF框架，结合冻结自监督DINOv3 Transformer和可训练CNN编码器-解码器，通过交叉融合模块平衡局部外观和上下文表示，在SynthRAD2023骨盆数据集上实现最先进的MRI→CT和CBCT→CT转换性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型缺乏全局语义理解，而Transformer在小规模医学数据集上容易过拟合，需要平衡局部特征和全局语义表示。

Method: DGCF框架：冻结DINOv3 Transformer + 可训练CNN编码器-解码器 + 可学习交叉融合模块 + 多级DINOv3感知损失(MLDP)。

Result: 在SynthRAD2023骨盆数据集上，DGCF在MS-SSIM、PSNR和基于分割的指标上达到最先进性能，适用于MRI→CT和CBCT→CT转换任务。

Conclusion: 这是首个使用DINOv3表示进行医学图像转换的工作，展示了自监督Transformer指导在语义感知CT合成中的潜力。

Abstract: Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\rightarrow$CT and CBCT$\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.

</details>


### [144] [Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models](https://arxiv.org/abs/2511.12099)
*Tianle Cheng,Zeyan Zhang,Kaifeng Gao,Jun Xiao*

Main category: cs.CV

TL;DR: 提出Adaptive Begin-of-Video Tokens (ada-BOV)方法，解决自回归视频扩散模型在生成长视频时的一致性和动态质量挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成长视频时面临两个主要问题：基于分块的方法存在去噪延迟和误差累积，流式去噪方法则存在一致性脆弱和动态质量差的问题。

Method: 1) 引入自适应BOV令牌，通过类似自适应层归一化的调制吸收去噪的前帧；2) 提出流式去噪细化策略，解耦采样轨迹长度与注意力窗口约束；3) 设计扰动增强训练噪声调度，平衡收敛速度与鲁棒性。

Result: 在多个指标上取得了令人信服的定性和定量结果，显著提升了长视频生成的一致性和动态质量。

Conclusion: ada-BOV方法有效解决了自回归视频扩散模型在长视频生成中的关键挑战，在保持全局一致性的同时改善了局部动态质量。

Abstract: Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.

</details>


### [145] [Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation](https://arxiv.org/abs/2511.12100)
*Yannan Chen,Ruoyu Chen,Bin Zeng,Wei Wang,Shiming Liu,Qunli Zhang,Zheng Hu,Laiyuan Wang,Yaowei Wang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出SS-CA方法，通过反事实解释增强训练，解决视觉模型依赖有限因果特征的问题，提升模型泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型仅依赖有限充分因果特征进行预测，对分布偏移或关键特征缺失敏感。模型的关键区域被遮挡后会导致误分类，而人类仍能识别，表明模型学习到的依赖关系不够因果充分。

Method: 基于子集选择的LIMA归因方法开发Counterfactual LIMA，识别能选择性改变模型预测的最小空间区域集合。利用这些归因，提出数据增强策略，将识别区域替换为自然背景，并在增强样本和原始样本上联合训练模型。

Result: 在多个ImageNet变体上的实验表明，SS-CA提高了ID测试数据的泛化能力，在OOD基准（如ImageNet-R和ImageNet-S）上表现优异。在噪声等扰动下，SS-CA训练的模型也展现出增强的泛化能力。

Conclusion: SS-CA方法有效利用可解释性洞察纠正模型缺陷，既提升了性能又增强了鲁棒性，证明了将反事实解释整合到训练过程中的有效性。

Abstract: In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.

</details>


### [146] [BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation](https://arxiv.org/abs/2511.12103)
*Sayad Ibna Azad,Md. Atiqur Rahman*

Main category: cs.CV

TL;DR: BdSL-SPOTER是一个基于姿态的transformer框架，用于准确高效地识别孟加拉手语，在BdSLW60基准测试上达到97.92%的Top-1验证准确率，比Bi-LSTM基线提升22.82%，同时保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 为孟加拉手语(BdSL)开发一个准确且高效的手语识别框架，解决低资源区域手语识别问题，并为现实世界的无障碍应用提供实用解决方案。

Method: 扩展SPOTER范式，采用文化特定的预处理和紧凑的四层transformer编码器，优化可学习位置编码，并使用课程学习来增强在有限数据上的泛化能力并加速收敛。

Result: 在BdSLW60基准测试上达到97.92%的Top-1验证准确率，比Bi-LSTM基线提升22.82%，同时参数数量更少、FLOPs更低、FPS更高，计算成本低。

Conclusion: BdSL-SPOTER为现实世界的无障碍应用提供了实用框架，并可作为其他低资源区域手语的可扩展模型。

Abstract: We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.

</details>


### [147] [TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery](https://arxiv.org/abs/2511.12104)
*Tammy Glazer,Gilles Q. Hacheme,Akram Zaytar,Luana Marotti,Amy Michaels,Girmaw Abebe Tadesse,Kevin White,Rahul Dodhia,Andrew Zolli,Inbal Becker-Reshef,Juan M. Lavista Ferres,Caleb Robinson*

Main category: cs.CV

TL;DR: TEMPO是一个全球性的时间分辨数据集，使用深度学习从高分辨率卫星图像中提取建筑密度和高度信息，提供2018年第一季度至2025年第二季度的季度更新。


<details>
  <summary>Details</summary>
Motivation: 需要大规模监测全球建筑发展模式和气候变化影响，为全球韧性和适应工作提供支持，同时降低计算成本。

Method: 将现有建筑足迹和高度数据与季度PlanetScope卫星图像配对，训练多任务深度学习模型，预测37.6米/像素分辨率的建筑密度和高度。

Result: 验证显示F1分数在85%-88%之间，时间稳定性高（五年趋势一致性得分0.96），能够以较低计算成本捕获季度建筑变化。

Conclusion: TEMPO数据集能够高效监测全球建筑发展，为气候适应和韧性建设提供重要数据支持。

Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.

</details>


### [148] [Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection](https://arxiv.org/abs/2511.12107)
*Tianxiang Zhang,Peipeng Yu,Zhihua Xia,Longchen Dai,Xiaoyu Zhou,Hui Gao*

Main category: cs.CV

TL;DR: 提出了DFF-Adapter方法，通过轻量级多头LoRA模块适配DINOv2，同时处理真实性检测和细粒度伪造方法分类，仅需3.5M可训练参数即可达到或超越现有复杂方法的检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法将DINOv2作为通用二元分类器，忽略了不同深度伪造方法产生的独特伪影特征，需要更精细化的检测方法。

Method: 在DINOv2的每个transformer块中集成轻量级多头LoRA模块，构建共享分支将细粒度伪造方法线索传播到真实性检测头，实现多任务协同优化。

Result: 仅使用3.5M可训练参数，检测精度达到或超越了当前复杂的先进方法。

Conclusion: DFF-Adapter通过参数高效的方式，利用细粒度伪造方法分类增强伪影敏感性，显著提升了深度伪造检测性能。

Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.

</details>


### [149] [MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images](https://arxiv.org/abs/2511.12110)
*Qinyue Tong,Ziqian Lu,Jun Liu,Rui Zuo,Zheming Lu*

Main category: cs.CV

TL;DR: 提出MEMR-Seg任务和MediRound模型，通过多轮对话实现实体级医学图像分割，解决了传统方法缺乏交互性和多轮推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为任务特定且缺乏交互性，文本提示方法局限于单轮对话，无法进行多轮推理。

Method: 构建MR-MedSeg数据集(17.7万轮多轮医学分割对话)，提出MediRound基线模型，引入轻量级判断与校正机制来缓解多轮分割中的错误传播。

Result: 实验结果表明该方法有效解决了MEMR-Seg任务，性能优于传统医学参考分割方法。

Conclusion: MEMR-Seg任务和MediRound模型为医学图像分割提供了多轮交互推理的新范式，显著提升了分割的灵活性和准确性。

Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.

</details>


### [150] [RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving](https://arxiv.org/abs/2511.12117)
*Ruiqi Cheng,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: RadarMP是一种利用4D毫米波雷达低电平回波信号进行精确3D场景运动感知的新方法，通过统一架构联合建模雷达目标检测和运动估计任务，在恶劣天气条件下实现可靠的自动驾驶感知。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达具有全天候工作能力，但稀疏和噪声的雷达点云导致运动感知不精确，限制了自动驾驶在光学传感器性能下降时的感知能力。

Method: 提出RadarMP方法，使用两帧连续雷达回波信号，在统一架构中联合建模雷达目标检测和运动估计，设计基于多普勒频移和回波强度的自监督损失函数来监督空间和运动一致性。

Result: 在公共数据集上的实验表明，RadarMP在不同天气和光照条件下实现可靠的运动感知，优于基于雷达的解耦运动感知方法。

Conclusion: RadarMP增强了全场景自动驾驶系统的感知能力，为恶劣天气条件下的可靠运动感知提供了有效解决方案。

Abstract: Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.

</details>


### [151] [OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description](https://arxiv.org/abs/2511.12131)
*Quanxing Xu,Ling Zhou,Feifei Zhang,Jinyu Tian,Rubing Huang*

Main category: cs.CV

TL;DR: 提出OAD-Promoter方法，通过减轻语言偏见和提升领域迁移鲁棒性来增强基于LLM的视觉问答系统，在少样本和零样本场景下取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: LLM在视觉问答中存在语言偏见问题，导致预测不可靠且在分布外泛化方面表现不佳

Method: 包含三个模块：OEG模块生成全局描述和对象集中样本，MKA模块从存储示例中检索相关知识，OAD Prompt整合前两个模块输出优化LLM推理

Result: 实验表明OAD-Promoter显著提升了基于LLM的VQA方法在少样本和零样本设置下的性能

Conclusion: OAD-Promoter通过减轻语言偏见和增强领域迁移能力，有效提升了LLM在视觉问答中的表现

Abstract: Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.

</details>


### [152] [Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware](https://arxiv.org/abs/2511.12136)
*Karol C. Jurzec,Tomasz Szydlo,Maciej Wielgosz*

Main category: cs.CV

TL;DR: 提出了一种轻量级的基于C语言的脉冲神经网络推理运行时，通过优化数据布局和利用稀疏性，在边缘设备上实现了高效部署。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络具有事件驱动特性和能效优势，但在边缘设备上的训练和部署仍面临挑战，需要轻量级高效的运行时解决方案。

Method: 将SNNTorch训练的模型转换为紧凑的C表示，采用静态缓存友好的数据布局和预分配策略，并利用稀疏脉冲活动修剪不活跃的神经元和突触。

Result: 在N-MNIST和ST-MNIST数据集上保持与Python基线相同的功能，在桌面CPU上实现约10倍加速，通过剪枝获得额外收益，内存大幅减少，可在Arduino Portenta H7等微控制器上部署。

Conclusion: 当配备优化的运行时和脉冲驱动的模型压缩时，脉冲神经网络可以在传统嵌入式平台上高效执行。

Abstract: Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/

</details>


### [153] [MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering](https://arxiv.org/abs/2511.12142)
*Seokwon Song,Minsu Park,Gunhee Kim*

Main category: cs.CV

TL;DR: MAVIS是首个评估多模态源归属系统的基准，包含157K视觉问答实例，开发了基于信息量、基础性和流畅性的自动评估指标，发现多模态RAG在信息量和流畅性上优于单模态RAG，但在图像文档的基础性上存在差距。


<details>
  <summary>Details</summary>
Motivation: 现有源归属研究主要关注纯文本场景，忽视了多模态的作用，需要开发能够理解视觉问题意图、检索多模态证据并生成带引用的长文本答案的系统。

Method: 构建包含157K视觉问答实例的数据集，每个答案都标注了事实级别的多模态文档引用，开发了三个维度的自动评估指标，并比较了不同提示方法下的多模态RAG系统。

Result: 多模态RAG比单模态RAG生成更信息和流畅的答案，但对图像文档的基础性较弱；在不同提示方法下存在信息量与基础性的权衡；缓解图像文档解释中的上下文偏差是关键研究方向。

Conclusion: MAVIS基准揭示了多模态源归属系统的关键挑战，特别是在图像文档基础性方面的差距，提出了缓解上下文偏差作为未来重要研究方向。

Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS

</details>


### [154] [Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain](https://arxiv.org/abs/2511.12150)
*Yuqi Xie,Shuhan Ye,Yi Yu,Chong Wang,Qixin Zhang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian,Guoqi Li*

Main category: cs.CV

TL;DR: TMKT提出了一种跨模态训练框架，通过时间步混合策略在RGB和DVS输入之间进行插值，结合轻量级的模态感知目标，实现了更平滑的知识迁移和更好的脉冲图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机和脉冲神经网络的结合有望实现高效视觉智能，但稀缺的事件数据和DVS输出的稀疏性阻碍了有效训练。RGB到DVS的先验知识迁移由于模态间分布差异大而表现不佳。

Method: 提出了时间步混合知识迁移(TMKT)框架，包含概率性时间步混合(TSM)策略，利用SNN的异步特性在RGB和DVS输入的不同时间步进行插值。引入两个轻量级模态感知目标：模态感知指导(MAG)和混合比感知(MRP)。

Result: TMKT实现了更平滑的知识迁移，缓解了训练中的模态不匹配问题，在脉冲图像分类任务中取得了优越性能。在多个基准测试和SNN骨干网络上的广泛实验证明了方法的有效性。

Conclusion: TMKT通过时间步混合策略和模态感知目标，有效解决了跨模态知识迁移中的分布差异问题，为事件相机和脉冲神经网络的结合提供了有效的训练框架。

Abstract: The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.

</details>


### [155] [FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing](https://arxiv.org/abs/2511.12151)
*Kaixiang Yang,Boyang Shen,Xin Li,Yuchen Dai,Yuxuan Luo,Yueran Ma,Wei Fang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FIA-Edit是一个基于频率交互注意力的免反演图像编辑框架，通过频率表示交互和特征注入模块实现高保真编辑，并在医学图像增强中取得突破。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的免反演方法虽然效率高，但缺乏有效的源信息整合，导致背景保留差、空间不一致和过度编辑问题。

Method: 提出频率交互注意力框架，包含频率表示交互模块（在自注意力中交换源和目标特征的频率分量）和特征注入模块（在交叉注意力中显式引入源侧查询、键、值和文本嵌入）。

Result: 在RTX 4090上每张512*512图像编辑仅需约6秒，在视觉质量、背景保真度和可控性方面优于现有方法，并首次将文本引导图像编辑扩展到临床应用中。

Conclusion: FIA-Edit实现了高效高保真的图像编辑，为医学数据增强开辟了新途径，在下游出血分类任务中带来显著提升。

Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.

</details>


### [156] [Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function](https://arxiv.org/abs/2511.12162)
*Shuo Yin,Zhiyuan Yin,Yuqing Hou,Rui Liu,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: 提出Center-Reassigned Hashing (CRH)端到端框架，通过动态重新分配预设码本中的哈希中心来优化哈希函数，避免两阶段方法的复杂性和性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有的哈希中心方法存在随机初始化忽略类间语义关系的问题，而两阶段方法虽然能缓解但引入了额外复杂性和阶段间不一致导致的性能下降。

Method: CRH框架从预设码本中动态重新分配哈希中心，同时联合优化哈希函数，无需显式的中心优化阶段，并通过多头机制增强哈希中心的表示能力。

Result: 在三个基准测试上的实验表明，CRH能够学习到语义上有意义的哈希中心，并在检索任务中优于最先进的深度哈希方法。

Conclusion: CRH通过端到端的动态哈希中心重分配机制，有效整合语义关系到学习过程中，实现了更好的检索性能。

Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

</details>


### [157] [Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective](https://arxiv.org/abs/2511.12170)
*Wang Luo,Di Wu,Hengyuan Na,Yinlin Zhu,Miao Hu,Guocong Quan*

Main category: cs.CV

TL;DR: 提出了一种新的点云补全范式Completion-by-Correction，通过预训练图像到3D模型生成拓扑完整的形状先验，然后在特征空间进行校正，而不是传统的补全-修复方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于多模态的补全-修复方法存在结构不一致和拓扑伪影问题，因为缺乏足够的几何和语义约束。

Method: 提出PGNet多阶段框架：双特征编码以锚定生成先验，合成结构对齐的粗支架，通过分层校正逐步细化几何细节。

Result: 在ShapeNetViPC数据集上，平均Chamfer Distance降低23.5%，F-score提升7.1%，优于现有最优方法。

Conclusion: Completion-by-Correction范式将补全任务从无约束合成转变为引导式细化，实现了结构一致且与观测对齐的重建。

Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).

</details>


### [158] [MixAR: Mixture Autoregressive Image Generation](https://arxiv.org/abs/2511.12181)
*Jinyuan Hu,Jiayou Zhang,Shaobo Cui,Kun Zhang,Guangyi Chen*

Main category: cs.CV

TL;DR: MixAR是一个新颖的自回归图像生成框架，通过混合离散和连续表示来提升生成质量，解决了纯连续表示建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的离散token方法会丢失细粒度信息，而纯连续表示在广阔无结构的空间中难以高效建模。

Method: 提出MixAR框架，使用离散token作为先验指导连续自回归建模，探索了DC-SA、DC-CA、DC-Mix等混合策略，并提出TI-Mix来弥合训练和推理的分布差距。

Result: 实验表明DC-Mix策略在计算效率和生成保真度之间取得良好平衡，TI-Mix带来一致的改进。

Conclusion: 混合离散-连续表示的自回归建模能有效提升图像生成质量，同时保持计算效率。

Abstract: Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.

</details>


### [159] [MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis](https://arxiv.org/abs/2511.12193)
*Abdelrahman Elsayed,Ahmed Jaheen,Mohammad Yaqub*

Main category: cs.CV

TL;DR: MMRINet是一个轻量级脑肿瘤分割架构，使用线性复杂度的Mamba状态空间模型替代二次复杂度的注意力机制，在BraTS-Lighthouse SSA 2025中仅用约250万参数就实现了0.752的平均Dice分数和12.23的平均HD95。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，深度3D网络计算成本过高，需要开发轻量高效的脑肿瘤分割方法。

Method: 使用线性复杂度的Mamba状态空间模型进行高效体积上下文建模，提出双路径特征精炼模块最大化特征多样性，采用渐进式特征聚合实现有效的多尺度融合。

Result: 在BraTS-Lighthouse SSA 2025中，模型仅用约250万参数就实现了平均Dice分数0.752和平均HD95 12.23的强性能。

Conclusion: MMRINet展示了在低资源临床环境中实现高效准确分割的潜力，适合资源受限的医疗环境部署。

Abstract: Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: github.com/BioMedIA-MBZUAI/MMRINet.

</details>


### [160] [Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System](https://arxiv.org/abs/2511.12196)
*Aditi Bhalla,Christian Hellert,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 提出一个两阶段跨视角、跨模态的无监督领域自适应框架，用于驾驶员活动识别，解决视角变化和领域偏移问题，在RGB视频数据上相比基线方法提升50%准确率。


<details>
  <summary>Details</summary>
Motivation: 驾驶员分心是交通事故的主要原因，现有深度学习方法在真实部署中面临视角变化和领域偏移的挑战，现有方法通常单独处理这些问题，缺乏联合解决方案。

Method: 两阶段框架：第一阶段使用对比学习在多视角数据中学习视角不变和动作区分特征；第二阶段使用信息瓶颈损失进行跨模态领域自适应，无需新领域的标注数据。

Result: 使用Video Swin和MViT视频变换器在Drive&Act数据集上评估，相比监督对比学习方法在RGB视频数据上提升近50%准确率，比无监督领域自适应方法提升5%。

Conclusion: 该联合框架能有效解决驾驶员活动识别中的跨视角和跨模态挑战，为模型在不同车辆配置中的鲁棒部署提供了可行方案。

Abstract: Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.

</details>


### [161] [Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation](https://arxiv.org/abs/2511.12200)
*Sujun Sun,Haowen Gu,Cheng Xie,Yanxu Ren,Mingwu Ren,Haofeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种分层语义学习框架来解决跨域少样本分割中的语义粒度差异问题，通过双风格随机化和分层语义挖掘模块增强模型对不同粒度语义的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有的跨域少样本分割方法主要关注源域和目标域之间的风格差异，但忽略了分割粒度差异，导致对目标域中新类别的语义区分能力不足。

Method: 提出了分层语义学习框架，包括双风格随机化模块模拟目标域数据风格变化，分层语义挖掘模块利用多尺度超像素挖掘不同粒度下的类内一致性和类间区分性，以及原型置信度调制阈值模块缓解分割模糊问题。

Result: 在四个流行的目标域数据集上进行了广泛实验，结果表明该方法达到了最先进的性能。

Conclusion: 所提出的分层语义学习框架有效解决了跨域少样本分割中的语义粒度差异问题，显著提升了模型性能。

Abstract: Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.

</details>


### [162] [OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs](https://arxiv.org/abs/2511.12201)
*Feng Chen,Yefei He,Shaoxuan He,Yuanyu He,Jing Liu,Lequan Lin,Akide Liu,Zhaoyang Li,Jiyuan Zhang,Zhenbang Sun,Bohan Zhuang,Qi Wu*

Main category: cs.CV

TL;DR: OmniSparse是一个训练感知的细粒度稀疏注意力框架，用于长视频多模态大语言模型，在训练和推理中都能实现动态token预算分配，达到与全注意力相当的性能，同时获得2.7倍预填充加速和2.4倍解码内存减少。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏注意力方法主要针对推理时加速，在预定义的稀疏模式下选择关键token，但无法弥合训练-推理差距，且缺乏在查询、键值对和注意力头等多个维度进行细粒度token选择的能力，导致性能次优和加速收益有限。

Method: OmniSparse包含三个自适应互补机制：1) 通过懒惰-主动分类进行查询选择，保留捕获广泛语义相似性的主动查询；2) 基于头级动态预算分配的KV选择，根据最平坦头确定共享预算；3) KV缓存精简，根据头级解码查询模式选择性获取视觉KV缓存以减少冗余。

Result: 实验结果表明，OmniSparse在保持与全注意力相当性能的同时，实现了预填充阶段2.7倍加速和解码阶段2.4倍内存减少。

Conclusion: OmniSparse框架成功解决了现有稀疏注意力方法的局限性，通过训练感知的细粒度稀疏机制，在长视频多模态大语言模型中实现了显著的加速和内存优化，同时保持了模型性能。

Abstract: Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.

</details>


### [163] [LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image](https://arxiv.org/abs/2511.12202)
*Zhuojiang Cai,Yiheng Zhang,Meitong Guo,Mingdao Wang,Yuwang Wang*

Main category: cs.CV

TL;DR: 提出LSS3D方法，通过可学习空间偏移解决多视图不一致性和非正面输入问题，实现高质量图像到3D生成


<details>
  <summary>Details</summary>
Motivation: 现有基于多视图扩散的3D生成方法存在形状和纹理不对齐问题，导致几何细节不完整和纹理重影，且对倾斜视角输入鲁棒性差

Method: 为每个视图分配可学习空间偏移参数，通过重建网格引导调整视图实现空间一致性，并加入输入视图作为额外约束增强鲁棒性

Result: 在几何和纹理评估指标上取得领先结果，对更灵活的输入视角具有更好的鲁棒性

Conclusion: LSS3D方法能有效处理多视图不一致性和非正面输入，生成具有更完整几何细节和清晰纹理的高质量3D模型

Abstract: Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.

</details>


### [164] [GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction](https://arxiv.org/abs/2511.12204)
*Jiaqi Wu,Yaosen Chen,Shuyuan Zhu*

Main category: cs.CV

TL;DR: 提出Geometry-guided Multi-View Diffusion Model，通过几何信息提取和强度调整机制，解决多视图图像生成中的一致性和高分辨率问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于单图像扩展的多视图生成方法在保持跨视图一致性和生成高分辨率输出方面面临显著计算挑战。

Method: 设计多视图几何信息提取模块（深度图、法线图、前景分割掩码），解耦几何增强注意力机制，自适应学习策略，迭代细化过程和动态几何信息强度调整机制。

Result: 生成在视图间一致且细节丰富的图像，提高了整体图像质量和细节保持能力。

Conclusion: 该模型通过几何引导有效解决了多视图图像生成中的一致性和质量挑战，在3D重建、虚拟现实等领域具有应用价值。

Abstract: Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.

</details>


### [165] [A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR](https://arxiv.org/abs/2511.12206)
*Nishant Vasantkumar Hegde,Aditi Agarwal,Minal Moharir*

Main category: cs.CV

TL;DR: 开发基于AI的交通违规自动检测系统，使用YOLOv8进行目标检测和EasyOCR进行车牌识别，可检测头盔违规、摩托车后视镜缺失等违规行为。


<details>
  <summary>Details</summary>
Motivation: 手动执法交通违规（如头盔法规和车辆安全标准）资源密集且不一致，需要自动化解决方案来提高执法效率和道路安全。

Method: 使用YOLOv8进行目标检测，EasyOCR进行车牌识别，在自定义标注数据集上训练，采用图像增强技术增加多样性，并通过Streamlit界面实现实时监控。

Result: 模型整体精度0.9147，召回率0.886，mAP@50为0.843，mAP@50-95为0.503，表明在严格IoU阈值下仍具有强检测能力。

Conclusion: 该系统为自动化交通规则执法提供了实用有效的解决方案，并讨论了实际部署的考虑因素。

Abstract: Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.

</details>


### [166] [Mixture of States: Routing Token-Level Dynamics for Multimodal Generation](https://arxiv.org/abs/2511.12207)
*Haozhe Liu,Ding Liu,Mingchen Zhuge,Zijian Zhou,Tian Xie,Sen He,Yukang Yang,Shuming Liu,Yuren Cong,Jiadong Guo,Hongyu Xu,Ke Xu,Kam-Woh Ng,Juan C. Pérez,Juan-Manuel~Pérez-Rúa,Tao Xiang,Wei Liu,Shikun Liu,Jürgen Schmidhuber*

Main category: cs.CV

TL;DR: MoS是一种新的多模态扩散模型融合范式，通过可学习的token-wise路由器在模态间创建状态交互，实现高效的多模态融合。


<details>
  <summary>Details</summary>
Motivation: 现有多模态扩散模型在模态融合方面存在效率问题，需要更灵活、计算高效的方法来精确对齐不同模态的特征。

Method: 使用可学习的token-wise路由器，基于去噪时间步和输入创建模态间隐藏状态的交互，采用top-k稀疏选择和ε-greedy训练策略。

Result: 在文本到图像生成和编辑任务中达到最先进水平，仅用3B-5B参数就能匹配或超越参数多4倍的模型。

Conclusion: MoS为多模态扩散模型提供了一种灵活且计算高效的扩展范式。

Abstract: We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $ε$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.

</details>


### [167] [FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention](https://arxiv.org/abs/2511.12215)
*Peng Zhang,Zhihui Lai,Wenting Chen,Xu Wu,Heng Kong*

Main category: cs.CV

TL;DR: FaNe是一个语义增强的医学视觉语言预训练框架，通过语义感知的正样本挖掘、文本条件稀疏注意力池化和硬负样本感知对比损失，解决了假阴性和细粒度跨模态对齐不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言预训练方法存在由语义相似文本引起的假阴性问题，以及细粒度跨模态对齐不足的局限性。

Method: 提出语义感知正样本挖掘策略（基于文本-文本相似度和自适应归一化）、文本条件稀疏注意力池化模块（通过文本引导的局部视觉表征实现细粒度图像-文本对齐）、硬负样本感知对比损失（自适应重加权语义相似负样本）。

Result: 在五个下游医学影像基准测试中，FaNe在图像分类、目标检测和语义分割任务上均达到最先进性能。

Conclusion: FaNe框架有效解决了医学视觉语言预训练中的假阴性和细粒度对齐问题，验证了所提方法的有效性。

Abstract: Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.

</details>


### [168] [Suppressing VLM Hallucinations with Spectral Representation Filtering](https://arxiv.org/abs/2511.12220)
*Ameen Ali,Tamim Zoabi,Lior Wolf*

Main category: cs.CV

TL;DR: SRF是一种无需训练的后处理方法，通过分析模型表示中的协方差结构来抑制视觉语言模型的幻觉现象，在多个VLM模型上显著减少幻觉率且不影响推理速度。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型经常产生幻觉，描述图像中不存在的对象、属性或关系，这源于对语言先验的过度依赖和不精确的跨模态对齐。

Method: 通过特征协方差的特征分解识别低秩幻觉模式，然后在深层VLM层的前馈投影权重中应用软谱滤波器来衰减这些模式，均衡特征方差同时保持语义保真度。

Result: 在LLaVA-1.5、MiniGPT-4和mPLUG-Owl2等VLM模型上，SRF在MSCOCO、POPE-VQA等视觉任务基准上持续降低幻觉率，达到最先进的忠实度且不降低描述质量。

Conclusion: SRF作为一种轻量级、无需训练的后处理方法，能有效抑制VLM的幻觉现象，无需架构修改且不产生推理开销，在各种VLM家族中表现一致。

Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.

</details>


### [169] [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)
*Dongdong Zhao,Qiben Xu,Ranxin Fang,Baogang Song*

Main category: cs.CV

TL;DR: 提出了DHMI，首个针对深度哈希的扩散式模型反演框架，能够在黑盒设置下成功重构高分辨率、高质量的图像，揭示了深度哈希系统的严重隐私风险。


<details>
  <summary>Details</summary>
Motivation: 深度哈希虽然提高了检索效率，但引入了严重的隐私风险，特别是从哈希码重构原始训练数据的能力可能导致生物特征伪造和隐私泄露。然而，针对深度哈希模型的模型反演攻击尚未被探索，其安全影响未被检验。

Method: DHMI首先对辅助数据集进行聚类得到语义哈希中心作为代理锚点，然后引入基于代理引导的去噪优化方法，利用融合分类一致性和哈希邻近度的新攻击指标动态选择候选样本，通过代理模型集群指导候选样本的细化。

Result: 在多个数据集上的实验表明，DHMI即使在最具挑战性的黑盒设置下也能成功重构高分辨率、高质量的图像，在黑盒场景下优于现有的最先进模型反演攻击方法。

Conclusion: DHMI证实了深度哈希系统固有的关键隐私风险，并展示了其在实际应用中的有效性，为深度哈希模型的安全性评估提供了重要参考。

Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.

</details>


### [170] [Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets](https://arxiv.org/abs/2511.12255)
*Huy M. Le,Dat Tien Nguyen,Phuc Binh Nguyen,Gia-Bao Le-Tran,Phu Truong Thien,Cuong Dinh,Minh Nguyen,Nga Nguyen,Thuy T. N. Nguyen,Huy Gia Ngo,Tan Nhat Nguyen,Binh T. Nguyen,Monojit Choudhury*

Main category: cs.CV

TL;DR: Fusionista2.0是一个优化的视频检索系统，通过重新设计核心模块和用户界面，将检索时间减少75%，同时提高准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: Video Browser Showdown挑战赛要求系统在严格时间限制下提供准确结果，需要开发快速高效的视频检索系统。

Method: 使用ffmpeg进行快速关键帧提取，Vintern-1B-v3.5进行多语言OCR，faster-whisper进行实时语音识别，轻量级视觉语言模型进行问答，并重新设计用户界面。

Result: 检索时间减少高达75%，准确性和用户满意度均得到提升。

Conclusion: Fusionista2.0被确认为大规模视频搜索的竞争性且用户友好的系统。

Abstract: The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.

</details>


### [171] [Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment](https://arxiv.org/abs/2511.12256)
*Tolga Demiroglu,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 提出基于MedSigLIP的提示条件框架，通过FiLM和多尺度池化注入文本先验，在LDCTIQA2023数据集上取得优异性能，超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 解决低剂量CT图像质量评估中的数据效率问题，通过文本提示引导模型学习临床意图，实现快速适应。

Method: 使用提示条件框架，通过FiLM注入文本先验，结合全局、局部和纹理感知的多尺度池化，采用成对排序损失训练。

Result: 在LDCTIQA2023数据集上获得PLCC=0.9575、SROCC=0.9561、KROCC=0.8301，超越已发表的最佳挑战提交结果。

Conclusion: 提示引导方法在医学图像质量评估中具有显著优势，能够实现数据高效学习和快速适应临床需求。

Abstract: We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.

</details>


### [172] [A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation](https://arxiv.org/abs/2511.12259)
*Puzhen Wu,Hexin Dong,Yi Lin,Yihao Ding,Yifan Peng*

Main category: cs.CV

TL;DR: 提出了一种新颖的双阶段疾病感知框架，用于从胸部X光片生成放射学报告，通过疾病感知语义标记和视觉语言对齐来提升临床准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在医学图像分析中缺乏足够的疾病感知能力和视觉语言对齐，导致忽略关键病理特征且生成报告临床准确性不足。

Method: 双阶段框架：第一阶段通过交叉注意力和多标签分类学习疾病感知语义标记，同时通过对比学习对齐视觉和语言表示；第二阶段引入疾病-视觉注意力融合模块和双模态相似性检索机制。

Result: 在CheXpert Plus、IU X-ray和MIMIC-CXR等基准数据集上实现了最先进的性能，在临床准确性和语言质量方面有显著提升。

Conclusion: 该疾病感知框架能够有效解决现有方法在胸部X光片报告生成中的局限性，为医学图像分析提供了更准确和可靠的解决方案。

Abstract: Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.

</details>


### [173] [CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2511.12263)
*Jingyao Li,Jingyun Wang,Molin Tan,Haochen Wang,Cilin Yan,Likun Shi,Jiayin Cai,Xiaolong Jiang,Yao Hu*

Main category: cs.CV

TL;DR: 提出了首个跨视频推理基准CrossVid，用于全面评估多模态大语言模型在多个视频间进行时空推理的能力，包含4个维度10个任务、5,331个视频和9,015个问答对。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准主要关注单视频分析，无法评估模型在多个视频间聚合和比较信息的能力。虽然近期有基准评估多视角视频，但任务有限，无法全面评估真实世界跨视频推理场景。

Method: 构建CrossVid基准，涵盖广泛的分层任务（4个高级维度和10个具体任务），提供5,331个视频和9,015个具有挑战性的问答对，支持单选、多选和开放性问题格式。

Result: 在各类开源和闭源MLLMs上的实验显示，Gemini-2.5-Pro表现最佳，平均准确率为50.4%。深度案例分析表明当前大多数MLLMs在跨视频推理任务上表现不佳，主要原因是无法整合或比较分布在多个视频中的证据。

Conclusion: CrossVid基准突显了当前MLLMs在跨视频推理能力上的局限性，为未来增强MLLMs的跨视频推理能力提供了指导和潜力。

Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.

</details>


### [174] [ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks](https://arxiv.org/abs/2511.12267)
*Ruixun Liu,Bowen Fu,Jiayi Song,Kaiyu Li,Wanchen Li,Lanxuan Xue,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 提出了ZoomEarth框架，通过主动感知范式处理超高分辨率遥感图像，使用自适应裁剪缩放和区域引导奖励机制，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有动态分辨率和token剪枝方法受限于被动感知范式，在处理更精细视觉输入时会产生更多冗余。需要探索主动感知范式，让模型能够重新访问信息丰富的区域。

Method: 提出ZoomEarth自适应裁剪缩放框架，采用区域引导奖励机制，通过监督微调和组相对策略优化进行训练。

Result: 在LRS-GRO基准测试和三个公共UHR遥感基准测试的零样本设置中达到最先进性能，并能通过简单工具接口与下游模型无缝集成。

Conclusion: ZoomEarth框架展示了强大的多功能性和可扩展性，能够有效处理超高分辨率遥感图像，并为各种下游任务提供支持。

Abstract: Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.

</details>


### [175] [TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation](https://arxiv.org/abs/2511.12270)
*Yaxuan Jiao,Qing Xu,Yuxiang Luo,Xiangjian He,Zhen Chen,Wenting Duan*

Main category: cs.CV

TL;DR: TM-UNet是一种轻量级医学图像分割框架，通过多尺度token-memory块将2D空间特征转换为token序列，利用矩阵记忆单元选择性保留和传播上下文信息，实现线性复杂度的长距离依赖建模。


<details>
  <summary>Details</summary>
Motivation: 尽管基于transformer的方法在医学图像分割中取得了显著成果，但其高计算成本阻碍了临床部署。需要开发更高效的轻量级分割框架。

Method: 提出多尺度token-memory(MSTM)块，通过空间扫描将2D特征转换为token序列，使用矩阵记忆单元选择性保留上下文信息，结合指数门控识别token有效性，通过并行池化操作实现多尺度上下文提取。

Result: TM-UNet在多种医学分割任务中优于最先进方法，同时显著降低计算成本。

Conclusion: TM-UNet通过创新的token-memory机制实现了高效的全局推理，为医学图像分割提供了一种计算效率高的解决方案。

Abstract: Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.

</details>


### [176] [D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs](https://arxiv.org/abs/2511.12280)
*Shuochen Chang,Xiaofeng Zhang,Qingyang Liu,Li Niu*

Main category: cs.CV

TL;DR: D³ToM是一种用于加速扩散多模态大语言模型推理的动态令牌合并方法，通过在每个去噪步骤中动态合并冗余视觉令牌来减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 扩散多模态大语言模型虽然具有强大的非自回归生成能力，但推理速度显著慢于自回归模型，因为每个去噪步骤都需要对整个序列进行双向自注意力计算，导致立方级的解码复杂度。

Method: 提出基于决策器引导的动态令牌合并方法，使用前一个去噪步骤生成的令牌构建重要性映射，保留最显著的令牌并通过相似性聚合合并其余令牌，同时采用随去噪步骤动态变化的合并比例。

Result: 实验表明D³ToM在保持竞争性性能的同时显著加速了推理过程。

Conclusion: D³ToM是一种即插即用的模块，能够在不改变模型参数的情况下物理缩短视觉令牌序列，为扩散多模态大语言模型提供了高效的推理加速解决方案。

Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.

</details>


### [177] [One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving](https://arxiv.org/abs/2511.12291)
*Andrea Bertogalli,Giacomo Boracchi,Luca Magri*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态外参标定框架，可同时估计事件相机、LiDAR和RGB相机之间的相对位姿，特别关注具有挑战性的事件相机标定。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶场景中，精确的多传感器对齐至关重要，现有方法通常依赖分离的成对标定过程，无法实现一次性联合标定。

Method: 设计并构建了新型3D标定靶标，包含平面特征、ChArUco图案和主动LED模式，分别针对LiDAR、RGB相机和事件相机的特性。通过这一独特设计实现一次性联合外参标定。

Result: 在定制数据集上进行了广泛实验评估，使用先进的自动驾驶传感器设置记录数据，验证了方法的准确性和鲁棒性。

Conclusion: 该方法能够准确标定自动驾驶中的复杂视觉系统，为多传感器融合提供了可靠的标定解决方案。

Abstract: We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.

</details>


### [178] [Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method](https://arxiv.org/abs/2511.12301)
*Chi Liu,Jincheng Liu,Congcong Zhu,Minghao Wang,Sheng Shen,Jia Gu,Tianqing Zhu,Wanlei Zhou*

Main category: cs.CV

TL;DR: 本文提出了频率重校准(FreRec)方法来解决医学图像生成数据增强中的频率失配问题，通过统计高频替换和重建高频映射来改善生成图像质量，提升下游分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 医学AI开发依赖大数据集但常面临数据稀缺问题。生成数据增强(GDA)可合成医学图像，但存在偏见风险，特别是频率失配可能导致生成图像引入有害特征并损害下游任务。

Method: 提出FreRec方法：1) 统计高频替换(SHR)粗略对齐高频分量；2) 重建高频映射(RHM)提升图像质量并重建高频细节。该方法作为独立后处理步骤，兼容任何生成模型。

Result: 在脑部MRI、胸部X光和眼底图像等多种医学数据集上的实验表明，FreRec相比未校准的AI合成样本显著提升了医学图像分类性能。

Conclusion: FreRec能有效减少频率分布差异，改善生成数据增强的可靠性，且作为独立模块可无缝集成到常见医学GDA流程中。

Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.

</details>


### [179] [LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors](https://arxiv.org/abs/2511.12304)
*Qifeng Chen,Jiarun Liu,Rengan Xie,Tao Tang,Sicong Du,Yiru Zhao,Yuchi Huo,Sheng Yang*

Main category: cs.CV

TL;DR: LiDAR-GS++是一种基于高斯泼溅的LiDAR重建方法，通过扩散先验增强，用于公共城市道路的实时高保真重模拟，解决了单次扫描重建不完整导致的视点外推伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于高斯泼溅的LiDAR渲染方法在视点外推时会出现伪影，这是由于单次遍历扫描的重建不完整导致的。

Method: 引入可控的LiDAR生成模型，以粗略外推渲染为条件生成额外的几何一致扫描，并采用有效的蒸馏机制进行扩展重建。

Result: 在多个公共数据集上的实验表明，LiDAR-GS++在插值和外推视点上都达到了最先进的性能，超越了现有的GS和NeRF方法。

Conclusion: 通过将重建扩展到欠拟合区域，该方法确保了外推新视点的全局几何一致性，同时保留了传感器捕获的详细场景表面。

Abstract: Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.

</details>


### [180] [Learning Time in Static Classifiers](https://arxiv.org/abs/2511.12321)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 提出了一种简单有效的框架，为前馈分类器添加时间推理能力，无需修改模型架构或引入循环模块。通过SEQ学习范式将训练数据组织成时间连贯的轨迹，学习类别特定的时间原型，并使用可微分的soft-DTW损失对齐预测序列。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视觉数据通常随时间逐渐演变（如姿态、光照、物体状态或场景上下文的变化），但传统分类器基于时间独立性假设训练，无法捕捉这种动态特性。

Method: 采用支持-范例-查询（SEQ）学习范式，将训练数据组织成时间连贯的轨迹。学习类别特定的时间原型，通过可微分的soft-DTW损失对齐预测序列，并使用多目标函数促进语义一致性和时间平滑性。

Result: 该方法在静态和时序任务中都表现优异：提升了细粒度和超细粒度图像分类的性能，并在视频异常检测中提供了精确、时间一致的预测。

Conclusion: 尽管方法简单，但通过损失设计引入了强时间归纳偏差，以模块化和数据高效的方式桥接了静态和时序学习，仅需在预提取特征上使用简单分类器。

Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.

</details>


### [181] [SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models](https://arxiv.org/abs/2511.12331)
*Sepehr Kazemi Ranjbar,Kumail Alhamoud,Marzyeh Ghassemi*

Main category: cs.CV

TL;DR: 提出一种无需训练的框架，将否定建模为嵌入空间中的子空间而非单点，显著提升视觉语言模型的否定理解能力


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理否定提示时表现不佳，现有方法通过微调大型否定数据集会损害模型在肯定提示上的零样本性能

Method: 将CLIP等模型的嵌入空间划分为语义一致的子空间，围绕A和N的嵌入构建两个球面帽，通过靠近A且远离N的区域中心方向来评分图像

Result: 在检索、多项选择和文本到图像任务中，否定理解能力平均提升约30%，缩小了肯定和否定提示之间的差距

Conclusion: 该方法在保持零样本性能的同时有效解决了视觉语言模型的否定理解问题，无需重新训练模型

Abstract: Vision-Language Models (VLMs) struggle with negation. Given a prompt like "retrieve (or generate) a street scene without pedestrians," they often fail to respect the "not." Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as "A but not N," we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.

</details>


### [182] [Ground Plane Projection for Improved Traffic Analytics at Intersections](https://arxiv.org/abs/2511.12342)
*Sajjad Pakdamansavoji,Kumar Vaibhav Jha,Baher Abdulhai,James H Elder*

Main category: cs.CV

TL;DR: 通过将基础设施摄像头检测到的车辆反投影到地面平面进行3D坐标分析，可以提高转弯运动计数的准确性，特别是多摄像头弱融合方法效果更佳。


<details>
  <summary>Details</summary>
Motivation: 准确的交叉口转弯运动计数对信号控制、交通管理和城市规划至关重要，现有基于图像平面的计算机视觉系统存在局限性。

Method: 将单/多基础设施摄像头检测到的车辆反投影到地面平面，在真实世界3D坐标中分析轨迹分类和转弯运动计数，并采用多摄像头弱融合方法。

Result: 单摄像头反投影比图像平面分析更准确，多摄像头弱融合能获得更高精度。

Conclusion: 交通分析应在地面平面而非图像平面上进行，反投影方法能显著提升转弯运动计数准确性。

Abstract: Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane

</details>


### [183] [CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification](https://arxiv.org/abs/2511.12346)
*Asmit Bandyopadhyay,Anindita Das Bhattacharjee,Rakesh Das*

Main category: cs.CV

TL;DR: CLAReSNet是一种混合架构，结合多尺度卷积提取和Transformer式注意力机制，通过自适应潜在瓶颈解决高光谱图像分类中的高维度和样本不平衡问题，在Indian Pines和Salinas数据集上达到99.71%和99.96%的准确率。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高光谱维度、复杂光谱-空间相关性和有限训练样本（特别是严重类别不平衡）等关键挑战。CNN擅长局部特征提取，Transformer捕获长程依赖，但单独应用效果欠佳。

Method: 提出CLAReSNet混合架构：多尺度卷积主干+深度残差块+增强卷积注意力模块提取层次空间特征；光谱编码层结合双向RNN和多尺度光谱潜在注意力（MSLA），通过自适应潜在令牌分配将复杂度从O(T²D)降至O(Tlog(T)D)；层次交叉注意力融合动态聚合多级表示。

Result: 在Indian Pines和Salinas数据集上分别达到99.71%和99.96%的总体准确率，显著超越HybridSN、SSRN和SpectralFormer等现有方法。

Conclusion: CLAReSNet在有限样本和严重类别不平衡条件下表现出色，学习到的嵌入具有优越的类间可分性和紧凑的类内聚类，验证了该方法的有效性。

Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.

</details>


### [184] [Explainable AI-Generated Image Detection RewardBench](https://arxiv.org/abs/2511.12363)
*Michael Yang,Shijian Deng,William T. Doan,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CV

TL;DR: 提出了首个评估多模态大语言模型在AI生成图像检测解释质量判断能力的基准XAIGID-RewardBench，包含约3000个标注三元组，结果显示当前最佳模型得分88.76%，与人类98.30%的一致性存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 传统基于分类的AI生成图像检测方法无法提供人类专家可理解的解释，降低了检测工具的可信度和说服力。虽然MLLMs被用于生成解释，但它们在评估自身或其他MLLMs生成解释质量方面的能力尚未得到充分研究。

Method: 构建包含约3000个标注三元组的基准数据集，这些数据来自不同图像生成模型和作为检测器的MLLMs，用于评估当前MLLMs作为奖励模型（评判者）的能力。

Result: 当前最佳奖励模型在该基准上得分为88.76%，而人类标注者间一致性达到98.30%，表明当前MLLMs的推理能力与人类水平仍存在明显差距。

Conclusion: 需要进一步改进MLLMs的推理能力以缩小与人类水平的差距，该基准为评估和提升MLLMs在AI生成图像检测解释质量判断方面的能力提供了重要工具。

Abstract: Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.

</details>


### [185] [Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.12365)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: DT-R1是一个强化学习框架，通过训练大语言模型构建复杂多模态视觉输入的数字孪生表示，并基于这些高级表示进行统一视觉推理。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法依赖特定任务的监督微调和架构设计，导致无法统一解决方案，限制了跨任务和跨模态的泛化能力。

Method: 使用GRPO强化学习算法训练大语言模型构建数字孪生表示，并采用验证结构完整性和输出准确性的新型奖励函数。

Result: 在六个视觉推理基准测试中（涵盖两种模态和四种任务类型），DT-R1始终优于最先进的特定任务模型。

Conclusion: DT-R1开创了基于数字孪生表示的强化学习实现视觉推理的新方向。

Abstract: Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.

</details>


### [186] [Fast Reasoning Segmentation for Images and Videos](https://arxiv.org/abs/2511.12368)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: FastReasonSeg是一种通过数字孪生表示实现高效推理分割的方法，通过解耦感知与推理，使用监督微调和强化学习进行知识蒸馏，在保持高性能的同时大幅减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法需要数十亿参数的多模态大语言模型，超出了边缘设备的计算能力，而传统的蒸馏方法无法有效传递多步推理能力。

Method: 使用数字孪生表示解耦感知与推理，先通过教师模型生成的推理链进行监督微调，然后使用联合奖励（分割精度和推理质量对齐）进行强化微调。

Result: 在四个基准测试中达到最先进的推理分割性能，0.6B参数的蒸馏变体优于20倍参数的模型，实现7.79 FPS吞吐量和仅2.1GB内存消耗。

Conclusion: 该方法实现了高效的实时推理分割，能够在资源受限的环境中部署，为具身智能系统在真实世界环境中的自主操作提供了可行方案。

Abstract: Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.

</details>


### [187] [Changes in Real Time: Online Scene Change Detection with Multi-View Fusion](https://arxiv.org/abs/2511.12370)
*Chamuditha Jayanga Galappaththige,Jason Lai,Lloyd Windrim,Donald Dansereau,Niko Sünderhauf,Dimity Miller*

Main category: cs.CV

TL;DR: 提出首个姿态无关、无标签、多视角一致的在线场景变化检测方法，在10+FPS下超越现有离线方法的性能


<details>
  <summary>Details</summary>
Motivation: 现有在线场景变化检测方法精度远低于离线方法，需要开发能在无约束视角下实时检测相关变化的高性能在线方法

Method: 引入自监督融合损失从多线索和观测推断场景变化，基于PnP的快速姿态估计，以及针对3D高斯溅射场景表示的快速变化引导更新策略

Result: 在复杂真实世界数据集上的广泛实验表明，该方法超越了在线和离线基线方法，实现了新的最先进性能

Conclusion: 该方法首次实现了姿态无关、无标签、多视角一致的在线场景变化检测，在保持实时性的同时超越了离线方法的性能

Abstract: Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.

</details>


### [188] [Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models](https://arxiv.org/abs/2511.12371)
*Yiqing Shen,Chenxiao Fan,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: 提出推理文本到视频检索新范式，通过数字孪生表示和LLM推理处理隐含查询，在专用基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频检索方法只能处理显式查询，无法应对需要推理的隐含查询，限制了实际应用场景。

Method: 两阶段框架：首先通过组合对齐识别候选视频，然后使用LLM推理和即时细化处理信息缺口，采用数字孪生表示视频内容。

Result: 在ReasonT2VBench-135上达到81.2% R@1，比最强基线提升50多个百分点，同时在三个传统基准上达到SOTA。

Conclusion: 数字孪生表示和LLM推理能有效处理隐含查询，显著提升文本到视频检索性能，为复杂推理任务提供新思路。

Abstract: The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).

</details>


### [189] [AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification](https://arxiv.org/abs/2511.12382)
*Ansh Makwe,Akansh Agrawal,Prateek Jain,Akshan Agrawal,Priyanka Bagade*

Main category: cs.CV

TL;DR: 提出了AGGRNet框架，通过提取信息性和非信息性特征来理解细粒度视觉模式，改善复杂医学图像分析任务的分类性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析面临复杂视觉模式、标记数据稀缺和专家解释差异等挑战，现有注意力模型难以有效区分细微类别，导致错误诊断。

Method: 提出AGGRNet框架，提取信息性和非信息性特征，以更好地理解细粒度视觉模式并改进分类。

Result: 在多个医学影像数据集上达到最先进性能，在Kvasir数据集上比SOTA模型提升高达5%。

Conclusion: AGGRNet能有效解决医学图像分析中的细粒度分类挑战，显著提升分类性能。

Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.

</details>


### [190] [Leveraging Quantum-Based Architectures for Robust Diagnostics](https://arxiv.org/abs/2511.12386)
*Shabnam Sodagari,Tommy Long*

Main category: cs.CV

TL;DR: 本研究开发了一种混合量子-经典框架，使用CT图像诊断和区分肾结石、囊肿和肿瘤，结合预训练的ResNet50编码器和量子卷积神经网络，在8量子位和12量子位配置下均达到99%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算增强医学影像诊断性能，特别是针对肾脏病变（结石、囊肿、肿瘤）的精确分类和鉴别诊断。

Method: 采用混合量子-经典框架：预处理的CT图像通过预训练ResNet50提取特征，然后通过角度编码转换为量子位，由量子卷积神经网络处理。使用去噪和对比度受限自适应直方图均衡化进行图像预处理，通过数据增强和加权采样解决类别不平衡问题。

Result: 两种架构均快速收敛且学习曲线稳定，训练和验证性能高度一致。测试准确率达0.99，12量子位配置在囊肿和肿瘤检测方面表现更优：囊肿达到完美召回率，肿瘤F1分数为0.9956。混淆矩阵分析显示所有类别分类可靠，误分类极少。

Conclusion: 将经典预处理和深度特征提取与量子电路相结合，能够显著提升医学诊断性能，量子辅助诊断在肾脏病变分类中表现出色。

Abstract: The objective of this study is to diagnose and differentiate kidney stones, cysts, and tumors using Computed Tomography (CT) images of the kidney. This study leverages a hybrid quantum-classical framework in this regard. We combine a pretrained ResNet50 encoder, with a Quantum Convolutional Neural Network (QCNN) to explore quantum-assisted diagnosis. We pre-process the kidney images using denoising and contrast limited adaptive histogram equalization to enhance feature extraction. We address class imbalance through data augmentation and weighted sampling. Latent features extracted by the encoder are transformed into qubits via angle encoding and processed by a QCNN. The model is evaluated on both 8-qubit and 12-qubit configurations. Both architectures achieved rapid convergence with stable learning curves and high consistency between training and validation performance. The models reached a test accuracy of 0.99, with the 12-qubit configuration providing improvements in overall recall and precision, particularly for Cyst and Tumor detection, where it achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix analysis further confirmed reliable classification behavior across all classes, with very few misclassifications. Results demonstrate that integrating classical pre-processing and deep feature extraction with quantum circuits enhances medical diagnostic performance.

</details>


### [191] [Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation](https://arxiv.org/abs/2511.12389)
*Divake Kumar,Patrick Poggi,Sina Tayebati,Devashri Naik,Nilesh Ahuja,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: 提出了一种轻量级推理时间框架UGITS，通过分解特征空间中的偶然不确定性和认知不确定性，实现无需采样、集成或额外前向传播的可靠不确定性估计，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 大多数估计器将所有不确定性模式压缩为单一置信度分数，无法可靠地指导何时分配更多计算或调整推理过程。

Method: 使用正则化全局密度模型估计偶然不确定性，通过局部支持不足、流形谱塌缩和跨层特征不一致三个互补组件形成认知不确定性，这些组件经验正交且无需额外计算。

Result: 在MOT17上减少约60%计算成本且精度损失可忽略，不确定性分解比总不确定性基线提高13.6个百分点的计算节省。

Conclusion: 该方法实现了实用的自调节视觉推理，通过正交不确定性分解显著提升计算效率。

Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.

</details>


### [192] [DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452)
*Xiaoyu Lin,Aniket Ghorpade,Hansheng Zhu,Justin Qiu,Dea Rrozhani,Monica Lama,Mick Yang,Zixuan Bian,Ruohan Ren,Alan B. Hong,Jiatao Gu,Chris Callison-Burch*

Main category: cs.CV

TL;DR: DenseAnnotate是一个音频驱动的在线标注平台，通过语音标注创建密集细粒度的图像和3D资产标注，解决了传统文本标注在表达性和效率上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型训练数据主要依赖稀疏的互联网挖掘或手动输入标注，无法充分捕捉图像视觉内容。密集标注更有价值但稀缺，传统文本标注方法在表达性、速度和专业领域（如多元文化图像和3D资产）存在明显不足。

Method: 开发音频驱动标注平台，标注者通过语音描述观察内容，同时将语音短语与图像区域或3D场景部分同步关联。平台整合语音转文字和注意力区域标记功能。

Result: 通过1,000多名标注者在两个领域的案例研究，创建了包含3,531张图像、898个3D场景、7,460个3D对象的多模态数据集，涵盖20种语言的音频对齐密集标注。基于该数据集训练的模型在多语言能力上提升5%，文化对齐提升47%，3D空间能力提升54%。

Conclusion: 该平台为未来视觉语言研究提供了可行的标注方法，可应用于各种任务和数据类型，显著提升了模型在专业领域的能力表现。

Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.

</details>


### [193] [MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting](https://arxiv.org/abs/2511.12400)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: MSLoRA是一种与主干网络无关的参数高效适配器，通过重新加权特征响应而非重新调整主干网络来实现迁移学习，适用于CNN和ViT架构。


<details>
  <summary>Details</summary>
Motivation: 现有的低秩适应方法主要局限于视觉变换器(ViTs)，难以跨架构泛化，需要一种统一的方法来适应CNN和ViT。

Method: 结合低秩线性投影和多尺度非线性变换，通过点乘和残差连接融合，联合调制空间和通道注意力，保持预训练权重冻结。

Result: 在分类、检测和分割任务上持续提升迁移性能，仅需不到5%的主干参数，实现稳定优化、快速收敛和强跨架构泛化。

Conclusion: MSLoRA通过重新加权而非重新调整，为冻结视觉主干的高效适应提供了简单通用的方法。

Abstract: We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather
  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision
  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and
  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly
  modulates spatial and channel attention. The two components are fused through pointwise multiplication and
  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained
  weights frozen.
  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,
  detection, and segmentation tasks with roughly less than 5\% of backbone parameters.
  The design further enables stable optimization, fast convergence, and strong cross-architecture
  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach
  for efficient adaptation of frozen vision backbones.

</details>


### [194] [Co-Layout: LLM-driven Co-optimization for Interior Layout](https://arxiv.org/abs/2511.12474)
*Chucheng Xiang,Ruchao Bao,Biyin Feng,Wenzheng Wu,Zhongyuan Liu,Yirui Guan,Ligang Liu*

Main category: cs.CV

TL;DR: 提出结合大语言模型和网格整数规划的自动化室内设计框架，联合优化房间布局和家具摆放。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段设计流程效果不佳，需要开发能够联合优化房间布局和家具摆放的自动化设计方法。

Method: 使用LLM提取设计约束，编码到基于Modulor的网格表示中，采用粗到细优化策略，从低分辨率网格开始逐步细化。

Result: 在多种场景下，联合优化方法显著优于现有两阶段设计流程，且通过粗到细策略实现了计算效率提升。

Conclusion: 该框架成功实现了房间布局和家具摆放的联合优化，在质量和效率方面均有显著改进。

Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.

</details>


### [195] [VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving](https://arxiv.org/abs/2511.12405)
*Hyunki Seong,Seongwoo Moon,Hojin Ahn,Jehun Kang,David Hyunchul Shim*

Main category: cs.CV

TL;DR: 提出VLA-R框架，通过整合开放世界感知和视觉-动作检索范式，实现端到端自动驾驶在非结构化户外环境中的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶在非结构化户外环境中常遇到训练时未见的情况，需要强大的泛化能力来应对开放世界场景。

Method: 使用冻结的视觉语言模型进行开放世界检测和分割，通过Q-Former瓶颈聚合细粒度视觉表示和语言对齐特征，引入视觉-动作对比学习方案对齐视觉语言和动作嵌入。

Result: 在真实机器人平台上实验表明，即使在有限数据情况下，在非结构化、未见环境中也表现出强大的泛化和探索性能。

Conclusion: VLA-R框架成功整合了开放世界感知和视觉-动作检索，为端到端自动驾驶在开放世界环境中的泛化问题提供了有效解决方案。

Abstract: Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.

</details>


### [196] [Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection](https://arxiv.org/abs/2511.12410)
*Xi Xiao,Zhuxuanzi Wang,Mingqiao Mo,Chen Liu,Chenrui Ma,Yanshu Li,Smita Krishnaswamy,Xiao Wang,Tianyang Wang*

Main category: cs.CV

TL;DR: PROBE是一个自监督框架，通过视觉探测目标域无需标签，使用自监督提示增强模块和域感知提示对齐目标，在四个基准测试中优于监督、自监督和适应基线方法。


<details>
  <summary>Details</summary>
Motivation: 自动化路面缺陷检测的部署常因跨域泛化能力差而受阻。监督检测器需要昂贵的新环境重新标注，而标准自监督方法捕获通用特征但仍易受域偏移影响。

Method: 提出PROBE框架：1）自监督提示增强模块(SPEM)，从无标签目标数据中提取缺陷感知提示来指导冻结的ViT骨干网络；2）域感知提示对齐(DAPA)目标，对齐提示条件下的源域和目标域表示。

Result: 在四个具有挑战性的基准测试中，PROBE始终优于强监督、自监督和适应基线方法，实现了稳健的零样本迁移、对域变化的强韧性，以及在少样本适应中的高数据效率。

Conclusion: 自监督提示是构建可扩展和自适应视觉检测系统的实用方向。

Abstract: The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main

</details>


### [197] [Towards Rotation-only Imaging Geometry: Rotation Estimation](https://arxiv.org/abs/2511.12415)
*Xinrui Li,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于旋转流形的旋转优化框架，用于从运动恢复结构(SfM)，通过将平移表示为旋转的函数，将成像几何表示压缩到旋转流形上，在双视图和多视图场景中实现了比现有方法更优越的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的仅姿态成像几何通过姿态调整展示了更好的SfM性能，本文继续从仅姿态视角出发，探索场景结构、旋转和平移之间的关键关系，旨在实现更准确、高效和可靠的3D视觉计算。

Method: 提出基于重投影误差的旋转优化框架，将平移表示为旋转的函数，将成像几何表示压缩到旋转流形上，适用于双视图和多视图场景。

Result: 实验结果表明，该方法在旋转估计精度和鲁棒性方面优于当前最先进的方法，甚至可与多次束调整迭代结果相媲美。

Conclusion: 这项工作通过旋转流形上的优化框架，为3D视觉计算贡献了更准确、高效和可靠的方法，有望推动SfM技术的进一步发展。

Abstract: Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.

</details>


### [198] [Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance](https://arxiv.org/abs/2511.12419)
*Wenjie Li,Jinglei Shi,Jin Han,Heng Guo,Zhanyu Ma*

Main category: cs.CV

TL;DR: DHGM是一个基于扩散模型的高频引导模型，通过整合预训练扩散先验和高通滤波器，同时去除雨滴伪影并增强结构细节，解决了天气恢复和超分辨率之间的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 真实世界图像常受恶劣天气影响而退化，但现有的天气恢复方法可能会牺牲对分析小物体至关重要的高频细节。简单地级联恢复和超分辨率方法存在内在冲突：恢复旨在去除高频天气噪声，而超分辨率旨在从现有细节中生成高频纹理。

Method: 提出DHGM模型，以去雨为例，整合预训练扩散先验与高通滤波器，同时进行雨滴伪影去除和结构细节增强。

Result: 大量实验表明，DHGM在性能上优于现有方法，且成本更低。

Conclusion: DHGM能够有效生成干净且高分辨率的图像，解决了天气恢复和超分辨率之间的冲突问题。

Abstract: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.

</details>


### [199] [MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation](https://arxiv.org/abs/2511.12422)
*Nuolin Sun,Linyuan Wang,Haonan Wei,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: MFI-ResNet通过压缩-扩展策略，将ResNet中的多层结构简化为MeanFlow模块，在减少参数的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 受流匹配模型MeanFlow启发，探索生成式流场如何有效表征ResNet中的特征变换过程，为理解生成建模与判别学习的关系提供新视角。

Method: 采用压缩-扩展策略：压缩阶段将每个ResNet阶段的多层结构简化为1-2个MeanFlow模块构建轻量元模型；扩展阶段对前三个阶段进行选择性孵化，使其匹配基线ResNet的残差块配置，最后一个阶段保持MeanFlow形式并进行微调。

Result: 在CIFAR-10和CIFAR-100数据集上，相比ResNet-50，MFI-ResNet分别减少参数46.28%和45.59%，同时准确率分别提升0.23%和0.17%。

Conclusion: 生成式流场能够有效表征ResNet中的特征变换过程，为理解生成建模与判别学习的关系提供了新视角，证明了参数效率和判别性能可以同时提升。

Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.

</details>


### [200] [RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning](https://arxiv.org/abs/2511.12428)
*Jingqi Xu,Jingxi Lu,Chenghao Li,Sreetama Sarkar,Souvik Kundu,Peter A. Beerel*

Main category: cs.CV

TL;DR: 提出了RedVTP方法，通过响应驱动的视觉令牌剪枝策略提升扩散视觉语言模型的推理效率，在不损失准确性的情况下显著提高吞吐量和降低延迟


<details>
  <summary>Details</summary>
Motivation: 扩散视觉语言模型虽然支持并行令牌解码，但大量视觉令牌严重阻碍了推理效率，而视觉令牌剪枝在自回归VLMs中已有研究，但在DVLMs中仍待探索

Method: 利用DVLMs的推理动态，通过掩码响应令牌的注意力来估计视觉令牌重要性，基于重要性分数在步骤间保持一致的观察，在第一步推理后剪枝不重要的视觉令牌

Result: RedVTP将LLaDA-V和LaViDa的令牌生成吞吐量分别提升至186%和28.05%，推理延迟分别降低64.97%和21.87%，且在某些情况下还能提高准确性

Conclusion: RedVTP是一种有效的视觉令牌剪枝策略，能显著提升DVLMs的推理效率，同时保持甚至改善模型性能

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.

</details>


### [201] [Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion](https://arxiv.org/abs/2511.12432)
*Xilai Li,Xiaosong Li,Weijun Jiang*

Main category: cs.CV

TL;DR: 提出了UP-Fusion框架，通过通道扰动和预训练知识集成来解决多模态图像融合中的梯度冲突问题，在保持模态区分性的同时提高融合质量。


<details>
  <summary>Details</summary>
Motivation: 统一模型在多模态图像融合中因模态差异大导致梯度冲突，而引入模态特定编码器又降低了跨任务泛化能力，需要平衡融合质量和泛化性能。

Method: 1. 语义感知通道剪枝模块(SCPM)：利用预训练模型的语义感知能力过滤和增强多模态特征通道；2. 几何仿射调制模块(GAM)：使用原始模态特征对初始融合特征进行仿射变换；3. 文本引导通道扰动模块(TCPM)：在解码过程中重塑通道分布。

Result: 大量实验表明，该算法在多模态图像融合和下游任务上均优于现有方法。

Conclusion: UP-Fusion框架通过通道扰动和预训练知识集成，有效解决了多模态图像融合中的梯度冲突问题，在保持模态区分性的同时提高了融合质量和泛化能力。

Abstract: Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.

</details>


### [202] [Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning](https://arxiv.org/abs/2511.12438)
*ANK Zaman,Prosenjit Chatterjee,Rajat Sharma*

Main category: cs.CV

TL;DR: 开发基于深度卷积神经网络和OpenCV的实时驾驶员疲劳检测系统，通过面部特征识别疲劳状态，准确率分别达到99.6%和97%。


<details>
  <summary>Details</summary>
Motivation: 长途驾驶容易导致驾驶员疲劳，疲劳驾驶对个人和其他道路使用者构成严重安全威胁，需要实时检测系统来预防事故。

Method: 使用实时摄像头采集驾驶员面部图像，通过OpenCV分析面部特征（如眼睛开合度和打哈欠动作），利用预训练的深度卷积神经网络模型检测疲劳状态。

Result: 在NTHU-DDD和Yawn-Eye数据集上分别实现了99.6%和97%的疲劳检测分类准确率，系统能够实时发出警报。

Conclusion: 该方法提供了一种非侵入式、低成本且有效的疲劳检测方案，有助于预防道路事故，保护生命安全。

Abstract: A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.

</details>


### [203] [CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training](https://arxiv.org/abs/2511.12446)
*Jiahe Qian,Yuhao Shen,Zhangtianyi Chen,Juexiao Zhou,Peisong Wang*

Main category: cs.CV

TL;DR: CoTBox-TTT是一种测试时训练方法，通过视觉思维链信号识别问题相关区域，并保持答案在原始图像和局部裁剪中的一致性，无需额外标签即可提升医疗VQA的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗视觉问答系统在领域偏移下可靠性不足，产生与图像证据弱相关的答案，且重新训练或添加标签在部署时不切实际。

Method: 采用证据优先的测试时训练方法，在推理时适配视觉语言模型但保持所有骨干网络冻结，仅更新少量连续软提示，通过视觉思维链识别问题相关区域并确保答案一致性。

Result: 在医疗VQA实验中，该方法实用性强，例如在LLaVA模型上添加CoTBox-TTT后，在pathVQA上的闭端准确率提高了12.3%。

Conclusion: CoTBox-TTT是一种无需标签、即插即用的方法，能够有效提升医疗VQA系统在真实部署中的可靠性。

Abstract: Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.

</details>


### [204] [MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2511.12449)
*Zhanheng Nie,Chenghan Fu,Daoze Zhang,Junxian Wu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON2.0是一个动态模态平衡的多模态表示学习框架，通过模态驱动的专家混合模块、双级对齐方法和基于MLLM的图像-文本协同增强策略，解决了电商多模态理解中的模态不平衡、内在对齐关系利用不足和数据噪声问题。


<details>
  <summary>Details</summary>
Motivation: 解决电商多模态理解中的三个关键挑战：(i) 模态混合训练导致的模态不平衡；(ii) 产品内视觉和文本信息内在对齐关系的利用不足；(iii) 电商多模态数据中噪声处理的局限性。

Method: 1. 模态驱动的专家混合模块自适应处理输入样本；2. 双级对齐方法利用产品内语义对齐特性；3. 基于MLLM的图像-文本协同增强策略结合动态样本过滤。

Result: 在MBE2.0基准和多个公共数据集上实现了最先进的零样本性能，注意力热图可视化提供了改进多模态对齐的定性证据。

Conclusion: MOON2.0框架有效解决了电商多模态理解的关键挑战，在多个基准测试中表现出色，为电商产品理解提供了强大的多模态表示学习解决方案。

Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

</details>


### [205] [MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning](https://arxiv.org/abs/2511.12480)
*Jingshan Hong,Haigen Hu,Huihuang Zhang,Qianwei Zhou,Zhao Li*

Main category: cs.CV

TL;DR: 提出MaskAnyNet，将图像掩码内容作为辅助知识而非丢弃，通过重新学习机制同时利用可见和掩码信息，提升语义多样性和细粒度细节保留。


<details>
  <summary>Details</summary>
Motivation: 传统图像掩码存在像素利用不足和关键特征丢失问题，而掩码图像建模显示掩码区域具有与原图的强上下文一致性，表明掩码区域可作为语义多样性来源。

Method: 提出MaskAnyNet，结合掩码和重新学习机制，通过额外分支从重新组合的掩码区域联合学习，利用掩码区域的语义多样性来丰富特征。

Result: 在CNN和Transformer骨干网络上的实验显示，在多个基准测试中均获得一致提升，进一步分析证实该方法通过重用掩码内容提高了语义多样性。

Conclusion: 将掩码内容视为辅助知识而非忽略，能够有效利用掩码区域的语义多样性，提升模型性能并保留细粒度细节。

Abstract: In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.

</details>


### [206] [Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion](https://arxiv.org/abs/2511.12498)
*Jongseong Bae,Junwoo Ha,Jinnyeong Heo,Yeongin Lee,Ha Young Kim*

Main category: cs.CV

TL;DR: 提出了C3DFusion模块，通过显式对齐当前帧和历史帧的3D提升点特征，生成隐藏区域感知的3D特征几何，显著提升了相机基3D语义场景补全在边界区域的重建能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于相机的3D语义场景补全方法主要关注增强帧内区域，但在重建自车侧面的关键帧外区域方面表现不佳，尽管历史帧通常包含这些不可见区域的宝贵上下文信息。

Method: 提出C3DFusion模块，通过历史上下文模糊化和当前中心特征密集化两种互补技术进行增强的时间融合：前者通过衰减尺度来抑制不准确扭曲的历史点特征噪声，后者通过增加体积贡献来增强当前点特征。

Result: 在SemanticKITTI和SSCBench-KITTI-360数据集上显著优于最先进方法，并在应用于其他基线模型时表现出强大的泛化能力，获得显著的性能提升。

Conclusion: C3DFusion模块简单集成到标准SSC架构中即可展现强大效果，有效解决了边界区域重建问题，为3D语义场景补全提供了有效的时空融合解决方案。

Abstract: Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.

</details>


### [207] [Visible Structure Retrieval for Lightweight Image-Based Relocalisation](https://arxiv.org/abs/2511.12503)
*Fereidoon Zangeneh,Leonard Bruns,Amit Dekel,Alessandro Pieropan,Patric Jensfelt*

Main category: cs.CV

TL;DR: 提出了一种新的结构重定位范式，通过神经网络直接从图像观测映射到可见场景结构，避免了图像检索或搜索启发式方法，降低了计算和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有结构重定位方法依赖搜索启发式或图像检索来减少搜索空间，导致复杂流水线或存储需求随观测数量增长，需要更高效的解决方案。

Method: 训练一个紧凑的可见结构检索网络，通过前向传播直接从查询图像获取地图中可见的3D结构点子集，减少2D-3D对应搜索空间。

Result: 该方法在定位精度上达到最先进水平，同时显著降低了计算和存储开销。

Conclusion: 提出的新范式使基于结构的重定位更加可行，通过神经网络直接映射实现了高效且紧凑的定位解决方案。

Abstract: Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.

</details>


### [208] [DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection](https://arxiv.org/abs/2511.12511)
*Jialiang Shen,Jiyang Zheng,Yunqi Xue,Huajie Chen,Yu Yao,Hui Kang,Ruiqi Liu,Helin Gong,Yang Yang,Dadong Wang,Tongliang Liu*

Main category: cs.CV

TL;DR: 提出基于师生知识蒸馏的模糊鲁棒AI生成图像检测框架，通过冻结教师网络(DINOv3)的特征表示，让学生网络在模糊图像上学习产生一致的表示，有效解决运动模糊导致的检测性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在真实世界运动模糊条件下性能严重下降，因为模糊会扭曲纹理特征并抑制高频伪影，限制了实际应用。

Method: 使用在清晰图像上训练的DINOv3作为冻结教师网络，通过知识蒸馏将其特征和logit响应传递到在模糊图像上训练的学生网络，使学生能在运动模糊下产生一致表示。

Result: 在运动模糊和清晰条件下的基准测试中达到最先进性能，显示出改进的泛化能力和实际应用性。

Conclusion: 提出的模糊鲁棒检测框架通过师生知识蒸馏有效提升了AI生成图像检测器在真实世界模糊条件下的性能，具有更好的实际应用价值。

Abstract: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.

</details>


### [209] [MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics](https://arxiv.org/abs/2511.12525)
*Jing Li,Yifan Wang,Jiafeng Yan,Renlong Zhang,Bin Yang*

Main category: cs.CV

TL;DR: 提出了一个基于大语言模型的退化感知图像融合框架(MdaIF)，通过混合专家系统处理多退化场景下的红外与可见光图像融合问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能考虑恶劣天气条件下可见光图像的退化问题，且依赖固定网络架构，限制了在不同退化场景下的适应性。

Method: 使用预训练视觉语言模型提取语义先验，提出退化感知通道注意力模块(DCAM)进行多模态特征交互，并利用混合专家系统实现有效的专家路由。

Result: 大量实验验证了MdaIF的有效性，在复杂退化场景下表现出优于现有最先进方法的性能。

Conclusion: 该框架能够自适应地处理多种退化场景下的图像融合问题，显著提升了融合性能。

Abstract: Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.

</details>


### [210] [D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation](https://arxiv.org/abs/2511.12528)
*Zheyuan Zhang,Jiwei Zhang,Boyu Zhou,Linzhimeng Duan,Hong Chen*

Main category: cs.CV

TL;DR: 提出D²-VPR框架，通过知识蒸馏和可变形聚合器，在保持视觉基础模型强大特征提取能力的同时，显著减少模型参数和计算开销，实现更好的性能-效率平衡。


<details>
  <summary>Details</summary>
Motivation: 解决DINOv2等视觉基础模型在视觉位置识别中虽然性能优越但模型复杂度和计算开销大，难以在资源受限设备上部署的问题。

Method: 采用两阶段训练策略结合知识蒸馏和微调，引入蒸馏恢复模块对齐师生模型特征空间；设计基于自上而下注意力的可变形聚合器，动态调整感兴趣区域以提升对不规则结构的适应性。

Result: 在保持竞争力的性能同时，参数数量减少约64.2%，FLOPs减少约62.6%（相比CricaVPR）。

Conclusion: D²-VPR框架成功实现了视觉位置识别任务中性能与效率的良好平衡，为资源受限环境下的部署提供了有效解决方案。

Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.

</details>


### [211] [ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding](https://arxiv.org/abs/2511.12530)
*Yuan Zhou,Litao Hua,Shilong Jin,Wentao Huang,Haoran Duan*

Main category: cs.CV

TL;DR: ReaSon是一个基于因果信息瓶颈的强化学习框架，用于视频理解中的关键帧选择，通过优化预测充分性和因果必要性来提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于视觉语言模型的输入令牌限制和视频帧中相关信息的时间稀疏性，关键帧选择对视频理解至关重要。现有方法往往忽略了关键帧的因果决定性。

Method: 提出ReaSon框架，将关键帧选择建模为优化问题：1）使用可学习策略网络从候选帧中选择关键帧确保预测充分性；2）通过反事实干预评估因果必要性；3）设计符合CIB原则的复合奖励来指导强化学习。

Result: 在NExT-QA、EgoSchema和Video-MME数据集上的广泛实验表明，ReaSon在有限帧设置下持续优于现有最先进方法。

Conclusion: ReaSon通过因果信息瓶颈原则有效解决了关键帧选择问题，验证了其有效性和泛化能力。

Abstract: Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.

</details>


### [212] [HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models](https://arxiv.org/abs/2511.12547)
*Zhiguang Lu,Qianqian Xu,Peisong Wen,Siran Da,Qingming Huang*

Main category: cs.CV

TL;DR: 提出了HiGFA方法，通过分层引导策略在扩散模型采样过程中结合文本、轮廓和细粒度分类器引导，以生成高质量的细粒度图像用于数据增强。


<details>
  <summary>Details</summary>
Motivation: 标准文本引导方法在细粒度任务中缺乏特异性，可能生成误导性样本，影响细粒度分类器性能。需要确保合成图像准确捕捉类别定义的细微特征。

Method: HiGFA在采样早期使用强文本和轮廓引导建立整体场景和结构，在后期激活细粒度分类器引导，并根据预测置信度动态调整所有引导信号的强度。

Result: 在多个细粒度视觉分类数据集上的实验证明了HiGFA的有效性。

Conclusion: HiGFA通过分层、置信度驱动的协调机制，能够生成多样且忠实的合成图像，在全局结构形成和精确细节细化之间实现智能平衡。

Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.

</details>


### [213] [EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis](https://arxiv.org/abs/2511.12554)
*Yijie Guo,Dexiang Hong,Weidong Chen,Zihan She,Cheng Ye,Xiaojun Chang,Zhendong Mao*

Main category: cs.CV

TL;DR: EmoVerse是一个大规模开源数据集，通过多层次的基于知识图谱的注释实现可解释的视觉情感分析，包含超过21.9万张图像，支持离散和连续情感表示。


<details>
  <summary>Details</summary>
Motivation: 现有视觉情感分析研究缺乏开源和可解释的数据集，通常仅对整个图像分配单一离散情感标签，无法揭示视觉元素如何贡献情感。

Method: 通过将情感分解为背景-属性-主体(B-A-S)三元组并将每个元素定位到视觉区域，使用多阶段标注流程确保高可靠性，并引入可解释模型将视觉线索映射到维度情感空间。

Result: 构建了包含21.9万张图像的大规模数据集，支持词级和主体级情感推理，提供分类情感状态和维度情感空间的双重标注。

Conclusion: EmoVerse数据集、标注流程和模型为推进可解释的高层次情感理解提供了全面基础。

Abstract: Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.

</details>


### [214] [SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition](https://arxiv.org/abs/2511.12559)
*Qing Cai,Guihao Yan,Fan Zhang,Cheng Zhang,Zhi Liu*

Main category: cs.CV

TL;DR: 提出SEMC框架，结合结构感知特征融合和专家引导对比学习，解决超声标准平面识别中浅层结构信息利用不足和细粒度语义差异捕捉困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效利用浅层结构信息，且难以通过图像增强生成的对比样本捕捉细粒度语义差异，导致超声标准平面的结构和判别细节识别效果不佳。

Method: 提出语义-结构融合模块(SSFM)来利用多尺度结构信息，增强模型感知细粒度结构细节的能力；设计专家混合对比识别模块(MCRM)，使用MoE机制在多层次特征上进行分层对比学习和分类。

Result: 在内部数据集和两个公共数据集上的实验结果表明，SEMC在各种指标上都优于最近的最先进方法。

Conclusion: SEMC框架通过结构增强和专家混合对比学习，显著提升了超声标准平面识别的性能。

Abstract: Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.

</details>


### [215] [Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection](https://arxiv.org/abs/2511.12572)
*Mohamed Youssef,Lukas Brunner,Klaus Rundhammer,Gerald Czech,Oliver Bimber*

Main category: cs.CV

TL;DR: 提出了一种结合信号处理和机器学习的新方法，用于通过遮挡的森林植被重建地表温度，实现无人机自主野火监测和早期火灾检测。


<details>
  <summary>Details</summary>
Motivation: 实现全自动空中野火监测，在烟雾或火焰可见之前早期检测地面火灾，解决合成孔径传感带来的热模糊问题。

Method: 训练视觉状态空间模型从模糊数据中恢复部分遮挡土壤和火灾热点的细微热信号，使用潜在扩散模型生成大量真实地表温度模拟数据，结合温度增强和程序化热森林模拟。

Result: 在模拟数据上，相比传统热成像和未校正SA成像，RMSE降低了2-2.5倍；在野外实验中，对高温热点检测的改进更显著，RMSE增益分别达到12.8倍和2.6倍。

Conclusion: 该方法不仅能重建火灾热点，还能重建完整的热信号形态，在部分遮挡情况下优于传统成像方法，并展示了在搜索救援等其他热信号应用中的泛化能力。

Abstract: We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.

</details>


### [216] [Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection](https://arxiv.org/abs/2511.12575)
*Jiayi Zhu,Yihao Huang,Yue Cao,Xiaojun Jia,Qing Guo,Felix Juefei-Xu,Geguang Pu,Bin Wang*

Main category: cs.CV

TL;DR: 提出了一种基于文本攻击的视觉语言模型地理隐私保护方法，通过在图像外部添加欺骗性文本，有效降低LVLMs的地理位置推断准确率，同时保持图像视觉质量。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）能够从社交媒体图像中推断用户地理位置，造成严重隐私泄露风险。现有对抗性图像扰动方法需要强失真，会显著降低图像视觉质量和分享价值。

Method: 采用两阶段语义感知的排版攻击方法，在视觉内容外部添加文本扩展，生成具有欺骗性的文本来干扰地理位置推断。

Result: 在三个数据集上的广泛实验表明，该方法显著降低了五个最先进商业LVLMs的地理位置预测准确率。

Conclusion: 该方法为对抗新兴地理隐私威胁提供了一种实用且视觉保持的保护策略。

Abstract: Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.

</details>


### [217] [TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction](https://arxiv.org/abs/2511.12578)
*Yukuo Ma,Cong Liu,Junke Wang,Junqi Liu,Haibin Huang,Zuxuan Wu,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TempoMaster是一个新颖的长视频生成框架，将长视频生成建模为下一帧率预测问题，通过从低帧率到高帧率的渐进式生成实现高效并行合成。


<details>
  <summary>Details</summary>
Motivation: 解决长视频生成中的长程时间一致性和生成效率问题，传统方法难以同时保证视频质量和生成效率。

Method: 首先生成低帧率视频片段作为粗粒度蓝图，然后逐步提高帧率以细化视觉细节和运动连续性；在生成过程中，每个帧率级别内使用双向注意力，在帧率间进行自回归预测。

Result: 在长视频生成任务中达到了新的最先进水平，在视觉质量和时间一致性方面表现优异。

Conclusion: TempoMaster通过将长视频生成建模为帧率预测问题，有效解决了长程时间一致性和生成效率的平衡问题，为长视频生成提供了新的解决方案。

Abstract: We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.

</details>


### [218] [Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting](https://arxiv.org/abs/2511.12588)
*Zuqi Huang,Mengxin Tian,Huan Liu,Wentao Li,Baobao Liang,Jie Wu,Fang Yan,Zhaoqing Tang,Zhongyu Li*

Main category: cs.CV

TL;DR: 提出了CountIHC框架，通过排名感知的教师选择策略和多知识蒸馏，结合视觉-语言对齐进行多类细胞计数，在IHC图像中实现准确的细胞计数。


<details>
  <summary>Details</summary>
Motivation: 解决IHC图像中细胞计数的挑战，包括染色体重叠、生物标记物变异和细胞形态多样性，同时探索基础模型在此任务中的潜力。

Method: 使用排名感知的教师选择策略(RATS)评估教师模型计数能力，通过多知识蒸馏获得紧凑学生模型；采用视觉-语言对齐方法，利用结构化文本提示生成语义锚点来指导类别特异性密度图回归。

Result: 在12种IHC生物标记物和5种组织类型上超越现有方法，与病理学家评估高度一致，在H&E染色数据上也表现有效。

Conclusion: CountIHC框架成功解决了IHC图像中多类细胞计数问题，方法具有可扩展性，为生物医学图像分析提供了有效解决方案。

Abstract: Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.

</details>


### [219] [Fine-Grained Representation for Lane Topology Reasoning](https://arxiv.org/abs/2511.12590)
*Guoqing Xu,Yiheng Li,Yang Yang*

Main category: cs.CV

TL;DR: TopoFG是一个细粒度的车道拓扑推理框架，通过分层先验提取、区域聚焦解码和鲁棒边界点拓扑推理，精确建模复杂车道结构，在OpenLane-V2基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常用单个查询表示每个车道，基于车道查询间的相似性推断拓扑连接性，但难以准确建模复杂车道结构，导致拓扑预测不可靠。

Method: 提出TopoFG框架，包含三个阶段：分层先验提取器从BEV掩码提取全局空间先验和车道内关键点序列提取局部顺序先验；区域聚焦解码器构建细粒度查询并采样RoI区域参考点；鲁棒边界点拓扑推理基于边界点查询特征建模车道连接性并采用拓扑去噪策略。

Result: 在OpenLane-V2基准测试中，TopoFG在subsetA上达到48.0% OLS，在subsetB上达到45.4% OLS，创造了新的最先进性能。

Conclusion: 通过将空间和顺序先验集成到细粒度查询中，并对边界点拓扑推理应用去噪策略，该方法能够精确建模复杂车道结构并提供可信的拓扑预测。

Abstract: Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions.Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries.However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction.In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling.RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane.RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity.By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions.Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.

</details>


### [220] [Seg-VAR: Image Segmentation with Visual Autoregressive Modeling](https://arxiv.org/abs/2511.12594)
*Rongkun Zheng,Lu Qi,Xi Chen,Yi Wang,Kun Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Seg-VAR将分割任务重新定义为条件自回归掩码生成问题，通过多尺度建模和潜在学习过程，在多个分割任务和基准测试中超越了之前的判别性和生成性方法。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归建模在图像生成方面已取得进展，但其在需要精确空间感知的分割任务中的潜力尚未被探索。

Method: 提出Seg-VAR框架，包含三个核心组件：图像编码器生成潜在先验、空间感知的seglat编码器将分割掩码映射为离散潜在标记、解码器从潜在表示重建掩码。采用多阶段训练策略。

Result: 实验表明Seg-VAR在各种分割任务和验证基准上优于之前的判别性和生成性方法。

Conclusion: 通过将分割构建为序列层次预测任务，Seg-VAR为将自回归推理集成到空间感知视觉系统中开辟了新途径。

Abstract: While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.

</details>


### [221] [LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet](https://arxiv.org/abs/2511.12602)
*Ria Shekhawat,Sushrut Patwardhan,Raghavendra Ramachandra,Praveen Kumar Chandaliya,Kishor P. Upla*

Main category: cs.CV

TL;DR: 提出了一种基于师生框架的单图像形态攻击检测方法，使用CNN教师模型优化ViT学生模型，并集成LoRA进行微调以提高效率。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统对安全至关重要，但仍然容易受到形态攻击的威胁，这种攻击通过合成图像融合多个个体的生物特征。

Method: 采用师生框架，CNN教师模型优化ViT学生模型，集成LoRA进行微调以减少计算成本。在由三个公开人脸数据集构建的形态数据集上进行实验，包含十种不同的形态生成算法。

Result: 与六种最先进的S-MAD技术相比，所提方法在检测性能和计算效率方面均表现出优越性。

Conclusion: 该方法在保持高检测精度的同时显著提高了计算效率，为形态攻击检测提供了有效的解决方案。

Abstract: Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.

</details>


### [222] [Pixels or Positions? Benchmarking Modalities in Group Activity Recognition](https://arxiv.org/abs/2511.12606)
*Drishya Karki,Merey Ramazanova,Anthony Cioppa,Silvio Giancola,Bernard Ghanem*

Main category: cs.CV

TL;DR: 该研究提出了SoccerNet-GAR多模态数据集，比较了基于视频和基于跟踪的群体活动识别方法，发现跟踪方法在准确性和效率上都优于视频方法。


<details>
  <summary>Details</summary>
Motivation: 群体活动识别(GAR)主要研究视频模态，但位置跟踪模态相对较少探索。需要标准化基准来公平比较这两种模态在GAR中的表现。

Method: 构建了SoccerNet-GAR多模态数据集，包含94,285个同步标注的群体活动。提出了两种单模态方法：竞争性视频分类器和基于图神经网络的跟踪分类器，其中跟踪模型采用角色感知图架构。

Result: 跟踪模型达到67.2%的平衡准确率，而最佳视频基线为58.1%。跟踪模型训练速度快4.25倍，参数少438倍(197K vs 86.3M)。

Conclusion: 跟踪模态在群体活动识别中优于视频模态，强调了模态选择和角色感知建模对GAR的重要性。

Abstract: Group Activity Recognition (GAR) is well studied on the video modality for surveillance and indoor team sports (e.g., volleyball, basketball). Yet, other modalities such as agent positions and trajectories over time, i.e. tracking, remain comparatively under-explored despite being compact, agent-centric signals that explicitly encode spatial interactions. Understanding whether pixel (video) or position (tracking) modalities leads to better group activity recognition is therefore important to drive further research on the topic. However, no standardized benchmark currently exists that aligns broadcast video and tracking data for the same group activities, leading to a lack of apples-to-apples comparison between these modalities for GAR. In this work, we introduce SoccerNet-GAR, a multimodal dataset built from the $64$ matches of the football World Cup 2022. Specifically, the broadcast videos and player tracking modalities for $94{,}285$ group activities are synchronized and annotated with $10$ categories. Furthermore, we define a unified evaluation protocol to benchmark two strong unimodal approaches: (i) a competitive video-based classifiers and (ii) a tracking-based classifiers leveraging graph neural networks. In particular, our novel role-aware graph architecture for tracking-based GAR directly encodes tactical structure through positional edges and temporal attention. Our tracking model achieves $67.2\%$ balanced accuracy compared to $58.1\%$ for the best video baseline, while training $4.25 \times$ faster with $438 \times$ fewer parameters ($197K$ \vs $86.3M$). This study provides new insights into the relative strengths of pixels and positions for group activity recognition. Overall, it highlights the importance of modality choice and role-aware modeling for GAR.

</details>


### [223] [Open-World Test-Time Adaptation with Hierarchical Feature Aggregation and Attention Affine](https://arxiv.org/abs/2511.12607)
*Ziqiong Liu,Yushun Tang,Junyang Ji,Zhihai He*

Main category: cs.CV

TL;DR: 提出了一种分层阶梯网络和注意力仿射网络，通过提取OOD特征和自适应调整注意力机制来提升测试时适应性，在领域偏移和未见类别样本下显著改善分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决测试时适应方法在面对未见类别样本时性能下降的问题，避免将OOD样本误分类为已知类别导致预测准确率下降和适应过程受损。

Method: 使用分层阶梯网络从所有Transformer层聚合的类别令牌中提取OOD特征，通过加权概率融合增强OOD检测；引入注意力仿射网络自适应优化自注意力机制；采用加权熵机制动态抑制低置信度样本的影响。

Result: 在基准数据集上的实验结果表明，该方法在广泛使用的分类数据集上显著提升了性能。

Conclusion: 所提出的方法有效提升了模型在测试时面对领域偏移和未见类别样本时的适应能力和分类性能。

Abstract: Test-time adaptation (TTA) refers to adjusting the model during the testing phase to cope with changes in sample distribution and enhance the model's adaptability to new environments. In real-world scenarios, models often encounter samples from unseen (out-of-distribution, OOD) categories. Misclassifying these as known (in-distribution, ID) classes not only degrades predictive accuracy but can also impair the adaptation process, leading to further errors on subsequent ID samples. Many existing TTA methods suffer substantial performance drops under such conditions. To address this challenge, we propose a Hierarchical Ladder Network that extracts OOD features from class tokens aggregated across all Transformer layers. OOD detection performance is enhanced by combining the original model prediction with the output of the Hierarchical Ladder Network (HLN) via weighted probability fusion. To improve robustness under domain shift, we further introduce an Attention Affine Network (AAN) that adaptively refines the self-attention mechanism conditioned on the token information to better adapt to domain drift, thereby improving the classification performance of the model on datasets with domain shift. Additionally, a weighted entropy mechanism is employed to dynamically suppress the influence of low-confidence samples during adaptation. Experimental results on benchmark datasets show that our method significantly improves the performance on the most widely used classification datasets.

</details>


### [224] [OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding](https://arxiv.org/abs/2511.12614)
*Artem Moroz,Vít Zeman,Martin Mikšík,Elizaveta Isianova,Miroslav David,Pavel Burget,Varun Burde*

Main category: cs.CV

TL;DR: 提出了一个统一的端到端框架，将物体检测和姿态估计与灵活的物体建模过程相结合。系统支持使用传统3D CAD模型或从多视角图像快速重建神经辐射场(NeRF)来生成物体表示，并通过CNOS检测器和OPFormer姿态估计模块实现精确的6D姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决物体检测和姿态估计在模型可用和不可用场景下的统一处理问题，提高系统在实际应用中的灵活性和实用性。

Method: 1) 物体建模阶段：使用3D CAD模型或从多视角图像重建NeRF；2) 检测阶段：CNOS检测器定位目标物体；3) 姿态估计：OPFormer基于transformer架构，结合基础模型特征提取、多模板视图编码和NOCS几何先验，建立2D-3D对应关系。

Result: 在BOP基准测试中表现出良好的准确性和效率平衡，在模型可用和不可用场景下都具有实际应用价值。

Conclusion: 该集成系统在准确性和效率之间取得了良好平衡，展示了在模型可用和不可用场景下的实际应用潜力。

Abstract: We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.

</details>


### [225] [C3Net: Context-Contrast Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12627)
*Baber Jan,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: C3Net是一个专门用于伪装目标检测的双路径解码器架构，通过边缘精炼路径和上下文定位路径协同工作，解决了伪装目标检测中的六大核心挑战，在多个数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 伪装目标检测面临传统分割方法和现代基础模型都无法有效处理的挑战，因为伪装目标与背景在颜色、纹理和模式上高度相似。本文识别了六个根本性挑战：内在相似性、边缘破坏、极端尺度变化、环境复杂性、上下文依赖性和显著-伪装目标区分。

Method: 提出C3Net双路径解码器架构：边缘精炼路径使用梯度初始化边缘增强模块从早期特征恢复精确边界；上下文定位路径采用基于图像的上下文引导机制实现内在显著性抑制；注意力融合模块通过空间门控协同结合两条路径。

Result: 在COD10K数据集上S-measure达到0.898，CAMO数据集达到0.904，NC4K数据集达到0.913，实现了最先进的性能，同时保持高效处理。

Conclusion: C3Net证明了复杂多方面的检测挑战需要架构创新，通过专门组件协同工作实现全面覆盖，超越了孤立改进的效果。

Abstract: Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.

</details>


### [226] [Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation](https://arxiv.org/abs/2511.12631)
*Yushe Cao,Dianxi Shi,Xing Fu,Xuechao Zou,Haikuo Peng,Xueqi Li,Chun Yu,Junliang Xing*

Main category: cs.CV

TL;DR: MDiTFace是一个基于扩散变换器的多模态人脸生成框架，通过统一标记化策略处理语义掩码和文本输入，采用解耦注意力机制显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统多模态特征融合方法难以实现有效的跨模态交互，导致生成效果不佳。需要解决异构模态表示之间的差异问题。

Method: 使用统一标记化策略处理语义掩码和文本输入；设计堆叠的多变量变换器块同步处理所有条件；引入解耦注意力机制，将内部计算分离为动态和静态路径。

Result: 实验表明MDiTFace在人脸保真度和条件一致性方面显著优于其他竞争方法，同时将掩码条件引入的计算开销降低了94%以上。

Conclusion: MDiTFace通过统一的标记化策略和解耦注意力机制，有效解决了多模态人脸生成中的跨模态交互问题，在保持性能的同时大幅降低了计算成本。

Abstract: While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.

</details>


### [227] [Denoising Vision Transformer Autoencoder with Spectral Self-Regularization](https://arxiv.org/abs/2511.12633)
*Xunzhi Xiang,Xingye Tian,Guiyu Zhang,Yabo Chen,Shaofeng Zhang,Xuebo Wang,Xin Tao,Qi Fan*

Main category: cs.CV

TL;DR: 提出Denoising-VAE，通过频谱自正则化策略抑制高维潜在空间中的冗余高频噪声，解决了VAE在高维潜在空间中重建保真度与生成性能之间的权衡问题，显著提升扩散模型的训练收敛速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统VAE在高维潜在空间中存在优化困境：高维空间能提高重建保真度但往往损害生成性能。现有方法使用外部视觉基础模型进行正则化，但高维潜在如何影响生成模型优化仍不清楚。

Method: 提出频谱自正则化策略抑制冗余高频噪声，同时保持重建质量；开发Denoising-VAE（基于ViT的自编码器，不依赖VFMs）；引入频谱对齐策略优化基于Denoising-VAE的生成模型。

Result: 扩散模型收敛速度比SD-VAE快约2倍；在ImageNet 256×256基准上达到最先进的重建质量（rFID=0.28，PSNR=27.26）和竞争性生成性能（gFID=1.82）。

Conclusion: 揭示了高维潜在空间中冗余高频分量阻碍扩散模型训练收敛的关键问题，提出的Denoising-VAE通过频谱正则化有效解决了这一困境，在重建和生成性能上都取得了显著提升。

Abstract: Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\times$256 benchmark.

</details>


### [228] [Medical Knowledge Intervention Prompt Tuning for Medical Image Classification](https://arxiv.org/abs/2511.12639)
*Ye Du,Nanxi Yu,Shujun Wang*

Main category: cs.CV

TL;DR: CILMP是一种将大型语言模型集成到视觉语言模型提示调优中的方法，通过提取疾病特定表示并创建实例自适应提示，在医疗图像分类任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示调优方法无法精确区分不同类型的医学概念，缺乏特定疾病相关特征。大型语言模型擅长提供专业医学知识，因此将其集成到提示调优过程中。

Method: 提出CILMP方法：从LLMs提取疾病特定表示，在低秩线性子空间中进行干预，创建疾病特定提示，并加入条件机制生成实例自适应提示。

Result: 在多个医疗图像数据集上的广泛实验表明，CILMP始终优于最先进的提示调优方法。

Conclusion: CILMP有效桥接LLMs和VLMs，将医学知识转移到VLM提示中，提高了医疗图像分类的性能和适应性。

Abstract: Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.

</details>


### [229] [DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry](https://arxiv.org/abs/2511.12653)
*Cheng Liao*

Main category: cs.CV

TL;DR: DPVO-QAT++是一个分层量化优化框架，通过可学习尺度参数化、前端FP16/FP32和后端全精度的异构精度设计，以及GPU原生内核融合，显著降低了内存占用并提高了处理速度，同时保持了原始模型的轨迹精度。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的视觉SLAM系统具有出色的几何推理能力，但其高昂的计算开销严重限制了在资源受限的自主平台上的部署。

Method: 采用分层量化优化框架，包括可学习尺度参数化、视觉里程计前端和后端的异构精度设计（前端使用FP16/FP32浮点伪量化，后端使用全精度），以及用于伪量化的GPU原生内核融合（自定义CUDA内核）。

Result: 在TartanAir数据集上，平均FPS提升52.1%，中位延迟降低29.1%，峰值GPU内存占用降低64.9%；在EuRoC数据集上，平均FPS提升30.1%，中位延迟降低23.1%，峰值GPU内存占用降低37.7%，同时保持与原始模型相当的轨迹精度。

Conclusion: DPVO-QAT++有效弥合了高精度深度视觉里程计与实际部署效率要求之间的差距，为该技术在真实嵌入式平台上的应用提供了可行的工程范式。

Abstract: Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.
  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.

</details>


### [230] [Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis](https://arxiv.org/abs/2511.12658)
*Zeqin Yu,Haotao Xie,Jian Zhang,Jiangqun Ni,Wenkan Su,Jiwu Huang*

Main category: cs.CV

TL;DR: 提出FSTS框架，通过分析真实篡改行为参数，使用傅里叶级数启发的层次建模方法合成多样化的训练数据，显著提升文本图像伪造定位模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本图像伪造定位方法泛化能力差，主要原因是真实世界数据集规模有限，且合成数据与真实篡改复杂性存在分布差距。

Method: FSTS框架：收集16,750个真实篡改实例，通过结构化流水线记录编辑痕迹；分析参数建立层次建模框架，将个体篡改参数表示为基操作配置的组合，群体分布通过聚合这些行为构建；基于傅里叶级数启发的可解释近似方法合成训练数据。

Result: 在四个评估协议上的广泛实验表明，使用FSTS数据训练的模型在真实世界数据集上实现了显著改进的泛化能力。

Conclusion: FSTS通过结构化建模真实篡改行为，能够合成更反映真实伪造痕迹的多样化训练数据，有效提升文本图像伪造定位模型的泛化性能。

Abstract: Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \href{https://github.com/ZeqinYu/FSTS}{Project Page}.

</details>


### [231] [Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans](https://arxiv.org/abs/2511.12662)
*Hongbin Huang,Junwei Li,Tianxin Xie,Zhuang Li,Cekai Weng,Yaodong Yang,Yue Luo,Li Liu,Jing Tang,Zhijing Shao,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出一个高保真、实时的对话数字人系统，结合逼真的3D虚拟形象、个性驱动的语音合成和知识驱动的对话生成，通过异步执行管道实现低延迟多模态交互。


<details>
  <summary>Details</summary>
Motivation: 当前数字人系统难以同时实现视觉逼真性和实时响应性，需要开发能支持自然及时交互的高保真数字人系统。

Method: 采用异步执行管道协调多模态组件，结合唤醒词检测、情感表达韵律和上下文感知响应生成，使用检索增强方法包括历史增强和基于意图的路由。

Result: 构建了一个集成的数字人系统，支持响应式和可信的数字人交互，适用于通信、教育和娱乐等沉浸式应用。

Conclusion: 该系统通过创新的多模态协调和检索增强方法，成功实现了高保真且实时的数字人交互，为沉浸式应用提供了可行的解决方案。

Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.

</details>


### [232] [DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality](https://arxiv.org/abs/2511.12671)
*Tushar Anand,Advik Sinha,Abhijit Das*

Main category: cs.CV

TL;DR: 提出了一种基于非因果选择性状态空间的实时光学流和视差估计模型，通过融合成对输入图像实现准确的密集感知任务


<details>
  <summary>Details</summary>
Motivation: 解决实时应用中光学流和视差估计的准确性与效率平衡问题，满足实时密集3D感知任务的需求

Method: 使用非因果Mamba块构建模型，融合成对输入图像，在非因果选择性状态空间中进行处理

Result: 模型在保持高精度的同时显著减少推理时间，降低GPU使用率，适用于实时应用场景

Conclusion: 该模型能够用于统一的实时准确3D密集感知估计任务，在实际场景中验证有效

Abstract: In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD

</details>


### [233] [Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis](https://arxiv.org/abs/2511.12675)
*Saar Stern,Ido Sobol,Or Litany*

Main category: cs.CV

TL;DR: 提出了一种基于Zero123特征的任务感知评估框架，包含参考评分D_PRISM和无参考评分MMD_PRISM，用于评估新视角合成(NVS)的可靠性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标难以准确评估生成图像是否既真实又忠实于源视图和视角变换，标准指标经常错误地排名不正确的结果。

Method: 利用Zero123强基础模型的特征，结合轻量级调优步骤增强判别能力，提出两种互补的评估指标。

Result: 在三个基准测试(Toys4K、GSO、OmniObject3D)上对六种NVS方法进行评估，MMD_PRISM产生了清晰稳定的排名，较低分数一致表示更强的模型。

Conclusion: 该框架为新视角合成评估提供了原则性和实用的方法，为更可靠的新视角合成进展铺平了道路。

Abstract: The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\text{PRISM}}$, and a reference-free score, $\text{MMD}_{\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\text{MMD}_{\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.

</details>


### [234] [BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections](https://arxiv.org/abs/2511.12676)
*Subin Varghese,Joshua Gao,Asad Ur Rahman,Vedhus Hoskere*

Main category: cs.CV

TL;DR: 提出了BridgeEQA基准测试，用于评估具身智能体在桥梁检查场景中的开放词汇问答能力，包含2200个问题-答案对，并提出了新的评估指标和EMVR方法。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能体在真实世界环境中进行问答的困难，特别是缺乏能够真实反映实际操作条件的基准测试。桥梁检查领域自然需要多尺度推理、长距离空间理解和复杂语义关系。

Method: 提出了Embodied Memory Visual Reasoning (EMVR)方法，将检查任务建模为在基于图像的场景图上的顺序导航：图像作为节点，智能体通过马尔可夫决策过程遍历视图、比较证据并进行推理。

Result: 评估显示最先进的视觉语言模型在情景记忆EQA设置下存在显著的性能差距。EMVR方法在基准测试上表现出优于基线模型的性能。

Conclusion: BridgeEQA基准测试为具身问答提供了真实世界的评估平台，EMVR方法展示了在复杂检查任务中的有效性，数据集和代码已公开发布。

Abstract: Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.
  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.
  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.

</details>


### [235] [R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection](https://arxiv.org/abs/2511.12691)
*Shuaike Shen,Ke Liu,Jiaqing Xie,Shangde Gao,Chunhua Shen,Ge Liu,Mireia Crispin-Ortuzar,Shangqi Gao*

Main category: cs.CV

TL;DR: R²Seg是一个无需训练的两阶段框架，通过推理-拒绝过程提升医学图像分割在分布外肿瘤上的鲁棒性，无需参数更新即可显著改善分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割基础模型在分布外数据上表现不佳，容易产生碎片化假阳性结果，需要提升对OOD肿瘤的鲁棒性。

Method: 两阶段方法：1) 推理阶段使用LLM引导的解剖推理规划器定位器官锚点并生成多尺度ROI；2) 拒绝阶段在ROI内对基础模型生成的候选区域进行双样本统计测试，仅保留与正常组织显著不同的候选区域。

Result: 在多中心多模态肿瘤分割基准测试中，R²Seg在Dice系数、特异性和敏感性方面显著优于强基线方法和原始基础模型。

Conclusion: R²Seg提供了一种无需训练的高效方法，能够有效抑制假阳性，提升OOD肿瘤分割的鲁棒性，同时避免灾难性遗忘问题。

Abstract: Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.

</details>


### [236] [HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models](https://arxiv.org/abs/2511.12693)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: HEDGE是一个用于视觉语言模型幻觉检测的统一框架，通过结合视觉扰动、语义聚类和不确定性度量来识别多模态模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型虽然能够进行开放式视觉问答，但容易产生幻觉，需要系统性的检测方法来评估多模态可靠性。

Method: HEDGE框架整合了采样、失真合成、聚类（基于蕴含和嵌入）和度量计算，形成可复现的流水线，适用于各种多模态架构。

Result: 评估显示统一融合模型（如Qwen2.5-VL）的幻觉可检测性最高，受限标记化架构（如Med-Gemma）最低。VASE度量与嵌入聚类结合时提供最稳健的幻觉信号。

Conclusion: HEDGE将幻觉检测构建为几何鲁棒性问题，为评估多模态可靠性提供了原则性、计算感知的基础，并通过开源库支持可复现的基准测试。

Abstract: Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .

</details>


### [237] [X-VMamba: Explainable Vision Mamba](https://arxiv.org/abs/2511.12694)
*Mohamed A. Mabrok,Yalda Zafari*

Main category: cs.CV

TL;DR: 提出了一个基于可控性的可解释性框架来分析视觉状态空间模型（SSMs），通过两种互补方法量化输入序列对内部状态动态的影响，揭示了SSMs在医学图像处理中的层次化特征细化过程。


<details>
  <summary>Details</summary>
Motivation: 尽管状态空间模型（特别是Mamba架构）在序列建模中表现出色，但由于缺乏类似注意力的透明机制，理解这些视觉SSMs如何处理空间信息仍然具有挑战性。

Method: 提出了两种互补方法：适用于任何SSM架构的基于雅可比矩阵的方法，以及针对对角SSMs的基于格拉姆矩阵的方法。两种方法都在单次前向传播中运行，具有线性复杂度，无需架构修改或超参数调整。

Result: 在三种不同的医学成像模态上的实验表明，SSMs自然地实现了从早期层的扩散低层纹理到深层层的聚焦、临床有意义模式的层次化特征细化。分析揭示了与诊断标准对齐的领域特定可控性特征、跨网络层次的渐进空间选择性，以及扫描策略对注意力模式的显著影响。

Conclusion: 该框架将可控性分析确立为跨所有领域的SSMs的统一、基础性可解释性范式，在计算机视觉、自然语言处理和跨领域任务中具有广泛应用。

Abstract: State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication

</details>


### [238] [Counting Through Occlusion: Framework for Open World Amodal Counting](https://arxiv.org/abs/2511.12702)
*Safaeid Hossain Arib,Rabeya Akter,Abdul Monaf Chowdhury,Md Jubair Ahmed Sourov,Md Mehedi Hasan*

Main category: cs.CV

TL;DR: CountOCC是一个解决遮挡条件下物体计数问题的模态计数框架，通过多模态引导重建被遮挡物体特征，在多个数据集上显著提升了遮挡场景下的计数精度。


<details>
  <summary>Details</summary>
Motivation: 现有物体计数方法在遮挡场景下表现不佳，因为主干网络会编码遮挡表面而非目标物体，导致特征表示损坏。需要开发能够显式重建被遮挡物体特征的方法。

Method: 提出CountOCC框架，通过整合可见片段的空间上下文与文本和视觉嵌入的语义先验，在多个金字塔层级上生成遮挡位置的类别区分性特征。引入视觉等价性目标，确保遮挡和未遮挡视图产生空间对齐的注意力图。

Result: 在FSC 147数据集上，验证集和测试集的MAE分别降低26.72%和20.80%；在CARPK数据集上MAE降低49.89%；在CAPTUREReal数据集上MAE降低28.79%，均达到SOTA性能。

Conclusion: CountOCC通过互补机制在遮挡条件下保持区分性特征，实现了跨视觉领域的鲁棒模态计数，显著提升了遮挡场景下的物体计数精度。

Abstract: Object counting has achieved remarkable success on visible instances, yet state-of-the-art (SOTA) methods fail under occlusion, a pervasive challenge in real world deployment. This failure stems from a fundamental architectural limitation where backbone networks encode occluding surfaces rather than target objects, thereby corrupting the feature representations required for accurate enumeration. To address this, we present CountOCC, an amodal counting framework that explicitly reconstructs occluded object features through hierarchical multimodal guidance. Rather than accepting degraded encodings, we synthesize complete representations by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. We further introduce a visual equivalence objective that enforces consistency in attention space, ensuring that both occluded and unoccluded views of the same scene produce spatially aligned gradient-based attention maps. Together, these complementary mechanisms preserve discriminative properties essential for accurate counting under occlusion. For rigorous evaluation, we establish occlusion-augmented versions of FSC 147 and CARPK spanning both structured and unstructured scenes. CountOCC achieves SOTA performance on FSC 147 with 26.72% and 20.80% MAE reduction over prior baselines under occlusion in validation and test, respectively. CountOCC also demonstrates exceptional generalization by setting new SOTA results on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction, validating robust amodal counting across diverse visual domains. Code will be released soon.

</details>


### [239] [FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling](https://arxiv.org/abs/2511.12708)
*Kaiser Hamid,Can Cui,Khandakar Ashrafi Akbar,Ziran Wang,Nade Liang*

Main category: cs.CV

TL;DR: FSDAM是一个少样本驾驶员注意力建模框架，仅需约100个标注样本即可同时预测注意力和生成解释，比现有方法减少两个数量级的标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员注意力模型依赖大规模注视数据集，但这些数据集收集和整理成本高昂。需要开发在数据受限场景下仍能有效工作的模型。

Method: 采用双路径架构，分别处理空间预测和标题生成，通过跨模态对齐保持语义一致性。

Result: FSDAM在注意力预测方面达到竞争性性能，生成连贯且上下文感知的解释，并在多个驾驶基准测试中展示强大的零样本泛化能力。

Conclusion: 这项工作表明，在有限监督下可以实现有效的注意力条件生成，为在数据受限场景中实际部署可解释的驾驶员注意力系统开辟了新可能性。

Abstract: Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.

</details>


### [240] [Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning](https://arxiv.org/abs/2511.12735)
*Ankita Raj,Chetan Arora*

Main category: cs.CV

TL;DR: 本文首次研究了开放词汇目标检测器(OVODs)的后门攻击，提出了TrAP多模态后门注入策略，通过联合优化图像和文本模态的提示参数以及视觉触发器，在保持模型泛化能力的同时植入隐藏后门。


<details>
  <summary>Details</summary>
Motivation: 随着OVODs在机器人、自动驾驶等高风险应用中的普及，理解其安全风险变得至关重要。本文旨在揭示由提示调优引入的新攻击面。

Method: 提出TrAP策略，联合优化图像和文本模态的提示参数以及视觉触发器，采用课程式训练策略逐步缩小触发器尺寸，使用轻量级可学习提示令牌植入后门而不需要重新训练基础模型权重。

Result: 在多个数据集上的实验表明，TrAP在目标误分类和目标消失攻击中均实现了高攻击成功率，同时在下游数据集上相比零样本设置提高了干净图像的检测性能。

Conclusion: TrAP展示了OVODs面临的新型安全威胁，提示调优可能成为后门攻击的有效载体，需要在部署这类模型时加强安全考虑。

Abstract: Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.

</details>


### [241] [Direct Visual Grounding by Directing Attention of Visual Tokens](https://arxiv.org/abs/2511.12738)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 提出了KL注意力损失（KLAL）来直接监督视觉语言模型中视觉token的注意力分布，通过KL散度对齐真实注意力图，结合标准的下一个token预测损失，显著提升了视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 发现视觉语言模型中与查询最相关的视觉token在LLM模块的最终层很少受到答案token的关注，这可能导致视觉问答错误。标准的NTP损失不足以指导对视觉token的注意力。

Method: 提出KL注意力损失函数，直接监督视觉token的注意力分布，将其与真实注意力图（来自合成任务几何或真实图像标注）对齐，结合NTP损失训练模型。

Result: 在几何任务、指向和指代表达理解等任务上，无论是合成数据还是真实数据都取得了显著改进。还创建了一个评估VLM线条追踪能力的新数据集，发现即使商业VLM在该任务上表现也不佳。

Conclusion: 直接监督视觉token的注意力分布能有效提升VLMs在视觉任务上的性能，KL注意力损失为改善VLM的视觉注意力机制提供了有效方法。

Abstract: Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.

</details>


### [242] [Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests](https://arxiv.org/abs/2511.12740)
*Amirhossein Hassanzadeh,Bartosz Krawczyk,Michael Saunders,Rob Wible,Keith Krause,Dimah Dera,Jan van Aardt*

Main category: cs.CV

TL;DR: 本研究探索从体素化的LiDAR点云数据中推断低级别体素内容信息（目标占用百分比），提出基于核点卷积的多目标回归方法，并针对类别不平衡问题采用密度相关成本敏感学习。


<details>
  <summary>Details</summary>
Motivation: 体素化虽然能降低LiDAR数据处理的计算成本，但会导致细尺度结构信息丢失。本研究旨在验证是否可以从高级别的体素化LiDAR数据中恢复低级别的体素内容信息。

Method: 使用DIRSIG软件生成的模拟LiDAR点云数据，提出基于核点卷积的多目标回归方法，采用密度相关成本敏感学习解决类别不平衡问题，并应用加权均方误差、焦点回归和正则化优化模型。

Result: 敏感性分析显示，较大体素尺寸（如2米）由于变异性降低而误差较小，而较小体素尺寸（如0.25或0.5米）误差较高，特别是在树冠区域。树皮和树叶目标在细分辨率下的误差显著高于粗分辨率。

Conclusion: 体素尺寸的选择取决于具体应用需求。本研究填补了深度不平衡学习模型在多目标回归和森林3D LiDAR点云模拟数据集方面的空白。

Abstract: Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.

</details>


### [243] [SAGE: Saliency-Guided Contrastive Embeddings](https://arxiv.org/abs/2511.12744)
*Colton R. Crum,Adam Czajka*

Main category: cs.CV

TL;DR: SAGE是一种通过对比嵌入将人类显著性先验整合到神经网络训练中的方法，在图像空间之外使用潜在空间嵌入来引导训练，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖模型内部机制来整合显著性指导，但研究表明这些机制可能不可靠。挑战源于仅将指导方法置于图像空间中。

Method: 使用对比嵌入将人类显著性整合到网络训练中，应用显著性保持和显著性退化信号增强，通过对比三元组损失引导模型关注显著特征，并对logit分布进行完整性检查。

Result: 在开放集和封闭集场景中，相比最先进的基于显著性的方法，分类性能均有提升，且在不同骨干网络和任务上表现出良好的泛化能力。

Conclusion: SAGE通过将人类显著性指导从图像空间转移到潜在空间，有效提升了模型性能，证明了该方法在各种任务中的广泛适用性。

Abstract: Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.

</details>


### [244] [Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion](https://arxiv.org/abs/2511.12757)
*Nicholas Karris,Luke Durell,Javier Flores,Tegan Emerson*

Main category: cs.CV

TL;DR: 论文发现Stable Diffusion的CLIP嵌入具有排列不变性，因此可以将其视为Wasserstein空间中的点云而非欧几里得空间中的矩阵。基于此提出了使用最优传输理论进行嵌入插值的方法，相比传统插值方法能生成更平滑的图像过渡。


<details>
  <summary>Details</summary>
Motivation: 发现Stable Diffusion对CLIP嵌入矩阵的行具有排列不变性，这启发了将嵌入解释为Wasserstein空间中的点云而非欧几里得空间中的矩阵的新视角。

Method: 将嵌入插值问题重新构建为最优传输问题，通过求解最优传输问题计算嵌入之间的最短路径（测地线），从而在嵌入空间中实现更自然和几何平滑的过渡。

Result: 实验表明，基于最优传输的插值方法相比其他标准插值方法能产生更平滑和连贯的中间（插值）图像。

Conclusion: 将嵌入视为点云（而非矩阵）能更好地反映和利用嵌入空间的几何特性，最优传输方法确实能提供更平滑的图像插值效果。

Abstract: It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.

</details>


### [245] [RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.12767)
*Cătălin-Alexandru Rîpanu,Andrei-Theodor Hotnog,Giulia-Stefania Imbrea,Dumitru-Clementin Cercel*

Main category: cs.CV

TL;DR: 本文介绍了首个罗马尼亚孤立手语识别数据集RoCoISLR，包含9000多个视频样本和近6000个标准化词汇，并评估了7种最先进的视频识别模型，其中基于Transformer的架构表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模标准化的罗马尼亚孤立手语识别数据集，限制了该领域的研究进展，需要填补这一空白。

Method: 构建RoCoISLR数据集，包含9000+视频样本和近6000个标准化词汇；在一致实验设置下评估7种视频识别模型（I3D、SlowFast、Swin Transformer、TimeSformer、Uniformer、VideoMAE、PoseConv3D）。

Result: 基于Transformer的架构优于卷积基线；Swin Transformer达到34.1%的Top-1准确率；与WLASL2000语料库相比，突显了低资源手语中长尾类分布的挑战。

Conclusion: RoCoISLR为系统性的罗马尼亚孤立手语识别研究提供了初步基础，证明了该数据集的实用性和挑战性。

Abstract: Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.

</details>


### [246] [Lightweight Optimal-Transport Harmonization on Edge Devices](https://arxiv.org/abs/2511.12785)
*Maria Larchenko,Dmitry Guskov,Alexander Lobashev,Georgy Derevyanko*

Main category: cs.CV

TL;DR: 提出轻量级颜色协调方法MKL-Harmonizer，支持设备端实时推理，用于增强现实中的无缝图像合成


<details>
  <summary>Details</summary>
Motivation: 解决增强现实中颜色协调算法缺乏实时解决方案的问题，支持在AR流水线中集成颜色协调功能

Method: 利用经典最优传输理论，训练紧凑编码器预测Monge-Kantorovich传输映射

Result: 在真实AR合成图像上，该方法获得了最佳综合评分，优于现有最先进方法

Conclusion: 提出的轻量级方法支持设备端推理，并发布了专用AR数据集和工具包以支持进一步研究

Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.

</details>


### [247] [Enhancing Neuro-Oncology Through Self-Assessing Deep Learning Models for Brain Tumor Unified Model for MRI Segmentation](https://arxiv.org/abs/2511.12801)
*Andrew Zhou*

Main category: cs.CV

TL;DR: 提出一个不确定性感知框架，增强nnUNet以提供体素级不确定性估计，同时结合正常和癌症数据集实现全脑结构分割，为临床决策提供肿瘤定位和不确定性信息。


<details>
  <summary>Details</summary>
Motivation: 解决当前脑肿瘤分割方法缺乏不确定性估计和手术所需周围健康脑结构分割的问题，提升临床应用的可靠性。

Method: 在nnUNet基础上增加不确定性通道，使用BraTS2023数据集训练，同时结合正常和癌症数据集构建统一模型进行全脑结构分割。

Result: 不确定性估计达到0.750的相关性和0.047的RMSD，不影响肿瘤分割精度；全脑结构分割DSC达0.81，肿瘤分割DSC达0.86。

Conclusion: 该框架首次同时输出肿瘤在自然环境中的分割结果和叠加的不确定性图，为AI辅助手术决策提供关键洞察和错误修正指导。

Abstract: Accurate segmentation of brain tumors is vital for diagnosis, surgical planning, and treatment monitoring. Deep learning has advanced on benchmarks, but two issues limit clinical use: no uncertainty estimates for errors and no segmentation of healthy brain structures around tumors for surgery. Current methods fail to unify tumor localization with anatomical context and lack confidence scores. This study presents an uncertainty-aware framework augmenting nnUNet with a channel for voxel-wise uncertainty. Trained on BraTS2023, it yields a correlation of 0.750 and RMSD of 0.047 for uncertainty without hurting tumor accuracy. It predicts uncertainty in one pass, with no extra networks or inferences, aiding clinical decisions. For whole-brain context, a unified model combines normal and cancer datasets, achieving a DSC of 0.81 for brain structures and 0.86 for tumor, with robust key-region performance. Combining both innovations gives the first model outputting tumor in natural surroundings plus an overlaid uncertainty map. Visual checks of outputs show uncertainty offers key insights to evaluate predictions and fix errors, helping informed surgical decisions from AI.

</details>


### [248] [MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12810)
*Leena Alghamdi,Muhammad Usman,Hafeez Anwar,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: 提出了一种多尺度递归网络MSRNet，通过金字塔视觉Transformer提取多尺度特征，结合注意力机制和递归解码策略，在伪装目标检测任务中取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前伪装目标检测方法在复杂场景中（特别是小目标和多目标情况）仍存在检测精度不足的问题，需要改进。

Method: 使用金字塔视觉Transformer作为骨干网络提取多尺度特征，通过注意力机制融合多尺度特征，采用递归解码策略和细粒度融合单元优化特征表示。

Result: 在两个基准数据集上达到了最先进性能，在另外两个数据集上排名第二，能够有效检测小目标和多目标伪装物体。

Conclusion: 多尺度学习和递归特征优化的联合使用能够显著提升伪装目标检测性能，特别是在处理复杂场景中的小目标和多目标时表现优异。

Abstract: Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.

</details>


### [249] [SAGA: Source Attribution of Generative AI Videos](https://arxiv.org/abs/2511.12834)
*Rohit Kundu,Vishal Mohanty,Hao Xiong,Shan Jia,Athula Balachandran,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: SAGA是首个用于大规模AI生成视频源归属的综合框架，通过多粒度归属识别特定生成模型，仅需0.5%的源标记数据即可达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的普及导致超逼真合成视频泛滥，传统二元真伪检测器已无法应对滥用风险，迫切需要能识别具体生成模型的源归属方法。

Method: 提出新颖的视频Transformer架构，利用鲁棒视觉基础模型特征捕捉时空伪影；采用数据高效的预训练-归属策略；引入Temporal Attention Signatures可解释性方法。

Result: 在公共数据集上的广泛实验表明，SAGA在合成视频溯源方面设立了新基准，在跨域场景下表现优异，匹配全监督性能。

Conclusion: SAGA为法证和监管应用提供了关键且可解释的洞察，是首个能大规模进行AI生成视频源归属的框架。

Abstract: The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.

</details>


### [250] [Video Finetuning Improves Reasoning Between Frames](https://arxiv.org/abs/2511.12868)
*Ruiqi Yang,Tian Yun,Zihan Wang,Ellie Pavlick*

Main category: cs.CV

TL;DR: 本文提出视觉思维链(vCoT)方法，通过生成连续帧间的过渡事件描述来增强多模态大语言模型的视频理解能力。研究发现视频微调模型已隐式掌握帧间过渡推理，而图像模型通过vCoT可获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型从图像扩展到视频时，往往只是简单拼接帧标记，缺乏对视频时序关系的深入理解。本文旨在探索视频微调对多模态LLMs的真正价值。

Method: 提出视觉思维链(vCoT)方法，生成连续帧间的过渡事件描述作为显式推理过程。系统比较仅图像模型与视频微调模型在有/无过渡线索下的表现。

Result: vCoT显著提升仅图像模型在长视频问答中的性能，但对视频微调模型增益有限，表明后者已隐式掌握帧间过渡推理。视频模型还能将时序推理能力迁移到静态场景，在关系视觉推理任务中超越图像模型基线。

Conclusion: 视频微调使多模态LLMs能够隐式学习帧间过渡推理，这种时序推理能力可迁移到静态视觉任务，为视频理解提供了新的视角。

Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

</details>


### [251] [View-aware Cross-modal Distillation for Multi-view Action Recognition](https://arxiv.org/abs/2511.12870)
*Trung Thanh Nguyen,Yasutomo Kawanishi,Vijay John,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: 提出了ViCoKD框架，通过跨模态知识蒸馏解决部分重叠多视图动作识别问题，让模态和标注受限的学生模型从全监督多模态教师模型中学习知识


<details>
  <summary>Details</summary>
Motivation: 现有的多视图方法主要针对完全重叠的传感器设置，而现实世界中部分重叠的设置（动作只在部分视图中可见）研究不足，且许多系统只提供有限输入模态和序列级标注而非密集帧级标注

Method: 使用跨模态适配器和跨模态注意力让学生模型利用多模态相关性；提出视图感知一致性模块处理视图不对齐问题，通过人类检测掩码和置信度加权的Jensen-Shannon散度强制共视动作的预测对齐

Result: 在真实世界MultiSensor-Home数据集上的实验表明，ViCoKD在多种骨干网络和环境设置下始终优于竞争性蒸馏方法，在受限条件下甚至超越了教师模型

Conclusion: ViCoKD框架有效解决了部分重叠多视图动作识别中的模态和标注限制问题，通过知识蒸馏和视图一致性机制显著提升了性能

Abstract: The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.

</details>


### [252] [Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views](https://arxiv.org/abs/2511.12878)
*Junyi Ma,Wentao Bao,Jingyi Xu,Guanzhong Sun,Yu Zheng,Erhang Zhang,Xieyuanli Chen,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出EgoLoc方法，在无需物体掩码和动作类别标注的情况下，零样本定位第一人称视频中手与物体接触和分离的关键时刻。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注交互动作的行为范式（"如何交互"），但更精细的捕捉手与目标物体接触和分离的关键时刻（"何时交互"）问题尚未充分探索，这对混合现实中的沉浸式交互体验和机器人运动规划至关重要。

Method: 提出EgoLoc方法，采用手动力学引导采样生成高质量视觉提示，利用视觉语言模型识别接触/分离属性、定位特定时间戳，并提供闭环反馈进行进一步优化。

Result: 在公共数据集和新基准上的综合实验表明，EgoLoc在第一人称视频中实现了可信的时间交互定位，并能有效促进第一人称视觉和机器人操作任务中的多个下游应用。

Conclusion: EgoLoc消除了对物体掩码和动词-名词分类的需求，实现了可泛化的零样本实现，为第一人称视频中的时间交互定位提供了有效解决方案。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., "how to interact"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., "when to interact") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [253] [Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings](https://arxiv.org/abs/2511.12880)
*Zihao Lin,Zhenshan Shi,Sasa Zhao,Hanwei Zhu,Lingyu Zhu,Baoliang Chen,Lei Mo*

Main category: cs.CV

TL;DR: 提出一个基于数据和认知驱动的自动绘画创造力评估框架，通过分析绘画内容和风格两个维度来预测创造力分数，并实现可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前绘画创造力评估主要依赖专家主观评分，既费时又主观。受认知科学启发，创造力可以从绘画内容和风格两个互补维度来理解。

Method: 首先扩展现有创造力标注数据集，增加内容类别标注。然后提出多模态多任务学习框架，同时预测创造力分数、分类内容类型和提取风格特征，引入条件学习机制动态调整视觉特征提取。

Result: 实验结果表明，该模型在性能上优于现有基于回归的方法，并能提供与人类判断一致的可解释可视化结果。

Conclusion: 该框架实现了自动化和可解释的绘画创造力评估，为心理学、教育和认知科学领域的创造力研究提供了有效工具。

Abstract: Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025

</details>


### [254] [ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation](https://arxiv.org/abs/2511.12893)
*Kaixin Zhang,Ruiqing Yang,Yuan Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: ActVAR是一个动态激活框架，通过双重稀疏化（权重和token序列）提升VAR模型效率，在ImageNet 256×256基准上实现21.2% FLOPs减少且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 现有静态剪枝方法会永久移除权重或token，破坏预训练依赖关系，导致性能下降。VAR模型随着序列长度增长面临计算成本激增的问题。

Method: 将前馈网络分解为轻量级专家子网络，使用可学习路由器动态选择token特定的专家子集；同时通过门控token选择器识别高更新潜力token进行计算，重构未选择token以保持全局上下文和序列对齐；采用两阶段知识蒸馏策略训练。

Result: 在ImageNet 256×256基准测试中，ActVAR实现了高达21.2%的FLOPs减少，同时保持最小性能下降。

Conclusion: ActVAR通过动态激活框架有效解决了VAR模型计算效率问题，在不牺牲模型容量的前提下显著提升了推理效率。

Abstract: Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\%$ FLOPs reduction with minimal performance degradation.

</details>


### [255] [Reconstructing 3D Scenes in Native High Dynamic Range](https://arxiv.org/abs/2511.12895)
*Kaixuan Zhang,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: 提出了首个直接从原生HDR观测数据重建3D场景的方法NH-3DGS，通过新颖的亮度-色度分解技术，在重建流程中保持完整动态范围，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 专业数字媒体创作需要HDR成像，但现有3D场景重建主要基于LDR数据，限制了在专业工作流程中的应用。现有方法依赖多曝光融合或逆色调映射，增加了捕获复杂性并依赖合成监督。

Method: 提出Native High dynamic range 3D Gaussian Splatting (NH-3DGS)，采用新颖的亮度-色度分解颜色表示，直接从原生HDR相机数据进行优化，在整个重建流程中保持完整动态范围。

Result: 在合成和真实多视角HDR数据集上的实验表明，NH-3DGS在重建质量和动态范围保持方面显著优于现有方法，能够直接从原生HDR捕获实现专业级3D重建。

Conclusion: NH-3DGS是首个直接从原生HDR观测数据重建3D场景的方法，通过亮度-色度分解技术实现了专业级的重建质量，为专业数字媒体工作流程提供了有效解决方案。

Abstract: High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.

</details>


### [256] [FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI](https://arxiv.org/abs/2511.12899)
*Hao Li,Zhenfeng Zhuang,Jingyu Lin,Yu Liu,Yifei Chen,Qiong Peng,Lequan Yu,Liansheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于频域分解预处理（FDP）的无监督脑MRI异常检测方法，通过分析病理特征的频域特性来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于脑解剖结构的多样性和标注数据的稀缺性，监督式脑MRI异常检测面临挑战。现有无监督方法使用模拟噪声训练生成模型，但这些模拟异常缺乏真实临床病变的生物物理保真度和形态复杂性。

Method: 通过系统性的频域分析发现异常具有独特的频率模式，提出了频率分解预处理（FDP）框架，利用频域重建同时实现病理抑制和解剖结构保留。

Result: 实验结果显示FDP能持续提升异常检测性能，与LDM结合时DICE分数提高了17.63%，并在多个基线方法上都保持稳健改进。

Conclusion: FDP是首个利用频域重建的无监督异常检测方法，能够无缝集成到现有异常模拟技术中，显著提升检测性能同时保持诊断保真度。

Abstract: Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.

</details>


### [257] [DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2511.12908)
*Junbo Zou,Haotian Xia,Zhen Ye,Shengjie Zhang,Christopher Lai,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: DeepSport是首个端到端训练的多模态大语言模型框架，专为多任务、多运动的视频理解设计，通过主动迭代推理和专门帧提取工具实现"视频思考"，在6.7k问题测试基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决体育视频理解的独特挑战：感知高速动态、理解复杂规则和长时序推理。现有方法要么是单一运动专用，要么局限于特定任务，或者依赖缺乏稳健学习推理过程的训练无关范式。

Method: 提出数据蒸馏管道从10个不同数据源合成高质量思维链轨迹（78k训练数据），采用两阶段训练策略：监督微调后接强化学习（带新型门控工具使用奖励），通过专门帧提取工具实现主动迭代推理。

Result: 在6.7k问题测试基准上的广泛实验表明，DeepSport达到最先进性能，显著优于专有模型和开源模型基线。

Conclusion: 这项工作为领域特定视频推理建立了新基础，以应对多样化体育的复杂性。

Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.

</details>


### [258] [CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection](https://arxiv.org/abs/2511.12909)
*Yaohua Zha,Xue Yuerong,Chunlin Fan,Yuansong Wang,Tao Dai,Ke Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出了一种基于曲率增强自监督学习的3D异常检测框架CASL，仅使用点云曲率作为异常分数就能超越传统方法，无需特定任务设计即可实现领先的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D异常检测方法缺乏通用性，而传统自监督点云模型在异常检测任务上表现不佳，需要开发更通用的3D模型。

Method: 基于U-Net架构的曲率增强自监督学习框架，通过多尺度曲率提示指导解码器预测点空间坐标，无需专门异常检测机制。

Result: 在异常检测任务上取得领先性能，且学习到的表征能很好地泛化到标准3D理解任务如点云分类。

Conclusion: 曲率在3D异常检测中起关键作用，CASL框架展示了无需任务特定设计即可实现高性能异常检测的可行性。

Abstract: Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.

</details>


### [259] [Explore How to Inject Beneficial Noise in MLLMs](https://arxiv.org/abs/2511.12917)
*Ruishu Zhu,Sida Huang,Ziheng Jiao,Hongyuan Zhang*

Main category: cs.CV

TL;DR: 提出了一种通过注入有益随机噪声的新型微调策略MuNG，仅需调整1-2%额外参数即可超越全参数微调和其他现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法往往忽略跨模态异质性，限制了多模态大语言模型的潜力。

Method: 从变分推理角度重新表述MLLMs的推理过程，设计多模态噪声生成器动态分析图像-文本对的跨模态关系，生成任务自适应的有益噪声。

Result: 在QwenVL和LLaVA上的实验表明，该方法超越全参数微调和其他现有微调方法，性能显著提升。

Conclusion: 通过注入有益噪声有效抑制不相关语义成分，显著改善跨模态表示对齐，增强下游任务性能。

Abstract: Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\sim2\%$ additional parameters. The relevant code is uploaded in the supplementary.

</details>


### [260] [CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation](https://arxiv.org/abs/2511.12919)
*Dexin Zuo,Ang Li,Wei Wang,Wenxian Yu,Danping Zou*

Main category: cs.CV

TL;DR: CoordAR是一个用于未见物体6D姿态估计的自回归框架，仅需单张参考视图而非完整3D模型，通过离散化坐标映射和概率预测解决对称性和遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于单参考视图的6D姿态估计方法依赖实值坐标回归，存在全局一致性不足和对称/遮挡场景处理困难的问题，需要新的不确定性建模方法。

Method: 提出CoordAR框架：1) 坐标映射离散化实现概率预测；2) RGB外观与坐标线索解耦编码；3) 基于位置对齐查询特征和部分生成序列的自回归transformer解码器。

Result: 在多个基准测试中显著优于现有方法，在对称性、遮挡等真实场景挑战下表现出强鲁棒性。

Conclusion: CoordAR通过自回归概率建模有效解决了单参考6D姿态估计中的全局一致性和不确定性挑战，为无3D模型的物体姿态估计提供了新思路。

Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.

</details>


### [261] [Generative Photographic Control for Scene-Consistent Video Cinematic Editing](https://arxiv.org/abs/2511.12921)
*Huiqiang Sun,Liao Shen,Zhan Peng,Kun Wang,Size Wu,Yuhang Zang,Tianqi Liu,Zihao Huang,Xingyu Zeng,Zhiguo Cao,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: CineCtrl是首个视频电影编辑框架，提供对专业相机参数（如散景、快门速度）的精细控制，通过解耦交叉注意力机制实现独立控制，并构建大规模数据集进行训练。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型主要局限于相机运动控制，难以控制景深、曝光等摄影元素，而这些元素对电影叙事至关重要。

Method: 提出解耦交叉注意力机制分离相机运动和摄影输入，开发综合数据生成策略包括模拟摄影效果和真实世界采集，构建大规模训练数据集。

Result: 模型能够生成具有精确控制、用户指定摄影相机效果的高保真视频。

Conclusion: CineCtrl成功实现了对专业相机参数的精细控制，为视频生成中的电影风格编辑提供了有效解决方案。

Abstract: Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.

</details>


### [262] [Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes](https://arxiv.org/abs/2511.12932)
*Feng Lv,Haoxuan Feng,Zilu Zhang,Chunlong Xia,Yanfeng Li*

Main category: cs.CV

TL;DR: 提出了一个统一的文本驱动框架，用于交通场景的图像生成和编辑，通过可控掩码机制和多视角数据增强几何多样性，采用两阶段训练策略提升文本-图像对齐和细节质量。


<details>
  <summary>Details</summary>
Motivation: 解决智能交通系统中文本驱动图像生成和编辑面临的挑战：交通元素语义丰富度不足、摄像机视角有限、合成图像视觉保真度低、文本描述与生成内容对齐差。

Method: 使用可控掩码机制统一图像生成和编辑任务；结合车辆侧和路边多视角数据增强几何多样性；采用两阶段训练策略（大规模粗粒度数据概念学习+细粒度描述数据微调）；引入掩码区域加权损失动态关注关键小区域。

Result: 在交通场景的文本驱动图像生成和编辑任务中取得了领先性能。

Conclusion: 提出的统一框架有效解决了交通场景图像生成和编辑中的关键问题，显著提升了生成质量和文本-图像对齐效果。

Abstract: With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.

</details>


### [263] [PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos](https://arxiv.org/abs/2511.12935)
*Dianbing Xi,Guoyuan An,Jingsen Zhu,Zhijian Liu,Yuan Liu,Ruiyuan Zhang,Jiayuan Lu,Rui Wang,Yuchi Huo*

Main category: cs.CV

TL;DR: PFAvatar是一种从"每日穿搭"照片重建高质量3D虚拟形象的新方法，通过两阶段流程：先微调姿态感知扩散模型，再蒸馏到NeRF表示，实现快速个性化（5分钟）和高保真重建。


<details>
  <summary>Details</summary>
Motivation: 解决从现实世界OOTD照片（包含多样姿态、遮挡和复杂背景）重建3D虚拟形象的挑战，避免传统方法中图像分解导致的inconsistency问题。

Method: 两阶段方法：1) 使用ControlNet进行姿态估计和Condition Prior Preservation Loss微调扩散模型；2) 通过规范SMPL-X空间采样和Multi-Resolution 3D-SDS优化NeRF表示，处理遮挡和保留高频纹理。

Result: 相比现有方法，PFAvatar在重建保真度、细节保留和遮挡/截断鲁棒性方面表现更优，实现48倍加速（5分钟完成个性化），支持虚拟试穿、动画等下游应用。

Conclusion: PFAvatar通过端到端学习完整外观和连续辐射场表示，显著推进了从真实世界OOTD相册生成实用3D虚拟形象的能力，具有广泛的应用价值。

Abstract: We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.

</details>


### [264] [ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios](https://arxiv.org/abs/2511.12938)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: ProtoAnomalyNCD是一个基于原型学习的框架，用于发现和分类多种未见过的异常类型，可集成到各种异常检测方法中，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法主要判断是否存在异常，但实际应用需要发现和分类多种异常类型。由于工业异常语义细微且现有方法未充分利用图像先验，直接聚类方法效果不佳。

Method: 1. 使用Grounded SAM和文本提示定位物体区域作为先验；2. 引入异常图引导注意力块，设计区域引导因子区分背景、物体区域和异常区域；3. 在统一原型学习框架下发现未见异常类别并实现多类型异常分类。

Result: 在MVTec AD、MTD和Real-IAD数据集上优于最先进方法，并能扩展到检测未见异常值，实现任务级统一。

Conclusion: ProtoAnomalyNCD通过结合物体区域定位和异常图先验，有效增强异常特征并抑制背景噪声，在发现和分类多种异常类型方面表现出色。

Abstract: Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.

</details>


### [265] [Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking](https://arxiv.org/abs/2511.12939)
*Wei Jiang,Jiahao Cui,Yizheng Wu,Zhan Peng,Zhiyu Pan,Zhiguo Cao*

Main category: cs.CV

TL;DR: 提出了一种基于半监督学习的高动态范围图像重建方法，通过教师模型生成伪HDR标签，并使用不确定性掩码机制过滤不可靠区域，仅需6.7%的HDR真值就能达到全监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的HDR图像重建方法需要LDR-HDR图像对，但这些配对数据难以获取，因此需要研究如何在有限HDR真值下实现可比较性能的注释高效方法。

Method: 采用半监督学习框架，教师模型为无真值的LDR样本生成伪HDR标签，学生模型从伪标签学习。提出基于不确定性的像素级和块级掩码过程来过滤伪标签中的不可靠区域。

Result: 该方法不仅优于之前的注释高效算法，而且仅使用6.7%的HDR真值就能达到最新全监督方法的可比性能。

Conclusion: 通过不确定性掩码机制有效缓解了确认偏差问题，证明了半监督学习在HDR图像重建任务中的有效性，显著减少了所需的标注数据量。

Abstract: Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.

</details>


### [266] [Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention](https://arxiv.org/abs/2511.12940)
*Taiye Chen,Zihan Ding,Anjian Li,Christina Zhang,Zeqi Xiao,Yisen Wang,Chi Jin*

Main category: cs.CV

TL;DR: 提出RAD框架，通过将LSTM集成到扩散变换器中，解决了长视频生成中的记忆遗忘和时空不一致问题，实现了在固定内存预算下更有效的长期信息保留。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成长视频时存在记忆压缩和检索不足的问题，导致超出窗口大小后出现遗忘和时空不一致性，需要增强历史信息的保留能力。

Method: 在扩散变换器框架中引入循环神经网络（RNN），特别是结合注意力机制的LSTM，并提出RAD框架，在训练和推理时一致执行逐帧自回归进行记忆更新和检索。

Result: 在Memory Maze和Minecraft数据集上的实验表明，RAD在长视频生成方面具有优越性，证明了LSTM在序列建模中的有效性。

Conclusion: RAD框架通过LSTM增强记忆保留能力，有效解决了长视频生成中的遗忘问题，在固定内存预算下实现了更好的长期一致性。

Abstract: Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.

</details>


### [267] [T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving](https://arxiv.org/abs/2511.12956)
*Chen Ma,Ningfei Wang,Junhao Zheng,Qing Guo,Qian Wang,Qi Alfred Chen,Chao Shen*

Main category: cs.CV

TL;DR: DiffSign是一个基于文本到图像(T2I)的交通标志识别(TSR)系统对抗性外观攻击框架，旨在生成物理鲁棒、高效、可迁移、实用且隐蔽的攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性外观攻击存在显著局限性：像素级扰动方法缺乏隐蔽性且过拟合特定模型，T2I扩散模型方法效果有限且泛化能力差。

Method: 提出精心设计的攻击流程，集成CLIP损失和掩码提示以提高攻击聚焦性和可控性；提出两种新颖的风格定制方法来引导视觉外观，改善域外交通标志攻击泛化能力和隐蔽性。

Result: 在多种真实世界条件下进行广泛评估，包括不同距离、角度、光照条件和标志类别，平均物理世界攻击成功率达到83.3%。

Conclusion: DiffSign在攻击可迁移性方面表现出高效性，能够生成物理鲁棒、高度有效、可迁移、实用且隐蔽的外观攻击。

Abstract: Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.
  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.

</details>


### [268] [EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics](https://arxiv.org/abs/2511.12962)
*Daniel Cavadia*

Main category: cs.CV

TL;DR: EndoSight AI是一个深度学习架构，用于实时检测和分割胃肠道息肉，在GPU上达到35+FPS的推理速度，检测mAP为88.3%，分割Dice系数达69%。


<details>
  <summary>Details</summary>
Motivation: 内窥镜检查中精确实时检测胃肠道息肉对结直肠癌早期诊断和预防至关重要。

Method: 基于公开Hyper-Kvasir数据集开发深度学习架构，采用热感知训练程序确保模型鲁棒性，结合临床相关性能指标。

Result: 息肉检测平均精度达88.3%，分割Dice系数最高69%，GPU推理速度超过35帧/秒。

Conclusion: 该集成AI解决方案可无缝部署到内窥镜工作流程中，有望提高胃肠道医疗的诊断准确性和临床决策能力。

Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

</details>


### [269] [CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models](https://arxiv.org/abs/2511.12964)
*Mehrab Mustafy Rahman,Jayanth Mohan,Tiberiu Sosea,Cornelia Caragea*

Main category: cs.CV

TL;DR: CalibrateMix是一种针对半监督学习模型的校准方法，通过识别易学和难学样本并进行有针对性的mixup，在保持分类准确率的同时显著改善模型校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法存在校准不佳的问题，模型会产生过度自信的预测。虽然mixup在监督学习中已证明能改善校准，但在半监督学习中由于伪标签的不可靠性，随机mixup面临挑战。

Method: 利用训练动态识别易学和难学样本，然后对这些样本进行有针对性的mixup操作，而不是随机mixup伪标签。

Result: 在多个基准图像数据集上的实验表明，该方法实现了更低的预期校准误差(ECE)和更高的准确率。

Conclusion: CalibrateMix能有效改善半监督学习模型的校准性能，同时保持或提升分类准确率。

Abstract: Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.

</details>


### [270] [GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.12968)
*Ning Han,Zhenyu Ge,Feng Han,Yuhua Sun,Chengqing Li,Jingjing Chen*

Main category: cs.CV

TL;DR: 提出了GrOCE框架，一种无需训练的概念擦除方法，通过图语义推理实现精确和自适应的概念移除。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法要么依赖昂贵的微调，要么进行粗糙的语义分离，往往会降低无关概念的质量，且难以适应不断变化的概念集。

Method: GrOCE将概念及其相互关系建模为动态语义图，包含三个组件：动态拓扑图构建、自适应聚类识别和选择性边切断。

Result: 在概念相似度和FID指标上达到最先进性能，实现了高效、准确且稳定的概念擦除。

Conclusion: GrOCE提供了一种无需重新训练的概念擦除解决方案，能够精确隔离不需要的内容同时保持全局语义。

Abstract: Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fréchet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.

</details>


### [271] [HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology](https://arxiv.org/abs/2511.12969)
*Ziqiao Weng,Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee AD Cooper,Weidong Cai,Bo Zhou*

Main category: cs.CV

TL;DR: HiFusion是一个深度学习框架，通过分层建模和跨尺度融合从H&E染色病理图像预测空间转录组基因表达，在多个基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学技术复杂且成本高昂，现有方法难以捕捉spot内的生物异质性，且容易受到周围组织形态学噪声的影响。

Method: HiFusion包含两个互补组件：分层spot内建模模块通过多分辨率子块分解提取细粒度形态表征；上下文感知跨尺度融合模块使用交叉注意力选择性整合生物相关区域上下文。

Result: 在两个基准ST数据集上的广泛实验表明，HiFusion在2D切片交叉验证和更具挑战性的3D样本特定场景中都达到了最先进的性能。

Conclusion: HiFusion作为一个稳健、准确且可扩展的解决方案，具有从常规组织病理学进行ST推断的潜力。

Abstract: Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.

</details>


### [272] [MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning](https://arxiv.org/abs/2511.12976)
*Yoonjae Seo,Ermal Elbasani,Jaehong Lee*

Main category: cs.CV

TL;DR: MCAQ-YOLO是一种形态复杂度感知的量化框架，通过五个形态学指标来指导空间自适应比特分配，在目标检测任务中实现比均匀量化更高的精度和收敛效率。


<details>
  <summary>Details</summary>
Motivation: 大多数神经网络量化方法在空间区域上应用均匀的比特精度，忽略了视觉数据的异构结构和纹理复杂性。

Method: 使用分形维度、纹理熵、梯度方差、边缘密度和轮廓复杂度五个形态学指标来表征局部视觉形态，并基于这些指标与量化敏感性的相关性动态调整比特精度。同时采用基于课程的量化感知训练方案逐步增加量化难度。

Result: 在安全设备数据集上达到85.6% mAP@0.5，平均4.2比特，压缩比7.6倍，比均匀4比特量化高3.5个百分点，每张图像仅增加1.8毫秒运行时间。在COCO和Pascal VOC上的跨数据集验证也显示了一致的性能提升。

Conclusion: 形态驱动的空间量化可以增强计算受限、安全关键视觉识别任务的效率和鲁棒性。

Abstract: Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.

</details>


### [273] [ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes](https://arxiv.org/abs/2511.12977)
*Yixuan Yang,Luyang Xie,Zhen Luo,Zixiang Zhao,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: ArtiWorld是一个从3D场景中自动识别可关节化物体并转换为可执行URDF模型的流程，核心是Arti4URDF，利用点云、大语言模型和URDF导向提示设计，在保持3D形状的同时将刚性物体转换为交互式关节化物体。


<details>
  <summary>Details</summary>
Motivation: 现有3D模拟资产大多是刚性的，手动转换为关节化物体成本高昂，需要自动化的解决方案来构建交互式模拟器和机器人学习环境。

Method: 使用场景感知流程，从文本场景描述中定位候选可关节化物体，利用3D点云、大语言模型先验知识和URDF导向提示设计，重建可执行URDF模型。

Result: 在3D模拟物体、完整3D模拟场景和真实世界扫描场景三个层面评估，均优于现有方法，达到最先进性能，保持物体几何形状并正确捕捉交互性。

Conclusion: 为直接从现有3D资产构建交互式、机器人就绪的模拟环境提供了实用路径。

Abstract: Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.

</details>


### [274] [Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach](https://arxiv.org/abs/2511.12978)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: 提出了CCI方法，利用CLIP的patch嵌入将空间块聚类成语义连贯的簇，通过掩码和评估预测相对变化来识别概念重要性。该方法在忠实性基准测试中达到新SOTA，并引入COVAR基准来系统评估前景和背景变化对模型的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对比视觉语言模型（如CLIP）在零样本识别方面表现强劲，但容易受到虚假相关性的影响，特别是对背景的过度依赖。需要开发更好的解释性方法来诊断和解决这些问题。

Method: 提出CCI方法：1）使用CLIP的patch嵌入将空间块聚类；2）掩码这些簇；3）评估预测的相对变化。结合GroundedSAM自动分类预测为前景或背景驱动。还开发了COVAR基准来系统变化前景和背景。

Result: CCI在忠实性基准测试中大幅超越先前方法，如在MS COCO检索的删除AUC指标上获得超过两倍的改进。通过CCI与COVAR结合，对18个CLIP变体进行了全面评估，揭示了模型脆弱性的来源。

Conclusion: CCI方法为理解CLIP模型的决策提供了强大的诊断工具，COVAR基准能够系统评估前景和背景变化的影响，为开发更鲁棒的视觉语言模型指明了方向。

Abstract: Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.

</details>


### [275] [UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective](https://arxiv.org/abs/2511.12988)
*Furui Xu,Shaobo Wang,Jiajun Zhang,Chenghao Sun,Haixiang Tang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出UNSEEN框架，从泛化角度进行数据集剪枝，通过使用未在训练中见过的模型来评分样本，解决传统方法中评分密集分布的问题，并在多步场景中优化核心集质量。


<details>
  <summary>Details</summary>
Motivation: 深度学习数据集规模增长带来计算挑战，传统数据集剪枝方法依赖训练阶段的模型性能评分，导致样本评分密集分布在狭窄范围内，降低了样本区分度。

Method: 提出UNSEEN框架：1）从泛化角度评分，使用未接触过样本的模型进行评分；2）扩展到多步场景，通过在不同核心集上训练的模型进行增量选择；3）动态优化核心集质量。

Result: 在CIFAR-10、CIFAR-100和ImageNet-1K上的实验表明，UNSEEN显著优于现有SOTA方法。在ImageNet-1K上，减少30%训练数据的同时实现无损性能。

Conclusion: UNSEEN框架通过泛化视角和多步优化，有效解决了传统数据集剪枝方法的局限性，在保持性能的同时显著减少了计算需求。

Abstract: The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.

</details>


### [276] [Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection](https://arxiv.org/abs/2511.12992)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 提出WSAE-Net方法，通过加权语义图和自适应候选编辑序列优化视觉反事实解释，提高语义相关性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统视觉反事实解释方法忽视替换区域与目标对象的语义相关性，损害模型可解释性并阻碍编辑流程。

Method: 使用加权语义图减少非语义特征单元计算，通过自适应候选编辑序列确定最优计算顺序，保持替换特征单元与目标对象的语义相关性。

Result: 实验表明该方法性能优越，有助于更清晰深入地理解视觉反事实解释。

Conclusion: WSAE-Net通过语义感知的编辑策略显著提升了视觉反事实解释的质量和效率。

Abstract: In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.

</details>


### [277] [PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching](https://arxiv.org/abs/2511.12998)
*Zewei Chang,Zheng-Peng Duan,Jianxing Zhang,Chun-Le Guo,Siyu Liu,Hyungju Chun,Hyunhee Park,Zikun Liu,Chongyi Li*

Main category: cs.CV

TL;DR: PerTouch是一个基于扩散模型的图像润色框架，支持语义级图像润色，通过参数映射实现细粒度控制，并利用VLM驱动的代理处理用户指令，结合反馈驱动重思考和场景感知记忆机制来更好地对齐用户意图。


<details>
  <summary>Details</summary>
Motivation: 解决图像润色中平衡可控性和主观性的挑战，既要增强视觉质量，又要符合用户的个性化审美偏好。

Method: 使用包含特定语义区域属性值的参数映射作为输入，构建显式的参数到图像映射；引入语义替换和参数扰动机制改善语义边界感知；开发VLM驱动的代理处理强弱用户指令；配备反馈驱动重思考和场景感知记忆机制。

Result: 广泛实验证明了每个组件的有效性，PerTouch在个性化图像润色方面表现出优越性能。

Conclusion: PerTouch框架能够有效支持语义级图像润色，同时保持全局美学，更好地与用户意图对齐并捕捉长期偏好。

Abstract: Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.

</details>


### [278] [Medal S: Spatio-Textual Prompt Model for Medical Segmentation](https://arxiv.org/abs/2511.13001)
*Pengcheng Shi,Jiawei Chen,Jiaqi Liu,Xinglin Zhang,Tao Chen,Lei Li*

Main category: cs.CV

TL;DR: Medal S是一个医学分割基础模型，支持原生分辨率空间和文本提示的端到端训练框架，在多个医学影像模态上实现高效的多类别分割。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本方法缺乏空间感知能力的问题，通过通道对齐机制减少分辨率不匹配带来的不准确性，提升多类别医学分割的性能和效率。

Method: 采用通道对齐机制将体积提示与文本嵌入对齐，使用轻量级3D卷积模块进行体素空间细化，支持并行空间提示处理，并提出动态重采样、两阶段推理策略等优化技术。

Result: 在五模态验证集上，DSC达到75.44（vs SAT 69.83），NSD达到77.34（vs 71.06），F1达到38.24（vs 24.88），并行空间提示相比顺序提示减少90%以上推理时间。

Conclusion: Medal S通过协调空间精度与语义文本指导，在医学多类别分割任务中展现出卓越的效率和准确性，优于基于顺序提示的方法。

Abstract: We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.

</details>


### [279] [Infinite-Story: A Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2511.13002)
*Jihun Park,Kyoungmin Lee,Jongmin Gim,Hyeonseo Jo,Minseok Oh,Wonhyeok Choi,Kyumin Hwang,Jaeyeul Kim,Minwoo Choi,Sunghoon Im*

Main category: cs.CV

TL;DR: Infinite-Story是一个无需训练的文本到图像生成框架，专门用于多提示词故事场景，通过身份提示替换和统一注意力引导机制解决身份和风格不一致问题，实现快速推理（1.72秒/图像）。


<details>
  <summary>Details</summary>
Motivation: 解决多提示词故事生成中的身份不一致和风格不一致问题，现有方法需要微调或推理速度慢。

Method: 基于尺度自回归模型，使用身份提示替换缓解文本编码器上下文偏差，以及包含自适应风格注入和同步引导适应的统一注意力引导机制。

Result: 在身份和风格一致性方面达到最先进性能，推理速度比现有最快模型快6倍以上（1.72秒/图像）。

Conclusion: Infinite-Story是一个高效实用的视觉故事生成框架，无需训练即可实现高质量一致性生成。

Abstract: We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.

</details>


### [280] [SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias](https://arxiv.org/abs/2511.13005)
*Wenqian Ye,Di Wang,Guangtao Zheng,Bohan Liu,Aidong Zhang*

Main category: cs.CV

TL;DR: 提出了SAGE方法，通过引导式提示选择来缓解CLIP模型中的多模态伪相关偏差，无需训练、微调或外部标注，提高零样本分类的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在多模态学习中存在伪相关偏差，倾向于依赖虚假特征（如背景）而非核心特征进行推理，这严重影响了在分布外数据上的鲁棒性。现有方法通常需要下游数据微调或先验知识，限制了CLIP的开箱即用性。

Method: 首先理论分析多模态伪相关偏差在零样本分类中的影响，然后提出SAGE方法：通过探索提示模板空间，选择能够诱导类别间最大语义分离的提示，从而改善最差组鲁棒性。

Result: 在四个真实世界基准数据集和五个流行骨干模型上的广泛实验表明，SAGE持续提升了零样本性能和泛化能力，优于先前无需外部知识或模型更新的零样本方法。

Conclusion: SAGE是一种简单有效的零样本方法，能够在不依赖训练、微调或外部标注的情况下缓解多模态伪相关偏差，显著提升CLIP模型的鲁棒性和泛化性能。

Abstract: Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.

</details>


### [281] [Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis](https://arxiv.org/abs/2511.13011)
*Qingsen Ma,Chen Zou,Dianyun Wang,Jia Wang,Liuyu Xiang,Zhaofeng He*

Main category: cs.CV

TL;DR: DTGS是一个统一框架，将Retinex启发的光照分解与热引导的3D高斯泼溅相结合，用于极低光照条件下的新视角合成，解决了标准3DGS在欠曝光输入下的几何和颜色一致性问题。


<details>
  <summary>Details</summary>
Motivation: 在极低光照条件下，新视角合成面临几何、颜色一致性和辐射稳定性的严重退化。标准3DGS直接应用于欠曝光输入时会失败，因为跨视图的独立增强会导致光照不一致和几何失真。

Method: DTGS通过循环增强-重建机制，在增强、几何和热监督之间进行联合优化。包含热监督分支动态平衡增强、结构和热损失，以及嵌入3DGS循环的Retinex分解模块提供物理可解释的反射-光照分离。

Result: 在构建的RGBT-LOW多视图低光热数据集上的广泛实验表明，DTGS显著优于现有的低光增强和3D重建基线，在极端光照下实现了优异的辐射一致性、几何保真度和颜色稳定性。

Conclusion: DTGS通过紧密耦合光照分解与热引导的3D高斯泼溅，成功解决了极低光照条件下的新视角合成问题，为低光3D重建提供了有效的解决方案。

Abstract: Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.

</details>


### [282] [You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection](https://arxiv.org/abs/2511.13013)
*Guoyi Zhang,Guangsheng Xu,Siyang Chen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: 提出BP-FPN，一种基于反向传播的特征金字塔架构，通过梯度隔离低层捷径和方向梯度正则化，显著提升红外小目标检测性能，在多个数据集上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要关注时空特征聚合，但性能提升有限，根本瓶颈在于单帧特征表示模糊而非时空建模本身。

Method: BP-FPN引入梯度隔离低层捷径(GILS)有效整合细粒度目标细节而不引发捷径学习，以及方向梯度正则化(DGR)在反向传播中强制层次特征一致性。

Result: 在多个公共数据集上的广泛实验表明，BP-FPN始终建立新的最先进性能。

Conclusion: 这是首个完全从反向传播角度为该任务设计的FPN架构，具有理论依据、计算开销可忽略，并能无缝集成到现有框架中。

Abstract: Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.

</details>


### [283] [Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues](https://arxiv.org/abs/2511.13015)
*King-Man Tam,Satoshi Ikehata,Yuta Asano,Zhaoyi An,Rei Kawakami*

Main category: cs.CV

TL;DR: GeoUniPS是一个通用光度立体网络，通过整合合成监督和来自大规模3D重建模型的几何先验，解决了传统方法在复杂真实场景中多光照线索不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 传统通用光度立体方法在复杂真实场景中，当多光照线索不可靠时（如偏置光照、阴影或自遮挡区域）表现不佳，需要更好的几何先验来提升性能。

Method: 设计了Light-Geometry双分支编码器，从冻结的3D重建模型中提取多光照线索和几何先验；引入PS-Perp数据集支持透视投影学习；整合合成监督与几何先验。

Result: 在多个数据集上实现了最先进的性能，特别是在复杂真实场景中，定量和定性评估都表现出色。

Conclusion: GeoUniPS通过利用3D重建模型作为视觉几何基础模型，成功提升了通用光度立体在复杂真实场景中的性能，证明了几何先验的有效性。

Abstract: Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.

</details>


### [284] [MeanFlow Transformers with Representation Autoencoders](https://arxiv.org/abs/2511.13019)
*Zheyuan Hu,Chieh-Hsin Lai,Ge Wu,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: 提出了一种在表示自编码器（RAE）潜在空间中训练MeanFlow的高效方法，通过一致性中期训练和两阶段方案解决了梯度爆炸问题，显著降低了训练和推理成本，在ImageNet上取得了优异的单步生成性能。


<details>
  <summary>Details</summary>
Motivation: MeanFlow在潜在空间中训练存在计算需求高、不稳定、推理成本高（SD-VAE解码器占主导）以及需要复杂引导超参数等问题，需要开发更高效的训练和采样方案。

Method: 在RAE潜在空间中训练MeanFlow，采用一致性中期训练进行轨迹感知初始化，使用两阶段方案：从预训练流匹配教师蒸馏加速收敛，然后可选的自举阶段使用单点速度估计器减少偏差。

Result: 在ImageNet 256上实现了1步FID 2.03（优于vanilla MF的3.43），采样GFLOPS减少38%，总训练成本降低83%；在ImageNet 512上达到1步FID 3.23，在所有基线中GFLOPS最低。

Conclusion: 该方法无需引导，简化了训练配置，显著降低了训练和采样计算成本，在保持高质量生成的同时实现了高效的少步生成。

Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.

</details>


### [285] [SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction](https://arxiv.org/abs/2511.13020)
*Yufei Wen,Yuting Zhang,Jingdan Kang,Hao Ren,Weibin Cheng,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: 提出SpectralAdapt半监督域适应框架，通过光谱密度掩码和光谱端元表示对齐，解决医疗HSI重建中的域偏移、光谱退化和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决医疗应用中人类HSI数据稀缺的问题，通过域适应方法利用丰富的通用领域HSI数据来提升人类中心HSI重建性能。

Method: SpectralAdapt框架包含：1) 光谱密度掩码(SDM)根据光谱复杂性自适应掩码RGB通道；2) 光谱端元表示对齐(SERA)使用物理可解释端元作为域不变锚点；3) 动量更新确保适应性和稳定性。

Result: 在基准数据集上实验显示，该方法在光谱保真度、跨域泛化能力和训练稳定性方面均有持续改进。

Conclusion: SSDA是医疗高光谱成像中解决数据稀缺问题的有效方案，SpectralAdapt框架成功缓解了域偏移、光谱退化和数据稀缺等挑战。

Abstract: Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.

</details>


### [286] [REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding](https://arxiv.org/abs/2511.13026)
*Jiaze Li,Hao Yin,Wenhui Tan,Jingyang Chen,Boshen Xu,Yuxun Qu,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: REVISOR是一个用于长视频理解的多模态反思框架，通过跨模态协作反思机制增强MLLMs的推理能力，无需额外微调或外部模型即可在多个基准测试中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 传统纯文本反思机制在长视频理解中存在局限性：1) 仅反思文本信息不足以处理丰富的动态视觉输入；2) 缺乏跨模态交互能力，无法在反思过程中充分整合视觉信息。

Method: 提出REVISOR框架，支持文本和视觉模态的协作反思过程。设计了DADR奖励机制，集成到GRPO训练策略中，确保模型推理与所选视频证据之间的因果对齐。

Result: 在VideoMME、LongVideoBench、MLVU和LVBench四个基准测试中取得了令人印象深刻的结果，显著提升了MLLMs的长视频理解能力。

Conclusion: REVISOR通过跨模态反思机制有效解决了长视频理解中的视觉信息整合问题，为MLLMs在复杂多模态任务中的推理能力提供了新的增强途径。

Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.

</details>


### [287] [Towards 3D Object-Centric Feature Learning for Semantic Scene Completion](https://arxiv.org/abs/2511.13031)
*Weihua Wang,Yubo Cui,Xiangru Lin,Zhiheng Li,Zheng Fang*

Main category: cs.CV

TL;DR: Ocean是一个面向对象的3D语义场景补全框架，通过将场景分解为单个对象实例来提高语义占用预测的准确性，在复杂环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉3D语义场景补全方法通常采用以自我为中心的范式，在整个场景中聚合和扩散特征，但往往忽略了细粒度的对象级细节，导致在复杂环境中出现语义和几何模糊问题。

Method: 首先使用轻量级分割模型MobileSAM从输入图像中提取实例掩码；然后引入3D语义组注意力模块，利用线性注意力在3D空间中聚合面向对象的特征；设计全局相似性引导注意力模块来处理分割错误和缺失实例；最后提出实例感知局部扩散模块，通过生成过程改进实例特征并在BEV空间中细化场景表示。

Result: 在SemanticKITTI和SSCBench-KITTI360基准测试上的广泛实验表明，Ocean实现了最先进的性能，mIoU分数分别达到17.40和20.28。

Conclusion: Ocean通过面向对象的方法有效解决了现有3D语义场景补全方法在复杂环境中的局限性，显著提升了语义占用预测的准确性。

Abstract: Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.

</details>


### [288] [Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts](https://arxiv.org/abs/2511.13032)
*Sheng Liu,Yuanzhi Liang,Jiepeng Wang,Sidan Du,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Uni-Inter是一个统一的人类运动生成框架，支持人-人、人-物、人-场景等多种交互场景，通过统一的交互体积表示实现异构实体的空间编码和关系推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特定任务设计且泛化能力有限，需要开发一个统一的任务无关架构来处理多种交互场景。

Method: 引入统一交互体积(UIV)作为体积表示，将异构交互实体编码到共享空间场中，通过关节级概率预测来生成运动。

Result: 在三个代表性交互任务上的实验表明，Uni-Inter实现了竞争性性能，并能很好地泛化到新的实体组合。

Conclusion: 统一建模复合交互为复杂环境中的可扩展运动合成提供了有前景的方向。

Abstract: We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.

</details>


### [289] [uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data](https://arxiv.org/abs/2511.13036)
*Dahyun Chung,Donghyun Shin,Yujin Sung,Seunggi Moon,Jinwoo Jeon,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 提出了一种轻量级、数据高效的多语言视觉-语言对齐框架，无需图像-文本对或文本-文本对，仅训练1.7M参数的投影模块，通过英语表示作为语义锚点实现多语言对齐。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP模型在低资源语言上扩展受限的问题，现有多语言视觉语言模型在捷克语、芬兰语、克罗地亚语、匈牙利语和罗马尼亚语等代表性不足语言上检索性能较低。

Method: 冻结预训练图像编码器和多语言文本编码器，仅训练紧凑的投影模块，使用对比损失以英语表示为语义锚点进行对齐。

Result: 在多个多语言检索基准测试中表现出色，在五个代表性不足语言上取得显著性能提升，现有模型在这些语言上通常表现不佳。

Conclusion: 基于枢轴的参数高效对齐策略对包容性多模态学习具有有效性，证明了轻量级框架在多语言视觉-语言对齐中的可行性。

Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.

</details>


### [290] [MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization](https://arxiv.org/abs/2511.13039)
*Zhenying Fang,Richang Hong*

Main category: cs.CV

TL;DR: 提出MGCA-Net方法，通过多粒度类别感知解决开放词汇时序动作定位问题，在基类和新型动作类别上都实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多在单一粒度上识别动作类别，这会降低基类和新型动作类别的识别准确性。需要解决多粒度类别感知问题以提升定位性能。

Method: 提出多粒度类别感知网络(MGCA-Net)，包含定位器、动作存在预测器、传统分类器和粗到细分类器。通过不同粒度的分类器分别处理基类和新型动作类别。

Result: 在THUMOS'14和ActivityNet-1.3基准测试中实现了最先进的性能，同时在零样本时序动作定位设置下也达到了最佳结果。

Conclusion: MGCA-Net通过多粒度类别感知有效增强了定位性能，为开放词汇时序动作定位提供了一种有效的解决方案。

Abstract: Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.

</details>


### [291] [DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation](https://arxiv.org/abs/2511.13047)
*Yan Gong,Jianli Lu,Yongsheng Gao,Jie Zhao,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.CV

TL;DR: DiffPixelFormer是一种用于RGB-D室内场景语义分割的差分像素感知Transformer，通过增强模态内表示和建模模态间交互来提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D融合方法依赖计算密集的跨注意力机制，对模态内和模态间特征关系建模不足，导致特征对齐不精确和判别表示有限。

Method: 提出Intra-Inter Modal Interaction Block (IIMIB)，通过自注意力捕获模态内长程依赖，使用Differential-Shared Inter-Modal (DSIM)模块建模模态间交互，解耦模态特定和共享线索，实现像素级跨模态对齐。采用动态融合策略平衡模态贡献。

Result: 在SUN RGB-D和NYUDv2基准测试中，DiffPixelFormer-L分别达到54.28%和59.95%的mIoU，比DFormer-L分别提升1.78%和2.75%。

Conclusion: DiffPixelFormer通过同时增强模态内表示和建模模态间交互，在RGB-D室内语义分割任务中取得了优越性能。

Abstract: Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.

</details>


### [292] [ViSS-R1: Self-Supervised Reinforcement Video Reasoning](https://arxiv.org/abs/2511.13054)
*Bo Fang,Yuxin Song,Qiangqiang Wu,Haoyuan Sun,Wenhao Wu,Antoni B. Chan*

Main category: cs.CV

TL;DR: 提出Pretext-GRPO自监督强化学习算法和ViSS-R1框架，通过视觉转换任务强制MLLM深度处理视觉信息，提升视频推理能力


<details>
  <summary>Details</summary>
Motivation: 当前基于R1的多模态大语言模型在视频任务中过度依赖文本推理，未能充分利用丰富的视觉信息，容易产生捷径学习和幻觉问题

Method: 1. Pretext-GRPO：在R1流程中引入自监督强化学习，对转换后的视觉输入正确解决前置任务给予正向奖励；2. ViSS-R1：将前置任务自监督学习直接集成到MLLM的R1后训练范式中，同时处理前置问题和真实用户查询

Result: 在六个广泛使用的视频推理和理解基准测试中表现出有效性和优越性

Conclusion: 该方法通过强制模型深度处理视觉信息，显著提升了复杂视频推理的鲁棒性和准确性

Abstract: Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.

</details>


### [293] [Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries](https://arxiv.org/abs/2511.13055)
*Ruixin Liu,Zejian Yuan*

Main category: cs.CV

TL;DR: MonoUnc是一个无需鸟瞰图的单目3D车道线检测器，通过局部车道结构建模随机不确定性，在ONCE-3DLanes和OpenLane数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖简化的几何假设（如独立点预测或全局平面建模），无法捕捉真实场景中的结构变化和随机不确定性。

Method: 将3D车道线投影到前视图空间并用参数曲线近似，基于曲线预测动态生成曲线点查询嵌入，将相邻点形成的线段建模为3D高斯分布，并设计3D高斯匹配损失函数。

Result: 在ONCE-3DLanes和OpenLane数据集上，MonoUnc在所有基准测试中都优于之前的最先进方法，特别是在更严格的评估标准下。

Conclusion: MonoUnc通过显式建模局部车道结构的不确定性，有效解决了单目3D车道检测中的随机不确定性问题，取得了最先进的性能。

Abstract: Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.

</details>


### [294] [FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation](https://arxiv.org/abs/2511.13063)
*Zhenghua Li,Hang Chen,Zihao Sun,Kai Li,Xiaolin Hu*

Main category: cs.CV

TL;DR: 提出了一种将Segment Anything 2 (SAM2)从自然图像领域迁移到电子显微镜图像神经结构分割的框架，通过特征引导注意力模块和双亲和度解码器，在冻结SAM2权重时达到SOTA水平，微调后显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜图像中的神经结构分割面临形态复杂、信噪比低和标注稀缺的挑战，现有方法在准确性和泛化性上受限。希望利用在大量自然图像上预训练的视觉基础模型的先验知识来解决这一问题。

Method: 使用SAM2提取通用特征，引入特征引导注意力模块利用SAM2的语义线索指导轻量级精细编码器关注困难区域，最后通过双亲和度解码器生成粗粒度和精炼的亲和度图。

Result: 实验结果表明，在冻结SAM2权重时性能与SOTA方法相当，在EM数据上微调后显著超越现有SOTA方法。

Conclusion: 研究表明，结合针对性领域自适应指导，将自然图像预训练的表征迁移可以有效解决神经元分割中的特定挑战。

Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

</details>


### [295] [RobustGait: Robustness Analysis for Appearance Based Gait Recognition](https://arxiv.org/abs/2511.13065)
*Reeshoon Sayera,Akash Kumar,Sirshapan Mitra,Prudvi Kamtam,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 提出了RobustGait框架，用于系统评估基于外观的步态识别系统在真实世界干扰和轮廓变化下的鲁棒性，涵盖四种扰动类型、多种轮廓提取方法和不同架构模型。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法在受控数据集上表现良好，但缺乏对其在真实世界干扰和轮廓变化下鲁棒性的系统性评估。

Method: 开发RobustGait评估框架，在四个维度进行评估：扰动类型（数字、环境、时间、遮挡）、轮廓提取方法、步态识别模型架构能力和部署场景，引入15种干扰类型和5个严重级别。

Result: 发现RGB级别噪声能更好反映真实世界退化；步态精度对轮廓提取器偏差高度敏感；鲁棒性同时依赖于扰动类型和架构设计；噪声感知训练和知识蒸馏能提升性能。

Conclusion: RobustGait揭示了步态识别系统中的关键鲁棒性问题，噪声感知训练和知识蒸馏是提升部署就绪系统的有效策略。

Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.

</details>


### [296] [Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.13079)
*Jiacheng Tang,Mingyue Feng,Jiachao Liu,Yaonong Wang,Jian Pu*

Main category: cs.CV

TL;DR: 提出了AdaptiveAD架构，通过双分支结构解耦场景感知和自车状态，解决现有端到端自动驾驶规划系统过度依赖自车状态的问题，提升泛化能力和场景理解鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模块化自动驾驶规划系统过度依赖自车状态，导致泛化能力受限和场景理解不鲁棒。根本原因是架构设计中自车状态过早融合到BEV编码器中，形成了信息捷径。

Method: 采用多上下文融合策略的双分支架构：一个分支基于多任务学习进行场景驱动推理（不含自车状态），另一个分支基于规划任务进行自车驱动推理。通过场景感知融合模块自适应整合两个分支的决策。还引入了路径注意力机制、BEV单向蒸馏和自回归在线映射等辅助任务。

Result: 在nuScenes数据集上的广泛评估表明，AdaptiveAD实现了最先进的开放环路规划性能，显著减轻了对自车状态的过度依赖，并在多样化场景中展现出令人印象深刻的泛化能力。

Conclusion: AdaptiveAD通过架构层面的解耦设计有效解决了自动驾驶规划中对自车状态的过度依赖问题，为构建更鲁棒和泛化的端到端自动驾驶系统提供了可行方案。

Abstract: Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.

</details>


### [297] [Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations](https://arxiv.org/abs/2511.13081)
*Yehonatan Elisha,Seffi Cohen,Oren Barkan,Noam Koenigstein*

Main category: cs.CV

TL;DR: 提出了RFxG分类法，将显著性解释按参考框架（点式vs对比式）和粒度（细粒度vs粗粒度）两个维度组织，揭示了现有评估指标的局限性，并开发了新的忠实度指标来系统评估解释质量。


<details>
  <summary>Details</summary>
Motivation: 显著性图在深度学习视觉解释中广泛应用，但对其预期目的和与不同用户查询的对齐缺乏共识，这种模糊性阻碍了解释方法的有效评估和实际效用。

Method: 引入RFxG分类法框架，提出四个新的忠实度指标，在十个最先进的显著性方法、四个模型架构和三个数据集上进行综合评估。

Result: 发现现有评估指标过度优先考虑点式忠实度，而忽视了对比推理和语义粒度，通过RFxG视角揭示了现有方法的局限性。

Conclusion: 通过倡导向用户意图驱动的评估转变，为开发既忠实于底层模型行为又与人类理解和查询复杂性有意义的视觉解释提供了概念基础和实践工具。

Abstract: Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations.Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.

</details>


### [298] [MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images](https://arxiv.org/abs/2511.13099)
*Doanh C. Bui,Ba Hung Ngo,Hoai Luan Pham,Khang Nguyen,Maï K. Nguyen,Yasuhiko Nakashima*

Main category: cs.CV

TL;DR: MergeSlide是一个用于全切片图像终身学习的框架，将终身学习视为模型合并问题，利用视觉语言病理学基础模型，通过正交持续合并策略和任务到类别提示对齐推理来缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 全切片图像体积巨大，终身学习可以减少数据传输和处理所需的资源和努力，通过顺序训练统一模型来处理癌症相关任务。

Method: 1) 使用类别感知提示定义新任务；2) 使用无MLP的主干网络进行少量轮次微调；3) 采用正交持续合并策略将模型合并到统一模型中；4) 在类别增量学习设置下使用任务到类别提示对齐推理。

Result: 在六个TCGA数据集流上的实验表明，MergeSlide优于基于排练的持续学习和视觉语言零样本基线方法。

Conclusion: MergeSlide是一个简单有效的终身学习框架，通过模型合并方法在保持性能的同时缓解灾难性遗忘问题。

Abstract: Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.

</details>


### [299] [CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation](https://arxiv.org/abs/2511.13102)
*Yu Zhu,Dan Zeng,Shuiwang Li,Qijun Zhao,Qiaomu Shen,Bo Tang*

Main category: cs.CV

TL;DR: 提出CapeNext框架解决类别无关姿态估计中静态关节嵌入的两个局限：跨类别歧义和细粒度变化区分不足，通过层次化跨模态交互和双流特征精化提升性能


<details>
  <summary>Details</summary>
Motivation: 现有CAPE方法使用固定文本关键点描述作为语义先验，但存在两个问题：(1)多义词导致的跨类别歧义（如'腿'在人和家具中视觉表现不同）(2)对细粒度类内变化区分不足（如姿势和毛发差异）

Method: 提出新框架，创新性地整合层次化跨模态交互与双流特征精化，通过文本描述和特定图像中的类别级和实例特定线索增强关节嵌入

Result: 在MP-100数据集上的实验表明，无论使用何种网络骨干，CapeNext都大幅优于最先进的CAPE方法

Conclusion: 所提出的框架有效解决了静态关节嵌入的局限性，显著提升了类别无关姿态估计的性能

Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.

</details>


### [300] [PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking](https://arxiv.org/abs/2511.13105)
*Seungjae Kim,SeungJoon Lee,MyeongAh Cho*

Main category: cs.CV

TL;DR: PlugTrack是一个新颖的多目标跟踪框架，通过自适应融合卡尔曼滤波器和数据驱动的运动预测器来提升跟踪性能，特别是在混合线性/非线性运动场景中。


<details>
  <summary>Details</summary>
Motivation: 现实世界跟踪场景同时包含线性和非线性运动模式，但现有方法要么使用计算高效的卡尔曼滤波器（无法处理非线性运动），要么使用数据驱动方法（计算开销大且泛化能力有限），需要一种能自适应融合两者优势的解决方案。

Method: 提出PlugTrack框架，通过多感知运动理解生成自适应混合因子，动态融合卡尔曼滤波器和数据驱动运动预测器的输出，无需修改现有运动预测器。

Result: 在MOT17/MOT20上取得显著性能提升，在DanceTrack上达到最先进水平，卡尔曼滤波器在非线性运动数据集中仍能在34%的情况下优于数据驱动方法。

Conclusion: PlugTrack是首个通过自适应融合桥接经典和现代运动预测范式的多目标跟踪框架，证明了混合方法的有效性。

Abstract: Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.

</details>


### [301] [Low-Level Dataset Distillation for Medical Image Enhancement](https://arxiv.org/abs/2511.13106)
*Fengzhi Xu,Ziyuan Yang,Mengyu Sun,Joey Tianyi Zhou,Yi Zhang*

Main category: cs.CV

TL;DR: 提出了首个用于医学图像增强的低级数据集蒸馏方法，通过构建共享解剖先验和个性化生成模块，在保护隐私的同时实现高效训练


<details>
  <summary>Details</summary>
Motivation: 现有医学图像增强方法需要大规模数据集学习复杂像素级映射，但训练和存储成本高。现有数据集蒸馏方法主要针对高级任务，而低级任务涉及多对多映射，需要像素级保真度，使得低级数据集蒸馏成为欠定问题

Method: 利用患者间解剖相似性构建共享解剖先验，通过结构保持个性化生成模块将患者特定解剖信息整合到蒸馏数据中，同时保持像素级保真度。使用梯度对齐将患者特定知识注入蒸馏数据

Result: 提出的方法能够生成包含抽象训练信息的蒸馏数据集，无需访问原始患者数据，保护隐私的同时实现高效训练

Conclusion: 该方法首次解决了医学图像增强中的低级数据集蒸馏问题，通过共享解剖先验和个性化生成，在保护隐私的同时显著降低了训练和存储成本

Abstract: Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.

</details>


### [302] [DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection](https://arxiv.org/abs/2511.13108)
*Jiazhen Yan,Ziqiang Li,Fan Wang,Boyu Wang,Zhangjie Fu*

Main category: cs.CV

TL;DR: 提出DGS-Net框架，通过梯度空间分解解决CLIP模型在AI生成图像检测任务中的灾难性遗忘问题，在50个生成模型上取得优于现有方法6.6%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 生成模型快速发展导致AI生成图像泛滥，引发虚假信息、隐私侵犯等担忧。虽然CLIP等模型提供强迁移表示，但微调会导致灾难性遗忘，损害预训练先验并限制跨域泛化。

Method: 提出蒸馏引导的梯度手术网络(DGS-Net)，通过梯度空间分解分离优化过程中的有害和有益下降方向，将任务梯度投影到有害方向的正交补空间，并与冻结CLIP编码器蒸馏的有益方向对齐。

Result: 在50个生成模型上的实验表明，该方法平均优于最先进方法6.6%，实现了卓越的检测性能和跨不同生成技术的泛化能力。

Conclusion: DGS-Net通过统一优化先验保持和无关抑制，有效解决了CLIP模型在AI生成图像检测中的灾难性遗忘问题，提升了检测性能和跨域泛化能力。

Abstract: The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.

</details>


### [303] [Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing](https://arxiv.org/abs/2511.13110)
*Shuaibin Fan,Senming Zhong,Wenchao Yan,Minglong Xue*

Main category: cs.CV

TL;DR: 提出了一种基于隐式神经退化表示的无监督去雾方法，通过结合通道独立和通道依赖机制来增强非线性依赖学习能力，并设计密集残差增强模块来消除冗余信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂场景时难以平衡不均匀雾分布的特征表示和全局一致性建模，需要更好地学习雾在空间变化中的退化表示。

Method: 基于Kolmogorov-Arnold表示定理，结合通道独立和通道依赖机制；设计隐式神经表示将雾退化建模为连续函数；构建密集残差增强模块。

Result: 在多个公共和真实世界数据集上实现了具有竞争力的去雾性能，在复杂场景中获得了良好的视觉感知效果。

Conclusion: 该方法通过隐式神经退化表示实现了高质量图像恢复，无需显式特征提取和物理模型依赖。

Abstract: Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.

</details>


### [304] [Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining](https://arxiv.org/abs/2511.13113)
*Zhaocheng Yu,Kui Jiang,Junjun Jiang,Xianming Liu,Guanglu Sun,Yi Xiao*

Main category: cs.CV

TL;DR: 提出了MPHM网络用于图像去雨，通过整合文本和视觉先验，结合分层Mamba模块，在保持语义和空间细节方面实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有去雨方法在语义和空间细节保真度方面存在不足，影响计算机视觉系统在自动驾驶等应用中的性能。

Method: 提出MPHM网络，整合CLIP文本先验和DINOv2视觉先验，采用渐进式先验融合注入策略，并设计分层Mamba模块进行特征表示。

Result: 在Rain200H数据集上获得0.57 dB PSNR增益，在真实雨天场景中表现出优越的泛化能力。

Conclusion: MPHM网络通过多先验融合和分层Mamba设计，有效提升了图像去雨的性能和细节保真度。

Abstract: Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.

</details>


### [305] [A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features](https://arxiv.org/abs/2511.13115)
*Hanzhe Liang,Jie Zhou,Can Gao,Bingyang Guo,Jinbao Wang,Linlin Shen*

Main category: cs.CV

TL;DR: 提出了一种旋转不变特征(RIF)框架用于3D异常检测，通过点坐标映射和轻量级卷积变换网络解决点云方向和位置变化带来的特征不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D异常检测方法在处理方向和位置变化的点云时，由于特征变化显著而面临挑战，需要开发旋转不变的特征表示。

Method: 1. 点坐标映射(PCM)技术将点映射到旋转不变空间；2. 轻量级卷积变换特征网络(CTF-Net)提取旋转不变特征；3. 使用迁移学习和3D数据增强预训练特征提取器。

Result: 在Anomaly-ShapeNet数据集上平均P-AUROC提升17.7%，在Real3D-AD数据集上平均P-AUROC提升1.6%，展现出强泛化能力。

Conclusion: RIF框架通过旋转不变特征有效解决了3D异常检测中的方向位置变化问题，在多个数据集上取得先进性能，具有工业应用潜力。

Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.

</details>


### [306] [CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model](https://arxiv.org/abs/2511.13121)
*Yuqi Zhang,Guanying Chen,Jiaxing Chen,Chuanyu Fu,Chuan Huang,Shuguang Cui*

Main category: cs.CV

TL;DR: CloseUpShot：基于扩散的框架，通过点条件视频扩散从稀疏输入实现特写场景的新视角合成，解决了特写场景下像素扭曲条件稀疏和背景泄漏的问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏输入视图下重建3D场景和合成新视角具有挑战性。现有方法主要针对适度视角变化设计，在特写场景中由于输入信息严重受限而难以捕捉细粒度细节。

Method: 提出分层扭曲和遮挡感知噪声抑制来增强视频扩散模型的调节图像质量；引入全局结构指导，利用密集融合点云为扩散过程提供一致几何上下文。

Result: 在多个数据集上的广泛实验表明，该方法在特写新视角合成方面优于现有方法，验证了设计的有效性。

Conclusion: CloseUpShot通过改进的调节机制和全局几何约束，有效解决了特写场景下稀疏输入的新视角合成问题。

Abstract: Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.

</details>


### [307] [Region-Point Joint Representation for Effective Trajectory Similarity Learning](https://arxiv.org/abs/2511.13125)
*Hao Long,Silin Zhou,Lisi Chen,Shuo Shang*

Main category: cs.CV

TL;DR: RePo是一种联合编码区域级和点级特征的轨迹相似度计算方法，通过捕捉空间上下文和细粒度移动模式，在各项评估指标上平均比现有最佳方法提升22.2%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法虽然降低了传统轨迹相似度计算的计算复杂度，但未能充分利用轨迹信息的完整谱系进行相似度建模。

Method: 提出RePo方法：1）区域级表示：将GPS轨迹映射为网格序列，通过结构特征和视觉特征增强的语义上下文捕捉空间上下文；2）点级表示：使用三个轻量级专家网络从密集GPS序列中提取局部、相关和连续移动模式；3）通过路由器网络自适应融合点级特征，再与区域级特征通过交叉注意力结合生成最终轨迹嵌入；4）使用带困难负样本的对比损失进行训练。

Result: 实验结果显示，RePo在所有评估指标上平均比现有最佳基线方法提升22.2%的准确率。

Conclusion: RePo通过联合编码区域级和点级特征，有效捕捉了轨迹的空间上下文和细粒度移动模式，显著提升了轨迹相似度计算的性能。

Abstract: Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.

</details>


### [308] [VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language](https://arxiv.org/abs/2511.13127)
*Zonghao Ying,Moyang Chen,Nizhang Li,Zhiqiang Wang,Wenxin Zhang,Quanchen Zou,Zonglei Jing,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: VEIL是一个针对文本到视频模型的越狱攻击框架，通过包含中性场景锚点、潜在听觉触发器和风格调制器的模块化提示设计，利用跨模态关联模式诱导模型生成违反安全策略的视频。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型的越狱攻击通常通过添加明显不安全的对抗性扰动来实现，容易被检测和防御。本文旨在探索如何通过看似良性的提示来诱导模型生成语义上不安全的视频。

Method: 提出VEIL框架，采用模块化提示设计：中性场景锚点提供表面场景描述；潜在听觉触发器利用音频-视觉共现先验；风格调制器通过电影指令增强效果。将攻击生成形式化为约束优化问题，并使用引导搜索算法求解。

Result: 在7个文本到视频模型上的广泛实验表明，该方法在商业模型中的平均攻击成功率提高了23%。

Conclusion: VEIL框架有效揭示了文本到视频模型的安全盲点，通过看似良性的提示成功诱导模型生成违反策略的内容，强调了需要更强大的安全防护机制。

Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.

</details>


### [309] [Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack](https://arxiv.org/abs/2511.13132)
*Chenyang Li,Wenbing Tang,Yihao Huang,Sinong Simon Zhan,Ming Hu,Xiaojun Jia,Yang Liu*

Main category: cs.CV

TL;DR: 提出了基于室内照明的对抗攻击框架ILA，通过操纵全局光照来干扰视觉语言导航代理，包括静态照明攻击和动态照明攻击两种模式。


<details>
  <summary>Details</summary>
Motivation: 现有对抗评估通常依赖不常见的纹理扰动，缺乏实际相关性。室内照明作为内在场景属性对导航有重要影响，但被忽视。

Method: ILA框架包含两种攻击模式：SILA（静态照明攻击，光照强度恒定）和DILA（动态照明攻击，在关键时刻开关灯引起光照突变）。

Result: 在两个最先进的VLN模型和三个导航任务上的评估显示，ILA显著提高了失败率并降低了轨迹效率。

Conclusion: 揭示了VLN代理对现实室内光照变化存在先前未被识别的脆弱性。

Abstract: Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.

</details>


### [310] [MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation](https://arxiv.org/abs/2511.13135)
*Junjie Yang,Yuhao Yan,Gang Wu,Yuxuan Wang,Ruoyu Liang,Xinjie Jiang,Xiang Wan,Fenglei Fan,Yongquan Zhang,Feiwei Qin,Changmiao Wan*

Main category: cs.CV

TL;DR: 提出了MedGEN-Bench基准测试，用于评估医疗AI的多模态生成能力，包含6422个图像-文本对，涵盖6种成像模式和16个临床任务，采用三层评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有医疗视觉基准存在局限性：查询模糊、诊断推理过于简化、忽视图像生成能力。需要更全面的多模态评估标准。

Method: 构建包含6422个专家验证图像-文本对的MedGEN-Bench数据集，分为视觉问答、图像编辑和上下文多模态生成三种格式，采用像素级指标、语义文本分析和临床相关性评分三层评估框架。

Result: 系统评估了10个组合框架、3个统一模型和5个视觉语言模型，展示了不同模型在多模态医疗任务上的表现。

Conclusion: MedGEN-Bench为医疗AI研究提供了更全面的多模态评估标准，推动了上下文交织指令和开放式生成能力的发展。

Abstract: As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.

</details>


### [311] [WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection](https://arxiv.org/abs/2511.13138)
*Longhui Zheng,Qiming Xia,Xiaolu Chen,Zhaoliang Liu,Chenglu Wen*

Main category: cs.CV

TL;DR: WinMamba：一种基于Mamba的3D特征编码骨干网络，通过窗口尺度自适应模块和窗口偏移策略，在保持计算效率的同时增强长距离空间依赖关系捕获能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D目标检测方法在最大化计算效率和捕获长距离空间依赖性之间存在平衡难题。Mamba模型虽然能以较低成本捕获长距离依赖，但现有方法采用固定窗口内的轴对齐扫描，不可避免地丢弃了空间信息。

Method: 提出WinMamba块组成的Mamba-based 3D特征编码骨干网络。包含窗口尺度自适应模块（补偿不同分辨率下的体素特征）和窗口偏移策略（在线性状态空间中获取丰富上下文线索），并配备可学习位置编码。

Result: 在KITTI和Waymo数据集上的广泛实验表明，WinMamba显著优于基线方法。消融研究进一步验证了WSF和AWF模块在提高检测精度方面的独立贡献。

Conclusion: WinMamba通过创新的窗口尺度自适应和窗口偏移策略，成功解决了3D目标检测中效率与长距离依赖捕获的平衡问题，为自动驾驶应用提供了有效的解决方案。

Abstract: 3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.

</details>


### [312] [Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks](https://arxiv.org/abs/2511.13145)
*Cesar Portocarrero Rodriguez,Laura Vandeweyen,Yosuke Yamamoto*

Main category: cs.CV

TL;DR: 该研究探索使用计算机视觉技术进行道路损坏分割，评估GAN生成合成数据的效果，并比较CNN和MaskFormer模型的性能。


<details>
  <summary>Details</summary>
Motivation: 美国基础设施状况不佳，道路系统评级为D。传统道路检测方法效率低下、成本高昂，而自动驾驶车辆提供的实时视觉数据为应用计算机视觉进行道路监测提供了机会。

Method: 使用GAN生成合成数据进行模型训练评估，应用CNN进行道路损坏分割，并研究基于transformer的MaskFormer模型。

Result: GAN生成的数据能提升模型性能，MaskFormer在mAP50和IoU两个指标上优于CNN模型。

Conclusion: 计算机视觉方法特别是MaskFormer模型在道路损坏分割方面具有良好表现，GAN生成的合成数据可作为有效的训练数据补充。

Abstract: The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.

</details>


### [313] [Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification](https://arxiv.org/abs/2511.13150)
*Rifen Lin,Alex Jinpeng Wang,Jiawei Mo,Min Li*

Main category: cs.CV

TL;DR: CSIP-ReID是首个基于骨架驱动的视频行人重识别预训练框架，通过对比学习对齐骨架和视觉特征，结合原型融合更新器和骨架引导时序建模，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态预训练方法依赖视频-文本对，但存在两个根本限制：(1)缺乏真正的多模态预训练，(2)文本无法捕捉细粒度时序运动信息。骨架序列能提供与视频帧对齐的时空信息模态。

Method: 两阶段方法：第一阶段使用对比学习在序列级别对齐骨架和视觉特征；第二阶段引入动态原型融合更新器(PFU)优化多模态身份原型，融合运动和外观线索；并提出骨架引导时序建模(SGTM)模块从骨架数据中提取时序线索并整合到视觉特征中。

Result: 在标准视频ReID基准测试(MARS、LS-VID、iLIDS-VID)上达到新的SOTA结果；在纯骨架ReID任务(BIWI、IAS)上表现出强大的泛化能力，显著优于先前方法。

Conclusion: CSIP-ReID开创了无标注和运动感知的ReID预训练范式，为多模态表示学习开辟了新前沿。

Abstract: Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.

</details>


### [314] [SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration](https://arxiv.org/abs/2511.13168)
*Haodong Wang,Tao Zhuo,Xiuwei Zhang,Hanlin Yin,Wencong Wu,Yanning Zhang*

Main category: cs.CV

TL;DR: SOMA是一个SAR与光学图像密集配准框架，通过集成结构梯度先验和混合匹配策略，显著提升了配准精度。


<details>
  <summary>Details</summary>
Motivation: SAR与光学图像由于成像机制和视觉特征差异，像素级配准具有挑战性。传统梯度信息在手工描述符中很重要，但在深度学习框架中未被有效利用。

Method: 提出特征梯度增强器(FGE)在特征空间中嵌入多尺度、多方向梯度滤波器；提出全局-局部仿射流匹配器(GLAM)结合仿射变换和流式细化。

Result: 在SEN1-2数据集上CMR@1px提升12.29%，在GFGE_SO数据集上提升18.50%，表现出强鲁棒性和良好泛化能力。

Conclusion: SOMA通过有效利用梯度信息和混合匹配策略，显著改善了SAR-光学图像配准性能。

Abstract: Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.

</details>


### [315] [THIR: Topological Histopathological Image Retrieval](https://arxiv.org/abs/2511.13170)
*Zahra Tabatabaei,Jon Sporring*

Main category: cs.CV

TL;DR: THIR是一个基于拓扑数据分析的无监督医学图像检索框架，利用持续同调中的Betti数来表征组织病理学图像的内在结构模式，无需训练即可实现高效检索。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性主要死因，早期诊断和准确临床决策至关重要。传统深度学习方法需要大量标注数据和GPU资源，限制了在临床环境中的应用。

Method: 使用立方体持续性从RGB组织病理学图像中提取拓扑指纹，将环的演化编码为紧凑、可解释的特征向量，通过计算拓扑描述符之间的距离进行相似性检索。

Result: 在BreaKHis数据集上的实验表明，THIR优于最先进的监督和无监督方法，在标准CPU上20分钟内处理整个数据集。

Conclusion: THIR提供了一个快速、可扩展且无需训练的临床图像检索解决方案，特别适合资源受限的医疗环境。

Abstract: According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.
  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.

</details>


### [316] [HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution](https://arxiv.org/abs/2511.13175)
*Chao Yang,Boqian Zhang,Jinghao Xu,Guang Jiang*

Main category: cs.CV

TL;DR: 提出HDW-SR方法，使用小波分解实现高频引导的扩散网络，专注于高频信息恢复，在图像超分辨率中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的单图像超分辨率方法在高频域缺乏足够引导，导致细节模糊，需要改进高频信息恢复。

Method: 使用小波分解替换传统U-Net，仅在残差图上进行扩散；引入小波下采样实现多尺度频率分解；设计动态阈值块优化高频选择；利用小波变换的可逆性进行低损失特征重建。

Result: 在合成和真实数据集上的实验表明，HDW-SR在超分辨率性能上具有竞争力，特别擅长恢复细粒度图像细节。

Conclusion: HDW-SR通过高频引导的扩散网络有效提升了图像超分辨率的质量，特别是在细节恢复方面表现优异。

Abstract: Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.

</details>


### [317] [GenTract: Generative Global Tractography](https://arxiv.org/abs/2511.13183)
*Alec Sargood,Lemuel Puglisi,Elinor Thompson,Mirco Musolesi,Daniel C. Alexander*

Main category: cs.CV

TL;DR: GenTract是首个用于全局纤维束成像的生成模型，将纤维束成像构建为生成任务，直接从dMRI映射到完整的解剖学合理纤维束。相比现有方法，在精度上表现显著优越，特别是在低分辨率和噪声环境下。


<details>
  <summary>Details</summary>
Motivation: 传统局部纤维束成像方法容易累积误差且假阳性率高，而全局方法计算成本高昂。需要一种既能保持高精度又能在不完美数据上可靠工作的解决方案。

Method: 将纤维束成像构建为生成任务，学习从dMRI到完整纤维束的直接映射。比较了基于扩散和流匹配的两种范式，并与最先进的基线方法进行对比。

Result: GenTract的精度比次优方法TractOracle高2.1倍。在具有挑战性的低分辨率和噪声设置下，其优势更加明显，比最接近的竞争对手高出一个数量级。

Conclusion: GenTract在研究级数据上产生高精度纤维束图，同时在不完美的低分辨率数据上保持可靠性，代表了全局纤维束成像的有前景解决方案。

Abstract: Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.

</details>


### [318] [Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://arxiv.org/abs/2511.13189)
*Diego Ortego,Marlon Rodríguez,Mario Almagro,Kunal Dahiya,David Jiménez,Juan C. SanMiguel*

Main category: cs.CV

TL;DR: 本文提出ViXML框架，将视觉信息高效整合到极端多标签分类中，通过解码器模型和视觉基础模型的结合，在保持计算效率的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 极端多标签分类中，基础模型的潜力尚未充分挖掘，需要在效率和性能间取得平衡。现有方法主要使用小型编码器模型，本文旨在探索如何有效利用更大的解码器模型和视觉信息。

Method: 提出ViXML框架：1) 使用数十亿参数规模的解码器模型；2) 通过每张图像池化单个嵌入的方式高效整合视觉基础模型；3) 将文本数据集扩展为包含视觉元数据的多模态数据集。

Result: 在四个公开数据集上的实验表明：ViXML在最大数据集上的P@1指标比之前最优方法提升8.21%；小型编码器结合视觉信息在多数情况下优于纯文本解码器。

Conclusion: 视觉信息在极端多标签分类中具有重要价值，ViXML框架成功实现了多模态能力的解锁，同时保持了计算效率，为未来基准测试提供了新标准。

Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

</details>


### [319] [Video Spatial Reasoning with Object-Centric 3D Rollout](https://arxiv.org/abs/2511.13190)
*Haoran Tang,Meng Cao,Ruyang Liu,Xiaoxi Liang,Linglong Li,Ge Li,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出Object-Centric 3D Rollout (OCR)方法，通过结构化扰动3D几何来增强多模态大语言模型的视频空间推理能力，在VSI-Bench上达到47.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在查询锁定推理问题，仅关注提示中明确提到的对象而忽略关键上下文线索，导致视频空间推理能力不足。

Method: OCR方法在训练期间对选定对象的3D几何引入结构化扰动，通过降级对象特定视觉线索并将改变的几何投影到2D空间，迫使模型进行整体场景推理。设计了基于rollout的训练流程，联合利用原始视频和区域噪声视频来优化空间推理轨迹。

Result: 3B参数模型在VSI-Bench上达到47.5%准确率，优于多个7B基线模型。消融实验证实OCR优于先前的rollout策略。

Conclusion: OCR方法通过结构化几何扰动有效解决了查询锁定推理问题，显著提升了多模态大语言模型的视频空间推理能力。

Abstract: Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).

</details>


### [320] [Birth of a Painting: Differentiable Brushstroke Reconstruction](https://arxiv.org/abs/2511.13191)
*Ying Jiang,Jiayin Lu,Yunuo Chen,Yumeng He,Kui Wu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: 提出了一种可微分的笔触重建框架，统一了绘画、风格化纹理和涂抹操作，能够真实再现人类绘画-涂抹循环过程。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法主要关注最终图像生成或基于补丁的过程模拟，缺乏明确的笔触结构，无法产生平滑逼真的阴影效果。

Method: 使用可微分笔触重建框架，通过并行可微分绘画渲染器优化单色和双色贝塞尔笔触，结合风格生成模块合成几何条件纹理，并引入可微分涂抹算子实现自然色彩混合和阴影。

Result: 在油画、水彩、水墨和数字绘画上的广泛实验表明，该方法能产生逼真且富有表现力的笔触重建、平滑的色调过渡和丰富的风格化外观。

Conclusion: 该方法为表现性数字绘画创作提供了一个统一模型，能够真实再现绘画过程并产生高质量的艺术效果。

Abstract: Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.

</details>


### [321] [Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection](https://arxiv.org/abs/2511.13195)
*Soyul Lee,Seungmin Baek,Dongbo Min*

Main category: cs.CV

TL;DR: MonoDLGD是一个新颖的难度感知标签引导去噪框架，通过根据检测不确定性自适应扰动和重建真实标签来解决单目3D目标检测中的深度估计不准确问题。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测由于深度线索的固有模糊性而存在根本性问题。现有DETR方法虽然尝试通过全局注意力和辅助深度预测来缓解，但仍受限于不准确的深度估计，且忽视了实例级检测难度（如遮挡、距离、截断），导致检测性能不佳。

Method: 提出难度感知标签引导去噪框架MonoDLGD：1）根据检测不确定性自适应扰动真实标签——对简单实例施加更强扰动，对困难实例施加较弱扰动；2）重建扰动后的标签以提供显式几何监督；3）联合优化标签重建和3D目标检测，促进几何感知表示学习。

Result: 在KITTI基准测试上的广泛实验表明，MonoDLGD在所有难度级别上都达到了最先进的性能。

Conclusion: MonoDLGD通过难度感知的标签扰动和重建策略，有效解决了单目3D目标检测中的深度估计不准确问题，提高了对不同复杂度目标的鲁棒性。

Abstract: Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.

</details>


### [322] [Self-Supervised Ultrasound Screen Detection](https://arxiv.org/abs/2511.13197)
*Alberto Gomez,Jorge Oliveira,Ramon Casero,Agis Chartsias*

Main category: cs.CV

TL;DR: 提出一种自监督管道，从超声显示器照片中提取超声图像，绕过DICOM瓶颈，实现快速算法测试和原型开发。


<details>
  <summary>Details</summary>
Motivation: 超声设备在内置显示器上显示图像，但常规传输到医院系统依赖DICOM格式，存在传输瓶颈。

Method: 使用自监督管道从超声显示器照片中提取和校正超声图像。

Result: 在概念验证研究中，校正后的图像保持了足够的视觉保真度，与原始DICOM图像相比，心脏视图分类的平衡准确率达到0.79。

Conclusion: 该方法成功绕过了DICOM瓶颈，为快速测试和开发新算法提供了可行方案。

Abstract: Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.

</details>


### [323] [RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2511.13204)
*Junhee Lee,ChaeBeen Bang,MyoungChul Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: RefineVAD是一个弱监督视频异常检测框架，通过联合建模时间运动模式和语义结构来识别多种异常类型，包含运动感知时间注意力重校准和类别导向细化两个核心模块。


<details>
  <summary>Details</summary>
Motivation: 现有方法将异常事件视为单一类别，忽略了真实世界异常的多样语义和时间特性。受人类感知异常方式的启发，需要同时解释不同异常类型的时间运动模式和语义结构。

Method: 包含两个核心模块：MoTAR模块通过基于位移的注意力和全局Transformer建模来估计运动显著性并动态调整时间关注；CORE模块通过跨注意力将片段级特征与可学习类别原型对齐，注入软异常类别先验。

Result: 在WVAD基准测试上的广泛实验验证了RefineVAD的有效性，并强调了整合语义上下文来引导特征细化朝向异常相关模式的重要性。

Conclusion: 通过联合利用时间动态和语义结构，RefineVAD能够明确建模运动如何演化以及它类似于什么语义类别，为弱监督视频异常检测提供了更精细的解决方案。

Abstract: Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.

</details>


### [324] [End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer](https://arxiv.org/abs/2511.13208)
*Yonghui Yu,Jiahang Cai,Xun Wang,Wenwu Yang*

Main category: cs.CV

TL;DR: PAVE-Net是一个端到端的多人视频姿态估计框架，消除了传统两阶段方法中的启发式操作，通过姿态感知注意力机制实现跨帧个体关联。


<details>
  <summary>Details</summary>
Motivation: 现有的多人视频姿态估计方法依赖检测、RoI裁剪和非极大值抑制等启发式操作，限制了准确性和效率。需要开发完全端到端的框架来消除这些限制。

Method: 提出PAVE-Net，包含空间编码器建模帧内关系和时空姿态解码器捕获跨帧全局依赖。采用姿态感知注意力机制，使每个姿态查询能够选择性地聚合跨连续帧的同一个体特征。

Result: 在PoseTrack2017上比基于图像的端到端方法提升6.0 mAP，与最先进的两阶段视频方法精度相当，同时显著提升效率。

Conclusion: PAVE-Net是首个多帧2D人体姿态估计的端到端方法，成功消除了传统两阶段流程中的启发式操作，在准确性和效率方面都表现出色。

Abstract: Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet

</details>


### [325] [3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale](https://arxiv.org/abs/2511.13211)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 3DAlign-DAER是一个统一框架，通过动态注意力策略和高效检索策略来对齐文本和3D几何，在细粒度语义对齐和大规模3D数据库检索方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有3D-文本跨模态对齐方法难以将细粒度文本语义与详细几何结构对齐，且在大规模3D数据库上性能显著下降。

Method: 提出动态注意力策略（DAP），使用分层注意力融合模块学习细粒度token到点的注意力，并通过蒙特卡洛树搜索动态校准注意力权重；在推理阶段使用高效检索策略（ERS）进行分层搜索。

Result: 在多个基准测试中表现出优越性能，构建了包含200万文本-3D对的大规模数据集Align3D-2M。

Conclusion: 3DAlign-DAER在细粒度3D-文本对齐和大规模检索任务中显著优于现有方法，将发布代码、模型和数据集。

Abstract: Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.

</details>


### [326] [Hybrid-Domain Adaptative Representation Learning for Gaze Estimation](https://arxiv.org/abs/2511.13222)
*Qida Tan,Hongyu Yang,Wenchao Du*

Main category: cs.CV

TL;DR: 提出HARL框架，通过混合域自适应表示学习解决基于外观的视线估计在跨域评估中的性能下降问题，利用多源混合数据集学习鲁棒的视线表示。


<details>
  <summary>Details</summary>
Motivation: 大多数基于外观的视线估计方法在跨域评估中性能显著下降，主要受表情、佩戴物和图像质量等与视线无关因素的干扰。

Method: 1) 通过无监督域自适应方式对齐高质量近眼图像特征，从低质量面部图像中解耦视线相关表示；2) 设计稀疏图融合模块探索视线方向与头部姿态之间的几何约束。

Result: 在EyeDiap、MPIIFaceGaze和Gaze360数据集上分别达到5.02°、3.36°和9.26°的最先进精度，并在跨数据集评估中表现出竞争力。

Conclusion: HARL框架能够有效学习鲁棒的视线表示，在多个数据集上实现最先进性能，且无需额外计算或推理成本。

Abstract: Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

</details>


### [327] [MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI](https://arxiv.org/abs/2511.13232)
*Malek Al Abed,Sebiha Demir,Anne Groteklaes,Elodie Germani,Shahrooz Faghihroohi,Hemmen Sabir,Shadi Albarqouni*

Main category: cs.CV

TL;DR: MRIQT是一个3D条件扩散框架，用于将便携式超低场MRI图像质量提升到高场MRI水平，通过物理一致的K空间降级模拟、v预测和SNR加权感知损失实现高质量图像转换。


<details>
  <summary>Details</summary>
Motivation: 便携式超低场MRI在新生儿护理中具有可及性优势，但其信噪比低、诊断质量差，需要提升图像质量以实现可靠的新生儿脑部评估。

Method: 结合K空间降级模拟物理一致的uLF MRI，使用v预测和分类器自由引导进行稳定生成，采用SNR加权的3D感知损失保持解剖保真度，基于注意力UNet架构进行结构保持转换。

Result: 在新生儿队列上训练，MRIQT在PSNR上超过最新GAN和CNN基线15.3%，85%的输出被医生评为良好质量且病理清晰可见。

Conclusion: MRIQT能够实现便携式超低场MRI的高保真度增强，为可靠的新生儿脑部评估提供支持。

Abstract: Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.

</details>


### [328] [MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.13242)
*Junjie Wu,Guohong Fu*

Main category: cs.CV

TL;DR: 提出了MMD-Thinker框架，通过自适应多维思维进行多模态虚假信息检测，解决了通用MLLMs在推理不足和推理偏见方面的限制。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息在社交媒体上泛滥，且随着AIGC的发展不断演变。现有基于通用MLLMs的检测方法存在推理不足和推理偏见两个关键问题。

Method: 采用两阶段框架：1) 设计专门的多模态虚假信息检测思维模式；2) 通过任务特定指令调优将定制思维模式注入通用MLLMs；3) 使用混合优势函数的强化学习策略增强推理能力。

Result: 在领域内和领域外基准数据集上实现了最先进的性能，同时保持灵活的推理和token使用。

Conclusion: MMD-Thinker通过自适应多维思维有效提升了多模态虚假信息检测能力，解决了现有方法的局限性。

Abstract: Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.

</details>


### [329] [Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention](https://arxiv.org/abs/2511.13249)
*Yu Wen,Shuyong Gao,Shuping Zhang,Miao Huang,Lili Tao,Han Yang,Haozhe Xing,Lihe Zhang,Boxue Hou*

Main category: cs.CV

TL;DR: RFMNet是一个用于指代伪装目标检测的新方法，通过多阶段特征融合和重叠窗口交叉注意力机制，在Ref-COD基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将参考图像转换为1D提示，作者希望探索通过丰富的显著图像特征和伪装目标特征的多上下文融合来提升性能。

Method: 提出RFMNet，利用参考显著图像的多编码阶段特征，在相应编码阶段与伪装特征进行交互融合；提出重叠窗口交叉注意力机制关注局部信息匹配；设计指代特征聚合模块进行渐进式解码分割。

Result: 在Ref-COD基准上的大量实验表明，该方法实现了最先进的性能。

Conclusion: 通过多阶段特征融合和局部注意力机制，RFMNet在指代伪装目标检测任务中表现出色。

Abstract: Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.

</details>


### [330] [GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models](https://arxiv.org/abs/2511.13259)
*Yushuo Zheng,Jiangyong Ying,Huiyu Duan,Chunyi Li,Zicheng Zhang,Jing Liu,Xiaohong Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: GeoX-Bench是一个用于评估大型多模态模型在跨视角地理定位和姿态估计任务中能力的基准数据集，包含10,859个全景-卫星图像对和755,976个问答对。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型在多种任务中表现出色，但其在跨视角地理定位和姿态估计领域的能力尚未被探索，而这些能力对导航、自动驾驶、户外机器人等领域具有重要价值。

Method: 构建了包含128个城市、49个国家的10,859个全景-卫星图像对和755,976个问答对的GeoX-Bench数据集，并评估了25个最先进的大型多模态模型在这些任务上的表现。

Result: 当前大型多模态模型在地理定位任务中表现良好，但在更复杂的姿态估计任务中效果显著下降。通过在GeoX-Bench训练数据上进行指令调优可以显著提升模型的跨视角地理感知能力。

Conclusion: GeoX-Bench揭示了大型多模态模型在跨视角地理定位和姿态估计方面的能力差距，特别是姿态估计任务需要进一步改进，指令调优是提升这些能力的有效方法。

Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.

</details>


### [331] [Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges](https://arxiv.org/abs/2511.13261)
*Junlong Li,Huaiyuan Xu,Sijie Cheng,Kejun Wu,Kim-Hui Yap,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了以自我为中心的程序AI助手概念，旨在通过第一人称视角逐步支持日常程序性任务，并识别了三个核心任务：错误检测、程序学习和问答。


<details>
  <summary>Details</summary>
Motivation: 受视觉语言模型和自我中心感知研究的推动，需要开发专门针对第一人称视角日常程序性任务的AI助手。

Method: 通过全面回顾现有技术、相关数据集和评估指标，构建新的分类法，并对代表性VLM方法进行实验评估。

Result: 识别了现有VLM助手与拟议EgoProceAssist之间的差距，建立了公开知识库持续收集最新研究。

Conclusion: 讨论了未来挑战和研究方向，强调自我中心程序AI助手在支持日常任务中的重要性。

Abstract: Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant

</details>


### [332] [SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2511.13264)
*Keshav Gupta,Akshat Sanghvi,Shreyas Reddy Palley,Astitva Srivastava,Charu Sharma,Avinash Sharma*

Main category: cs.CV

TL;DR: SymGS是一个基于对称感知的3D高斯泼溅压缩框架，通过引入可学习镜像来消除局部和全局的反射冗余，在保持渲染质量的同时实现108倍压缩比。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术在渲染速度和真实感方面表现出色，但其内存占用随场景复杂度快速增长，现有压缩方法主要利用基元级冗余进行压缩，但仍有提升空间。

Method: 提出SymGS框架，引入可学习镜像来识别和消除场景中的对称冗余，作为即插即用模块增强现有压缩方法（如HAC）。

Result: 相比HAC方法，在基准数据集上实现1.66倍压缩（大规模场景可达3倍），平均实现108倍压缩，同时保持渲染质量。

Conclusion: SymGS通过对称感知技术有效突破了现有压缩方法的极限，为3D高斯泼溅提供了高效的内存压缩解决方案。

Abstract: 3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}

</details>


### [333] [Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation](https://arxiv.org/abs/2511.13269)
*Lingfeng Zhang,Yuchen Zhang,Hongsheng Li,Haoxiang Fu,Yingbo Tang,Hangjun Ye,Long Chen,Xiaojun Liang,Xiaoshuai Hao,Wenbo Ding*

Main category: cs.CV

TL;DR: 提出了SpatialSky-Bench基准测试来评估视觉语言模型在无人机导航中的空间智能能力，并开发了Sky-VLM模型在各项任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在无人机场景中的空间智能能力尚未充分探索，存在导航和解释动态环境的有效性担忧。

Method: 创建了包含环境感知和场景理解两大类别、13个子类别的SpatialSky-Bench基准，并基于包含100万样本的SpatialSky-Dataset开发了专门用于无人机空间推理的Sky-VLM模型。

Result: 主流视觉语言模型在复杂无人机导航场景中表现不佳，而Sky-VLM在所有基准任务中均达到最先进性能。

Conclusion: Sky-VLM为开发适用于无人机场景的视觉语言模型铺平了道路，填补了现有模型在空间智能能力方面的空白。

Abstract: Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.

</details>


### [334] [Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models](https://arxiv.org/abs/2511.13276)
*Noam Tsfaty,Avishai Weizman,Liav Cohen,Moshe Tshuva,Yehudit Aperstein*

Main category: cs.CV

TL;DR: 提出基于视频级监督的双主干网络框架，通过卷积和Transformer表征结合与top-k池化，在UCF-Crime数据集上实现90.7% AUC的异常检测性能


<details>
  <summary>Details</summary>
Motivation: 解决监控视频中罕见且多样化异常检测的挑战，仅使用视频级监督（无需帧级标注）

Method: 双主干网络框架：结合卷积和Transformer表征，采用top-k池化策略

Result: 在UCF-Crime数据集上达到90.7% AUC

Conclusion: 该双主干框架能有效检测监控视频中的罕见多样化异常，仅需视频级监督即可实现高性能

Abstract: We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.

</details>


### [335] [SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.13278)
*Zihan Li,Tengfei Wang,Wentian Gan,Hao Zhan,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: SF-Recon是一种直接从多视角图像重建轻量级建筑表面的方法，无需后处理网格简化，通过3D高斯溅射、法向梯度引导优化和深度约束三角剖分实现高效重建。


<details>
  <summary>Details</summary>
Motivation: 传统多视角几何流程依赖密集重建、网格化和后续简化，过程繁琐且质量敏感，需要一种能直接从图像重建轻量级建筑表面的方法。

Method: 首先训练3D高斯溅射场获得视图一致表示，然后通过法向梯度引导的高斯优化选择与屋顶和墙壁边界对齐的基元，接着进行多视角边缘一致性修剪增强结构锐度，最后通过多视角深度约束Delaunay三角剖分转换为轻量级建筑网格。

Result: 在提出的SF数据集上，SF-Recon能够直接从多视角图像重建轻量级建筑模型，显著减少面和顶点数量，同时保持计算效率。

Conclusion: SF-Recon方法能够有效直接从多视角图像重建轻量级建筑表面模型，在保持结构保真度的同时大幅减少网格复杂度。

Abstract: Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/

</details>


### [336] [Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space](https://arxiv.org/abs/2511.13282)
*Kaiwen Wang,Kaili Zheng,Yiming Shi,Chenyi Guo,Ji Wu*

Main category: cs.CV

TL;DR: 提出了DTO方法解决多人人体网格恢复中的场景一致性缺失问题，并构建了DTO-Humans数据集和Metric-Aware HMR网络，在相对深度推理和人体网格恢复方面达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的野外人体网格伪真值生成流程是单人中心的，缺乏场景级一致性，导致同一图像中个体的深度和尺度存在冲突。

Method: 提出了深度条件平移优化(DTO)方法，在MAP框架下联合优化所有个体的相机空间平移；并提出了Metric-Aware HMR端到端网络，直接估计度量尺度的人体网格和相机参数。

Result: 构建了DTO-Humans数据集，包含56万张高质量场景一致的多人图像；在相对深度推理和人体网格恢复方面达到最先进性能。

Conclusion: DTO方法有效解决了多人场景中的深度和尺度一致性问题，Metric-Aware HMR网络能够直接输出度量尺度的结果，为多人人体网格恢复提供了新的解决方案。

Abstract: Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.

</details>


### [337] [TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing](https://arxiv.org/abs/2511.13283)
*Jongha Kim,Minseong Bae,Sanghyeok Lee,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: TabFlash：一种高效的多模态大语言模型，通过渐进式问题条件化、剪枝策略和令牌聚焦训练，在表格理解任务中实现最佳性能，同时减少计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM方法在处理表格图像时忽略了问题特定关注需求和冗余背景区域的问题，导致视觉表示信息不足且冗余。

Method: 1. 渐进式问题条件化：将问题信息以逐渐增加的频率注入ViT层；2. 剪枝策略：丢弃背景令牌提高效率；3. 令牌聚焦训练：鼓励模型在保留令牌中集中关键信息。

Result: TabFlash在表格理解任务中达到最先进性能，优于开源和专有MLLM，同时相比次优MLLM减少27% FLOPs和30%内存使用。

Conclusion: TabFlash通过结合渐进式问题条件化、剪枝和令牌聚焦，实现了高效且有效的表格理解，在性能和效率方面均表现出色。

Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.

</details>


### [338] [SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design](https://arxiv.org/abs/2511.13285)
*Yunjie Yu,Jingchen Wu,Junchen Zhu,Chunze Lin,Guibin Chen*

Main category: cs.CV

TL;DR: SkyReels-Text是一个无需字体标签或微调的字体可控框架，用于精确的海报文本编辑，支持同时编辑多个文本区域并保持非编辑区域的视觉外观。


<details>
  <summary>Details</summary>
Motivation: 解决现代图像编辑模型在细粒度、字体感知的文本编辑方面的不足，满足专业设计工作流程中对字体样式保持和精确文本修改的需求。

Method: 提出字体可控框架，用户只需提供裁剪的字形补丁即可控制字体样式，无需字体标签或推理时微调，支持多文本区域同时编辑。

Result: 在多个数据集（包括手写文本基准）上实现最先进的性能，在文本保真度和视觉真实感方面表现优异，提供对字体家族和风格细节的前所未有的控制。

Conclusion: 这项工作弥合了通用图像编辑与专业级排版设计之间的差距，为专业设计工作流程提供了强大的文本编辑工具。

Abstract: Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.

</details>


### [339] [CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving](https://arxiv.org/abs/2511.13297)
*Enhui Ma,Lijun Zhou,Tao Tang,Jiahuan Zhang,Junpeng Jiang,Zhan Zhang,Dong Han,Kun Zhan,Xueyang Zhang,XianPeng Lang,Haiyang Sun,Xia Zhou,Di Lin,Kaicheng Yu*

Main category: cs.CV

TL;DR: 提出了CorrectAD系统，利用扩散视频生成模型和3D布局来自动纠正端到端自动驾驶规划器的故障案例，显著降低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 解决端到端自动驾驶规划方法因长尾问题导致的鲁棒性不足，特别是罕见但安全关键的故障案例。

Method: 使用PM-Agent制定数据需求，提出DriveSora生成与3D布局对齐的高保真视频，构建完整的自纠正代理系统CorrectAD。

Result: 在nuScenes和内部数据集上，CorrectAD分别纠正了62.5%和49.8%的故障案例，碰撞率分别降低39%和27%。

Conclusion: CorrectAD是一个端到端模型无关的管道，可有效提升任何端到端规划器的性能，解决长尾问题。

Abstract: End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.

</details>


### [340] [DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving](https://arxiv.org/abs/2511.13309)
*Kaiwen Cai,Xinze Liu,Xia Zhou,Hengtong Hu,Jie Xiang,Luyao Zhang,Xueyang Zhang,Kun Zhan,Yifei Zhan,Xianpeng Lang*

Main category: cs.CV

TL;DR: DriveLiDAR4D是一个新颖的LiDAR生成流水线，能够生成时序一致的LiDAR场景，具有高度可控的前景物体和逼真的背景，在nuScenes数据集上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D LiDAR点云生成方法存在缺乏序列生成能力、无法产生精确定位的前景物体和逼真背景等局限性，阻碍了其实际应用。

Method: 提出了DriveLiDAR4D流水线，包含多模态条件和新型序列噪声预测模型LiDAR4DNet，以端到端方式实现具有完整场景操作能力的LiDAR场景序列生成。

Result: 在nuScenes和KITTI数据集上评估，在nuScenes数据集上获得FRD分数743.13和FVD分数16.96，相比当前最佳方法UniScene分别提升了37.2%和24.1%。

Conclusion: 这是首个以端到端方式解决具有完整场景操作能力的LiDAR场景序列生成的工作，在性能上显著超越了现有方法。

Abstract: The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.

</details>


### [341] [Computer Vision based group activity detection and action spotting](https://arxiv.org/abs/2511.13315)
*Narthana Sivalingam,Santhirarajah Sivasthigan,Thamayanthi Mahendranathan,G. M. R. I. Godaliyadda,M. P. B. Ekanayake,H. M. V. R. Herath*

Main category: cs.CV

TL;DR: 提出一个结合深度学习和图推理的群体活动检测框架，使用Mask R-CNN进行精确定位，通过图卷积网络建模演员关系，在拥挤和非拥挤场景中提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 多人场景中的群体活动检测面临复杂人体交互、遮挡和时间外观变化的挑战，需要有效建模个体间关系。

Method: 使用Mask R-CNN进行演员定位，融合掩码信息提取特征，构建演员关系图编码外观相似性和位置关系，应用图卷积网络进行关系推理。

Result: 在Collective Activity数据集上的实验表明，该方法在拥挤和非拥挤场景中均能提高识别性能。

Conclusion: 该方法展示了结合分割、特征提取和图关系推理在复杂视频理解任务中的潜力。

Abstract: Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.

</details>


### [342] [YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection](https://arxiv.org/abs/2511.13344)
*Ori Meiraz,Sharon Shalev,Avishai Weizman*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv9-T的混合专家框架，通过自适应路由实现动态特征专业化，相比单一YOLOv9-T模型获得了更高的mAP和AR


<details>
  <summary>Details</summary>
Motivation: 为了提升目标检测性能，需要解决单一模型在处理多样化特征时的局限性，通过专家混合框架实现更精细的特征处理

Method: 采用混合专家框架，集成多个YOLOv9-T专家模型，通过自适应路由机制实现动态特征分配和专业化处理

Result: 相比单一YOLOv9-T模型，该框架在平均精度(mAP)和平均召回率(AR)指标上均有显著提升

Conclusion: 混合专家框架通过动态特征专业化有效提升了目标检测性能，证明了多专家协作在检测任务中的优势

Abstract: This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.

</details>


### [343] [Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images](https://arxiv.org/abs/2511.13353)
*Lucas Gabriel Telesco,Danila Nejamkin,Estefanía Mata,Francisco Filizzola,Kevin Wignall,Lucía Franco Troilo,María de los Angeles Cenoz,Melissa Thompson,Mercedes Leguía,Ignacio Larrabide,José Ignacio Orlando*

Main category: cs.CV

TL;DR: 提出一种混合半监督学习方法，通过结合整体质量的人工标签和质量细节的伪标签，在多任务框架下改进视网膜图像质量评估，无需大量手动标注即可获得更可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜图像质量评估工具大多只分类整体图像质量，无法指示采集缺陷以指导重新拍摄，这主要是由于详细标注成本高昂。

Method: 使用在小型数据集上训练的教师模型生成伪标签，然后在多任务设置中使用这些伪标签微调预训练模型，采用ResNet-18骨干网络。

Result: 多任务模型在EyeQ数据集上F1分数达0.875，在DeepDRiD数据集上达0.778，优于单任务基线，在大多数细节预测任务上与教师模型性能相当。在新标注的EyeQ子集上表现与专家相似。

Conclusion: 所提出的半监督方法不仅改进了整体质量评估，还提供了关于采集条件（光照、清晰度、对比度）的可解释反馈，增强了可解释性且无需额外手动标注成本，为图像重新拍摄提供了临床可行的输出。

Abstract: Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.

</details>


### [344] [Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model](https://arxiv.org/abs/2511.13387)
*Fei Kong*

Main category: cs.CV

TL;DR: 本文提出了广义去噪扩散压缩模型(gDDCM)，将DDCM扩展到主流扩散模型及其变体，包括DDPM、基于分数的模型、一致性模型和整流流，在图像压缩任务中取得了改进的性能。


<details>
  <summary>Details</summary>
Motivation: DDCM虽然利用DDPM和特定规则采样的噪声实现了图像压缩，但无法应用于DDPM之外的其他方法，需要将其扩展到更广泛的扩散模型。

Method: 提出gDDCM框架，将DDCM的核心思想（在反向过程中使用特定规则采样的噪声替代随机噪声）扩展到DDPM、基于分数的模型、一致性模型和整流流等主流扩散模型。

Result: 在CIFAR-10和LSUN Bedroom数据集上的实验结果表明，gDDCM成功将DDCM推广到上述模型，并实现了性能提升。

Conclusion: gDDCM有效扩展了DDCM的应用范围，使其能够适用于多种主流扩散模型，为基于扩散模型的图像压缩提供了更通用的解决方案。

Abstract: Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.

</details>


### [345] [Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)](https://arxiv.org/abs/2511.13397)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: 提出了DTPQA基准测试，用于评估视觉语言模型在交通场景中的感知能力，特别是关注远距离物体识别，包含合成和真实世界两部分数据集。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域需要可靠的视觉语言模型，这些模型必须具备在复杂交通场景中的稳健感知能力，特别是对远距离物体的识别，这对安全驾驶决策至关重要。

Method: 创建了DTPQA基准测试，包含合成数据集（使用模拟器生成）和真实世界数据集（基于真实交通场景图像），每个样本包含图像、问题、真实答案和物体距离标注。

Result: 提供了完整的DTPQA数据集和生成脚本，可用于评估VLM在不同距离下的感知性能退化情况。

Conclusion: DTPQA基准测试为评估视觉语言模型在自动驾驶场景中的感知能力提供了专门工具，特别关注距离对感知性能的影响。

Abstract: The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.

</details>


### [346] [TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing](https://arxiv.org/abs/2511.13399)
*Yuchen Bao,Yiting Wang,Wenjian Huang,Haowei Wang,Shen Chen,Taiping Yao,Shouhong Ding,Jianguo Zhang*

Main category: cs.CV

TL;DR: TripleFDS是一个用于场景文本编辑的新框架，通过解耦文本样式、文本内容和背景三个属性，实现了更灵活和视觉一致的文本编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的场景文本编辑方法在可编辑属性解耦方面存在不足，通常只能处理单一方面的编辑（如文本内容），限制了可控性和视觉一致性。

Method: 提出TripleFDS框架和SCB Synthesis数据集，使用SCB Group作为基本训练单元，通过组间对比正则化和组内多特征正交性实现三特征解耦，在合成阶段进行特征重映射以防止重建中的"捷径"现象。

Result: 在主流STE基准测试中，TripleFDS实现了最先进的图像保真度（SSIM为44.54）和文本准确率（ACC为93.58%），并支持样式替换和背景转移等新操作。

Conclusion: TripleFDS通过三特征解耦实现了更灵活和高质量的场景文本编辑，在性能和功能上都超越了现有方法。

Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS

</details>


### [347] [What Color Is It? A Text-Interference Multimodal Hallucination Benchmark](https://arxiv.org/abs/2511.13400)
*Jinkun Zhao,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: 论文提出了一个名为"What Color Is It"的数据集，用于测试多模态大模型在颜色感知方面的视觉幻觉问题，并探索了解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型的快速发展，这些模型在视觉感知方面仍然容易受到信息干扰，特别是在颜色感知方面，这增加了幻觉风险。

Method: 通过构建"What Color Is It"数据集，使用简单方法触发多模态大模型中的单模态视觉幻觉，并分析其原因。

Result: 验证了多模态大模型在视觉模态中确实存在颜色感知方面的幻觉问题。

Conclusion: 提出了增强多模态大模型鲁棒性的潜在解决方案，以解决视觉模态中的幻觉问题。

Abstract: With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the "What Color Is It" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.

</details>


### [348] [Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source](https://arxiv.org/abs/2511.13417)
*Mykola Lavreniuk,Nataliia Kussul,Andrii Shelestov,Yevhenii Salii,Volodymyr Kuzin,Sergii Skakun,Zoltan Szantoi*

Main category: cs.CV

TL;DR: DelAnyFlow是一个分辨率无关的大规模农田边界提取方法，结合DelAny实例分割模型和结构化后处理流程，能够快速生成拓扑一致的矢量边界，在乌克兰603,000km²区域仅需6小时完成边界提取。


<details>
  <summary>Details</summary>
Motivation: 现有农田边界提取方法存在边界不完整、相邻农田合并、难以扩展等问题，需要开发可扩展的解决方案来支持土地管理和作物监测。

Method: 基于YOLOv11骨干网络构建DelAny实例分割模型，在FBIS 22M数据集（672,909个多分辨率图像块，2,290万个验证农田实例）上训练，结合结构化后处理、合并和矢量化流程。

Result: DelAny模型比SAM2准确率提高100%以上，推理速度快400倍；在乌克兰应用中提取了375万个（5米分辨率）和515万个（2.5米分辨率）农田边界，显著优于现有产品。

Conclusion: 该方法为缺乏数字地籍数据的地区提供了可扩展、成本效益高的农田边界提取解决方案，支持国家级应用。

Abstract: Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.

</details>


### [349] [VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task](https://arxiv.org/abs/2511.13420)
*Xingming Long,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出了VOPE方法，用于评估大型视觉语言模型在自愿想象任务中的幻觉问题，发现现有模型在想象任务中普遍存在严重幻觉，且现有缓解方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注事实描述任务中的幻觉问题，而忽视了自愿想象任务（如故事创作）中的幻觉评估，这些任务需要模型生成图像之外的新内容。

Method: 提出VOPE方法，通过基于重新检查的问题来评估LVLM如何解释其响应中想象对象的存在性，根据模型解释与图像中对象存在的一致性来判断是否产生幻觉。

Result: 应用VOPE评估主流LVLM和幻觉缓解方法发现：大多数LVLM在自愿想象中严重幻觉，对想象对象的存在性评估表现差；现有缓解方法在自愿想象任务中效果有限。

Conclusion: 自愿想象任务中的幻觉问题是一个重要的研究方向，需要开发专门针对此类任务的幻觉缓解方法。

Abstract: Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.

</details>


### [350] [FUSE: A Flow-based Mapping Between Shapes](https://arxiv.org/abs/2511.13431)
*Lorenzo Olearo,Giulio Viganò,Daniele Baieri,Filippo Maggioli,Simone Melzi*

Main category: cs.CV

TL;DR: 提出基于流匹配模型的3D形状间映射神经表示，支持跨表示形状匹配，无需大规模训练或数据驱动过程。


<details>
  <summary>Details</summary>
Motivation: 解决3D形状匹配中跨不同表示（点云、网格、SDF等）的挑战，提供计算高效且模态无关的映射表示。

Method: 将3D形状表示为从固定锚分布通过连续可逆流映射诱导的概率分布，通过逆流（源到锚）和前向流（锚到目标）的组合实现形状间连续映射。

Result: 在多样化基准和挑战性设置中一致实现高覆盖率和准确性，在人体点云扫描的UV映射和配准等任务中也表现出色。

Conclusion: 该框架为3D形状映射提供了高效、可逆且模态无关的表示，在形状匹配及相关任务中展现出强大性能。

Abstract: We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.

</details>


### [351] [Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline](https://arxiv.org/abs/2511.13442)
*Rui Zuo,Qinyue Tong,Zhe-Ming Lu,Ziqian Lu*

Main category: cs.CV

TL;DR: Foresee是一个无需训练的MLLM图像伪造分析框架，通过类型先验驱动策略和灵活特征检测器，在多种篡改类型上实现优越的定位精度和文本解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测方法泛化能力有限且缺乏可解释性，而大规模训练的MLLM方法计算成本高，未能充分利用原始MLLM的内在泛化潜力。

Method: 提出无需训练的MLLM管道，采用类型先验驱动策略和灵活特征检测器模块专门处理复制-移动篡改，释放原始MLLM在取证领域的潜力。

Result: 在多种篡改类型上实现优越定位精度，提供更丰富的文本解释，展现出更强的泛化能力，超越现有IFDL方法。

Conclusion: Foresee成功展示了原始MLLM在图像伪造分析中的潜力，无需额外训练即可实现高性能的篡改定位和解释。

Abstract: With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.

</details>


### [352] [Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling](https://arxiv.org/abs/2511.13478)
*Adam Hazimeh,Ke Wang,Mark Collier,Gilles Baechler,Efi Kokiopoulou,Pascal Frossard*

Main category: cs.CV

TL;DR: SliDer是一个使用视觉语言模型将幻灯片图像转换为可编辑SVG格式的框架，能够保持文档的语义结构并支持迭代优化。


<details>
  <summary>Details</summary>
Motivation: 现有的几何光栅-矢量转换方法在处理复杂文档时无法保持高层次结构，导致图像和文本元素的语义区分丢失，限制了文档的编辑性。

Method: 使用视觉语言模型检测和提取光栅输入中的图像和文本元素属性，将其组织成连贯的SVG格式，并在推理过程中通过迭代优化来改进预测。

Result: SliDer实现了0.069的重建LPIPS，在82.9%的情况下被人类评估者认为优于最强的零样本VLM基线。

Conclusion: SliDer能够有效地将静态光栅文档转换为可编辑的矢量格式，保持语义结构，为文档编辑性恢复提供了新方法。

Abstract: Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.

</details>


### [353] [InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE](https://arxiv.org/abs/2511.13488)
*Lipeng Wang,Hongxing Fan,Haohua Chen,Zehuan Huang,Lu Sheng*

Main category: cs.CV

TL;DR: InterMoE是一个基于动态时间选择性专家混合的新框架，用于生成高质量的人类交互动作，能够保持个体特征并忠实于文本描述。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成人类交互时往往无法保持独特的个体特征或完全遵循文本描述，这限制了在虚拟现实和机器人等应用中的价值。

Method: 采用动态时间选择性专家混合框架，通过路由机制协同使用高级文本语义和低级动作上下文，将时间运动特征分配给专门专家，让专家动态确定选择容量并关注关键时间特征。

Result: 在InterHuman数据集上FID分数降低9%，在InterX数据集上降低22%，实现了个体特定高保真3D人类交互生成的最先进性能。

Conclusion: InterMoE框架通过动态专家选择和特征路由，有效解决了保持个体特征和语义保真度的挑战，在人类交互生成任务上表现出色。

Abstract: Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.

</details>


### [354] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: LGIP基准测试评估视觉语言模型对语义保持改写和语义改变翻转的鲁棒性，发现EVA02-CLIP和大型OpenCLIP变体在不变性和敏感性方面表现最佳，而SigLIP系列存在较大问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在零样本任务中表现良好，但缺乏对其在受控语言扰动下响应可靠性的系统评估。

Method: 使用40k MS COCO图像和人工标注，自动生成语义保持的改写和基于规则的语义翻转（改变对象类别、颜色或数量），通过不变性误差、语义敏感性差距和正率统计来总结模型行为。

Result: EVA02-CLIP和大型OpenCLIP变体在不变性-敏感性边界上表现最佳，而SigLIP和SigLIP2显示出较大的不变性误差，且经常偏好翻转后的描述而非原始人工描述。

Conclusion: LGIP提供了一个模型无关的诊断工具，能够揭示标准检索指标无法发现的视觉语言模型语言鲁棒性问题。

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [355] [Mapping the Vanishing and Transformation of Urban Villages in China](https://arxiv.org/abs/2511.13507)
*Wenyu Zhang,Yao Tong,Yiqiu Liu,Rui Cao*

Main category: cs.CV

TL;DR: 本研究提出了一个基于深度学习的框架来监测中国城中村的时空变化，通过多时相遥感影像语义分割和拆迁后土地利用分类，揭示了城中村改造的复杂性和非线性特征。


<details>
  <summary>Details</summary>
Motivation: 中国城中村经历了大规模拆迁改造，但缺乏对拆迁土地是否有效再利用的系统评估，需要评估当前改造实践的效能和可持续性。

Method: 使用多时相遥感影像语义分割绘制城中村边界变化，将拆迁后土地利用分为六类：未完全拆迁、闲置土地、建筑工地、建筑物、绿地和其它，选取广州、郑州、西安和哈尔滨四个代表性城市作为研究区域。

Result: 1) 城中村改造过程经常被延长；2) 改造主要发生在城市外围区域，而城市核心区相对稳定；3) 揭示了三种时空转换路径：同步改造、延迟改造和逐步优化。

Conclusion: 城中村改造具有碎片化、复杂性和非线性特征，需要分层级和因地制宜的规划策略，研究结果为支持更包容、高效和可持续的城市更新提供了实证见解。

Abstract: Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the "remained-demolished-redeveloped" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.

</details>


### [356] [Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems](https://arxiv.org/abs/2511.13533)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: 提出了一种渐近极小极大方法用于多目标共形预测，在保证联合边际覆盖的同时提供紧密的预测区间，应用于多指标盲图像质量评估、多任务不确定性量化和多轮测量采集。


<details>
  <summary>Details</summary>
Motivation: 在病态成像逆问题中，不确定性量化是一个基本挑战。现有方法只能处理标量估计目标，而实际应用常涉及多个目标，需要开发多目标共形预测方法。

Method: 提出渐近极小极大多目标共形预测方法，通过优化算法确保联合边际覆盖的同时最小化预测区间宽度。

Result: 数值实验表明，该方法在合成数据和MRI数据上相对于现有多目标共形预测方法具有优势，能提供更紧密的预测区间。

Conclusion: 该方法为多目标不确定性量化提供了有效的解决方案，在医学成像等安全关键应用中具有重要价值。

Abstract: In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.

</details>


### [357] [Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew](https://arxiv.org/abs/2511.13535)
*Farhin Farhad Riya,Shahinul Hoque,Jinyuan Stella Sun,Olivera Kotevska*

Main category: cs.CV

TL;DR: 本文揭示了一种新的联邦学习攻击，通过微小颜色扰动在不影响模型准确性的情况下破坏可解释性，使显著性地图偏离语义区域。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在安全关键领域部署，可解释性技术变得至关重要。本文挑战了模型审计中"正确预测意味着忠实解释"的常见假设，证明可解释性本身可能成为攻击面。

Method: 提出色度扰动模块，通过系统性地改变前景和背景之间的颜色对比度来制作对抗样本，在联邦学习环境中累积这些扰动，以隐蔽持久的方式毒化全局模型的内部特征归因。

Result: 攻击将Grad-CAM解释中的峰值激活重叠降低了35%，同时在所有评估数据集上保持96%以上的分类准确率。标准训练流程无法检测或缓解这种解释退化。

Conclusion: 可解释性本身是一个新的攻击面，联邦学习环境中微小的颜色扰动难以察觉，但对模型解释保真度构成严重威胁，需要新的防御机制。

Abstract: As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.

</details>


### [358] [BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse](https://arxiv.org/abs/2511.13539)
*Yuanchao Wang,Tian Qin,Eduardo Valle,Bruno Abrahao*

Main category: cs.CV

TL;DR: BootOOD是一个完全自监督的OOD检测框架，通过从ID数据中合成伪OOD特征，利用神经崩溃现象，使用基于特征范数的轻量级辅助头进行OOD检测，在语义相似的OOD样本上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测器在处理与ID类别语义相似的OOD样本时表现不佳，需要一种能够专门处理这种语义挑战性OOD样本的检测方法。

Method: 通过ID表示的简单变换合成伪OOD特征，利用神经崩溃现象，引入基于特征范数的轻量级辅助头进行半径分类，将OOD检测与主分类器解耦。

Result: 在CIFAR-10、CIFAR-100和ImageNet-200上的实验表明，BootOOD优于先验的后处理方法，在没有异常暴露的情况下超越基于训练的方法，与最先进的异常暴露方法竞争力相当，同时保持或提高了ID准确率。

Conclusion: BootOOD提供了一种有效的自监督OOD检测解决方案，特别适用于处理语义相似的OOD样本，在保持ID性能的同时显著提升了OOD检测能力。

Abstract: Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.

</details>


### [359] [Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks](https://arxiv.org/abs/2511.13545)
*Md. Iqbal Hossain,Afia Sajeeda,Neeresh Kumar Perla,Ming Shao*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.

</details>


### [360] [TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images](https://arxiv.org/abs/2511.13552)
*Sining Chen,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出了TSE-Net，一种用于半监督单目高度估计的自训练框架，通过教师-学生-考试网络结构利用未标记数据提升模型性能


<details>
  <summary>Details</summary>
Motivation: 单目高度估计在遥感3D感知中很重要，但受限于标注数据的稀缺性和获取成本，需要利用大量未标记数据来提升模型泛化能力

Method: 采用教师-学生-考试网络的自训练框架：教师网络生成伪标签，学生网络在未标记数据上训练，考试网络作为学生网络的时序集成以稳定性能。教师网络结合回归和分类分支，使用分层双切策略处理高度值的长尾分布

Result: 在三个不同分辨率和成像模式的数据集上进行了评估，代码已开源

Conclusion: 提出的半监督学习框架有效解决了单目高度估计中标注数据稀缺的问题，通过利用未标记数据显著提升了模型性能

Abstract: Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.

</details>


### [361] [Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation](https://arxiv.org/abs/2511.13571)
*Ziyang Huang,Jiagang Chen,Jin Liu,Shunping Ji*

Main category: cs.CV

TL;DR: 提出了Opt3DGS框架，通过两阶段优化过程（自适应探索和曲率引导利用）解决3D高斯泼溅优化中的局部最优和收敛质量问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）在新视角合成中表现出色，但其核心优化问题未被充分探索，存在陷入局部最优和收敛质量不足的问题。

Method: 采用两阶段优化：探索阶段使用自适应加权随机梯度朗之万动力学增强全局搜索；利用阶段使用局部拟牛顿方向引导的Adam优化器进行精确收敛。

Result: 在多个基准数据集上的实验表明，Opt3DGS在不改变3DGS底层表示的情况下实现了最先进的渲染质量。

Conclusion: Opt3DGS通过改进3DGS的优化过程，有效解决了其优化挑战，提升了渲染性能。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.

</details>


### [362] [Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification](https://arxiv.org/abs/2511.13575)
*Linhan Zhou,Shuang Li,Neng Dong,Yonghang Tai,Yafei Zhang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了分层提示学习框架（HPL），通过任务感知的提示建模联合优化图像到图像（I2I）和文本到图像（T2I）的人员重识别任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将I2I和T2I任务分开处理，可能导致表示纠缠和性能不佳。两种任务具有不同挑战：I2I强调判别性身份学习，T2I需要准确的跨模态语义对齐。

Method: 1. 任务路由Transformer：在共享视觉编码器中引入双分类令牌，分别路由到I2I和T2I分支；2. 分层提示生成：集成身份级可学习令牌和实例级伪文本令牌；3. 跨模态提示正则化：在提示令牌空间强制语义对齐。

Result: 在多个ReID基准测试上的广泛实验验证了方法的有效性，在I2I和T2I任务上均达到了最先进的性能。

Conclusion: HPL框架通过统一的任务感知提示建模，有效解决了人员重识别中I2I和T2I任务的联合优化问题，实现了优异的跨模态检索性能。

Abstract: Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.

</details>


### [363] [Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images](https://arxiv.org/abs/2511.13586)
*Yinuo Xu,Yan Cui,Mingyao Li,Zhi Huang*

Main category: cs.CV

TL;DR: NuClass是一个病理学家工作流程启发的多尺度细胞分类框架，通过整合核形态和微环境上下文，解决了现有方法缺乏组织背景和细粒度标注的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像块的模型能捕捉核形态细节但缺乏组织背景信息，且人类标注通常粗糙且分布不均，难以获得细粒度的细胞亚型监督。

Method: NuClass包含两个主要组件：Path local（224×224像素裁剪关注核形态）和Path global（1024×1024像素邻域建模周围环境），通过可学习门控模块平衡局部细节和上下文线索，并采用不确定性引导目标促进互补学习。

Result: 在三个完全保留的队列上评估，NuClass最佳类别F1得分达96%，优于强基线方法。

Conclusion: 多尺度、不确定性感知融合能够弥合玻片级病理基础模型与可靠细胞级表型预测之间的差距。

Abstract: Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.
  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.
  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.

</details>


### [364] [VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping](https://arxiv.org/abs/2511.13587)
*Haotian Dong,Ye Li,Rongwei Lu,Chen Tang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 提出VVS框架，通过部分验证跳过加速视觉自回归模型的推理过程，将目标模型前向传递次数减少2.8倍，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成模型存在推理延迟问题，传统推测解码的"一步草稿、一步验证"范式限制了加速潜力，需要探索验证跳过机制来减少前向传递次数。

Method: 提出VVS框架，包含三个互补模块：动态截断的验证无关令牌选择器、令牌级特征缓存与重用、细粒度跳过步数调度。

Result: VVS将目标模型前向传递次数减少2.8倍，相比传统AR解码，在保持竞争力的生成质量的同时提供更优的速度-质量权衡。

Conclusion: VVS框架展示了重塑推测解码范式的强大潜力，通过验证跳过机制实现了视觉自回归生成的高效加速。

Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

</details>


### [365] [ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement](https://arxiv.org/abs/2511.13607)
*Xin Xu,Hao Liu,Wei Liu,Wei Wang,Jiayi Wu,Kui Jiang*

Main category: cs.CV

TL;DR: 提出ICLR框架，包含DIEM模块和CCL损失函数，通过改进色度与亮度的交互机制，在低光图像增强任务中取得优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有HVI色彩空间方法在色度与亮度分支交互中存在分布差异限制互补特征提取，以及亮度误差通过非线性参数传播到色度通道的问题；同时不同色度分支间在均匀颜色区域相关性弱，传统像素级损失导致梯度冲突。

Method: 提出双流交互增强模块(DIEM)从融合和增强两个维度改进互补信息提取；设计协方差校正损失(CCL)利用亮度残差统计惩罚色度误差，并通过约束色度分支协方差平衡梯度冲突。

Result: 在多个数据集上的实验结果表明，所提出的ICLR框架优于最先进的方法。

Conclusion: ICLR框架通过改进色度与亮度交互机制，有效解决了低光图像增强中的分布差异和梯度冲突问题，取得了优异的性能表现。

Abstract: Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.

</details>


### [366] [AtlasMorph: Learning conditional deformable templates for brain MRI](https://arxiv.org/abs/2511.13609)
*Marianne Rakic,Andrew Hoopes,S. Mazdak Abulnaga,Mert R. Sabuncu,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: 提出一个机器学习框架，使用卷积配准神经网络学习基于特定属性（如年龄、性别）生成模板的函数，并能生成相应的解剖分割图。


<details>
  <summary>Details</summary>
Motivation: 现有模板数量有限且计算成本高，导致分析常使用不具代表性的模板，特别是在群体变异较大时。

Method: 使用卷积配准神经网络学习条件模板生成函数，结合分割信息生成解剖标签图，并能进行图像配准。

Result: 在3D脑部MRI数据集上验证，能学习高质量代表性模板，带标签的条件模板比无条件模板配准效果更好，优于其他模板构建方法。

Conclusion: 该方法能有效生成条件模板，提高医学图像分析的准确性和代表性。

Abstract: Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.

</details>


### [367] [Tissue Aware Nuclei Detection and Classification Model for Histopathology Images](https://arxiv.org/abs/2511.13615)
*Kesi Xu,Eleni Chiou,Ali Varamesh,Laura Acqualagna,Nasir Rajpoot*

Main category: cs.CV

TL;DR: TAND是一个用于计算病理学中细胞核检测和分类的新框架，通过组织掩码条件增强点级监督，在PUMA基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖详细的专家标注且未充分利用组织上下文，导致细胞核检测和分类效果受限。

Method: 结合ConvNeXt编码器-解码器与冻结的Virchow-2组织分割分支，通过多尺度空间特征线性调制选择性调节分类流。

Result: 在PUMA基准测试中达到最先进性能，显著改善组织依赖性细胞类型（如上皮细胞、内皮细胞和基质细胞）的分类效果。

Conclusion: 这是首个基于学习组织掩码进行单细胞分类的方法，为减少标注负担提供了实用途径。

Abstract: Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.

</details>


### [368] [A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio](https://arxiv.org/abs/2511.13618)
*Ashlesha G. Sawant,Shreyash S. Kamble,Raj S. Kanade,Raunak N. Kanugo,Tanishq A. Kapse,Karan A. Bhapse*

Main category: cs.CV

TL;DR: 开发基于面部特征和眼部运动的驾驶员疲劳检测系统，通过实时监测眨眼频率和闭眼时长来预警疲劳驾驶


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是导致道路事故的主要原因之一，每年造成数千人死亡和受伤，需要开发有效的预警系统来提高道路安全

Method: 使用标准网络摄像头，结合MediaPipe Face Mesh框架进行面部特征点检测，采用眼部纵横比(EAR)方法分析眼部运动，通过OpenCV进行图像处理

Result: 实验分析表明系统具有高准确性和快速响应能力，能够有效检测长时间闭眼和低眨眼频率等疲劳特征

Conclusion: 该系统提供了一个高性能、低成本的驾驶员监控解决方案，可作为高级驾驶辅助系统(ADAS)的组成部分

Abstract: One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).

</details>


### [369] [Alpha Divergence Losses for Biometric Verification](https://arxiv.org/abs/2511.13621)
*Dimitrios Koutsianos,Ladislav Mosner,Yannis Panagakis,Themos Stafylakis*

Main category: cs.CV

TL;DR: 本文提出了两种基于α-散度的边际损失函数：Q-Margin（在参考度量中引入边际）和A3M（在对数中引入边际），解决了α-散度损失难以整合角度边际的问题，并在人脸和说话人验证任务中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于边际的softmax损失（如CosFace和ArcFace）在人脸和说话人验证中表现优异，但α-散度损失函数虽然能诱导稀疏解，却难以整合角度边际这一对验证任务至关重要的特性。

Method: 提出了两种整合角度边际的方法：通过参考度量（先验概率）的Q-Margin和通过对数（未归一化对数似然）的A3M。针对A3M的训练不稳定性，提出了简单有效的原型重新初始化策略。

Result: 在IJB-B和IJB-C人脸验证基准上取得了显著性能提升，在VoxCeleb说话人验证上同样表现出色。特别是在低误接受率（FAR）下显著优于强基线方法，这对银行认证等高安全性应用至关重要。

Conclusion: 提出的两种基于α-散度的边际损失函数成功解决了角度边际整合问题，在多个验证任务中表现出优越性能，特别是在低误接受率场景下具有重要实用价值。

Abstract: Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.

</details>


### [370] [CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2511.13644)
*Shrenik Patel,Daivik Patel*

Main category: cs.CV

TL;DR: CacheFlow是一种无需训练的长视频问答解决方案，通过动态令牌丢弃和压缩长期记忆机制，显著减少处理令牌数量，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理长视频问答时面临注意力机制和键值缓存随运行时间增长的问题，导致推理成本高昂或只能使用短视的滑动窗口方法。

Method: 结合动态令牌丢弃（基于帧间余弦相似度在线修剪令牌）和压缩长期记忆（使用小型循环编码器构建检索索引，离线存储完整KV对），采用基于共识的检索机制选择最相关块进行注意力计算。

Result: 在离线和流式视频问答基准测试中优于现有基线方法，同时处理令牌数量减少高达87%。

Conclusion: CacheFlow使视觉语言模型既能高效处理长视频，又能保持上下文感知能力，为实用的长视频理解铺平了道路。

Abstract: Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.

</details>


### [371] [Part-X-MLLM: Part-aware 3D Multimodal Large Language Model](https://arxiv.org/abs/2511.13647)
*Chunshi Wang,Junliang Ye,Yunhan Yang,Yang Li,Zizhuo Lin,Jun Zhu,Zhuo Chen,Yawei Luo,Chunchao Guo*

Main category: cs.CV

TL;DR: Part-X-MLLM是一个原生3D多模态大语言模型，通过结构化可执行语法将多样3D任务统一为程序，实现基于部件的生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 统一多样3D任务，通过结构化输出驱动下游几何感知模块，实现符号规划与几何合成的解耦。

Method: 采用双编码器架构预训练以分离结构与语义，在大型部件中心数据集上进行指令调优，自回归生成包含边界框、语义描述和编辑命令的连贯令牌序列。

Result: 模型在结构化规划生成方面表现出色，在基于问答、组合生成和局部编辑等任务中实现最先进性能。

Conclusion: 通过单一语言原生前端控制兼容几何引擎，为3D多模态任务提供了统一且高效的解决方案。

Abstract: We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/

</details>


### [372] [PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image](https://arxiv.org/abs/2511.13648)
*Ziang Cao,Fangzhou Hong,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: PhysX-Anything是首个面向仿真的物理3D生成框架，能从单张图像生成具有明确几何、关节和物理属性的仿真就绪3D资产，显著提升在具身AI中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法大多忽略了关键的物理和关节属性，限制了在具身AI中的应用。需要将3D建模从静态视觉表示转向可直接用于仿真和交互的物理化、可关节化资产。

Method: 提出首个基于视觉语言模型的物理3D生成模型，采用新的3D表示方法将几何token化，token数量减少193倍；构建PhysX-Mobility数据集，扩展物理3D数据类别2倍以上，包含2000多个真实世界物体。

Result: 在PhysX-Mobility数据集和真实世界图像上的实验表明，PhysX-Anything具有强大的生成性能和鲁棒泛化能力；在MuJoCo风格环境中的仿真实验验证了资产可直接用于接触密集的机器人策略学习。

Conclusion: PhysX-Anything能够显著赋能下游应用，特别是在具身AI和基于物理的仿真领域，为物理3D生成提供了新的解决方案。

Abstract: 3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.

</details>


### [373] [Distribution Matching Distillation Meets Reinforcement Learning](https://arxiv.org/abs/2511.13649)
*Dengyang Jiang,Dongyang Liu,Zanyi Wang,Qilong Wu,Xin Jin,David Liu,Zhen Li,Mengmeng Wang,Peng Gao,Harry Yang*

Main category: cs.CV

TL;DR: DMDR是一个结合强化学习和蒸馏的新框架，用于将多步扩散模型蒸馏为少步模型，在保持推理效率的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统的分布匹配蒸馏方法中，少步模型的性能往往受限于多步教师模型。为了突破这一限制，需要探索新的蒸馏框架。

Method: 将强化学习技术融入蒸馏过程，使用DMD损失作为正则化，设计动态分布引导和动态重噪声采样训练策略。

Result: DMDR在少步方法中实现了领先的视觉质量和提示一致性，甚至在某些情况下超越了多步教师模型的性能。

Conclusion: DMDR框架通过同时进行蒸馏和强化学习，成功释放了少步生成器的能力，为扩散模型的高效部署提供了有效解决方案。

Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.

</details>


### [374] [OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation](https://arxiv.org/abs/2511.13655)
*Henry Herzog,Favyen Bastani,Yawen Zhang,Gabriel Tseng,Joseph Redmon,Hadrien Sablon,Ryan Park,Jacob Morrison,Alexandra Buraczynski,Karen Farley,Joshua Hansen,Andrew Howe,Patrick Alan Johnson,Mark Otterlee,Ted Schmitt,Hunter Pitelka,Stephen Daspit,Rachel Ratner,Christopher Wilhelm,Sebastian Wood,Mike Jacobi,Hannah Kerner,Evan Shelhamer,Ali Farhadi,Ranjay Krishna,Patrick Beukema*

Main category: cs.CV

TL;DR: OlmoEarth是一个多模态、时空基础模型，专门为地球观测数据设计，在多个基准测试和实际任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据具有空间性（如图像）、序列性（如视频或文本）和多模态特性，需要专门的基础模型来处理这些独特挑战。

Method: 采用新颖的自监督学习公式、掩码策略和损失函数，专门为地球观测领域设计。

Result: 在24个任务中的15个任务上获得最佳嵌入性能，在29个任务中的19个任务上通过全微调获得最佳性能，优于其他12个基础模型。

Conclusion: OlmoEarth作为端到端平台的核心组件，为非营利组织和NGO提供前沿基础模型和强大数据管理工具，用于解决全球重大问题。

Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.

</details>


### [375] [Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting](https://arxiv.org/abs/2511.13684)
*Jiangnan Ye,Jiedong Zhuang,Lianrui Mu,Wenjie Zheng,Jiaqi Hu,Xingze Zou,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: GS-Light是一个基于高斯泼溅的高效文本引导3D场景重光照系统，通过训练免费的扩散模型处理多视角输入，结合几何语义先验生成高质量重光照结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本引导的3D场景重光照方面存在局限性，特别是在准确理解用户指定的光照方向、颜色和强度等需求方面。需要开发能够精确解析文本提示并保持多视角一致性的重光照方法。

Method: 使用大视觉语言模型解析文本提示为光照先验，结合深度、法线和语义分割等几何语义信息，生成初始潜代码指导扩散模型进行多视角重光照，最后微调3D高斯泼溅场景。

Result: 在室内外场景评估中，GS-Light在多视角一致性、图像质量、美学评分和语义相似度等指标上均优于现有基线方法，用户研究也证实了其优越性。

Conclusion: GS-Light提供了一种高效且准确的文本引导3D场景重光照解决方案，能够精确满足用户的光照需求，并在多个评估维度上表现出色。

Abstract: We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.

</details>


### [376] [TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models](https://arxiv.org/abs/2511.13704)
*Harold Haodong Chen,Disen Lan,Wen-Jie Shu,Qingyang Liu,Zihan Wang,Sirui Chen,Wenkai Cheng,Kanghao Chen,Hongfei Zhang,Zixin Zhang,Rongjin Guo,Yu Cheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出了TiViBench基准来评估图像到视频生成模型的推理能力，并开发了VideoTPO测试时优化策略来提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成基准主要评估视觉保真度和时间一致性，无法捕捉高阶推理能力。需要专门基准来评估视频生成模型的推理能力。

Method: 1) 设计TiViBench分层基准，在4个维度24个任务场景评估推理能力；2) 提出VideoTPO测试时策略，通过LLM自分析生成候选视频来识别优劣。

Result: 商业模型(如Sora 2、Veo 3.1)展现出更强的推理潜力，开源模型因训练规模和数据多样性受限而潜力未充分开发。VideoTPO显著提升推理性能且无需额外训练。

Conclusion: TiViBench和VideoTPO为评估和推进视频生成模型的推理能力奠定了基础，为该新兴领域的未来研究设定了框架。

Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.

</details>


### [377] [Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine](https://arxiv.org/abs/2511.13713)
*Xincheng Shuai,Zhenyuan Qin,Henghui Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: FFSE是一个3D感知的自回归框架，能够在真实图像上实现直观、物理一致的3D物体编辑，通过建模为一系列学习的3D变换序列，支持平移、缩放、旋转等任意操作，同时保持背景效果和场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在语义图像编辑方面取得了进展，但大多数方法在3D感知的物体操作方面表现不足，要么在图像空间操作，要么需要缓慢且容易出错的3D重建过程。

Method: FFSE将编辑建模为一系列学习的3D变换序列，采用自回归框架，并引入了3DObjectEditor混合数据集，该数据集包含来自不同物体和场景的模拟编辑序列，支持多轮和动态条件下的有效训练。

Result: 大量实验表明，FFSE在单轮和多轮3D感知编辑场景中显著优于现有方法。

Conclusion: FFSE框架成功实现了对真实世界图像的3D感知物体编辑，能够保持物理一致性和场景全局一致性，为多轮编辑提供了有效解决方案。

Abstract: Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.

</details>


### [378] [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/abs/2511.13714)
*Junwei Yu,Trevor Darrell,XuDong Wang*

Main category: cs.CV

TL;DR: UnSAMv2通过引入粒度控制嵌入和自监督学习，使SAM模型能够在无需人工标注的情况下实现任意粒度的分割，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型在分割粒度控制方面的局限性，用户通常需要手动细化结果才能达到所需细节水平，而收集所有粒度的密集标注成本过高。

Method: 扩展UnSAM的分治策略，发现丰富的掩码-粒度对，引入新颖的粒度控制嵌入来实现精确、连续的分割尺度控制，仅使用6K未标注图像和0.02%额外参数。

Result: 在11个基准测试中显著提升SAM-2性能：NoC90从5.69降至4.75，1-IoU从58.0提升至73.1，AR1000从49.6提升至68.3。

Conclusion: 少量未标注数据结合粒度感知的自监督学习方法可以释放视觉基础模型的潜力，实现任意粒度的分割控制。

Abstract: The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.

</details>


### [379] [Segment Anything Across Shots: A Method and Benchmark](https://arxiv.org/abs/2511.13715)
*Hengrui Hu,Kaining Ying,Henghui Ding*

Main category: cs.CV

TL;DR: 提出SAAS模型解决多镜头视频对象分割问题，通过过渡模拟数据增强策略和跨镜头分割能力，在YouMVOS和Cut-VOS基准上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频对象分割方法主要针对单镜头视频，难以处理镜头间的不连续性，限制了实际应用。多镜头标注数据稀缺也制约了相关研究发展。

Method: 提出过渡模拟数据增强策略(TMA)利用单镜头数据实现跨镜头泛化，开发SAAS模型有效检测和理解镜头转换，并构建Cut-VOS多镜头分割基准数据集。

Result: 在YouMVOS和Cut-VOS基准上的大量实验表明，SAAS通过有效模拟、理解和分割复杂过渡实现了最先进的性能。

Conclusion: SAAS模型能够有效处理多镜头视频对象分割问题，提出的数据增强策略和跨镜头理解能力为解决镜头间不连续性提供了有效方案。

Abstract: This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.

</details>


### [380] [Scaling Spatial Intelligence with Multimodal Foundation Models](https://arxiv.org/abs/2511.13719)
*Zhongang Cai,Ruisi Wang,Chenyang Gu,Fanyi Pu,Junxiang Xu,Yubo Wang,Wanqi Yin,Zhitao Yang,Chen Wei,Qingping Sun,Tongxi Zhou,Jiaqi Li,Hui En Pang,Oscar Qian,Yukun Wei,Zhiqian Lin,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Xiangyu Fan,Hanming Deng,Lewei Lu,Liang Pan,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: SenseNova-SI系列多模态基础模型通过构建800万样本数据集，在空间智能任务上取得突破性表现，同时保持强大的通用多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型取得了显著进展，但在空间智能方面仍存在明显不足，需要专门提升模型的空间推理和理解能力。

Method: 基于Qwen3-VL、InternVL3和Bagel等多模态基础模型，系统构建了包含800万多样化样本的SenseNova-SI-8M数据集，采用严格的空间能力分类体系。

Result: 在多个空间智能基准测试中表现优异：VSI-Bench 68.7%、MMSI 43.3%、MindCube 85.6%、ViewSpatial 54.6%、SITE 50.1%，同时保持强大的通用多模态理解能力（MMBench-En 84.9%）。

Conclusion: SenseNova-SI项目展示了通过数据扩展和多样化训练能够有效提升空间智能，并观察到新兴的泛化能力。该项目将持续更新，所有模型将公开发布以促进相关研究。

Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.

</details>


### [381] [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720)
*Tianhong Li,Kaiming He*

Main category: cs.CV

TL;DR: 本文提出JiT方法，直接预测干净数据而非噪声，利用流形假设使低容量网络能在高维空间有效工作，使用大块Transformer在像素级别实现强生成模型。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型预测噪声而非干净图像，但根据流形假设，自然数据位于低维流形而噪声数据不在，因此直接预测干净数据更有效。

Method: 使用简单的大块Transformer直接在像素级别操作，无需分词器、预训练或额外损失，通过16和32的大块尺寸在ImageNet上训练。

Result: 在ImageNet 256×256和512×512分辨率上取得有竞争力的结果，而预测高维噪声的方法会灾难性失败。

Conclusion: JiT方法回归基础，通过直接映射到流形实现自包含的Transformer扩散范式，证明简单方法在生成建模中的有效性。

Abstract: Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "$\textbf{Just image Transformers}$", or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [382] [ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments](https://arxiv.org/abs/2511.11586)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Xinming Wei,Cenlin Duan,Weisheng Zhao,Chunming Hu*

Main category: cs.DC

TL;DR: ACE-GNN是一个自适应图神经网络协同推理框架，专门针对动态边缘环境设计，通过系统级抽象和两种新颖的预测方法实现性能感知，支持运行时方案优化，并引入数据并行机制在流水线并行和数据并行之间自适应调度，显著提升系统性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于离线模型分割和流水线并行的GNN协同推理方法在动态边缘环境中性能受网络波动和多设备访问等环境因素影响严重，缺乏对这些动态因素的适应性。

Method: 1. 系统级抽象和两种新颖预测方法实现性能感知；2. 运行时方案优化；3. 引入数据并行机制，在流水线并行和数据并行之间自适应调度；4. 高效批处理推理策略和专用通信中间件。

Result: 在多样化应用和边缘设置下，ACE-GNN相比GCoDE实现了最高12.7倍的加速和82.3%的能耗节省，相比Fograph实现了11.7倍更好的能效。

Conclusion: ACE-GNN是首个针对动态边缘环境的自适应GNN协同推理框架，通过性能感知、自适应调度和优化策略，显著提升了边缘GNN应用的性能和能效。

Abstract: The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline model splitting and pipeline parallelism (PP), which enables more efficient computation and resource utilization during inference. However, the performance of these static deployment methods is significantly affected by environmental dynamics such as network fluctuations and multi-device access, which remain unaddressed. We present ACE-GNN, the first Adaptive GNN Co-inference framework tailored for dynamic Edge environments, to boost system performance and stability. ACE-GNN achieves performance awareness for complex multi-device access edge systems via system-level abstraction and two novel prediction methods, enabling rapid runtime scheme optimization. Moreover, we introduce a data parallelism (DP) mechanism in the runtime optimization space, enabling adaptive scheduling between PP and DP to leverage their distinct advantages and maintain stable system performance. Also, an efficient batch inference strategy and specialized communication middleware are implemented to further improve performance. Extensive experiments across diverse applications and edge settings demonstrate that ACE-GNN achieves a speedup of up to 12.7x and an energy savings of 82.3% compared to GCoDE, as well as 11.7 better energy efficiency than Fograph.

</details>


### [383] [Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks](https://arxiv.org/abs/2511.11598)
*Van-Vi Vo,Tien-Dung Nguyen,Duc-Tai Le,Hyunseung Choo*

Main category: cs.DC

TL;DR: 提出了一种基于Q学习的分布式最短路径树构建框架，用于物联网传感器网络的高效路由，相比传统集中式算法能降低通信开销并适应拓扑变化。


<details>
  <summary>Details</summary>
Motivation: 传统集中式路由算法（如Dijkstra）在动态分布式物联网环境中计算量大且不适用，需要一种能够自主学习的分布式路由方法。

Method: 采用分布式Q学习框架，传感器节点基于本地信息独立学习最优下一跳决策，状态定义基于节点位置和路由历史，奖励函数激励向汇聚节点前进并惩罚低效路径。

Result: 在100-500节点网络上的仿真显示，路由准确率接近最优（300节点以上超过99%），小网络中1-2跳的微小偏差对性能影响可忽略，相比集中式和泛洪方法显著降低通信开销。

Conclusion: Q学习为资源受限的物联网网络提供了自主、鲁棒的路由方案，是传统协议的可扩展替代方案。

Abstract: Efficient routing in IoT sensor networks is critical for minimizing energy consumption and latency. Traditional centralized algorithms, such as Dijkstra's, are computationally intensive and ill-suited for dynamic, distributed IoT environments. We propose a novel distributed Q-learning framework for constructing shortest-path trees (SPTs), enabling sensor nodes to independently learn optimal next-hop decisions using only local information. States are defined based on node positions and routing history, with a reward function that incentivizes progression toward the sink while penalizing inefficient paths. Trained on diverse network topologies, the framework generalizes effectively to unseen networks. Simulations across 100 to 500 nodes demonstrate near-optimal routing accuracy (over 99% for networks with more than 300 nodes), with minor deviations (1-2 extra hops) in smaller networks having negligible impact on performance. Compared to centralized and flooding-based methods, our approach reduces communication overhead, adapts to topology changes, and enhances scalability and energy efficiency. This work underscores the potential of Q-learning for autonomous, robust routing in resource-constrained IoT networks, offering a scalable alternative to traditional protocols.

</details>


### [384] [Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators](https://arxiv.org/abs/2511.11601)
*Elliott Wen,Sean Ma,Ewan Tempero,Jens Dietrich,Daniel Luo,Jiaxing Shen,Kaiqi Zhao,Bruce Sham,Yousong Song,Jiayi Hua,Jia Hong*

Main category: cs.DC

TL;DR: 首个对异构AI加速器上机器学习模型差异的实证研究，发现新兴AI平台(Mac、华为)相比NVIDIA支持更少算子、输出差异率更高、编译失败率更高，并识别了PyTorch和厂商特定问题


<details>
  <summary>Details</summary>
Motivation: 随着AI加速器市场多样化，新兴厂商提供成本效益替代方案，但缺乏对其与主流平台兼容性和性能一致性的实证研究

Method: 使用自动化流水线，从4000个真实模型合成超过10万个变体模型，在5种企业级AI加速器上执行测试

Result: Mac和华为平台支持算子比NVIDIA少17%，输出差异率超过5%，编译失败率更高，识别出7个PyTorch实现缺陷和40个平台特定问题

Conclusion: 在日益多样化的硬件生态系统中实现一致的机器学习行为面临重大挑战

Abstract: While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.

</details>


### [385] [Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review](https://arxiv.org/abs/2511.11603)
*Deep Bodra,Sushil Khairnar*

Main category: cs.DC

TL;DR: 对10种AI/ML算法在云资源分配中的比较分析，显示混合架构在多指标上优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法无法满足现代云基础设施的多目标优化需求，需要更智能的资源分配方案

Method: 系统评估了10种算法，分为四类：深度强化学习、神经网络架构、增强型传统机器学习、多智能体系统

Result: 在多个指标上显著改进，包括减少makespan、成本优化和能效提升，混合架构表现最佳

Conclusion: 混合AI/ML架构在复杂动态环境中表现最优，边缘计算环境部署准备度最高

Abstract: Cloud resource allocation has emerged as a major challenge in modern computing environments, with organizations struggling to manage complex, dynamic workloads while optimizing performance and cost efficiency. Traditional heuristic approaches prove inadequate for handling the multi-objective optimization demands of existing cloud infrastructures. This paper presents a comparative analysis of state-of-the-art artificial intelligence and machine learning algorithms for resource allocation. We systematically evaluate 10 algorithms across four categories: Deep Reinforcement Learning approaches, Neural Network architectures, Traditional Machine Learning enhanced methods, and Multi-Agent systems. Analysis of published results demonstrates significant performance improvements across multiple metrics including makespan reduction, cost optimization, and energy efficiency gains compared to traditional methods. The findings reveal that hybrid architectures combining multiple artificial intelligence and machine learning techniques consistently outperform single-method approaches, with edge computing environments showing the highest deployment readiness. Our analysis provides critical insights for both academic researchers and industry practitioners seeking to implement next-generation cloud resource allocation strategies in increasingly complex and dynamic computing environments.

</details>


### [386] [PACE Solver Description: twin_width_fmi](https://arxiv.org/abs/2511.11605)
*David Balaban,Adrian Miclăuş*

Main category: cs.DC

TL;DR: 提出了三种最小支配集启发式算法：贪心算法greedy-ln、模拟退火局部搜索，以及最终提交的hedom5算法，后者结合了迭代贪心构造、反向剪枝和局部改进步骤


<details>
  <summary>Details</summary>
Motivation: 为PACE 2025竞赛的最小支配集启发式赛道开发高效求解器，旨在通过组合多种启发式策略来获得更优的支配集解

Method: hedom5算法采用迭代贪心风格：首先使用紧凑CSR结构存储图并简化，然后基于优先队列的懒增益贪心构造，接着进行反向剪枝删除冗余顶点，最后执行预算限制的1-交换局部改进

Result: 开发了三种算法变体，其中hedom5作为最佳性能组件被提交，它结合了构造、剪枝和修复阶段而非单次遍历

Conclusion: hedom5通过迭代贪心构造、积极剪枝和局部改进的组合策略，在最小支配集问题上实现了有效的启发式求解

Abstract: In this paper we present \texttt{twin\_width\_fmi}'s solver for the heuristic track of PACE's 2025 competition on Minimum Dominating Set.
  As a baseline, we implement \texttt{greedy-ln}, a standard greedy dominating-set heuristic that repeatedly selects the vertex that newly dominates the largest number of currently undominated vertices. We then use this greedy solution as the starting point for a simulated annealing local search: we attempt vertex removals and exchanges and accept worsening moves with decaying probability, in order to escape local minima while preserving domination.
  Our best-performing component, which we ultimately submitted, is \texttt{hedom5}. The design of \texttt{hedom5} is inspired by recent iterative-greedy style domination heuristics~\cite{IterativeGreedy22} that alternate between constructive steps, pruning, and focused repair rather than relying on a single pass. In \texttt{hedom5}, the input graph is first stored in a compact CSR structure and simplified using fast reductions such as forcing neighbors of leaves and handling isolates. We then run a lazy gain-based greedy stage using a priority queue: each candidate vertex is scored by how many currently undominated vertices its closed neighborhood would newly dominate, and scores are only recomputed when necessary. After this constructive phase, we perform an aggressive backward pruning pass that iterates over the chosen dominators in reverse insertion order and deletes any vertex whose closed neighborhood is still fully dominated by the remaining set. Finally, we run a budgeted 1-swap local improvement step that attempts to replace a dominator by an alternative vertex that covers all of its uniquely covered vertices, thereby reducing the size of the dominating set. A brief safety patch at the end guarantees full domination.

</details>


### [387] [Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression](https://arxiv.org/abs/2511.11608)
*Mingyu Sung,Suhwan Im,Daeho Bang,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.DC

TL;DR: SLICER是一个无需重新训练的框架，通过压缩中间特征来减少边缘-云分割计算中的通信和服务器负载，在保持任务质量的同时显著降低上行流量和服务器GPU时间。


<details>
  <summary>Details</summary>
Motivation: 现代DNN的边缘-云模型分割方案通常使用固定、浅层的分割点，导致边缘计算资源利用不足，服务器端延迟和能耗集中。在自回归LLM推理中，每个token的前向传递会重复生成庞大的中间特征，加剧了这一问题。

Method: SLICER结合三种技术：(1)非对称top-K过滤稀疏化低幅值激活；(2)幅值分割将剩余非零值分组为等基数块；(3)自适应位量化在失真预算下选择每块位宽。

Result: 在标准视觉和LLM任务中，SLICER将上行流量减少高达10倍，服务器GPU时间减少高达4.4倍，同时任务质量保持在基线0-3个百分点内。

Conclusion: SLICER无需重新训练或架构更改即可附加到现成模型上，为可扩展、低延迟的分布式推理提供了即插即用解决方案。

Abstract: Modern DNNs often rely on edge-cloud model partitioning (MP), but widely used schemes fix shallow, static split points that underutilize edge compute and concentrate latency and energy on the server. The problem is exacerbated in autoregressive (AR) LLM inference, where per-token forward passes repeatedly generate bulky intermediate features (IFs). We introduce SLICER, a retraining-free, architecture-agnostic framework that compresses IFs to reduce both communication and server load in split computing. SLICER combines (i) asymmetric top-K filtering (ATKF) to sparsify low-magnitude activations, (ii) magnitude-splitting (MS) to group the remaining non-zeros into equal-cardinality blocks, and (iii) adaptive bit quantization (ABQ) that selects per-block bitwidths under a distortion budget. Across standard vision and LLM workloads (e.g., ImageNet/COCO; HellaSwag, PIQA, ARC-E/C, GSM8K, HumanEval), SLICER reduces uplink volume by up to 10x and server GPU time by up to 4.4x, while keeping task quality within ~0-3 pp of baseline. In multi-device settings and AR LLMs, SLICER scales by shifting meaningful compute to the edge and lowering bits-per-token and server time per token, stabilizing per-step traffic. The codec attaches to off-the-shelf models without retraining or architectural changes, offering a plug-and-play path to scalable, low-latency distributed inference. Code is provided in the supplementary material.

</details>


### [388] [Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems](https://arxiv.org/abs/2511.11612)
*Aasish Kumar Sharma,Julian Kunkel*

Main category: cs.DC

TL;DR: 评估21个公开LLM在HPC工作负载调度问题上的表现，发现3个模型能精确重现最优解，12个接近最优，6个存在错误。LLM在组合优化中表现出解释性强的推理能力，但精确时序和依赖执行仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在结构化、约束优化问题上的推理能力，特别是在HPC工作负载调度这一代表性组合优化问题中的表现。

Method: 让21个公开LLM基于相同的系统节点、任务需求和调度约束文本描述，进行任务分配、总完成时间计算和推理解释，以手动推导的9小时20秒最优解为基准。

Result: 3个模型精确重现最优解，12个在2分钟内接近最优，6个存在算术或依赖错误。所有模型都生成了可行的任务映射，但仅约一半严格遵循约束。19个模型生成了部分可执行验证代码，18个提供了连贯的逐步推理。

Conclusion: LLM在组合优化中能作为可解释的协同工具，而非自主求解器。领先模型能从自然语言直接重建最优调度，但大多数在精确时序、数据传输算术和依赖执行方面仍有困难。

Abstract: Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.

</details>


### [389] [Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI](https://arxiv.org/abs/2511.11614)
*Arturo Urías Jiménez*

Main category: cs.DC

TL;DR: FPGAs作为可重构硬件平台，相比固定架构的GPU和CPU，在AI加速中具有低延迟、高能效和硬件定制化优势，支持并行处理、确定性时序和近传感器推理。


<details>
  <summary>Details</summary>
Motivation: GPU主导的AI加速存在延迟、能效和硬件控制限制，需要可重构平台来满足确定性性能和深度定制需求。

Method: 利用FPGA的可重构特性，将AI算法直接映射到设备逻辑中，实现卷积、注意力机制和后处理的并行流水线，支持部分重配置和AI框架编译流程。

Result: FPGA能够实现确定性时序、降低功耗，支持近传感器推理，减少延迟和带宽需求，提高隐私保护，并释放数据中心的GPU资源。

Conclusion: FPGA通过硬件-算法协同设计，缩短从原型到部署的路径，为需要可预测性能和深度定制的AI工作负载提供了战略选择。

Abstract: AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.
  Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.

</details>


### [390] [AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism](https://arxiv.org/abs/2511.11617)
*Wendong Xu,Chujie Chen,He Xiao,Kuan Li,Jing Xiong,Chen Zhang,Wenyong Zhou,Chaofan Tao,Yang Bai,Bei Yu,Ngai Wong*

Main category: cs.DC

TL;DR: AnchorTP是一个弹性张量并行框架，通过解耦的守护进程保存模型参数和KV缓存，使用带宽感知规划器和流水线执行调度器，在GPU故障时实现快速恢复，显著减少服务中断时间。


<details>
  <summary>Details</summary>
Motivation: 多GPU张量并行推理服务对单GPU故障非常敏感，传统重启恢复方法会导致长时间服务中断，需要一种能够保持状态并快速恢复的弹性解决方案。

Method: 提出弹性张量并行(ETP)支持任意GPU数量的不等宽分区，兼容MoE模型；使用解耦守护进程保存参数和KV缓存；基于连续最小迁移算法的带宽感知规划器最小化重载字节；流水线执行调度器并行P2P传输和重载。

Result: 在典型故障场景下，AnchorTP相比重启重载方法，首次成功时间减少高达11倍，峰值时间减少高达59%。

Conclusion: AnchorTP通过状态保持和优化数据迁移策略，实现了LLM推理服务的快速故障恢复，显著提升了服务可用性。

Abstract: Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.

</details>


### [391] [DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack](https://arxiv.org/abs/2511.11619)
*Yuanjie Liu,Wenpeng Xing,Ye Zhou,Gaowei Chang,Changting Lin,Meng Han*

Main category: cs.DC

TL;DR: 提出了DIAP协议，一个去中心化的星际代理协议，通过绑定代理身份到IPFS/IPNS内容标识符和使用零知识证明，实现持久、可验证、无需信任的互操作性。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖中心化中介或缺乏去中心化身份解析机制，导致信任瓶颈和跨网络互操作性受限。

Method: 使用Rust SDK集成Noir（零知识证明）、DID-Key、IPFS和混合P2P堆栈，采用零依赖ZKP部署模型和预编译Noir电路。

Result: 建立了实用、高性能的基础设施，支持即时、可验证、隐私保护的身份证明。

Conclusion: 为下一代自主代理生态系统和代理间经济奠定了实践基础。

Abstract: The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability.
  We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates.
  We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs.
  This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies.

</details>


### [392] [AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs](https://arxiv.org/abs/2511.11621)
*Pedro Antunes,Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.DC

TL;DR: AIvailable是一个低成本、高可用的LLM即服务平台，能够在异构GPU节点上运行大型语言模型，专注于充分利用每个节点的VRAM资源。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统大多假设同质化、资源丰富的硬件环境，这在学术或资源受限环境中往往不现实，需要一种能在异构和传统GPU上高效运行的解决方案。

Method: 采用软件定义的方法，在NVIDIA和AMD等异构GPU节点上运行LLM，通过四个主要组件实现：客户端接口、服务前端、SDAI控制器和服务后端，提供动态的VRAM感知模型分配和重新分配。

Result: 实现了完全GPU加速的推理，无需CPU回退，通过统一客户端接口无缝与所有部署的LLM交互，确保资源高效利用和对故障或工作负载波动的弹性。

Conclusion: AIvailable通过重新利用传统GPU，支持多样化的开源LLM，有助于在学术实验室、私营公司等资源受限组织中普及生成式AI。

Abstract: The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs.

</details>


### [393] [Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges](https://arxiv.org/abs/2511.11624)
*Md Romyull Islam,Bobin Deng,Nobel Dhar,Tu N. Nguyen,Selena He,Yong Shi,Kun Suo*

Main category: cs.DC

TL;DR: 评估5个小型语言模型在边缘设备上的能效表现，发现Jetson Orin Nano GPU配置能效最高，Llama 3.2在准确性和能效间平衡最佳，TinyLlama适合低功耗环境但准确性较低。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署小型语言模型具有低延迟和独立网络连接的优势，但受限于计算资源和能源预算，需要评估其能效表现。

Method: 在Raspberry Pi 5、Jetson Nano和Jetson Orin Nano（CPU和GPU配置）上评估Llama 3.2、Phi-3 Mini、TinyLlama和Gemma 2五个代表性SLM的功率效率。

Result: Jetson Orin Nano GPU加速实现最高能效比，显著优于CPU配置；Llama 3.2在准确性和能效间平衡最佳；TinyLlama适合低功耗环境但准确性降低；Phi-3 Mini能耗最高。

Conclusion: GPU加速、内存带宽和模型架构是优化推理能效的关键因素，为AI、智能系统和移动自组织平台在能源受限环境中平衡准确性、推理延迟和能效提供实用见解。

Abstract: Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.

</details>


### [394] [Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies](https://arxiv.org/abs/2511.11628)
*Xinbo Wang,Shian Jia,Ziyang Huang,Jing Cao,Mingli Song*

Main category: cs.DC

TL;DR: ASA是一个动态调度框架，通过机器学习模型识别工作负载模式，从专门的调度器组合中动态选择最优策略，解决了传统单一静态调度策略无法适应异构硬件和多样化工作负载的问题。


<details>
  <summary>Details</summary>
Motivation: 现代操作系统调度器采用单一的静态策略，无法在异构硬件和多样化动态工作负载下提供最优性能，导致公平性、吞吐量和延迟方面的显著妥协。

Method: 提出自适应调度代理(ASA)框架：1）离线训练硬件无关的机器学习模型识别抽象工作负载模式；2）运行时使用时间加权概率投票算法识别工作负载，通过预配置的机器特定映射表切换到最优调度器。

Result: 在基于用户体验指标的新基准测试中，ASA在86.4%的测试场景中优于默认Linux调度器(EEVDF)，在78.6%的场景中其选择接近最优，位列前三名调度器。

Conclusion: ASA验证了通过动态选择专家调度器而非设计单一调度器的方法，是实现更智能、自适应和响应式操作系统调度器的实用路径。

Abstract: Modern operating system schedulers employ a single, static policy, which struggles to deliver optimal performance across the diverse and dynamic workloads of contemporary systems. This "one-policy-fits-all" approach leads to significant compromises in fairness, throughput, and latency, particularly with the rise of heterogeneous hardware and varied application architectures.
  This paper proposes a new paradigm: dynamically selecting the optimal policy from a portfolio of specialized schedulers rather than designing a single, monolithic one. We present the Adaptive Scheduling Agent (ASA), a lightweight framework that intelligently matches workloads to the most suitable "expert" scheduling policy at runtime. ASA's core is a novel, low-overhead offline/online approach. First, an offline process trains a universal, hardware-agnostic machine learning model to recognize abstract workload patterns from system behaviors. Second, at runtime, ASA continually processes the model's predictions using a time-weighted probability voting algorithm to identify the workload, then makes a scheduling decision by consulting a pre-configured, machine-specific mapping table to switch to the optimal scheduler via Linux's sched_ext framework. This decoupled architecture allows ASA to adapt to new hardware platforms rapidly without expensive retraining of the core recognition model.
  Our evaluation, based on a novel benchmark focused on user-experience metrics, demonstrates that ASA consistently outperforms the default Linux scheduler (EEVDF), achieving superior results in 86.4% of test scenarios. Furthermore, ASA's selections are near-optimal, ranking among the top three schedulers in 78.6% of all scenarios. This validates our approach as a practical path toward more intelligent, adaptive, and responsive operating system schedulers.

</details>


### [395] [Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications](https://arxiv.org/abs/2511.11640)
*Sed Centeno,Christopher Sprague,Arnab A Purkayastha,Ray Simar,Neeraj Magotra*

Main category: cs.DC

TL;DR: 该论文在MNIST数据集上实现了推测性反向传播，使用OpenMP并行编程平台，通过重叠前向和反向传播步骤加速神经网络训练，在CPU实验中获得了最高24%的执行时间加速和35%的步骤执行时间加速，同时准确率保持在基线3-4%以内。


<details>
  <summary>Details</summary>
Motivation: 推测性反向传播是一种有前景的加速神经网络训练的技术，通过在前向和反向传播重叠时进行推测性权重更新来减少训练时间，同时不显著影响准确率。

Method: 在MNIST数据集上实现推测性反向传播，使用OpenMP作为并行编程平台，利用其多线程能力同时执行前向和推测性反向传播步骤，并计划在FPGA上进行硬件加速演示。

Result: CPU实验结果显示，当阈值为0.25时，推测性反向传播实现了最高24%的执行时间加速，准确率在各种epoch下保持在基线3-4%以内。在单个步骤执行时间比较中，获得了最高35%的加速。

Conclusion: 推测性反向传播通过重叠前向和反向传播有效加速了神经网络训练，在保持准确率的同时显著提高了训练速度，证明了该技术在硬件加速方面的潜力。

Abstract: Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes.

</details>


### [396] [HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support](https://arxiv.org/abs/2511.11660)
*Zizheng Guo,Haichuan Liu,Xizhe Shi,Shenglu Hua,Zuodong Zhang,Chunyuan Zhao,Runsheng Wang,Yibo Lin*

Main category: cs.DC

TL;DR: HeteroSTA是首个CPU-GPU异构时序分析引擎，支持多种延迟计算模型、行业格式和端到端GPU加速，提供零开销的异构API。


<details>
  <summary>Details</summary>
Motivation: 开发一个无需依赖外部黄金工具的异构时序分析引擎，支持行业标准格式和全面的时序异常处理，同时提供高效的GPU加速。

Method: 采用CPU-GPU异构架构，支持多种延迟计算模型，兼容.sdc等行业格式，实现图基和路径基时序查询的端到端GPU加速，提供零开销的扁平化异构API。

Result: 作为独立工具、时序驱动DREAMPlace 4.0集成和时序驱动全局布线集成使用时，都表现出显著的运行速度提升和可比较的质量。

Conclusion: HeteroSTA成功实现了高效的异构时序分析，为学术和工业应用提供了公开可用的解决方案，在保持质量的同时大幅提升了运行速度。

Abstract: We introduce in this paper, HeteroSTA, the first CPU-GPU heterogeneous timing analysis engine that efficiently supports: (1) a set of delay calculation models providing versatile accuracy-speed choices without relying on an external golden tool, (2) robust support for industry formats, including especially the .sdc constraints containing all common timing exceptions, clock domains, and case analysis modes, and (3) end-to-end GPU-acceleration for both graph-based and path-based timing queries, all exposed as a zero-overhead flattened heterogeneous application programming interface (API). HeteroSTA is publicly available with both a standalone binary executable and an embeddable shared library targeting ubiquitous academic and industry applications. Example use cases as a standalone tool, a timing-driven DREAMPlace 4.0 integration, and a timing-driven global routing integration have all demonstrated remarkable runtime speed-up and comparable quality.

</details>


### [397] [Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks](https://arxiv.org/abs/2511.11664)
*Mingyu Sung,Suhwan Im,Vikas Palakonda,Jae-Mo Kang*

Main category: cs.DC

TL;DR: 提出一种轻量级压缩框架，利用rANS编码、非对称整数量化和稀疏张量表示来显著减少边缘计算中的传输开销，无需复杂概率建模或网络修改。


<details>
  <summary>Details</summary>
Motivation: 解决边缘计算中深度神经网络推理的通信瓶颈问题，在资源受限设备和云服务器之间传输中间特征时面临显著的带宽限制。

Method: 结合非对称整数量化和稀疏表示技术，采用分布无关的压缩流水线，利用张量固有稀疏性实现带宽减少，并通过优化张量重塑维度最大化压缩效率。

Result: 在多种神经网络架构和数据集上保持接近基线的准确率，在GPU加速实现中实现亚毫秒级的编码/解码延迟，并在NLP任务中验证了广泛适用性。

Conclusion: 该方法解决了在带宽受限环境中部署复杂AI系统的根本瓶颈，且不损害模型性能，具有广泛的适用性。

Abstract: Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverages Range Asymmetric Numeral Systems (rANS) encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead dramatically. Specifically, our approach combines asymmetric integer quantization with a sparse representation technique, eliminating the need for complex probability modeling or network modifications. The key contributions include: (1) a distribution-agnostic compression pipeline that exploits inherent tensor sparsity to achieve bandwidth reduction with minimal computational overhead; (2) an approximate theoretical model that optimizes tensor reshaping dimensions to maximize compression efficiency; and (3) a GPU-accelerated implementation with sub-millisecond encoding/decoding latency. Extensive evaluations across diverse neural architectures (ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, EfficientNetB0) demonstrate that the proposed framework consistently maintains near-baseline accuracy across CIFAR100 and ImageNet benchmarks. Moreover, we validated the framework's effectiveness on advanced natural language processing tasks by employing Llama2 7B and 13B on standard benchmarks such as MMLU, HellaSwag, ARC, PIQA, Winogrande, BoolQ, and OpenBookQA, demonstrating its broad applicability beyond computer vision. Furthermore, this method addresses a fundamental bottleneck in deploying sophisticated artificial intelligence systems in bandwidth-constrained environments without compromising model performance.

</details>


### [398] [OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents](https://arxiv.org/abs/2511.11672)
*Zengyi Qin,Jinyuan Chen,Yunze Man,Shengcao Cao,Ziqi Pang,Zhuoyuan Wang,Xin Sun,Gen Lin,Han Fang,Ling Zhu,Zixin Xie,Zibu Wei,Tianshu Ran,Haoran Geng,Xander Wu,Zachary Bright,Qizhen Sun,Rui Wang,Yuyang Cai,Song Wang,Jiace Zhao,Han Cao,Yeyang Zhou,Tianrui Liu,Ray Pan,Chongye Yang,Xiang Ren,Bo Zhang,Yutong Ban,Jitendra Malik,Brian Anthony,Pieter Abbeel*

Main category: cs.DC

TL;DR: OSGym是一个超可扩展的分布式数据引擎，用于在多样化计算机相关任务中训练智能体，能在学术可承受成本下扩展到上千个操作系统副本，支持工具使用、浏览器交互、软件工程等任务。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限环境下运行多个操作系统副本的高资源需求问题，为智能体训练提供动态运行环境，推动智能体研究的可扩展性和通用性发展。

Method: 采用分布式架构并行化上千个操作系统实例，支持多种模型训练算法，包括监督微调和强化学习，提供灵活的任务定制能力。

Result: 每分钟生成1420个多轮轨迹，每个OS副本每日成本仅0.2-0.3美元，训练出的模型性能优于现有最先进基线。

Conclusion: OSGym具有高可扩展性、通用性和经济可行性，能够有效推进未来智能体研究的规模和通用性发展，已开源供研究和商业使用。

Abstract: We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers three key advantages. (1) Scalability: Despite the intensive resource requirements of running multiple OS replicas, OSGym parallelizes over a thousand instances while maintaining operational efficiency under constrained resources, generating up to 1420 multi-turn trajectories per minute. (2) Generality and Customizability: OSGym supports a broad spectrum of tasks that run on OS platforms, including tool use, browser interactions, software engineering, and office applications, with flexible support for diverse model training algorithms. (3) Economic Viability: OSGym operates at only 0.2-0.3 USD per day per OS replica using accessible on-demand compute providers. It is fully open-source and freely available for both research and commercial use. Experiments show that OSGym enables comprehensive data collection, supervised fine-tuning, and reinforcement learning pipelines for computer agents. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance scalability and universality in future agent research.

</details>


### [399] [A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems](https://arxiv.org/abs/2511.11678)
*Yuze Liu,Yunhan Wang,Tiehua Zhang,Zhishu Shen,Cheng Peng,Libing Wu,Feng Xia,Jiong Jin*

Main category: cs.DC

TL;DR: Co-PLMs是一个新颖的协同调优框架，通过结构无关的相互学习实现异构语言模型之间的知识交换，使用蒸馏代理模型作为桥梁，在保护设备特定领域知识的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的智能应用激增，带宽有限的云服务器难以实时处理大量LLM工作负载且不损害用户数据隐私。云边联盟整合服务器LLM和移动边缘设备上的SLM成为解决方案，但SLM的跨域部署和架构异构性给性能提升带来挑战。

Method: 提出Co-PLMs框架，采用结构无关的相互学习方法，使用蒸馏代理模型作为异构服务器LLM和边缘设备SLM之间协同训练的桥梁，同时保留每个设备的领域特定洞察。

Result: 实验结果显示Co-PLMs优于最先进方法，在Rouge-L和EM指标上分别平均提升5.38%和4.88%。

Conclusion: Co-PLMs框架有效解决了异构语言模型协同训练中的挑战，通过蒸馏代理模型实现知识交换，显著提升了推理性能，为云边联盟中的模型协作提供了可行方案。

Abstract: The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.

</details>


### [400] [ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation](https://arxiv.org/abs/2511.11719)
*Mohammad Mahdi Kamani,Zhongwei Cheng,Lin Chen*

Main category: cs.DC

TL;DR: 提出Eccentric框架，通过边缘模型到云端模型的知识适应，在推理过程中降低计算和通信成本，同时实现最佳性能


<details>
  <summary>Details</summary>
Motivation: 边缘AI应用中，由于边缘设备计算资源有限，依赖云端系统不可避免。随着边缘设备数量增加，云端推理系统的计算和通信成本急剧上升，需要在计算、通信和性能之间进行权衡

Method: 基于边缘模型到云端模型的知识适应，学习具有不同权衡水平的模型，减少推理过程中的计算和通信成本

Result: 在分类和目标检测任务上的实证研究证实了该框架的有效性

Conclusion: Eccentric框架可视为一种新型压缩方法，适用于边缘-云端推理系统，能同时降低计算和通信成本

Abstract: The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework.

</details>


### [401] [A Meta-Heuristic Load Balancer for Cloud Computing Systems](https://arxiv.org/abs/2511.11721)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 提出一种云系统服务分配策略，避免节点过载并保持系统稳定，同时最小化成本。


<details>
  <summary>Details</summary>
Motivation: 解决云系统中服务分配时的节点过载问题，在保证系统稳定性的前提下降低运营成本。

Method: 建立云资源利用的抽象模型，考虑多种资源类型和服务迁移成本；开发原型元启发式负载均衡器；提出新颖的遗传算法，用其他元启发式算法的输出作为种群种子。

Result: 展示了原型负载均衡器，并呈现和讨论了实验结果。

Conclusion: 提出的策略能够有效分配云服务，避免节点过载，维持系统稳定性，同时实现成本最小化。

Abstract: This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.

</details>


### [402] [Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks](https://arxiv.org/abs/2511.11729)
*Ao Xu,Han Zhao,Weihao Cui,Quan Chen,Yukang Chen,Shulai Zhang,Shuang Chen,Jiemin Jiang,Zhibin Yu,Minyi Guo*

Main category: cs.DC

TL;DR: Harli是一个LLM服务系统，通过在解码实例上共同部署参数高效微调任务来提高GPU利用率，同时保证推理服务质量


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统中解码实例GPU利用率低，因为内存限制和动态工作负载中批处理不足，导致计算资源未充分利用

Method: 使用统一内存分配器实现运行时内存复用，两阶段延迟预测器建模解码延迟，以及QoS保证的吞吐量最大化调度器

Result: 相比最先进的服务系统，Harli平均提高微调吞吐量46.2%（最高92.0%），同时保持推理解码的严格QoS保证

Conclusion: 通过将计算密集的PEFT任务与内存受限的LLM解码实例共同部署，可以显著提高GPU利用率而不影响推理服务质量

Abstract: Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.
  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.

</details>


### [403] [Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput](https://arxiv.org/abs/2511.11733)
*Jingwei Song,Wanyi Chen,Xinyuan Song,Max,Chris Tong,Gufeng Chen,Tianyi Zhao,Eric Yang,Bill Shi,Lynn Ai*

Main category: cs.DC

TL;DR: 提出了去中心化推测解码（DSD）框架，通过并行验证多个候选标记将网络延迟转化为有用计算，在去中心化环境中加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 推测解码在集中式系统中有效，但在网络延迟占主导的去中心化环境中表现未充分研究。

Method: 使用轻量级草稿模型提出标记，在分布式节点上并行验证多个候选标记，并引入自适应推测验证策略根据标记语义重要性调整接受阈值。

Result: 理论减少约(N-1)t1(k-1)/k的跨节点通信成本，实践中在HumanEval和GSM8K上分别实现2.56倍和2.59倍加速，超越Eagle3基准同时保持准确性。

Conclusion: DSD将网络停滞转化为吞吐量，无需模型重训练或架构更改即可实现更快的分布式LLM推理。

Abstract: Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes.

</details>


### [404] [Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows](https://arxiv.org/abs/2511.11739)
*Christina Schenk,Miguel Hernández-del-Valle,Luis Calero-Lumbreras,Marcus Noack,Maciej Haranczyk*

Main category: cs.DC

TL;DR: 提出了一种噪声感知决策算法，通过量化设备特定噪声特征来管理变异性，使用分布分析和聚类选择单设备或多设备贝叶斯优化策略，提高可重复性和效率。


<details>
  <summary>Details</summary>
Motivation: 实验噪声的设备间变异性严重影响可重复性，特别是在自动化高通量系统中，这种变异性在更大规模（如建筑3D打印）中可能引发结构或经济故障。

Method: 使用分布分析和成对散度度量与聚类，选择单设备或鲁棒多设备贝叶斯优化策略，明确利用设备间差异来增强性能。

Result: 在三个名义相同的3D打印机上的实验案例显示减少了冗余、降低了资源使用并提高了可靠性。

Conclusion: 该框架为可扩展自动化实验平台建立了精确和资源感知优化的新范式。

Abstract: Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.

</details>


### [405] [How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems](https://arxiv.org/abs/2511.11749)
*Almond Kiruthu Murimi*

Main category: cs.DC

TL;DR: 该研究探讨了机器学习驱动的数据复制策略如何提升大规模分布式系统的容错能力，通过预测分析和强化学习实现自适应复制机制。


<details>
  <summary>Details</summary>
Motivation: 传统静态配置的复制方法难以适应动态工作负载和意外故障，导致资源利用效率低下和停机时间延长。

Method: 整合机器学习技术（预测分析和强化学习），提出能够预测系统故障并实时优化数据放置的自适应复制机制，通过文献综述、定性分析和与传统方法的比较评估进行研究。

Result: 识别出现有复制策略的关键局限性，凸显了机器学习在创建更具弹性、自优化系统方面的变革潜力。

Conclusion: 研究结果强调了在现实环境中实施机器学习驱动解决方案的前景和挑战，为云基础和企​​业系统的未来研究和实际部署提供了建议。

Abstract: This research paper investigates how machine learning-driven data replication strategies can enhance fault tolerance in large-scale distributed systems. Traditional replication methods, which rely on static configurations, often struggle to adapt to dynamic workloads and unexpected failures, leading to inefficient resource utilization and prolonged downtime. By integrating machine learning techniques-specifically predictive analytics and reinforcement learning. The study proposes adaptive replication mechanisms capable of forecasting system failures and optimizing data placement in real time. Through an extensive literature review, qualitative analysis, and comparative evaluations with traditional approaches, the paper identifies key limitations in existing replication strategies and highlights the transformative potential of machine learning in creating more resilient, self-optimizing systems. The findings underscore both the promise and the challenges of implementing ML-driven solutions in real-world environments, offering recommendations for future research and practical deployment in cloud-based and enterprise systems.

</details>


### [406] [TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing](https://arxiv.org/abs/2511.11843)
*Yiwei Zhao,Qiushi Lin,Hongbo Kang,Guy E. Blelloch,Laxman Dhulipala,Charles McGuffey,Phillip B. Gibbons*

Main category: cs.DC

TL;DR: 提出了TD-Orch任务数据编排框架，通过分布式推送-拉取技术实现任务与数据的双向流动，有效处理数据热点问题，在分布式图处理和键值存储应用中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决分布式应用中任务与数据分布不匹配的问题，特别是在数据热点场景下，传统调度方法难以实现高效的任务数据协同定位和执行。

Method: 采用分布式推送-拉取技术，支持任务和数据的双向流动，实现可扩展的负载均衡；构建了TDO-GP图处理系统作为应用实例。

Result: TD-Orch相比现有分布式调度基线实现最高2.7倍加速；TDO-GP相比现有开源分布式图系统平均实现4.1倍加速。

Conclusion: TD-Orch框架通过高效的任务数据编排机制，为分布式应用提供了可扩展的解决方案，特别适用于处理数据热点问题。

Abstract: In this paper, we highlight a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must get co-located with its target data (by moving tasks and/or data) and executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional f low of both tasks and data to achieve scalable load balance across machines even under highly skewed data request (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.7x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.

</details>


### [407] [Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs](https://arxiv.org/abs/2511.11885)
*Kausar Patherya,Ashutosh Dhekne,Francisco Romero*

Main category: cs.DC

TL;DR: Flash-Fusion是一个端到端的边缘-云系统，通过边缘统计摘要和云端查询规划，显著降低IoT数据收集和分析负担，实现95%延迟减少和98%令牌使用减少。


<details>
  <summary>Details</summary>
Motivation: 用户在使用LLM分析IoT数据时面临两大挑战：昂贵的数据收集基础设施产生海量低级别传感器数据，以及缓慢的数据分析需要迭代努力和技术专长。直接向LLM提供所有IoT遥测数据不切实际。

Method: 采用两个设计原则：(1) 基于边缘的统计摘要实现73.5%的数据减少；(2) 基于云端的查询规划，聚类行为数据并组装上下文丰富的提示。

Result: 在大学巴士车队部署评估中，相比向最先进LLM提供原始数据的基线，Flash-Fusion实现95%延迟减少和98%令牌使用及成本减少，同时保持高质量响应。

Conclusion: Flash-Fusion使跨学科用户（安全官员、城市规划师、车队经理、数据科学家）能够高效迭代IoT数据，无需手动查询编写或预处理负担。

Abstract: Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.
  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.

</details>


### [408] [KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference](https://arxiv.org/abs/2511.11907)
*Huawei Zhang,Chunwei Xia,Zheng Wang*

Main category: cs.DC

TL;DR: KVSwap是一个软件框架，通过将KV缓存卸载到磁盘来解决长上下文推理中的内存容量瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 移动和嵌入式AI应用需要处理长上下文输入，但在设备上本地运行语言模型时，KV缓存随上下文长度和批次大小线性增长，导致内存容量瓶颈。

Method: KVSwap将完整KV缓存存储在磁盘上，使用紧凑的内存元数据预测需要预加载的条目，重叠计算与硬件感知的磁盘访问，并根据存储设备特性编排读取模式。

Result: 评估显示，在各种代表性语言模型和存储类型下，KVSwap在严格内存预算下提供更高吞吐量，同时保持生成质量。

Conclusion: KVSwap能够有效突破长上下文推理的内存容量限制，为设备端AI应用提供可行的解决方案。

Abstract: Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.
  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.

</details>


### [409] [High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts](https://arxiv.org/abs/2511.12009)
*Guangchao Yao,Yali Li*

Main category: cs.DC

TL;DR: 提出了一种创新的GPU并行计算方法，在8张RTX 5090 GPU上仅用28.4天验证了27皇后问题的解，确认了PreuBer团队的结果，并将28皇后问题的预计求解时间缩短至约11个月。


<details>
  <summary>Details</summary>
Motivation: N皇后问题计数具有极高的计算复杂度，目前学术界仅严格验证到N<=26。27皇后问题由PreuBer团队用FPGA耗时约一年解决但结果未独立验证，现有GPU方法验证27皇后仍需约17个月，时间和计算资源成本过高。

Method: 在NVIDIA GPU平台上提出创新并行计算方法：迭代深度优先搜索算法、将所需栈结构完全映射到GPU共享内存、通过精心设计的内存访问模式有效避免bank冲突、采用各种优化技术实现最佳性能。

Result: 在8张RTX 5090 GPU上仅用28.4天成功验证27皇后问题，确认PreuBer计算结果正确；将28皇后问题预计求解时间缩短至约11个月；相比最先进GPU方法，在相同硬件配置(8 A100)上实现10倍以上加速，使用8 RTX 5090 GPU时实现26倍以上加速。

Conclusion: 该方法为解决这一长期停滞的问题带来了新的视角，显著降低了N皇后问题验证的时间和计算资源成本，使得28皇后问题的解决在计算上变得可行。

Abstract: The counting of solutions to the N-Queens problem is a classic NP-complete problem with extremely high computational complexity. As of now, the academic community has rigorously verified the number of solutions only up to N <= 26. In 2016, the research team led by PreuBer solved the 27-Queens problem using FPGA hardware, which took approximately one year, though the result remains unverified independently. Recent studies on GPU parallel computing suggest that verifying the 27-Queens solution would still require about 17 months, indicating excessively high time and computational resource costs. To address this challenge, we propose an innovative parallel computing method on NVIDIA GPU platform, with the following core contributions: (1) An iterative depth-first search (DFS) algorithm for solving the N-Queens problem; (2) Complete mapping of the required stack structure to GPU shared memory; (3) Effective avoidance of bank conflicts through meticulously designed memory access patterns; (4) Various optimization techniques are employed to achieve optimal performance. Under the proposed optimization framework, we successfully verified the 27-Queens problem in just 28.4 days using eight RTX 5090 GPUs, thereby confirming the correctness of PreuBer's computational results. Moreover, we have reduced the projected solving time for the next open case-the 28-Queens problem-to approximately 11 months, making its resolution computationally feasible. Compared to the state-of-the-art GPU methods, our method achieves over 10x speedup on identical hardware configurations (8 A100), while delivering over 26x acceleration when utilizing 8 RTX 5090 GPUs, and brings fresh perspectives to this long-stagnant problem.

</details>


### [410] [A Quick and Exact Method for Distributed Quantile Computation](https://arxiv.org/abs/2511.12025)
*Ivan Cao,Jaromir J. Saloni,David A. G. Harrison*

Main category: cs.DC

TL;DR: GK Select是一种在Spark中计算精确分位数的算法，它避免了全数据洗牌，在恒定次数的操作中完成，性能比Spark的完全排序快约10.5倍。


<details>
  <summary>Details</summary>
Motivation: Spark中计算分位数时，通常使用近似方法GK Sketch，但当需要精确分位数时，默认选项是昂贵的全局排序。

Method: 利用GK Sketch识别接近目标的枢轴，在每个分区中线性时间内提取误差范围内的所有值，然后对候选集进行树形归约。

Result: 分析表明GK Select与GK Sketch的执行器端时间复杂度相同但返回精确分位数。在30核AWS EMR集群上，对10^9个值、120个分区，GK Select实现了近似GK Sketch的延迟，性能比Spark完全排序快约10.5倍。

Conclusion: GK Select提供了一种高效的精确分位数计算方法，在保持近似算法性能的同时提供精确结果。

Abstract: Quantile computation is a core primitive in large-scale data analytics. In Spark, practitioners typically rely on the Greenwald-Khanna (GK) Sketch, an approximate method. When exact quantiles are required, the default option is an expensive global sort. We present GK Select, an exact Spark algorithm that avoids full-data shuffles and completes in a constant number of actions. GK Select leverages GK Sketch to identify a near-target pivot, extracts all values within the error bound around this pivot in each partition in linear time, and then tree-reduces the resulting candidate sets. We show analytically that GK Select matches the executor-side time complexity of GK Sketch while returning the exact quantile. Empirically, GK Select achieves sketch-level latency and outperforms Spark's full sort by approximately 10.5x on 10^9 values across 120 partitions on a 30-core AWS EMR cluster.

</details>


### [411] [Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding](https://arxiv.org/abs/2511.12031)
*Arun Ramachandran,Ramaswamy Govindarajan,Murali Annavaram,Prakash Raghavendra,Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang*

Main category: cs.DC

TL;DR: 提出BMC方法优化LLM推理中的KV缓存管理，通过定期分配冗余行实现原地更新，减少内存分配和复制开销，同时利用冗余计算进行推测解码，在CPU和GPU上均获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着GPU成本飙升，需要在CPU上进行LLM推理。传统KV缓存更新涉及分配、复制和原地更新，随着序列长度增加，分配和复制开销成为性能瓶颈。预分配大KV张量会导致冗余计算。

Method: 提出BMC方法：每r次迭代分配带r个冗余行的KV张量，支持原地更新避免复制开销；利用冗余行进行推测解码；建立分析模型选择最优r值。

Result: 在CPU上比HuggingFace基线提升3.2倍吞吐量；结合推测解码额外提升1.39倍；比vLLM和DeepSpeed分别提升1.36倍和2.29倍；在GPU上也表现良好。

Conclusion: BMC通过平衡内存和计算开销，有效优化LLM推理性能，在CPU和GPU上均能显著提升吞吐量。

Abstract: With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.

</details>


### [412] [Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications](https://arxiv.org/abs/2511.12185)
*Mills Staylor,Arup Kumar Sarker,Gregor von Laszewski,Geoffrey Fox,Yue Cheng,Judy Fox*

Main category: cs.DC

TL;DR: Cylon是一个高性能分布式数据框架，通过设计无服务器通信器解决AWS Lambda等无服务器函数在处理大数据集时的通信和性能问题，性能比传统服务器(EC2)和HPC低1%以下。


<details>
  <summary>Details</summary>
Motivation: 传统数据工程、机器学习和AI工作负载需要在数据中心的大型集群上运行，硬件和维护成本高。虽然云服务提供了无服务器函数如AWS Lambda，但在处理大数据集时依赖外部存储导致性能显著低于HPC集群的直接通信。

Method: 借鉴FMI库设计无服务器通信器，通过NAT Traversal TCP Hole Punching实现直接通信，解决无服务器函数的通信和性能问题。

Result: Cylon在AWS Lambda上的性能比服务器式AWS(EC2)和HPC低1%以下，在强扩展实验中表现不佳。

Conclusion: 无服务器函数在处理大数据集时存在显著的通信性能瓶颈，需要专门的通信机制来提升性能。

Abstract: Data is found everywhere, from health and human infrastructure to the surge of sensors and the proliferation of internet-connected devices. To meet this challenge, the data engineering field has expanded significantly in recent years in both research and industry. Traditionally, data engineering, Machine Learning, and AI workloads have been run on large clusters within data center environments, requiring substantial investment in hardware and maintenance. With the rise of the public cloud, it is now possible to run large applications across nodes without owning or maintaining hardware. Serverless functions such as AWS Lambda provide horizontal scaling and precise billing without the hassle of managing traditional cloud infrastructure. However, when processing large datasets, users often rely on external storage options that are significantly slower than direct communication typical of HPC clusters. We introduce Cylon, a high-performance distributed data frame solution that has shown promising results for data processing using Python. We describe how we took inspiration from the FMI library and designed a serverless communicator to tackle communication and performance issues associated with serverless functions. With our design, we demonstrate that the performance of AWS Lambda falls below one percent of strong scaling experiments compared to serverful AWS (EC2) and HPCs based on implementing direct communication via NAT Traversal TCP Hole Punching.

</details>


### [413] [Distributed Seasonal Temporal Pattern Mining](https://arxiv.org/abs/2511.12216)
*Van Ho-Long,Nguyen Ho,Anh-Vu Dinh-Duc,Ha Manh Tran,Ky Trung Nguyen,Tran Dung Pham,Quoc Viet Hung Nguyen*

Main category: cs.DC

TL;DR: 提出了第一个分布式季节性时序模式挖掘框架DSTPM，通过分布式层次查找哈希结构高效处理大规模时序数据，显著优于传统串行方法。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器产生的大量时序数据中存在重要的周期性模式（季节性时序模式），但传统挖掘方法无法捕捉季节性且缺乏反单调性，导致搜索空间指数级增长，现有串行方法难以扩展到大规模数据集。

Method: 开发分布式季节性时序模式挖掘框架DSTPM，利用分布式层次查找哈希结构等高效数据结构实现并行计算。

Result: 大量实验评估表明，DSTPM在运行时间和内存使用方面显著优于串行基线方法，并能有效扩展到非常大的数据集。

Conclusion: DSTPM是第一个用于从时序数据中挖掘季节性时序模式的分布式框架，解决了传统方法在大规模数据上的可扩展性问题。

Abstract: The explosive growth of IoT-enabled sensors is producing enormous amounts of time series data across many domains, offering valuable opportunities to extract insights through temporal pattern mining. Among these patterns, an important class exhibits periodic occurrences, referred to as \textit{seasonal temporal patterns} (STPs). However, mining STPs poses challenges, as traditional measures such as support and confidence cannot capture seasonality, and the lack of the anti-monotonicity property results in an exponentially large search space. Existing STP mining methods operate sequentially and therefore do not scale to large datasets. In this paper, we propose the Distributed Seasonal Temporal Pattern Mining (DSTPM), the first distributed framework for mining seasonal temporal patterns from time series. DSTPM leverages efficient data structures, specifically distributed hierarchical lookup hash structures, to enable efficient computation. Extensive experimental evaluations demonstrate that DSTPM significantly outperforms sequential baselines in runtime and memory usage, while scaling effectively to very large datasets.

</details>


### [414] [Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA](https://arxiv.org/abs/2511.12461)
*Fangqiang Du,Sixuan Chong,Zixuan Huang,Rui Qin,Fengnan Mi,Caibao Hu,Jiangang Chen*

Main category: cs.DC

TL;DR: 提出了一种基于数据流的SVD处理算法(DSB Jacobi)，显著减少片上BRAM使用并提高计算速度，为大规模数据流的实时SVD计算提供实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着矩阵维度快速增长，SVD计算成本显著增加，对数据分析和信号处理系统的效率构成严重挑战。现有硬件架构存在可扩展性有限、片上内存资源消耗高的问题，且通常忽视SVD相关的计算和数据传输挑战，使其不适用于嵌入式系统中大规模数据流矩阵的实时处理。

Method: 提出数据流基础的SVD处理算法(DSB Jacobi)，通过优化算法设计减少片上BRAM使用并提高计算效率。

Result: 与先前工作相比，实验结果表明所提方法将片上RAM消耗减少41.5%，计算效率提高23倍。

Conclusion: DSB Jacobi算法为大规模数据流的实时SVD计算提供了高效的解决方案，在资源消耗和计算速度方面均有显著改进。

Abstract: Singular value decomposition (SVD) is widely used for dimensionality reduction and noise suppression, and it plays a pivotal role in numerous scientific and engineering applications. As the dimensions of the matrix grow rapidly, the computational cost increases significantly, posing a serious challenge to the efficiency of data analysis and signal processing systems,especially in time-sensitive scenarios with large-scale datasets. Although various dedicated hardware architectures have been proposed to accelerate the computation of intensive SVD, many of these designs suffer from limited scalability and high consumption of on-chip memory resources. Moreover, they typically overlook the computational and data transfer challenges associated with SVD, enabling them unsuitable for real-time processing of large-scale data stream matrices in embedded systems. In this express, we propose a Data Stream-Based SVD processing algorithm (DSB Jacobi), which significantly reduces on-chip BRAM usage while improving computational speed, offering a practical solution for real-time SVD computation of large-scale data streams. Compared with previous works, our experimental results indicate that the proposed method reduces on-chip RAM consumption by 41.5 percent and improves computational efficiency by 23 times.

</details>


### [415] [A Decentralized Root Cause Localization Approach for Edge Computing Environments](https://arxiv.org/abs/2511.12486)
*Duneesha Fernando,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出了一种去中心化的根因定位方法，在边缘设备层面使用个性化PageRank算法进行异常定位，通过服务分组和集群内本地化分析，显著减少定位时间。


<details>
  <summary>Details</summary>
Motivation: 现有根因定位方法为云环境设计，依赖集中式分析，在边缘环境中会增加延迟和通信开销。边缘计算环境中的微服务应用容易出现性能异常，需要高效的定位方法。

Method: 将微服务分组为通信和共置感知的集群，在集群内使用PPR算法进行本地化根因定位。对于跨集群异常传播，引入集群间点对点近似处理。还提出了针对异构边缘环境的新型异常评分机制。

Result: 在公开边缘数据集MicroCERCL上的评估显示，该去中心化方法达到与集中式方法相当或更高的定位准确率，同时将定位时间减少高达34%。

Conclusion: 去中心化的基于图的根因定位可以为资源受限的边缘环境提供实用高效的异常诊断解决方案。

Abstract: Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.

</details>


### [416] [Iris: First-Class Multi-GPU Programming Experience in Triton](https://arxiv.org/abs/2511.12500)
*Muhammad Awad,Muhammad Osama,Brandon Potter*

Main category: cs.DC

TL;DR: Iris是一个完全用Python和Triton实现的多GPU通信库，通过tile-based对称内存抽象消除了性能与可编程性之间的权衡，使开发者能够用少量代码实现计算-通信重叠模式。


<details>
  <summary>Details</summary>
Motivation: 传统多GPU编程需要在性能和可编程性之间做出复杂权衡：高性能实现需要复杂的HIP/CUDA通信库，而简单抽象往往牺牲性能。

Method: Iris提供与Triton编程模型自然对齐的tile-based对称内存抽象，支持开发者编写单源内核，无缝交织计算和通信，实现从批量同步到细粒度工作组专业化的各种重叠模式。

Result: 评估显示Iris在微基准测试中实现接近最优的带宽利用率，在GEMM+All-Scatter工作负载上比PyTorch和RCCL快1.79倍。

Conclusion: 高级实现可以匹配甚至超过高度优化的库，同时显著简化多GPU编程。

Abstract: Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.

</details>


### [417] [Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2511.12667)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel*

Main category: cs.DC

TL;DR: 开发了一个基于Kubernetes的工具，支持非侵入式、延迟应用设计模式，无需修改服务代码，同时收集能源指标以支持能源感知决策。


<details>
  <summary>Details</summary>
Motivation: 随着数据网格架构的发展，组织越来越多地构建消费者特定的数据共享管道，使用模块化、基于云的转换服务。虽然可重用的转换服务可以提高成本和能源效率，但应用传统的云设计模式会降低服务在不同管道中的可重用性。

Method: 提出了一个基于Kubernetes的工具，该工具能够非侵入式地延迟应用设计模式，无需修改服务代码。该工具自动化模式注入并收集能源指标。

Result: 该工具支持能源感知决策，同时保持转换服务在各种管道结构中的可重用性。

Conclusion: 通过非侵入式的方法应用设计模式，可以在不牺牲可重用性的情况下实现能源效率的提升。

Abstract: As data mesh architectures grow, organizations increasingly build consumer-specific data-sharing pipelines from modular, cloud-based transformation services. While reusable transformation services can improve cost and energy efficiency, applying traditional cloud design patterns can reduce reusability of services in different pipelines. We present a Kubernetes-based tool that enables non-intrusive, deferred application of design patterns without modifying services code. The tool automates pattern injection and collects energy metrics, supporting energy-aware decisions while preserving reusability of transformation services in various pipeline structures.

</details>


### [418] [The Time to Consensus in a Blockchain: Insights into Bitcoin's "6 Blocks Rule''](https://arxiv.org/abs/2511.12687)
*Partha S. Dey,Aditya S. Gopalan,Vijay G. Subramanian*

Main category: cs.DC

TL;DR: 研究Nakamoto区块链中达成共识的时间，通过排队论方法分析诚实和恶意两种竞争增长过程，计算诚实过程永久超越恶意过程所需的时间。


<details>
  <summary>Details</summary>
Motivation: 理解Nakamoto区块链中达成共识的时间特性，特别是在存在随机延迟的诚实增长过程和恶意竞争过程的情况下。

Method: 使用排队论技术，考虑诚实增长过程受随机延迟影响，在简化的比特币模型中计算共识时间的拉普拉斯变换，并通过仿真验证。

Result: 获得了共识时间的拉普拉斯变换表达式，并通过仿真验证了理论结果的正确性。

Conclusion: 成功量化了Nakamoto区块链中达成共识的时间特性，为理解区块链安全性提供了理论分析框架。

Abstract: We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \emph{honest} and \emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.

</details>


### [419] [Learning Process Energy Profiles from Node-Level Power Data](https://arxiv.org/abs/2511.13155)
*Jonathan Bader,Julius Irion,Jannis Kappel,Joel Witzke,Niklas Fomin,Diellza Sherifi,Odej Kao*

Main category: cs.DC

TL;DR: 提出了一种基于eBPF和perf收集细粒度进程级资源指标，结合PDU获取的节点级能耗数据，通过回归模型学习进程资源使用与节点能耗关系，实现更细粒度的进程能耗预测方法。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗因高性能计算、云计算和AI需求激增而急剧上升，需要进程级能耗洞察来提升能效。现有方法如Intel RAPL受限于特定硬件且只能提供粗粒度的域级测量。

Method: 利用eBPF和perf收集细粒度进程级资源指标，与PDU获取的节点级能耗数据同步，通过回归模型统计学习进程资源使用与节点能耗的关系。

Result: 实现了更细粒度的进程能耗预测能力。

Conclusion: 该方法能够提供比现有机制更精细的进程级能耗分析，有助于数据中心能效优化。

Abstract: The growing demand for data center capacity, driven by the growth of high-performance computing, cloud computing, and especially artificial intelligence, has led to a sharp increase in data center energy consumption. To improve energy efficiency, gaining process-level insights into energy consumption is essential. While node-level energy consumption data can be directly measured with hardware such as power meters, existing mechanisms for estimating per-process energy usage, such as Intel RAPL, are limited to specific hardware and provide only coarse-grained, domain-level measurements. Our proposed approach models per-process energy profiles by leveraging fine-grained process-level resource metrics collected via eBPF and perf, which are synchronized with node-level energy measurements obtained from an attached power distribution unit. By statistically learning the relationship between process-level resource usage and node-level energy consumption through a regression-based model, our approach enables more fine-grained per-process energy predictions.

</details>


### [420] [Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices](https://arxiv.org/abs/2511.13253)
*Mordechai Guri*

Main category: cs.DC

TL;DR: Pico-Cloud是一种基于树莓派Zero等超小型硬件的微边缘云架构，提供容器虚拟化、服务发现和轻量级编排，支持低延迟、低功耗的本地操作。


<details>
  <summary>Details</summary>
Motivation: 解决传统集中式数据中心在边缘计算场景中的高延迟、高功耗问题，为轻量级分布式工作负载提供成本效益高、去中心化和可持续的平台。

Method: 基于树莓派Zero等单板计算机构建微边缘云架构，实现容器化虚拟化、服务发现和轻量级编排，直接在设备层运行。

Result: Pico-Cloud能够支持农村连接、教育集群和边缘AI推理等代表性用例，在计算、网络、存储和电源管理方面具有良好表现。

Conclusion: Pico-Cloud是一个成本效益高、去中心化且可持续的平台，适用于网络边缘的轻量级分布式工作负载。

Abstract: This paper introduces the Pico-Cloud, a micro-edge cloud architecture built on ultra-minimal hardware platforms such as the Raspberry Pi Zero and comparable single-board computers. The Pico-Cloud delivers container-based virtualization, service discovery, and lightweight orchestration directly at the device layer, enabling local operation with low latency and low power consumption without reliance on centralized data centers. We present its architectural model, outline representative use cases including rural connectivity, educational clusters, and edge AI inference, and analyze design challenges in computation, networking, storage, and power management. The results highlight Pico-Clouds as a cost-effective, decentralized, and sustainable platform for lightweight distributed workloads at the network edge.

</details>


### [421] [Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems](https://arxiv.org/abs/2511.13313)
*Sulaiman Muhammad Rashid,Ibrahim Aliyu,Jaehyung Park,Jinsul Kim*

Main category: cs.DC

TL;DR: 提出了一种基于DeepSets的分布式分层模型DeepSets-S，用于解决元宇宙边缘计算中的无线和计算资源联合管理问题，在保证置换等变性的同时实现了近最优的资源分配。


<details>
  <summary>Details</summary>
Motivation: 元宇宙需要低延迟和实时体验，但传统优化方法在动态边缘环境和高用户负载下效果不佳，需要新的资源管理方案。

Method: 构建了切片使能的网络内边缘架构，将问题分解为三个子问题，并训练基于DeepSets的分布式分层模型，设计了共享编码器和任务特定解码器的松弛感知归一化机制。

Result: DeepSets-S在子问题SP1/SP2上达到95.26%和95.67%的容错精度，在SP3上多类卸载精度为0.7486，相比精确求解器减少86.1%执行时间，系统成本与最优解差距在6.1%以内。

Conclusion: DeepSets-S能够有效处理可变大小设备集的资源分配问题，在保持置换等变性的同时实现近最优性能，显著提升了边缘计算资源利用率。

Abstract: The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-enabled in-network edge architecture that combines computing-in-the-network (COIN) with multi-access edge computing (MEC). In addition, we formulate the joint problem of wireless and computing resource management with optimal slice selection as a mixed-integer nonlinear program (MINLP). Because solving this model online is computationally intensive, we decompose it into three sub-problems (SP1) intra-slice allocation, (SP2) inter-slice allocation, and (SP3) offloading decision and train a distributed hierarchical DeepSets-based model (DeepSets-S) on optimal solutions obtained offline. In the proposed model, we design a slack-aware normalization mechanism for a shared encoder and task-specific decoders, ensuring permutation equivariance over variable-size wireless device (WD) sets. The learned system produces near-optimal allocations with low inference time and maintains permutation equivariance over variable-size device sets. Our experimental results show that DeepSets-S attains high tolerance-based accuracies on SP1/SP2 (Acc1 = 95.26% and 95.67%) and improves multiclass offloading accuracy on SP3 (Acc = 0.7486; binary local/offload Acc = 0.8824). Compared to exact solvers, the proposed approach reduces the execution time by 86.1%, while closely tracking the optimal system cost (within 6.1% in representative regimes). Compared with baseline models, DeepSets-S consistently achieves higher cost ratios and better utilization across COIN/MEC resources.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [422] [Uncertainty-Guided Live Measurement Sequencing for Fast SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11895)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: 提出了一种基于扩展卡尔曼滤波的自适应闭环测试方法，用于高效测试高分辨率SAR ADC的线性度，显著减少测试时间和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有ADC测试方法（如直方图法、正弦波测试等）需要密集数据采集和离线后处理，导致测试时间长、复杂度高。

Method: 使用扩展卡尔曼滤波实时优化迭代行为模型，动态选择测量点以最大化信息增益，直接估计决定INL行为的电容失配参数。

Result: 实验结果表明该方法大幅减少了总测试时间和计算开销。

Conclusion: 该方法适合集成到生产环境中，通过自适应目标选择和即时反馈消除了大规模数据收集和后测量分析的需求。

Abstract: This paper introduces a novel closed-loop testing methodology for efficient linearity testing of high-resolution Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs). Existing test strategies, including histogram-based approaches, sine wave testing, and model-driven reconstruction, often rely on dense data acquisition followed by offline post-processing, which increases overall test time and complexity. To overcome these limitations, we propose an adaptive approach that utilizes an iterative behavioral model refined by an Extended Kalman Filter (EKF) in real time, enabling direct estimation of capacitor mismatch parameters that determine INL behavior. Our algorithm dynamically selects measurement points based on current model uncertainty, maximizing information gain with respect to parameter confidence and narrowing sampling intervals as estimation progresses. By providing immediate feedback and adaptive targeting, the proposed method eliminates the need for large-scale data collection and post-measurement analysis. Experimental results demonstrate substantial reductions in total test time and computational overhead, highlighting the method's suitability for integration in production environments.

</details>


### [423] [Advanced Strategies for Uncertainty-Guided Live Measurement Sequencing in Fast, Robust SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11917)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: 增强版UGLMS方法显著提升了SAR ADC线性度测试速度，通过秩1 EKF更新、协方差膨胀策略、载波多项式扩展和基于迹的终止条件，在保持精度的同时将16位ADC测试时间缩短至36ms，速度提升8倍。


<details>
  <summary>Details</summary>
Motivation: 传统SAR ADC测试需要全范围扫描和离线后处理，效率低下。UGLMS方法旨在实现实时、生产就绪的ADC线性度测试，消除全范围扫描需求。

Method: 1) 用秩1 EKF更新替代矩阵求逆；2) 测量对齐的协方差膨胀策略加速收敛；3) 扩展静态失配模型为低阶载波多项式；4) 基于迹的终止条件自适应测试长度。

Result: 16位ADC仅需36ms即可重建完整INL/DNL，18位ADC在70ms内完成（带多项式扩展为120ms）。16位ADC达到相同精度速度提升8倍。

Conclusion: 增强版UGLMS实现了实时、生产就绪的SAR ADC线性度测试，显著提升了测试效率。

Abstract: This paper builds on our Uncertainty-Guided Live Measurement Sequencing (UGLMS) method. UGLMS is a closed-loop test strategy that adaptively selects SAR ADC code edges based on model uncertainty and refines a behavioral mismatch model in real time via an Extended Kalman Filter (EKF), eliminating full-range sweeps and offline post-processing. We introduce an enhanced UGLMS that delivers significantly faster test runtimes while maintaining estimation accuracy. First, a rank-1 EKF update replaces costly matrix inversions with efficient vector operations, and a measurement-aligned covariance-inflation strategy accelerates convergence under unexpected innovations. Second, we extend the static mismatch model with a low-order carrier polynomial to capture systematic nonlinearities beyond pure capacitor mismatch. Third, a trace-based termination adapts test length to convergence, preventing premature stops and redundant iterations. Simulations show the enhanced UGLMS reconstructs full Integral- and Differential-Non-Linearity (INL/DNL) in just 36 ms for 16-bit and under 70 ms for 18-bit ADCs (120 ms with the polynomial extension). Combining the faster convergence from covariance inflation with reduced per-iteration runtime from the rank-1 EKF update, the method reaches equal accuracy 8x faster for 16-bit ADCs. These improvements enable real-time, production-ready SAR ADC linearity testing.

</details>


### [424] [TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space](https://arxiv.org/abs/2511.12035)
*Wenxuan Miao,Yulin Sun,Aiyue Chen,Jing Lin,Yiwu Yao,Yiming Gan,Jieru Zhao,Jingwen Leng,Mingyi Guo,Yu Feng*

Main category: cs.AR

TL;DR: 提出一种基于时空相关性的轻量级自适应重用策略，通过重用空间或时间相关token的部分注意力分数来加速视频扩散变换器中的自注意力计算。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型主要基于视频扩散变换器(vDiT)，但由于自注意力机制导致推理延迟严重。先前研究主要关注减少自注意力中的冗余计算，但忽视了视频流中固有的时空相关性。

Method: 利用潜在空间中的时空相关性，提出轻量级自适应重用策略，通过重用空间或时间相关token的部分注意力分数来近似注意力计算。该方法基于vDiT中注意力模式主要由token通道级别的空间和时间相关性主导的观察。

Result: 在4个vDiT模型上实现了显著更高的计算节省(85%)，同时几乎保持相同的视频质量(VBench上损失<0.06%)。

Conclusion: 该方法通过有效利用视频中的时空相关性，在保持视频质量的同时大幅加速了vDiT的自注意力计算，优于现有最先进技术。

Abstract: The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.
  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\% loss on VBench).

</details>


### [425] [A digital SRAM-based compute-in-memory macro for weight-stationary dynamic matrix multiplication in Transformer attention score computation](https://arxiv.org/abs/2511.12152)
*Jianyi Yu,Yuxuan Wang,Xiang Fu,Fei Qiao,Ying Wang,Rui Yuan,Liyuan Liu,Cong Shi*

Main category: cs.AR

TL;DR: 提出了一种用于Transformer注意力计算的数字存内计算宏，通过重新设计注意力得分计算流程和使用位串行移位加法，在65nm工艺下实现了34.1 TOPS/W的能效和120.77 GOPS/mm²的面积效率。


<details>
  <summary>Details</summary>
Motivation: 存内计算技术可缓解AI处理器中计算与存储单元间大量数据移动导致的功耗和延迟瓶颈，但动态矩阵乘法不适合常见的权重固定存内计算范式。

Method: 基于组合QK权重矩阵重新设计注意力得分计算流程，将二项矩阵乘法分解为4组位串行移位和加法操作，采用零值位跳过、数据驱动字线激活、读写分离6T单元和位交替14T/28T加法器等优化技术。

Result: 在65nm工艺下实现0.35mm²面积，1.0V电源和100MHz时钟频率下达到42.27 GOPS峰值性能，功耗1.24mW，能效34.1 TOPS/W，面积效率120.77 GOPS/mm²。相比CPU和GPU分别提升25倍和13倍能效，相比其他Transformer存内计算设计至少提升7倍能效和2倍面积效率。

Conclusion: 该数字存内计算宏在能效和面积效率方面显著优于传统处理器和其他Transformer存内计算设计，展示了在边缘侧智能应用中的潜力。

Abstract: Compute-in-memory (CIM) techniques are widely employed in energy-efficient artificial intelligent (AI) processors. They alleviate power and latency bottlenecks caused by extensive data movements between compute and storage units. This work proposes a digital CIM macro to compute Transformer attention. To mitigate dynamic matrix multiplication that is unsuitable for the common weight-stationary CIM paradigm, we reformulate the attention score computation process based on a combined QK-weight matrix, so that inputs can be directly fed to CIM cells to obtain the score results. Moreover, the involved binomial matrix multiplication operation is decomposed into 4 groups of bit-serial shifting and additions, without costly physical multipliers in the CIM. We maximize the energy efficiency of the CIM circuit through zero-value bit-skipping, data-driven word line activation, read-write separate 6T cells and bit-alternating 14T/28T adders. The proposed CIM macro was implemented using a 65-nm process. It occupied only 0.35 mm2 area, and delivered a 42.27 GOPS peak performance with 1.24 mW power consumption at a 1.0 V power supply and a 100 MHz clock frequency, resulting in 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency. When compared to the CPU and GPU, our CIM macro is 25x and 13x more energy efficient on practical tasks, respectively. Compared with other Transformer-CIMs, our design exhibits at least 7x energy efficiency and at least 2x area efficiency improvements when scaled to the same technology node, showcasing its potential for edge-side intelligent applications.

</details>


### [426] [Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing](https://arxiv.org/abs/2511.12286)
*Khyati Kiyawat,Zhenxing Fan,Yasas Seneviratne,Morteza Baradaran,Akhil Shekar,Zihan Xia,Mingu Kang,Kevin Skadron*

Main category: cs.AR

TL;DR: Sangam是一个基于CXL连接的PIM-chiplet内存模块，可作为GPU的替代品或协同执行器，通过解耦逻辑和内存芯片来解决现有内存计算方案的局限性，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型变得越来越数据密集，推理过程特别是解码阶段受限于内存带宽，而现有的内存计算方案存在内存容量减少和处理能力有限的问题。

Method: 采用chiplet架构，将逻辑和DRAM芯片在异构技术节点制造并通过中介层连接，逻辑芯片维持对DRAM芯片的高带宽访问，集成脉动阵列和SRAM缓冲区来加速内存密集型GEMM操作。

Result: 相比H100 GPU，在LLaMA 2-7B、Mistral-7B和LLaMA 3-70B上分别实现了3.93、4.22、2.82倍的端到端查询延迟加速，10.3、9.5、6.36倍的解码吞吐量提升，以及数量级的能耗节省。

Conclusion: Sangam架构有效解决了现有PIM方案的局限性，为内存密集型LLM推理提供了高效的硬件加速方案。

Abstract: Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.

</details>


### [427] [Pushing the Memory Bandwidth Wall with CXL-enabled Idle I/O Bandwidth Harvesting](https://arxiv.org/abs/2511.12349)
*Divya Kiran Kadiyala,Alexandros Daglis*

Main category: cs.AR

TL;DR: SURGE是一种软件支持的架构技术，通过利用空闲的I/O带宽资源来提升内存带宽可用性，解决服务器CPU内存带宽受限问题。


<details>
  <summary>Details</summary>
Motivation: 随着服务器CPU核心数增加，内存系统面临带宽限制，而传统设计中内存和I/O带宽固定分配导致资源利用率低下。

Method: 利用CXL等灵活互连技术，在相同处理器接口上动态复用内存和I/O流量，实现带宽资源的灵活调配。

Result: SURGE增强架构可将内存密集型工作负载在带宽受限服务器上的性能提升高达1.3倍。

Conclusion: SURGE通过实现内存和I/O带宽的可互换性，有效提升了宝贵的片外带宽资源利用率。

Abstract: The continual increase of cores on server-grade CPUs raises demands on memory systems, which are constrained by limited off-chip pin and data transfer rate scalability. As a result, high-end processors typically feature lower memory bandwidth per core, at the detriment of memory-intensive workloads. We propose alleviating this challenge by improving the utility of the CPU's limited pins. In a typical CPU design process, the available pins are apportioned between memory and I/O traffic, each accounting for about half of the total off-chip bandwidth availability. Consequently, unless both memory and I/O are simultaneously highly utilized, such fragmentation leads to underutilization of the valuable off-chip bandwidth resources. An ideal architecture would offer I/O and memory bandwidth fungibility, allowing use of the aggregate off-chip bandwidth in the form required by each workload.
  In this work, we introduce SURGE, a software-supported architectural technique that boosts memory bandwidth availability by salvaging idle I/O bandwidth resources. SURGE leverages the capability of versatile interconnect technologies like CXL to dynamically multiplex memory and I/O traffic over the same processor interface. We demonstrate that SURGE-enhanced architectures can accelerate memory-intensive workloads on bandwidth-constrained servers by up to 1.3x.

</details>


### [428] [FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration](https://arxiv.org/abs/2511.12544)
*Mukul Lokhande,Akash Sankhe,S. V. Jaya Chand,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: FERMI-ML是一个用于TinyML加速的灵活资源高效内存原位SRAM宏，集成了9T XNOR位单元和22T压缩树累加器，支持可变精度MAC和CAM操作，在65nm工艺下实现1.93 TOPS吞吐量和364 TOPS/W能效。


<details>
  <summary>Details</summary>
Motivation: AIoT设备对低功耗和面积高效的TinyML推理需求日益增长，需要最小化数据移动同时保持高计算效率的内存架构。

Method: 提出FERMI-ML SRAM宏，采用9T XNOR位单元（5T存储+4T XNOR计算单元）和22T压缩树累加器，支持Posit-4或FP-4精度的原位计算和CAM查找操作。

Result: 65nm工艺后布局结果显示在0.9V电压下运行频率达350MHz，吞吐量1.93 TOPS，能效364 TOPS/W，在InceptionV4和ResNet-18上质量结果超过97.5%。

Conclusion: FERMI-ML展示了一个紧凑、可重构且能量感知的数字内存原位宏，能够支持混合精度的TinyML工作负载。

Abstract: The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads.

</details>


### [429] [SynapticCore-X: A Modular Neural Processing Architecture for Low-Cost FPGA Acceleration](https://arxiv.org/abs/2511.12616)
*Arya Parameshwara*

Main category: cs.AR

TL;DR: SynapticCore-X是一个面向低成本FPGA平台的模块化、资源高效的神经处理架构，集成了轻量级RISC-V控制核心和可配置的神经计算单元，提供完全开源的SystemVerilog微架构。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA加速器依赖重量级IP块，缺乏开源、可配置的神经处理架构，限制了学术和开源硬件研究在神经微架构领域的进入门槛。

Method: 设计集成了RV32IMC RISC-V控制核心和可配置神经计算单元，支持融合矩阵、激活和数据移动操作，提供可调并行性、暂存器深度和DMA突发行为。

Result: 在Zynq-7020上实现100MHz时序收敛，仅消耗6.1% LUTs、32.5% DSPs和21.4% BRAMs，硬件验证确认了寄存器级执行正确性和确定性控制路径行为。

Conclusion: SynapticCore-X证明在商用教育FPGA上可以原型化能效NPU类加速，降低了神经微架构学术和开源硬件研究的进入门槛。

Abstract: This paper presents SynapticCore-X, a modular and resource-efficient neural processing architecture optimized for deployment on low-cost FPGA platforms. The design integrates a lightweight RV32IMC RISC-V control core with a configurable neural compute tile that supports fused matrix, activation, and data-movement operations. Unlike existing FPGA accelerators that rely on heavyweight IP blocks, SynapticCore-X provides a fully open-source SystemVerilog microarchitecture with tunable parallelism, scratchpad memory depth, and DMA burst behavior, enabling rapid exploration of hardware-software co-design trade-offs. We document an automated, reproducible Vivado build pipeline that achieves timing closure at 100 MHz on the Zynq-7020 while consuming only 6.1% LUTs, 32.5% DSPs, and 21.4% BRAMs. Hardware validation on PYNQ-Z2 confirms correct register-level execution, deterministic control-path behavior, and cycle-accurate performance for matrix and convolution kernels. SynapticCore-X demonstrates that energy-efficient NPU-like acceleration can be prototyped on commodity educational FPGAs, lowering the entry barrier for academic and open-hardware research in neural microarchitectures.

</details>


### [430] [Dissecting and Re-architecting 3D NAND Flash PIM Arrays for Efficient Single-Batch Token Generation in LLMs](https://arxiv.org/abs/2511.12860)
*Yongjoo Jang,Sangwoo Hwang,Hojin Lee,Sangwoo Jung,Donghun Lee,Wonbo Shim,Jaeha Kung*

Main category: cs.AR

TL;DR: 提出使用3D NAND闪存处理内存(PIM)设备来卸载单批次token生成，以解决大语言模型在传统硬件上的内存和计算需求挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数数量激增导致内存和计算需求大幅增加，传统硬件DRAM容量有限且GPU成本高昂，难以有效服务这些模型。

Method: 探索3D NAND闪存配置，重新设计具有H树网络的PIM阵列以实现最佳延迟和单元密度，开发操作分块和映射方法用于LLM层。

Result: 相比四个RTX4090使用vLLM实现2.4倍加速，与四个A100性能相当且仅有4.9%延迟开销，可在4.98mm²芯片面积内集成而无需额外面积开销。

Conclusion: 3D NAND闪存PIM架构是解决大语言模型服务中内存容量和计算需求挑战的有效方案，具有显著性能提升和面积效率。

Abstract: The advancement of large language models has led to models with billions of parameters, significantly increasing memory and compute demands. Serving such models on conventional hardware is challenging due to limited DRAM capacity and high GPU costs. Thus, in this work, we propose offloading the single-batch token generation to a 3D NAND flash processing-in-memory (PIM) device, leveraging its high storage density to overcome the DRAM capacity wall. We explore 3D NAND flash configurations and present a re-architected PIM array with an H-tree network for optimal latency and cell density. Along with the well-chosen PIM array size, we develop operation tiling and mapping methods for LLM layers, achieving a 2.4x speedup over four RTX4090 with vLLM and comparable performance to four A100 with only 4.9% latency overhead. Our detailed area analysis reveals that the proposed 3D NAND flash PIM architecture can be integrated within a 4.98mm2 die area under the memory array, without extra area overhead.

</details>


### [431] [Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration](https://arxiv.org/abs/2511.12930)
*Changhun Oh,Seongryong Oh,Jinwoo Hwang,Yoonsung Kim,Hardik Sharma,Jongse Park*

Main category: cs.AR

TL;DR: Neo通过重用和更新排序算法及专用硬件加速器，解决了3D高斯泼溅渲染中排序阶段的内存带宽瓶颈，显著提升了资源受限设备上的实时渲染性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上实现3D高斯泼溅的实时渲染对AR/VR体验至关重要，但现有方案在高分辨率渲染时难以达到高帧率，排序阶段的高内存带宽需求是主要瓶颈。

Method: 提出重用和更新排序算法，利用连续帧间高斯排序的时间冗余性，通过跟踪和更新高斯深度排序而非从头重新排序，并设计了针对该算法的硬件加速器。

Result: 相比最先进的边缘GPU和ASIC解决方案，Neo分别实现了10.0倍和5.6倍的吞吐量提升，同时将DRAM流量减少了94.5%和81.3%。

Conclusion: Neo使高质量、低延迟的片上3D渲染更加实用可行。

Abstract: 3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.

</details>


### [432] [Think with Self-Decoupling and Self-Verification: Automated RTL Design with Backtrack-ToT](https://arxiv.org/abs/2511.13139)
*Zhiteng Chao,Yonghao Wang,Xinyu Zhang,Jiaxin Zhou,Tenghui Hua,Husheng Han,Tianmeng Yang,Jianan Mu,Bei Yu,Rui Zhang,Jing Ye,Huawei Li*

Main category: cs.AR

TL;DR: VeriBToT是一个专门用于自动生成Verilog的LLM推理范式，通过集成自上而下和面向验证的设计方法，实现了中间步骤的自解耦和自验证，构建了带有形式化算子的回溯思维树。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在Verilog生成方面存在挑战：复杂设计难以一次性生成成功，缺乏针对性解耦策略，且评估解耦子任务正确性困难。传统的CoT方法在IC设计自动化中效果不佳，需要人工干预，主要问题在于推理方向和步骤粒度与专家RTL设计知识不匹配。

Method: 提出VeriBToT范式，集成自上而下和面向验证的设计方法，实现自解耦和自验证中间步骤，构建带有形式化算子的回溯思维树。通过灵活的模块化、层次化和可重用性优化token成本。

Result: 相比传统CoT范式，VeriBToT方法提升了Verilog生成质量，同时通过灵活的模块化、层次化和可重用性优化了token成本。

Conclusion: VeriBToT为自动化IC设计提供了一个有效的LLM推理范式，解决了传统CoT方法在Verilog生成中的局限性，实现了更好的设计质量和效率。

Abstract: Large language models (LLMs) hold promise for automating integrated circuit (IC) engineering using register transfer level (RTL) hardware description languages (HDLs) like Verilog. However, challenges remain in ensuring the quality of Verilog generation. Complex designs often fail in a single generation due to the lack of targeted decoupling strategies, and evaluating the correctness of decoupled sub-tasks remains difficult. While the chain-of-thought (CoT) method is commonly used to improve LLM reasoning, it has been largely ineffective in automating IC design workflows, requiring manual intervention. The key issue is controlling CoT reasoning direction and step granularity, which do not align with expert RTL design knowledge. This paper introduces VeriBToT, a specialized LLM reasoning paradigm for automated Verilog generation. By integrating Top-down and design-for-verification (DFV) approaches, VeriBToT achieves self-decoupling and self-verification of intermediate steps, constructing a Backtrack Tree of Thought with formal operators. Compared to traditional CoT paradigms, our approach enhances Verilog generation while optimizing token costs through flexible modularity, hierarchy, and reusability.

</details>


### [433] [Coliseum project: Correlating climate change data with the behavior of heritage materials](https://arxiv.org/abs/2511.13343)
*A Cormier,David Roqui,Fabrice Surma,Martin Labouré,Jean-Marc Vallet,Odile Guillon,N Grozavu,Ann Bourgès*

Main category: cs.AR

TL;DR: COLISEUM项目提出了一种通过人工智能模型预测文化遗产材料行为的方法论，在法国三个不同气候和材料的遗址收集多模态数据，建立风化模型来预测气候变化对遗产材料的影响。


<details>
  <summary>Details</summary>
Motivation: 气候变化已经影响文化遗产材料，增加的气候变化会缩短古迹寿命。由于风化依赖多种因素，很难将其进展与气候变化联系起来，需要同时收集气候数据和监测劣化进展。

Method: 在法国三个文化遗址建立气候监测系统，使用微气候传感器连续记录参数变化，定期通过化学分析、制图测量和科学成像监测劣化状态，通过计算风化指数在风化矩阵中收集数据。

Result: 建立了三个遗址的监测系统，以斯特拉斯堡大教堂为例展示了仪器方法、初步诊断和首批结果。

Conclusion: 该方法论将允许使用IPCC不同气候变化情景的已知数据预测未来材料行为，为文化遗产保护提供科学依据。

Abstract: Heritage materials are already affected by climate change, and increasing climatic variations reduces the lifespan of monuments. As weathering depends on many factors, it is also difficult to link its progression to climatic changes. To predict weathering, it is essential to gather climatic data while simultaneously monitoring the progression of deterioration. The multimodal nature of collected data (images, text{\ldots}) makes correlations difficult, particularly on different time scales. To address this issue, the COLISEUM project proposes a methodology for collecting data in three French sites to predict heritage material behaviour using artificial intelligence computer models. Over time, prediction models will allow the prediction of future material behaviours using known data from different climate change scenarios by the IPCC (Intergovernmental Panel on Climate Change). Thus, a climate monitoring methodology has been set up in three cultural sites in France: Notre-Dame cathedral in Strasbourg ( 67), Bibracte archaeological site (71), and the Saint-Pierre chapel in Villefranche-sur-Mer (06). Each site has a different climate and specific materials. In situ, microclimatic sensors continuously record variations parameters over time. The state of alteration is monitored at regular intervals by means of chemical analyses, cartographic measurements and scientific imaging campaigns. To implement weathering models, data is gathered in alteration matrix by mean of a calculated weathering index. This article presents the instrumentation methodology, the initial diagnostic and the first results with the example of Strasbourg Cathedral site.

</details>


### [434] [T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization](https://arxiv.org/abs/2511.13676)
*Hyunwoo Oh,KyungIn Nam,Rajat Bhattacharjya,Hanning Chen,Tamoghno Das,Sanggeon Yun,Suyeon Jang,Andrew Ding,Nikil Dutt,Mohsen Imani*

Main category: cs.AR

TL;DR: T-SAR是首个在CPU上实现可扩展三元LLM推理的框架，通过重新利用SIMD寄存器文件进行动态寄存器内查找表生成，消除了内存瓶颈，显著提升了计算效率和能效。


<details>
  <summary>Details</summary>
Motivation: LLM的快速发展超过了边缘平台CPU的计算和内存能力，现有CPU解决方案依赖内存查找表限制了可扩展性，而FPGA或GPU加速器在边缘场景不实用。

Method: 重新利用SIMD寄存器文件进行动态寄存器内查找表生成，只需最小硬件修改，消除内存瓶颈并最大化数据级并行性。

Result: 在GEMM延迟和GEMV吞吐量上分别实现5.6-24.5倍和1.1-86.2倍的提升，SIMD单元仅增加3.2%功耗和1.4%面积开销，能效达到NVIDIA Jetson AGX Orin的2.5-4.9倍。

Conclusion: T-SAR为边缘平台上的高效LLM推理提供了一种实用方法，通过最小硬件修改实现了显著性能提升。

Abstract: Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.

</details>


### [435] [QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention](https://arxiv.org/abs/2511.13679)
*Hyunwoo Oh,Hanning Chen,Sanggeon Yun,Yang Ni,Wenjun Huang,Tamoghno Das,Suyeon Jang,Mohsen Imani*

Main category: cs.AR

TL;DR: QUILL是一个针对可变形transformer的硬件加速器，通过距离排序查询和预取技术将不规则内存访问转换为缓存友好的单次处理，显著提升吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 可变形transformer在检测任务中表现优异，但由于不规则内存访问和低算术强度，在硬件上映射效率低下。

Method: 采用距离排序查询(DOOQ)按空间邻近性排序查询，结合区域预取形成调度感知预取循环；融合MSDeformAttn引擎单次执行插值、Softmax、聚合和投影；小张量保留在片上，密集层使用集成GEMM。

Result: 相比RTX 4090实现7.29倍吞吐量提升和47.3倍能效提升；相比其他加速器实现3.26-9.82倍吞吐量提升和2.01-6.07倍能效提升；混合精度量化下精度损失≤0.9 AP。

Conclusion: 通过将稀疏性转换为局部性，再将局部性转换为利用率，QUILL实现了持续性的端到端加速。

Abstract: Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [436] [Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys](https://arxiv.org/abs/2511.12036)
*Satanu Ghosh,Collin Holgate,Neal R. Brodnik,Doug Downey,Samantha Daly,Tresa M. Pollock,Samuel Carton*

Main category: cs.CE

TL;DR: 使用偏好学习优化语言模型进行BCC/B2高温合金设计，通过热力学相计算提供物理基础奖励信号，替代传统启发式或人工反馈方法。


<details>
  <summary>Details</summary>
Motivation: 针对BCC/B2高温合金这一未充分探索的结构材料类别，开发基于物理基础反馈的语言模型设计方法，解决传统方法依赖昂贵人工反馈的问题。

Method: 使用三个开源模型(LLaMA-3.1、Gemma-2、OLMo-2)，通过直接偏好优化(DPO)方法，利用热力学相计算作为统一奖励信号进行多目标优化。

Result: 成功展示了语言模型能够通过物理基础反馈进行偏好调优，为结构合金设计提供科学依据，这是该领域的首次演示。

Conclusion: 该框架具有通用性和可扩展性，为物理科学领域的智能设计空间探索提供了可行路径。

Abstract: We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [437] [eFPE: Design, Implementation, and Evaluation of a Lightweight Format-Preserving Encryption Algorithm for Embedded Systems](https://arxiv.org/abs/2511.12225)
*Nishant Vasantkumar Hegde,Suneesh Bare,K B Ramesh,Aamir Ibrahim*

Main category: cs.CR

TL;DR: eFPE是一种轻量级格式保持加密算法，专为资源受限的嵌入式系统设计，使用8轮Feistel结构和新型轻量级PRF，可直接加密偶数位十进制字符串，无需填充或复杂转换。


<details>
  <summary>Details</summary>
Motivation: 资源受限的嵌入式系统需要安全且轻量级的数据保护方案，特别是在必须保持数据格式完整性的场景下。

Method: 采用8轮Feistel密码结构，配备新型轻量级伪随机函数(PRF)，该PRF使用两轮AES风格操作（字节替换、密钥异或和字节旋转）。在ARM7TDMI LPC2148微控制器上实现并评估。

Result: 总固件ROM占用4.73kB，RAM使用1.34kB；核心eFPE算法模块仅需3.55kB ROM和116B RAM。

Conclusion: eFPE是金融终端、医疗传感器和工业物联网设备等应用的理想解决方案，能够满足数据格式完整性、最小资源占用和低操作延迟的要求。

Abstract: Resource-constrained embedded systems demand secure yet lightweight data protection, particularly when data formats must be preserved. This paper introduces eFPE (Enhanced Format-Preserving Encryption), an 8-round Feistel cipher featuring a "novel lightweight Pseudorandom Function (PRF)" specifically designed for this domain. The PRF, architected with an efficient two-iteration structure of AES-inspired operations (byte-substitution, keyed XOR, and byte-rotation), underpins eFPE's ability to directly encrypt even-length decimal strings without padding or complex conversions, while aiming for IND-CCA2 security under standard assumptions. Implemented and evaluated on an ARM7TDMI LPC2148 microcontroller using Keil μVision 4, eFPE demonstrates the efficacy of its targeted design: a total firmware Read-Only Memory (ROM) footprint of 4.73 kB and Random Access Memory (RAM) usage of 1.34 kB. The core eFPE algorithm module itself is notably compact, requiring only 3.55 kB ROM and 116 B RAM. These characteristics make eFPE a distinct and highly suitable solution for applications like financial terminals, medical sensors, and industrial IoT devices where data format integrity, minimal resource footprint, and low operational latency are paramount.

</details>


### [438] [InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference](https://arxiv.org/abs/2511.13365)
*Ruijun Deng,Zhihui Lu,Qiang Duan*

Main category: cs.CR

TL;DR: InfoDecom是一个防御框架，通过分解和去除冗余信息，然后注入经过校准的噪声来保护分裂推理中的隐私，同时保持较好的模型性能。


<details>
  <summary>Details</summary>
Motivation: 分裂推理虽然能避免直接传输原始数据，但数据重建攻击仍能从客户端发送的粉碎数据中恢复原始输入，导致严重隐私泄露。现有防御方法通常导致显著的效用下降，特别是当客户端模型较浅时。

Method: 提出InfoDecom防御框架：首先分解并去除粉碎数据中的冗余信息，然后注入经过理论保证隐私的校准噪声。

Result: 实验表明，InfoDecom在计算机视觉任务中相比现有基线方法实现了更优的效用-隐私权衡。

Conclusion: InfoDecom通过针对性地处理冗余信息，有效解决了分裂推理中的隐私保护问题，同时保持了良好的模型性能。

Abstract: Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom.

</details>


### [439] [A Content-Preserving Secure Linguistic Steganography](https://arxiv.org/abs/2511.12565)
*Lingyun Xiang,Chengfu Ou,Xu He,Zhongliang Yang,Yuling Liu*

Main category: cs.CR

TL;DR: 提出了一种内容保持的语言隐写方法CLstega，通过可控分布变换嵌入秘密信息而不修改原始文本，实现完美安全的隐蔽通信。


<details>
  <summary>Details</summary>
Motivation: 现有语言隐写方法通过内容变换隐藏秘密信息，但会造成正常文本和隐写文本之间的细微差异，存在安全风险。需要一种不修改原始文本内容的安全隐写方法。

Method: CLstega使用增强掩码策略定位嵌入位置，设计动态分布隐写编码策略，通过微调掩码语言模型实现可控分布变换，从而嵌入秘密信息。

Result: 实验结果显示CLstega能实现100%的提取成功率，在安全性方面优于现有方法，有效平衡了嵌入容量和安全性。

Conclusion: CLstega方法确保了秘密信息的完美安全性，同时完全保持了原始文本的完整性，为安全隐蔽通信提供了有效解决方案。

Abstract: Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\textit{C}ontent-preserving \textit{L}inguistic \textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.

</details>


### [440] [AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research](https://arxiv.org/abs/2511.13333)
*Alexandru-Mihai Apostu,Andrei Preda,Alexandra Daniela Damir,Diana Bolocan,Radu Tudor Ionescu,Ioana Croitoru,Mihaela Gaman*

Main category: cs.CR

TL;DR: AutoMalDesc是一个自动化静态分析摘要框架，通过自步学习流水线生成威胁检测的自然语言解释，无需大量人工标注


<details>
  <summary>Details</summary>
Motivation: 尽管恶意软件检测系统已有显著进展，但生成全面的威胁检测自然语言解释仍是网络安全研究中的开放问题

Method: 采用迭代自步学习流水线，通过合成数据生成和验证循环逐步提升输出质量，基于少量专家标注示例进行初始训练

Result: 在5种脚本语言的3600个多样化样本上评估显示，迭代间有统计显著改进，摘要质量和分类准确性均有持续提升

Conclusion: 该框架在技术精确性和语言连贯性方面均得到验证，并发布了包含10万+脚本样本的完整数据集以促进研究可复现性

Abstract: Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.

</details>


### [441] [ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models](https://arxiv.org/abs/2511.13548)
*Siyang Cheng,Gaotian Liu,Rui Mei,Yilin Wang,Kejia Zhang,Kaishuo Wei,Yuqi Yu,Weiping Wen,Xiaojie Wu,Junhua Liu*

Main category: cs.CR

TL;DR: ForgeDAN是一个新颖的进化框架，通过多策略文本扰动、可解释语义适应度评估和双维度越狱判断，有效生成对抗性提示来绕过LLM的安全对齐机制。


<details>
  <summary>Details</summary>
Motivation: 现有自动化越狱生成方法存在突变多样性有限、适应度评估浅层和基于关键字的脆弱检测等问题，需要更有效的解决方案。

Method: 采用字符、词、句子级别的多策略文本扰动增强攻击多样性；使用基于文本相似度模型的可解释语义适应度评估；集成基于LLM的分类器进行双维度越狱判断。

Result: 评估显示ForgeDAN实现了高越狱成功率，同时保持自然性和隐蔽性，优于现有最先进解决方案。

Conclusion: ForgeDAN框架能够有效生成语义连贯且高效的对抗性提示，解决了现有方法的局限性，在LLM安全测试中表现出色。

Abstract: The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.

</details>


### [442] [BackWeak: Backdooring Knowledge Distillation Simply with Weak Triggers and Fine-tuning](https://arxiv.org/abs/2511.12046)
*Shanmin Wang,Dongdong Zhao*

Main category: cs.CR

TL;DR: BackWeak是一种简单、无需代理模型的KD后门攻击方法，通过使用弱触发器以极小学习率微调良性教师模型，即可在标准蒸馏过程中可靠地转移到各种学生架构。


<details>
  <summary>Details</summary>
Motivation: 现有KD后门方法通常复杂且计算密集，使用代理学生模型和模拟蒸馏来保证可转移性，构建的触发器类似UAPs，不具隐蔽性且具有强对抗性。本文质疑这种复杂性是否必要。

Method: 提出BackWeak攻击范式：使用弱触发器（不可感知的扰动，具有可忽略的对抗效果）以极小学习率微调良性教师模型，无需代理模型。

Result: 在多个数据集、模型架构和KD方法上的广泛实验表明，BackWeak高效、简单，且通常比先前复杂方法更具隐蔽性，能实现高攻击成功率。

Conclusion: 这项工作呼吁KD后门攻击研究者特别关注触发器的隐蔽性及其潜在的对抗特性。

Abstract: Knowledge Distillation (KD) is essential for compressing large models, yet relying on pre-trained "teacher" models downloaded from third-party repositories introduces serious security risks -- most notably backdoor attacks. Existing KD backdoor methods are typically complex and computationally intensive: they employ surrogate student models and simulated distillation to guarantee transferability, and they construct triggers in a way similar to universal adversarial perturbations (UAPs), which being not stealthy in magnitude, inherently exhibit strong adversarial behavior. This work questions whether such complexity is necessary and constructs stealthy "weak" triggers -- imperceptible perturbations that have negligible adversarial effect. We propose BackWeak, a simple, surrogate-free attack paradigm. BackWeak shows that a powerful backdoor can be implanted by simply fine-tuning a benign teacher with a weak trigger using a very small learning rate. We demonstrate that this delicate fine-tuning is sufficient to embed a backdoor that reliably transfers to diverse student architectures during a victim's standard distillation process, yielding high attack success rates. Extensive empirical evaluations on multiple datasets, model architectures, and KD methods show that BackWeak is efficient, simpler, and often more stealthy than previous elaborate approaches. This work calls on researchers studying KD backdoor attacks to pay particular attention to the trigger's stealthiness and its potential adversarial characteristics.

</details>


### [443] [AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.12149)
*Jiayu Li,Yunhan Zhao,Xiang Zheng,Zonghuan Xu,Yige Li,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: 提出了AttackVLA框架来统一评估VLA模型的安全漏洞，发现现有攻击方法存在局限性，并开发了BackdoorVLA实现针对性的长时程动作序列攻击。


<details>
  <summary>Details</summary>
Motivation: VLA模型整合感知、语言和控制带来了新的安全漏洞，但缺乏统一的评估框架，现有攻击方法在真实场景中有效性不明确，且难以进行公平比较。

Method: 提出AttackVLA统一框架，涵盖数据构建、模型训练和推理全流程；实现多种攻击方法；开发BackdoorVLA后门攻击，强制VLA执行攻击者指定的长时程动作序列。

Result: 在模拟和真实机器人环境中评估，BackdoorVLA平均目标成功率58.4%，在选定任务上达到100%；现有攻击方法主要导致非目标性失败或静态动作状态。

Conclusion: 提供了标准化VLA漏洞评估框架，展示了精确对抗操纵的潜力，推动VLA基础具身系统的安全研究。

Abstract: Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.

</details>


### [444] [SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization](https://arxiv.org/abs/2511.12982)
*Xuankun Rong,Wenke Huang,Tingfeng Wang,Daiguo Zhou,Bo Du,Mang Ye*

Main category: cs.CR

TL;DR: 提出SafeGRPO框架，通过将规则治理的奖励构建集成到GRPO中，实现可解释和可验证的推理安全优化，显著提升多模态安全意识、组合鲁棒性和推理稳定性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在扩展模态空间时引入了新的组合安全风险，即使单个输入是良性的，跨模态耦合也可能产生不安全语义，暴露当前MLLMs脆弱的安全意识。

Method: 基于构建的SafeTag-VL-3K数据集，SafeGRPO执行步骤引导的安全思考，强制结构化推理和行为对齐，将规则治理的奖励构建集成到GRPO中。

Result: 显著提高了多模态安全意识、组合鲁棒性和推理稳定性，在多样化基准测试中表现优异，且不牺牲一般能力。

Conclusion: SafeGRPO框架通过自我奖励的多模态安全对齐，有效解决了MLLMs的组合安全风险问题，实现了可解释和可验证的安全优化。

Abstract: Multimodal large language models (MLLMs) have demonstrated impressive reasoning and instruction-following capabilities, yet their expanded modality space introduces new compositional safety risks that emerge from complex text-image interactions. Such cross-modal couplings can produce unsafe semantics even when individual inputs are benign, exposing the fragile safety awareness of current MLLMs. While recent works enhance safety by guiding models to reason about potential risks, unregulated reasoning traces may compromise alignment; although Group Relative Policy Optimization (GRPO) offers self-rewarded refinement without human supervision, it lacks verifiable signals for reasoning safety. To address this, we propose SafeGRPO a self-rewarded multimodal safety alignment framework that integrates rule-governed reward construction into GRPO, enabling interpretable and verifiable optimization of reasoning safety. Built upon the constructed SafeTag-VL-3K dataset with explicit visual, textual, and combined safety tags, SafeGRPO performs step-guided safety thinking to enforce structured reasoning and behavior alignment, substantially improving multimodal safety awareness, compositional robustness, and reasoning stability across diverse benchmarks without sacrificing general capabilities.

</details>


<div id='cs.GL'></div>

# cs.GL [[Back]](#toc)

### [445] [LLM Architecture, Scaling Laws, and Economics: A Quick Summary](https://arxiv.org/abs/2511.11572)
*William H. Press*

Main category: cs.GL

TL;DR: 本文总结了当前大型语言模型的标准架构、扩展规律和成本估算，为2025年的LLM发展提供了系统性概述。


<details>
  <summary>Details</summary>
Motivation: 虽然这些内容并非全新知识，但在总结形式中并不容易获得，作者旨在提供一个关于LLM架构、扩展规律和成本估算的系统性概览。

Method: 通过总结当前LLM标准架构（包括QKV自注意力和典型Transformer架构），分析计算（浮点运算）和内存（参数加数据）的扩展规律，并提供2025年不同规模LLM参数的粗略成本估算。

Result: 提供了LLM架构的清晰总结、扩展规律的量化描述，以及2025年不同规模LLM的成本估算，包括对DeepSeek是否应被视为特殊案例的讨论。

Conclusion: 该论文为非专业读者提供了一个关于当前LLM技术状态、扩展特性和经济成本的全面但简洁的概述，填补了相关知识在总结形式中的空白。

Abstract: The current standard architecture of Large Language Models (LLMs) with QKV self-attention is briefly summarized, including the architecture of a typical Transformer. Scaling laws for compute (flops) and memory (parameters plus data) are given, along with their present (2025) rough cost estimates for the parameters of present LLMs of various scales, including discussion of whether DeepSeek should be viewed as a special case. Nothing here is new, but this material seems not otherwise readily available in summary form.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [446] [QPU Micro-Kernels for Stencil Computation](https://arxiv.org/abs/2511.12617)
*Stefano Markidis,Luca Pennati,Marco Pasquale,Gilbert Netzer,Ivy Peng*

Main category: cs.ET

TL;DR: 提出了QPU微内核方法：使用浅层量子电路执行模板节点更新，通过重复测量获得蒙特卡洛估计，用于求解偏微分方程。该方法将QPU作为采样加速器，保持经典时间循环，仅卸载局部更新操作。


<details>
  <summary>Details</summary>
Motivation: 传统的量子PDE求解器将完整时空问题编码在一个深层电路中，而微内核方法旨在保持经典时间循环，仅将局部更新操作卸载到量子处理器，从而降低资源需求并提高可并行性。

Method: 开发了两种微内核实现：伯努利微内核针对凸和模板，将值编码为单量子比特概率；分支微内核准备模板分支选择器并对单个读出量子比特应用寻址旋转。通过批处理和电路内融合来分摊提交和读出开销。

Result: 在无噪声量子电路模拟器上，随着样本数量增加，精度得到改善。在IBM Brisbane量子计算机上，单步扩散测试显示伯努利实现的误差低于分支实现，且QPU微内核执行时间占主导地位。

Conclusion: QPU微内核方法提供了一种实用的量子-经典混合PDE求解策略，通过浅层量子电路实现局部更新，资源需求固定且独立于全局网格，易于从经典主机编排和并行化。

Abstract: We introduce QPU micro-kernels: shallow quantum circuits that perform a stencil node update and return a Monte Carlo estimate from repeated measurements. We show how to use them to solve Partial Differential Equations (PDEs) explicitly discretized on a computational stencil. From this point of view, the QPU serves as a sampling accelerator. Each micro-kernel consumes only stencil inputs (neighbor values and coefficients), runs a shallow parameterized circuit, and reports the sample mean of a readout rule. The resource footprint in qubits and depth is fixed and independent of the global grid. This makes micro-kernels easy to orchestrate from a classical host and to parallelize across grid points. We present two realizations. The Bernoulli micro-kernel targets convex-sum stencils by encoding values as single-qubit probabilities with shot allocation proportional to stencil weights. The branching micro-kernel prepares a selector over stencil branches and applies addressed rotations to a single readout qubit. In contrast to monolithic quantum PDE solvers that encode the full space-time problem in one deep circuit, our approach keeps the classical time loop and offloads only local updates. Batching and in-circuit fusion amortize submission and readout overheads. We test and validate the QPU micro-kernel method on two PDEs commonly arising in scientific computing: the Heat and viscous Burgers' equations. On noiseless quantum circuit simulators, accuracy improves as the number of samples increases. On the IBM Brisbane quantum computer, single-step diffusion tests show lower errors for the Bernoulli realization than for branching at equal shot budgets, with QPU micro-kernel execution dominating the wall time.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [447] [SEE++: Evolving Snowpark Execution Environment for Modern Workloads](https://arxiv.org/abs/2511.12457)
*Gaurav Jain,Brandon Baker,Joe Yin,Chenwei Xie,Zihao Ye,Sidh Kulkarni,Sara Abdelrahman,Nova Qi,Urjeet Shrestha,Mike Halcrow,Dave Bailey,Yuxiong He*

Main category: cs.DB

TL;DR: Snowpark将其内部沙盒解决方案升级为gVisor，以支持日益多样化的数据工程和AI/ML工作负载，通过优化实现安全高效的Python等多语言执行环境。


<details>
  <summary>Details</summary>
Motivation: 随着Snowpark采用率增长，多样化工作负载对沙盒技术提出了更复杂的需求，需要更强大和灵活的沙盒解决方案来支持新一代工作负载。

Method: 将原有的内部沙盒解决方案迁移到gVisor，并进行针对性优化，构建新的沙盒架构。

Result: 成功升级沙盒架构，解决了迁移过程中遇到的各种挑战，实现了更安全、高性能的执行环境。

Conclusion: 升级后的架构增强了Snowpark执行环境的可扩展性和灵活性，能够更好地支持下一代Snowpark工作负载，并通过案例研究展示了新功能。

Abstract: Snowpark enables Data Engineering and AI/ML workloads to run directly within Snowflake by deploying a secure sandbox on virtual warehouse nodes. This Snowpark Execution Environment (SEE) allows users to execute arbitrary workloads in Python and other languages in a secure and performant manner. As adoption has grown, the diversity of workloads has introduced increasingly sophisticated needs for sandboxing. To address these evolving requirements, Snowpark transitioned its in-house sandboxing solution to gVisor, augmented with targeted optimizations. This paper describes both the functional and performance objectives that guided the upgrade, outlines the new sandbox architecture, and details the challenges encountered during the journey, along with the solutions developed to resolve them. Finally, we present case studies that highlight new features enabled by the upgraded architecture, demonstrating SEE's extensibility and flexibility in supporting the next generation of Snowpark workloads.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [448] [Towards Mitigating Systematics in Large-Scale Surveys via Few-Shot Optimal Transport-Based Feature Alignment](https://arxiv.org/abs/2511.11787)
*Sultan Hassan,Sambatra Andrianomena,Benjamin D. Wandelt*

Main category: astro-ph.IM

TL;DR: 提出了一种通过优化预训练模型特征对齐损失来对齐分布内外样本特征的新方法，以解决系统误差导致的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 系统误差会污染观测数据，导致其与理论模拟信号相比出现分布偏移，这给使用预训练模型标记此类观测数据带来了重大挑战。

Method: 通过优化预训练ID模型提取表示的特征对齐损失来对齐ID和OOD样本的学得特征，使用了均方误差和最优传输等可能的对齐损失函数。

Result: 在MNIST数据集和中性氢大规模地图上的实验表明，当ID和OOD样本之间的对等性未知时，最优传输在特征对齐方面特别有效，即使在数据有限的情况下也能很好地工作。

Conclusion: 该方法能够有效处理系统误差导致的分布偏移问题，特别适用于从大规模巡天中提取信息的实际应用场景。

Abstract: Systematics contaminate observables, leading to distribution shifts relative to theoretically simulated signals-posing a major challenge for using pre-trained models to label such observables. Since systematics are often poorly understood and difficult to model, removing them directly and entirely may not be feasible. To address this challenge, we propose a novel method that aligns learned features between in-distribution (ID) and out-of-distribution (OOD) samples by optimizing a feature-alignment loss on the representations extracted from a pre-trained ID model. We first experimentally validate the method on the MNIST dataset using possible alignment losses, including mean squared error and optimal transport, and subsequently apply it to large-scale maps of neutral hydrogen. Our results show that optimal transport is particularly effective at aligning OOD features when parity between ID and OOD samples is unknown, even with limited data-mimicking real-world conditions in extracting information from large-scale surveys. Our code is available at https://github.com/sultan-hassan/feature-alignment-for-OOD-generalization.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [449] [Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing](https://arxiv.org/abs/2511.12529)
*Sanchaita Hazra,Doeun Lee,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.HC

TL;DR: LLMs作为科学写作辅助工具的研究：通过随机对照试验发现，AI生成的摘要经过少量修改后可以达到与人类撰写摘要相当的接受度，但作者编辑行为主要受AI作者身份感知而非客观质量驱动，源披露在协作科学写作中具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在需要精确性、多模态综合和领域专业知识的科学写作中作为辅助工具的有效性，特别是摘要撰写方面。

Method: 设计了激励性随机对照试验，采用2x2被试间设计，考察摘要的隐含来源（人类vs AI）和来源披露情况对作者编辑行为和审稿人决策的影响。

Result: 作者对AI生成摘要的编辑量少于人类撰写摘要，主要受AI生成文本更高可读性的影响；披露来源信息后编辑量趋同；审稿人决策不受摘要来源影响，但与编辑数量显著相关；AI生成摘要经少量修改后可达人类撰写摘要的接受度。

Conclusion: AI生成摘要在科学写作中具有潜力，但作者编辑行为主要受AI作者身份感知驱动，而非客观质量评估，强调了在协作科学写作中披露来源的重要性。

Abstract: Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.

</details>


### [450] [Enhancing XR Auditory Realism via Multimodal Scene-Aware Acoustic Rendering](https://arxiv.org/abs/2511.11930)
*Tianyu Xu,Jihan Li,Penghe Zu,Pranav Sahay,Maruchi Kim,Jack Obeng-Marnu,Farley Miller,Xun Qian,Katrina Passarella,Mahitha Rachumalla,Rajeev Nongpiur,D. Shin*

Main category: cs.HC

TL;DR: SAMOSA是一个在XR设备上实时渲染空间音频的系统，通过融合房间几何、表面材料和语义声学上下文的多模态场景表示，动态适应物理环境来合成逼真的房间脉冲响应，提升XR听觉真实感。


<details>
  <summary>Details</summary>
Motivation: 现有XR空间音频渲染方法难以实时适应多样化物理场景，导致视觉和听觉线索不匹配，破坏用户沉浸感。

Method: 利用多模态场景表示融合实时估计的房间几何、表面材料和语义驱动的声学上下文，通过场景先验进行高效声学校准，合成高度逼真的房间脉冲响应。

Result: 技术评估显示SAMOSA在各种房间配置和声音类型下都能有效合成RIR，专家评估(N=12)证实了系统在增强XR听觉真实感方面的可行性和有效性。

Conclusion: SAMOSA系统通过动态适应物理环境，能够显著提升XR体验中的听觉真实感，解决了现有方法在实时适应多样化场景方面的局限性。

Abstract: In Extended Reality (XR), rendering sound that accurately simulates real-world acoustics is pivotal in creating lifelike and believable virtual experiences. However, existing XR spatial audio rendering methods often struggle with real-time adaptation to diverse physical scenes, causing a sensory mismatch between visual and auditory cues that disrupts user immersion. To address this, we introduce SAMOSA, a novel on-device system that renders spatially accurate sound by dynamically adapting to its physical environment. SAMOSA leverages a synergistic multimodal scene representation by fusing real-time estimations of room geometry, surface materials, and semantic-driven acoustic context. This rich representation then enables efficient acoustic calibration via scene priors, allowing the system to synthesize a highly realistic Room Impulse Response (RIR). We validate our system through technical evaluation using acoustic metrics for RIR synthesis across various room configurations and sound types, alongside an expert evaluation (N=12). Evaluation results demonstrate SAMOSA's feasibility and efficacy in enhancing XR auditory realism.

</details>


### [451] [Trust in Vision-Language Models: Insights from a Participatory User Workshop](https://arxiv.org/abs/2511.13458)
*Agnese Chiatti,Lara Piccolo,Sara Bernardini,Matteo Matteucci,Viola Schiaffonati*

Main category: cs.HC

TL;DR: 本文通过用户研讨会探讨了用户对视觉语言模型信任度的建立与演变问题，为未来用户-VLM交互研究提供初步见解。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的广泛部署，需要工具帮助用户判断何时信任这些系统，但目前用户信任如何建立和演变仍是开放问题。

Method: 采用以用户为中心的方法，与潜在VLM用户进行研讨会，收集初步结果。

Result: 从试点研讨会中获得洞察，为未来研究提供基础。

Conclusion: 这些初步发现将为未来研究提供背景化信任指标和参与者参与策略，以适应用户-VLM交互的具体情况。

Abstract: With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [452] [Optimal Multi-Constrained Workflow Scheduling for Cyber-Physical Systems in the Edge-Cloud Continuum](https://arxiv.org/abs/2511.07466)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.NI

TL;DR: 提出了一种基于连续时间混合整数线性规划的最优调度方法，用于最小化边缘-中心-云网络中的工作流应用延迟，相比增强版启发式算法平均延迟降低13.54%-33.03%。


<details>
  <summary>Details</summary>
Motivation: 边缘-中心-云范式虽然支持延迟关键型网络物理应用，但面临设备异构性、计算通信能力有限、不同传感执行能力等挑战，需要优化调度来降低整体延迟。

Method: 采用连续时间混合整数线性规划方法，全面建模边缘设备、中心设备和云服务器的异构多核处理器及各种传感、执行能力，考虑现有方法常忽略的多种约束条件。

Result: 在真实用例中平均延迟改进13.54%，在合成工作流中平均延迟降低33.03%，证明了方法的有效性和可扩展性。

Conclusion: 所提出的最优调度方法能显著降低边缘-中心-云网络物理系统中的工作流延迟，优于传统启发式算法，具有良好的实际应用价值。

Abstract: The emerging edge-hub-cloud paradigm has enabled the development of innovative latency-critical cyber-physical applications in the edge-cloud continuum. However, this paradigm poses multiple challenges due to the heterogeneity of the devices at the edge of the network, their limited computational, communication, and energy capacities, as well as their different sensing and actuating capabilities. To address these issues, we propose an optimal scheduling approach to minimize the overall latency of a workflow application in an edge-hub-cloud cyber-physical system. We consider multiple edge devices cooperating with a hub device and a cloud server. All devices feature heterogeneous multicore processors and various sensing, actuating, or other specialized capabilities. We present a comprehensive formulation based on continuous-time mixed integer linear programming, encapsulating multiple constraints often overlooked by existing approaches. We conduct a comparative experimental evaluation between our method and a well-established and effective scheduling heuristic, which we enhanced to consider the constraints of the specific problem. The results reveal that our technique outperforms the heuristic, achieving an average latency improvement of 13.54% in a relevant real-world use case, under varied system configurations. In addition, the results demonstrate the scalability of our method under synthetic workflows of varying sizes, attaining a 33.03% average latency decrease compared to the heuristic.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [453] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 本研究探讨使用LLM生成的合成新闻标题数据集替代真实数据支持NLP任务的可行性，特别关注负面情感文本，验证了合成数据在内容、语气、长度和风格上与真实负面新闻的匹配度。


<details>
  <summary>Details</summary>
Motivation: 克服真实数据获取困难和隐私问题，探索LLM生成数据在情感分析等NLP任务中的潜力，特别是在负面情感文本分析方面。

Method: 使用定制提示创建负面新闻标题语料库，通过专家评审和嵌入空间分析验证合成数据质量，采用多种评估指标与真实新闻标题进行基准测试。

Result: 合成标题在大多数指标上与真实标题匹配良好，仅在POS分析中的专有名词得分存在明显差异。

Conclusion: LLM生成的合成新闻标题数据集可以有效替代真实数据支持NLP任务，特别是在负面情感分析领域具有应用潜力。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [454] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB是一个评估大语言模型在气候变化领域专业知识的基准，发现前沿模型具有博士级别的知识综合能力，但在证据基础和归因方面存在严重问题。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型处理复杂专业知识的能力，特别是在气候变化这样的关键领域，需要可靠的基准来测试模型的知识质量和证据支持能力。

Method: 开发CLINB基准，包含真实用户问题和气候科学家制定的评估标准，采用基于模型的评估流程来测试多个前沿模型。

Result: 前沿模型展现出卓越的知识综合能力，质量超过专家辅助的混合答案，但在证据基础方面存在严重问题，引用和图像存在大量幻觉。

Conclusion: 弥合知识综合与可验证归因之间的差距对AI在科学工作流程中的部署至关重要，需要像CLINB这样的可靠基准来构建可信赖的AI系统。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [455] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying是一个用于网络欺凌研究和检测的合成多LLM对话数据集，通过大语言模型模拟真实欺凌互动，提供可扩展且伦理安全的替代方案。


<details>
  <summary>Details</summary>
Motivation: 为网络欺凌研究提供可扩展且伦理安全的数据收集替代方案，克服传统人类数据收集的局限性。

Method: 利用大语言模型生成模拟真实欺凌互动的多轮对话，提供对话结构、上下文感知标注和细粒度标签。

Result: 数据集在对话结构、词汇模式、情感/毒性、角色动态、伤害强度和欺凌类型分布等五个维度进行评估，并测试其作为独立训练数据和增强源的性能。

Conclusion: SynBullying是一个有效的网络欺凌研究工具，能够生成高质量的训练数据并增强现有分类器的性能。

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [456] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文批判性评估了现有NL-FOL翻译数据集和评估协议的局限性，提出了新的评估方法来区分真正的语义逻辑理解与表面模式识别，并发现对话导向的LLM在NL-FOL翻译方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 自然语言到一阶逻辑的翻译是一个长期挑战，虽然LLM的出现带来了突破希望，但现有文献对其能力评估结果相互矛盾，需要更准确的评估方法。

Method: 批判性分析现有评估数据集和协议，提出新的评估协议来区分语义级逻辑理解与表面模式识别、记忆和数据集污染。

Result: 使用新评估方法发现，最先进的对话导向LLM表现出强大的NL-FOL翻译能力和真正的句子级逻辑理解，而嵌入中心模型表现明显较差。

Conclusion: 对话导向的LLM在NL-FOL翻译任务中具有真正的语义逻辑理解能力，但需要更准确的评估方法来揭示其真实能力。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [457] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: Forgetting-MarI是一个LLM遗忘框架，通过惩罚边际信息来选择性移除特定数据的参数知识，同时保留其他数据的信息，提供可证明的不可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在不断扩大数据集上训练，需要从训练模型中移除特定数据影响以实现隐私保护和法规遵从，而现有遗忘方法往往因移除过多信息而降低模型性能。

Method: 引入Forgetting-MarI框架，通过惩罚边际信息来仅移除待遗忘数据贡献的额外信息，同时保留待保留数据支持的信息，提供明确的残余影响上界。

Result: 广泛实验表明该方法优于当前最先进的遗忘方法，实现可靠遗忘并在多样化基准测试中保持更好的通用模型性能。

Conclusion: 这一进展代表了在不影响AI系统有效性的前提下，使其更可控并符合隐私和版权法规的重要一步。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [458] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach是一个模型无关的自进化框架，为网页浏览代理提供跨会话的持久记忆，通过记忆检索和运行时建议注入，提升代理在复杂浏览任务中的长期规划和持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态LLM代理在网页导航中存在重复错误，缺乏跨会话的学习能力，限制了长期鲁棒性和样本效率。

Method: WebCoach包含三个核心组件：WebCondenser标准化导航日志为摘要，External Memory Store组织完整轨迹为经验记忆，Coach基于相似性和时效性检索经验并决定是否通过运行时钩子注入任务特定建议。

Result: 在WebVoyager基准测试中，WebCoach显著提升了三种不同LLM骨干的浏览器代理性能。使用38B模型时，任务成功率从47%提升到61%，同时减少或保持平均步骤数。较小基础模型配合WebCoach可达到与使用GPT-4o的相同代理相当的性能。

Conclusion: WebCoach通过持久跨会话记忆实现了网页浏览代理的自我进化，无需重新训练即可持续改进，显著提升了代理的长期鲁棒性和样本效率。

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [459] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: 评估语言模型在对话中构建和维护世界模型的能力，测试其在语言变化下的鲁棒性，并提出解释性框架和微调策略来改善模型表现。


<details>
  <summary>Details</summary>
Motivation: 现实对话包含丰富的语用元素，需要构建局部世界模型来理解实体提及、指代和含义。但目前不清楚语言模型是否能够构建和维护这种隐式表示。

Method: 对流行数据集中的对话应用七种最小语言变化，构建两个包含是非问题的基准测试。评估多种开源和闭源语言模型，并提出双视角解释性框架识别有害的Transformer层，然后设计基于层正则化的微调策略。

Result: 语言模型在语言变化下难以保持鲁棒准确性，特别是在跟踪实体等关键细节方面表现不佳。有害层通常编码虚假信号或依赖捷径。

Conclusion: 语言模型在对话世界模型构建方面存在局限性，但通过识别和抑制有害层可以改善其表现。

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [460] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP框架通过基于任务成功率的动态采样分配和步骤级优化，解决了多轮交互强化学习中轨迹级优化的低效问题。


<details>
  <summary>Details</summary>
Motivation: 轨迹级优化在多轮交互强化学习中存在效率低下和误导性学习信号的问题，包括对任务难度的统一采样、对失败轨迹中正确中间动作的惩罚以及高样本收集成本。

Method: 提出STEP框架：维护平滑的成功率记录指导自适应轨迹重采样，计算成功率加权优势，将轨迹分解为步骤级样本，并应用步骤级GRPO增强来优化低成功率任务的更新。

Result: 在OSWorld和AndroidWorld上的实验表明，STEP显著提高了样本效率和训练稳定性，在相同采样预算下收敛更快且泛化能力更好。

Conclusion: STEP通过动态采样分配和步骤级优化，有效解决了多轮交互强化学习中的效率问题，优于传统的轨迹级GRPO方法。

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [461] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: 该研究探讨了AI系统在道德困境中的不确定性，通过分析32个开源模型在电车问题中的表现，发现模型架构和训练方法对道德不确定性的影响大于道德维度本身。通过引入推理时的随机性，研究发现可以增加模型的总熵和互信息，从而改善AI与人类在道德判断上的一致性。


<details>
  <summary>Details</summary>
Motivation: 人类在面对道德困境时表现出显著的不确定性，但机器和AI代理中的这种不确定性程度尚未被充分探索。随着AI系统越来越多地嵌入伦理决策场景，理解其道德推理和内在不确定性对于构建可靠的AI系统至关重要。

Method: 研究分析了32个开源模型在经典电车问题中的响应，涵盖9个不同的道德维度。通过测量二元熵作为总熵、条件熵和互信息的线性组合来量化不确定性。在推理时通过'dropout'引入随机性来检验其效果。

Result: 研究发现模型置信度的方差在模型间的差异大于在道德维度内的差异。引入随机性机制后，总熵增加，主要是通过互信息的上升，而条件熵基本保持不变。该机制显著改善了人类-LLM道德对齐，互信息和对齐分数变化之间存在相关性。

Conclusion: 通过有意调节不确定性和降低LLMs在道德复杂场景中的置信度，可以更好地对齐模型生成的决策与人类偏好，这为构建更可靠的AI系统提供了重要启示。

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


### [462] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR是一个模块化、零样本的代理框架，通过分层提示分析、价值对齐推理和选择性重写，显著减少文本到图像生成中的不安全内容，同时保持生成质量和创造力。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉语言模型存在产生不安全、冒犯性或文化不适当内容的风险，现有防御方法在保持生成质量和降低成本方面面临挑战。

Method: VALOR框架整合了多层NSFW检测器、文化价值对齐模块和意图消歧器，检测到不安全内容时通过大语言模型在角色特定指令下选择性重写提示，必要时进行风格化再生。

Result: 在对抗性、模糊性和价值敏感提示上的实验显示，VALOR将不安全输出减少了高达100.00%，同时保持了提示的有用性和创造力。

Conclusion: VALOR是一种可扩展且有效的方法，可在开放世界环境中部署安全、对齐且有用的图像生成系统。

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [463] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception是一个基于拓扑属性的基准测试，用于严格评估大型视觉语言模型的全局视觉感知能力，发现现有模型在全局感知方面表现不佳，甚至不如随机猜测。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型中的视觉感知模块成为瓶颈，传统评估基准存在局部捷径问题，导致高估模型感知能力。需要一种无捷径的方法来评估全局视觉感知。

Method: 利用拓扑属性创建TopoPerception基准测试，拓扑依赖于图像全局结构且对局部特征不变，能够实现无捷径的全局感知评估。

Result: 在TopoPerception上评估最先进模型发现，即使在最粗的感知粒度下，所有模型表现都不优于随机机会，表明模型缺乏全局视觉特征感知能力。更强大的模型反而准确率更低。

Conclusion: 仅扩大模型规模不足以解决全局感知缺陷，可能需要新的训练范式或架构。TopoPerception不仅揭示了当前LVLMs的关键瓶颈，还为改进全局视觉感知提供了方向和工具。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [464] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O是一个端到端系统，可将组织解剖视频转化为手势序列，并发现与术后结果相关的模式，为数据驱动的手术反馈和临床决策支持奠定基础。


<details>
  <summary>Details</summary>
Motivation: 术内行为的细粒度分析及其对患者结果的影响是一个长期挑战，需要自动化的方法来理解手术过程与结果之间的关系。

Method: 利用基于transformer的空间和时间建模以及逐帧分类，F2O在机器人辅助根治性前列腺切除术的神经保留步骤中稳健地检测连续短手势。

Result: F2O在帧级别和视频级别的手势检测AUC分别达到0.80和0.81，其衍生特征预测术后结果的准确性与人工注释相当（0.79 vs 0.75）。

Conclusion: F2O能够自动捕获与勃起功能恢复相关的关键模式，如延长的组织剥离和减少的能量使用，为可解释的手术评估提供了基础。

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [465] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH是一个可解释的病理AI框架，通过自学习范式将多模态大语言模型从被动模式识别转变为证据关联的诊断推理，仅需少量标注数据即可生成癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 当前病理AI系统缺乏人类可读的推理过程，难以审计决策和防止错误，限制了临床采用。

Method: 采用两阶段自学习过程：多样化阶段扩展病理学风格解释，优化阶段为准确性精炼这些解释，无需白盒访问或权重更新。

Result: 在乳腺和前列腺数据集上的评估显示，RECAP-PATH生成的解释与专家评估一致，诊断准确性显著优于基线方法。

Conclusion: RECAP-PATH通过结合视觉理解和推理，提供了临床可信赖的AI，展示了证据关联解释的通用路径。

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [466] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: AURA系统使用合成视频数据开发，通过姿态估计检测ICU患者手部靠近气道导管和身体躁动的高风险行为，实现隐私保护的实时非计划拔管风险监测。


<details>
  <summary>Details</summary>
Motivation: 解决ICU中非计划拔管这一严重患者安全问题，同时克服真实ICU视频数据获取的伦理和隐私挑战。

Method: 利用文本到视频扩散技术生成合成ICU视频数据集，通过姿态估计识别两种高风险运动模式：手部进入气道导管附近区域（碰撞）和身体关键点速度量化的躁动。

Result: 专家评估确认合成数据的真实性，性能评估显示碰撞检测准确率高，躁动识别性能中等。

Conclusion: 这项工作展示了开发隐私保护、可复现的患者安全监测系统的新途径，具有在重症监护环境中部署的潜力。

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [467] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: Yanyun-3是一个通用代理框架，首次实现了在三个异构策略游戏环境中的自主跨平台操作，通过结合视觉语言推理和精确执行能力，在减少63%推理时间的同时将BLEU-4得分提升12.98倍。


<details>
  <summary>Details</summary>
Motivation: 策略游戏中的自动化操作需要能够在多样化用户界面和动态战场条件下实现强泛化能力的智能体，而视觉语言模型在复杂人机交互场景（如策略游戏）中的应用尚未充分探索。

Method: 集成Qwen2.5-VL的视觉语言推理能力和UI-TARS的精确执行能力，采用屏幕捕获、模型推理和动作执行的闭环流程，并通过系统消融研究评估不同多模态数据组合（静态图像、多图像序列和视频）的效果。

Result: 混合策略（融合多图像和视频数据并混合静态图像）显著优于完全融合：推理时间减少63%，BLEU-4得分从4.81%提升至62.41%（约12.98倍提升），代理展现出强大的实时性能和跨平台泛化能力。

Conclusion: 该工作不仅为策略游戏自动化提供了高效解决方案，还通过结构化多模态数据组织建立了增强视觉语言模型性能的通用范式，为具身智能中静态感知与动态推理的相互作用提供了新见解。

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [468] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: MEGA-GUI是一个多阶段GUI定位框架，通过分离粗粒度区域选择和细粒度元素定位来解决视觉杂乱和指令模糊问题，在密集和复杂基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位系统采用单块模型或一次性流水线，缺乏模块化，在视觉杂乱和模糊指令下表现不佳。

Method: 采用多阶段框架，将定位分解为粗粒度ROI选择和细粒度元素定位，由专门的视觉语言代理协调，包含双向ROI缩放算法和上下文感知重写代理。

Result: 在ScreenSpot-Pro基准测试中达到73.18%准确率，在OSWorld-G基准测试中达到68.63%，超越先前报告结果。

Conclusion: 模块化结构能够利用不同视觉尺度下视觉语言模型的互补优势，实现比单块方法更高的准确率。

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [469] [MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications](https://arxiv.org/abs/2511.13131)
*Gagan Raj Gupta,Anshul Kumar,Manish Rai,Apu Chakraborty,Ashutosh Modi,Abdelaali Chaoub,Soumajit Pramanik,Moyank Giri,Yashwanth Holla,Sunny Kumar,M. V. Kiran Sooraj*

Main category: cs.AI

TL;DR: 提出了MM-Telco，一个专门为电信领域定制的多模态基准测试套件和模型，用于解决LLMs在电信领域应用中的特定挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电信领域有巨大应用潜力，但面临领域特定的挑战，需要专门适配才能充分发挥作用。

Method: 开发了包含文本和图像任务的多模态基准测试套件，涵盖网络运营、网络管理、文档质量改进等实际用例，并对各种LLMs和VLMs进行基线实验。

Result: 在数据集上微调的模型性能显著提升，实验揭示了当前最先进多模态LLMs的薄弱环节。

Conclusion: MM-Telco为电信领域LLMs的进一步发展提供了重要指导，有助于加速LLMs在电信行业的应用。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.

</details>


### [470] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: DAP是一个离散令牌自回归规划器，通过联合预测BEV语义和自车轨迹，结合强化学习微调，在160M参数下实现了最先进的自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中数据扩展和模型规模扩展带来的可持续性能提升挑战，传统方法仅预测自车轨迹存在监督稀疏和场景演化约束弱的问题。

Method: 采用离散令牌自回归规划器，联合预测BEV语义和自车轨迹，结合强化学习微调，在完全离散令牌自回归框架下操作栅格化BEV和自车动作。

Result: 在160M参数预算下，在开环指标上达到最先进性能，在NAVSIM基准测试中提供有竞争力的闭环结果。

Conclusion: 完全离散令牌自回归公式为自动驾驶提供了一个紧凑且可扩展的规划范式。

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [471] [Predicting upcoming visual features during eye movements yields scene representations aligned with human visual cortex](https://arxiv.org/abs/2511.12715)
*Sushrut Thorat,Adrien Doerig,Alexander Kroner,Carmen Amme,Tim C. Kietzmann*

Main category: q-bio.NC

TL;DR: 通过主动视觉中的下一瞥预测任务，GPNs模型能够自监督学习与人类大脑对齐的场景表征，无需显式语义目标。


<details>
  <summary>Details</summary>
Motivation: 场景是复杂但有结构的部件集合，需要统一的表征来关联部件位置和共现关系。假设这种结构可以通过主动视觉的时间规律性来自监督学习。

Method: 提出Glimpse Prediction Networks (GPNs) - 循环模型，训练预测人类扫描路径上下一瞥的特征嵌入，利用共现和眼跳条件空间规律性。

Result: GPNs成功学习共现结构，对空间排列敏感，能整合多瞥信息形成统一场景表征，与人类fMRI响应高度对齐，优于显式语义目标训练的模型。

Conclusion: 主动视觉中的下一瞥预测是生物合理、自监督的路径，可从自然视觉经验学习大脑对齐的场景表征。

Abstract: Scenes are complex, yet structured collections of parts, including objects and surfaces, that exhibit spatial and semantic relations to one another. An effective visual system therefore needs unified scene representations that relate scene parts to their location and their co-occurrence. We hypothesize that this structure can be learned self-supervised from natural experience by exploiting the temporal regularities of active vision: each fixation reveals a locally-detailed glimpse that is statistically related to the previous one via co-occurrence and saccade-conditioned spatial regularities. We instantiate this idea with Glimpse Prediction Networks (GPNs) -- recurrent models trained to predict the feature embedding of the next glimpse along human-like scanpaths over natural scenes. GPNs successfully learn co-occurrence structure and, when given relative saccade location vectors, show sensitivity to spatial arrangement. Furthermore, recurrent variants of GPNs were able to integrate information across glimpses into a unified scene representation. Notably, these scene representations align strongly with human fMRI responses during natural-scene viewing across mid/high-level visual cortex. Critically, GPNs outperform architecture- and dataset-matched controls trained with explicit semantic objectives, and match or exceed strong modern vision baselines, leaving little unique variance for those alternatives. These results establish next-glimpse prediction during active vision as a biologically plausible, self-supervised route to brain-aligned scene representations learned from natural visual experience.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [472] [Modular GPU Programming with Typed Perspectives](https://arxiv.org/abs/2511.11939)
*Manya Bansal,Daniel Sainati,Joseph W. Cutler,Saman Amarasinghe,Jonathan Ragan-Kelley*

Main category: cs.PL

TL;DR: Prism是一个新的GPU语言，通过类型化视角在类型层面体现线程控制粒度，解决了GPU编程中单个线程控制与线程组集体操作之间的模块化问题。


<details>
  <summary>Details</summary>
Motivation: 现代GPU编程需要在控制单个线程行为和跟踪线程组集体操作之间取得平衡，这种张力使得模块化编程容易出错。封装集体操作的函数虽然按线程调用，但必须由线程组协作执行。

Method: 引入Prism语言，核心思想是类型化视角，在类型层面体现程序员控制线程行为的粒度。设计了Prism语言，实现了编译器，并在核心演算Bundl中建立了理论基础。

Result: 在Prism中实现了最先进的GPU内核，发现它提供了程序员所需的保证，能够自信地编写模块化代码而不牺牲性能。

Conclusion: Prism语言通过类型化视角恢复了模块化，同时为程序员提供了对集体操作的低级控制，这对于实现高性能至关重要。

Abstract: To achieve peak performance on modern GPUs, one must balance two frames of mind: issuing instructions to individual threads to control their behavior, while simultaneously tracking the convergence of many threads acting in concert to perform collective operations like Tensor Core instructions. The tension between these two mindsets makes modular programming error prone. Functions that encapsulate collective operations, despite being called per-thread, must be executed cooperatively by groups of threads.
  In this work, we introduce Prism, a new GPU language that restores modularity while still giving programmers the low-level control over collective operations necessary for high performance. Our core idea is typed perspectives, which materialize, at the type level, the granularity at which the programmer is controlling the behavior of threads. We describe the design of Prism, implement a compiler for it, and lay its theoretical foundations in a core calculus called Bundl. We implement state-of-the-art GPU kernels in Prism and find that it offers programmers the safety guarantees needed to confidently write modular code without sacrificing performance.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [473] [On the Excitability of Ultra-Low-Power CMOS Analog Spiking Neurons](https://arxiv.org/abs/2511.12753)
*Léopold Van Brandt,Grégoire Brandsteert,Denis Flandre*

Main category: cond-mat.stat-mech

TL;DR: 该论文分析了超低功耗模拟神经元的兴奋性特性，建立了基于临界电荷或膜电位阈值的兴奋标准，发现只有膜电位阈值是神经元固有的，与输入刺激无关。


<details>
  <summary>Details</summary>
Motivation: 研究尖峰神经元的兴奋性特性对于事件驱动的神经形态计算范式至关重要，需要分析代表性超低功耗模拟神经元的尖峰机制。

Method: 使用与工业晶体管紧凑模型兼容的常规SPICE仿真，分析非线性神经元动力学，建立基于临界电荷或膜电位阈值的兴奋标准。

Result: 发现只有膜电位阈值是神经元固有的特征，与输入刺激无关；而临界电荷则依赖于输入刺激。非线性动力学分析提供了深入见解但仍需进一步探索。

Conclusion: 膜电位阈值是表征神经元兴奋性的固有参数，非线性动力学和固有噪声的影响需要进一步研究。

Abstract: The excitability property of spiking neurons describes their capability to output an action potential as a real-time response to an input synaptic excitation current and is central to the event-based neuromorphic computing paradigm. The spiking mechanism is analysed in a representative ultra-low-power analog neuron from the circuit literature. Relying on conventional SPICE simulations compatible with industrial transistor compact models, we establish a excitation criterion, quantified either in terms of critical supplied charge or membrane potential threshold. Only the latter is found intrinsic to the neuron, i.e. independent of the input stimulus. Rigorous analysis of the nonlinear neuron dynamics provides insight but still needs to be explored further, as well as the effect of the intrinsic noise.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [474] [Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems](https://arxiv.org/abs/2511.12257)
*Elhadji Cisse Faye,Mame Diarra Fall,Nicolas Dobigeon,Eric Barat*

Main category: stat.CO

TL;DR: 提出了一种基于贝叶斯框架的泊松逆问题求解方法，通过数据增强和几何感知的蒙特卡洛采样算法处理非欧几里得几何结构，在去噪、去模糊和PET重建中表现优异。


<details>
  <summary>Details</summary>
Motivation: 泊松逆问题面临非利普希茨梯度和正性约束等挑战，需要开发能够处理底层几何结构的高效采样算法。

Method: 使用基于Burg熵的Bregman散度进行数据增强，构建增强后验分布，结合Gibbs采样和Hessian黎曼朗之万蒙特卡洛算法进行高效采样。

Result: 在去噪、去模糊和PET实验中，该方法在重建质量方面与基于优化和采样的方法相比具有竞争力。

Conclusion: 所提出的贝叶斯框架能够有效处理泊松逆问题的几何结构，通过几何感知的采样算法获得高质量重建结果。

Abstract: This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [475] [Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?](https://arxiv.org/abs/2511.13646)
*Chunqiu Steven Xia,Zhe Wang,Yan Yang,Yuxiang Wei,Lingming Zhang*

Main category: cs.SE

TL;DR: Live-SWE-agent是首个能够在运行时自主持续演化的软件代理，从基础代理脚手架开始，在解决真实世界软件问题时自主进化其脚手架实现，在SWE-bench Verified基准测试中达到75.4%的解决率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM软件代理需要专门设计且可能不是最优的，而自改进代理需要昂贵的离线训练且泛化能力有限。

Method: 从仅使用bash工具的基础代理开始，在解决真实软件问题时自主演化其脚手架实现，实现运行时在线自进化。

Result: 在SWE-bench Verified基准测试中达到75.4%解决率，超越所有现有开源软件代理；在SWE-Bench Pro基准测试中达到45.8%的最佳解决率。

Conclusion: Live-SWE-agent证明了软件代理能够在运行时自主持续演化的可行性，显著提升了软件代理的性能和适应性。

Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [476] [Evolving Prompts for Toxicity Search in Large Language Models](https://arxiv.org/abs/2511.12487)
*Onkar Shelar,Travis Desell*

Main category: cs.NE

TL;DR: ToxSearch是一个黑盒进化框架，通过同步稳态循环演化提示来测试模型安全性，发现词汇替换是最有效的攻击方式，且对抗性提示在不同模型间存在可转移性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型即使经过安全对齐后仍然容易受到对抗性提示的攻击，产生有害内容，需要系统性的红队测试方法来评估模型安全性。

Method: 使用黑盒进化框架，在同步稳态循环中演化提示，采用多种操作符（词汇替换、否定、回译、改写和两种语义交叉操作符），并通过审核预言机提供适应度指导。

Result: 词汇替换在产量-方差权衡方面表现最佳，语义相似性交叉作为精确的低吞吐量插入器，全局改写显示高方差但拒绝成本较高。对抗性提示在不同模型间存在可转移性，毒性大约减半，较小的LLaMA 3.2变体表现出最强的抵抗能力。

Conclusion: 小的可控扰动是系统性红队测试的有效载体，防御措施应该预见对抗性提示的跨模型重用，而不仅仅是单一模型的加固。

Abstract: Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [477] [Slow - Motion Video Synthesis for Basketball Using Frame Interpolation](https://arxiv.org/abs/2511.11644)
*Jiantang Huang*

Main category: eess.IV

TL;DR: 提出基于RIFE网络的实时慢动作合成系统，专门针对篮球比赛视频，通过微调在SportsSloMo数据集上实现高质量帧插值，在PSNR和SSIM指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统篮球转播帧率(30-60fps)限制了观众欣赏快速动作如扣篮和变向的能力，需要专门的慢动作合成技术。

Method: 从SportsSloMo数据集中提取篮球子集，使用人感知随机裁剪微调RIFE网络，构建实时慢动作合成系统。

Result: 微调后的RIFE模型获得平均PSNR 34.3 dB和SSIM 0.949，比Super SloMo高2.1 dB，比基准RIFE高1.3 dB，在RTX 4070 Ti Super上实现30fps的4倍慢动作生成。

Conclusion: 任务特定适配对体育慢动作至关重要，RIFE在准确性和速度之间提供了良好的平衡，适合消费级应用。

Abstract: Basketball broadcast footage is traditionally captured at 30-60 fps, limiting viewers' ability to appreciate rapid plays such as dunks and crossovers. We present a real-time slow-motion synthesis system that produces high-quality basketball-specific interpolated frames by fine-tuning the recent Real-Time Intermediate Flow Estimation (RIFE) network on the SportsSloMo dataset. Our pipeline isolates the basketball subset of SportsSloMo, extracts training triplets, and fine-tunes RIFE with human-aware random cropping. We compare the resulting model against Super SloMo and the baseline RIFE model using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) on held-out clips. The fine-tuned RIFE attains a mean PSNR of 34.3 dB and SSIM of 0.949, outperforming Super SloMo by 2.1 dB and the baseline RIFE by 1.3 dB. A lightweight Gradio interface demonstrates end-to-end 4x slow-motion generation on a single RTX 4070 Ti Super at approximately 30 fps. These results indicate that task-specific adaptation is crucial for sports slow-motion, and that RIFE provides an attractive accuracy-speed trade-off for consumer applications.

</details>


### [478] [A Deep Learning Framework for Thyroid Nodule Segmentation and Malignancy Classification from Ultrasound Images](https://arxiv.org/abs/2511.11937)
*Omar Abdelrazik,Mohamed Elsayed,Noorul Wahab,Nasir Rajpoot,Adam Shephard*

Main category: eess.IV

TL;DR: 提出一个完全自动化的两阶段框架，用于甲状腺结节超声图像的可解释恶性预测，通过TransUNet分割结节区域，再用ResNet-18分类器进行恶性预测。


<details>
  <summary>Details</summary>
Motivation: 解决甲状腺结节超声风险分层中观察者间变异性的问题，同时提供可解释的深度学习模型。

Method: 两阶段框架：1) TransUNet自动分割甲状腺结节；2) 使用分割掩码创建感兴趣区域，输入ResNet-18分类器进行恶性预测。

Result: 在349张临床图像数据集上通过5折交叉验证获得0.852的F1分数，优于使用手工形态特征的随机森林基线（F1=0.829）。

Conclusion: 这是第一个完全自动化的端到端管道，既能检测甲状腺结节又能预测恶性程度，证明从局部结节学习的隐式视觉特征比显式形状特征更具预测性。

Abstract: Ultrasound-based risk stratification of thyroid nodules is a critical clinical task, but it suffers from high inter-observer variability. While many deep learning (DL) models function as "black boxes," we propose a fully automated, two-stage framework for interpretable malignancy prediction. Our method achieves interpretability by forcing the model to focus only on clinically relevant regions. First, a TransUNet model automatically segments the thyroid nodule. The resulting mask is then used to create a region of interest around the nodule, and this localised image is fed directly into a ResNet-18 classifier. We evaluated our framework using 5-fold cross-validation on a clinical dataset of 349 images, where it achieved a high F1-score of 0.852 for predicting malignancy. To validate its performance, we compared it against a strong baseline using a Random Forest classifier with hand-crafted morphological features, which achieved an F1-score of 0.829. The superior performance of our DL framework suggests that the implicit visual features learned from the localised nodule are more predictive than explicit shape features alone. This is the first fully automated end-to-end pipeline for both detecting thyroid nodules on ultrasound images and predicting their malignancy.

</details>


### [479] [Recursive Threshold Median Filter and Autoencoder for Salt-and-Pepper Denoising: SSIM analysis of Images and Entropy Maps](https://arxiv.org/abs/2511.12212)
*Petr Boriskov,Kirill Rudkovskii,Andrei Velichko*

Main category: eess.IV

TL;DR: 本文研究使用中值滤波器和简单三层自编码器在递归阈值算法中去除图像椒盐噪声，提出两种可扩展方案，并验证了SSIMMap指标在客观模糊评估和去噪参数调整中的实用价值。


<details>
  <summary>Details</summary>
Motivation: 研究图像去噪方法，特别是针对椒盐噪声的去除，探索中值滤波器和自编码器在不同噪声水平下的性能，并引入新的评估指标SSIMMap来补充传统SSIMImg指标。

Method: 使用中值滤波器和简单三层自编码器结合递归阈值算法，提出两种可扩展方案：2MF（使用两个不同窗口大小的MF和最终阈值步骤）和MFs-AE（通过AE聚合多个MF的特征）。

Result: 递归阈值MF在强噪声（50-60%）下能稳健恢复图像，而简单AE仅能在低噪声水平（<30%）下恢复图像。2MF方案在低分辨率下有效突出局部细节，MFs-AE方案在高分辨率下有利于恢复整体场景结构。

Conclusion: MF由于其简单性和计算效率，在资源受限平台上更受青睐，而AE在没有预去噪的情况下表现不佳。SSIMMap在客观模糊评估和去噪参数调整中具有实用价值。

Abstract: This paper studies the removal of salt-and-pepper noise from images using median filter (MF) and simple three-layer autoencoder (AE) within recursive threshold algorithm. The performance of denoising is assessed with two metrics: the standard Structural Similarity Index SSIMImg of restored and clean images and a newly applied metric SSIMMap - the SSIM of entropy maps of these images computed via 2D Sample Entropy in sliding windows. We shown that SSIMMap is more sensitive to blur and local intensity transitions and complements SSIMImg. Experiments on low- and high-resolution grayscales images demonstrate that recursive threshold MF robustly restores images even under strong noise (50-60 %), whereas simple AE is only capable of restoring images with low levels of noise (<30 %). We propose two scalable schemes: (i) 2MF, which uses two MFs with different window sizes and a final thresholding step, effective for highlighting sharp local details at low resolution; and (ii) MFs-AE, which aggregates features from multiple MFs via an AE and is beneficial for restoring the overall scene structure at higher resolution. Owing to its simplicity and computational efficiency, MF remains preferable for deployment on resource-constrained platforms (edge/IoT), whereas AE underperforms without prior denoising. The results also validate the practical value of SSIMMap for objective blur assessment and denoising parameter tuning.

</details>


### [480] [Deep Unfolded BM3D: Unrolling Non-local Collaborative Filtering into a Trainable Neural Network](https://arxiv.org/abs/2511.12248)
*Kerem Basim,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: eess.IV

TL;DR: 提出了DU-BM3D方法，将传统BM3D算法展开为可训练架构，用可学习的U-Net去噪器替代固定协作滤波，结合了BM3D的非局部结构先验和深度学习的灵活性。


<details>
  <summary>Details</summary>
Motivation: BM3D利用非局部自相似性先验进行去噪但参数固定，而深度模型如U-Net更灵活但缺乏可解释性且在不同噪声机制下泛化能力差。

Method: 将BM3D展开为可训练架构，用可学习的U-Net去噪器替代固定协作滤波，保留BM3D的非局部结构先验同时支持端到端优化。

Result: 在低剂量CT去噪任务中，DU-BM3D在不同噪声水平下均优于传统BM3D和独立U-Net，获得更高的PSNR和SSIM，特别是在高噪声条件下表现更佳。

Conclusion: DU-BM3D成功结合了传统方法和深度学习的优势，在保持可解释性的同时提高了去噪性能和泛化能力。

Abstract: Block-Matching and 3D Filtering (BM3D) exploits non-local self-similarity priors for denoising but relies on fixed parameters. Deep models such as U-Net are more flexible but often lack interpretability and fail to generalize across noise regimes. In this study, we propose Deep Unfolded BM3D (DU-BM3D), a hybrid framework that unrolls BM3D into a trainable architecture by replacing its fixed collaborative filtering with a learnable U-Net denoiser. This preserves BM3D's non-local structural prior while enabling end-to-end optimization. We evaluate DU-BM3D on low-dose CT (LDCT) denoising and show that it outperforms classic BM3D and standalone U-Net across simulated LDCT at different noise levels, yielding higher PSNR and SSIM, especially in high-noise conditions.

</details>


### [481] [Multimodal RGB-HSI Feature Fusion with Patient-Aware Incremental Heuristic Meta-Learning for Oral Lesion Classification](https://arxiv.org/abs/2511.12268)
*Rupam Mukherjee,Rajkumar Daniel,Soujanya Hazra,Shirin Dasgupta,Subhamoy Mandal*

Main category: eess.IV

TL;DR: 提出了一种统一的口腔病变四分类器，结合深度RGB特征、高光谱重建、手工特征和人口统计学数据，通过增量启发式元学习器提升在低资源环境下的口腔癌早期检测性能。


<details>
  <summary>Details</summary>
Motivation: 在低资源环境中，由于标注数据有限，口腔癌和潜在恶性病变的早期检测具有挑战性。

Method: 使用微调的ConvNeXt-v2编码器处理口腔图像，进行RGB到HSI重建生成31波段高光谱立方体，提取血红蛋白敏感指数、纹理特征和光谱形状度量，与深度特征和临床特征融合，采用增量启发式元学习器结合校准的基础分类器。

Result: 在未见患者分割上，该框架实现了66.23%的宏观F1分数和64.56%的准确率。

Conclusion: 高光谱重建和不确定性感知元学习显著提高了现实世界口腔病变筛查的鲁棒性。

Abstract: Early detection of oral cancer and potentially malignant disorders is challenging in low-resource settings due to limited annotated data. We present a unified four-class oral lesion classifier that integrates deep RGB embeddings, hyperspectral reconstruction, handcrafted spectral-textural descriptors, and demographic metadata. A pathologist-verified subset of oral cavity images was curated and processed using a fine-tuned ConvNeXt-v2 encoder, followed by RGB-to-HSI reconstruction into 31-band hyperspectral cubes. Haemoglobin-sensitive indices, texture features, and spectral-shape measures were extracted and fused with deep and clinical features. Multiple machine-learning models were assessed with patient-wise validation. We further introduce an incremental heuristic meta-learner (IHML) that combines calibrated base classifiers through probabilistic stacking and patient-level posterior smoothing. On an unseen patient split, the proposed framework achieved a macro F1 of 66.23% and an accuracy of 64.56%. Results demonstrate that hyperspectral reconstruction and uncertainty-aware meta-learning substantially improve robustness for real-world oral lesion screening.

</details>


### [482] [RAA-MIL: A Novel Framework for Classification of Oral Cytology](https://arxiv.org/abs/2511.12269)
*Rupam Mukherjee,Rajkumar Daniel,Soujanya Hazra,Shirin Dasgupta,Subhamoy Mandal*

Main category: eess.IV

TL;DR: 首个弱监督深度学习框架用于口腔细胞学全玻片图像的患者级别诊断，通过多实例学习和区域亲和注意力机制提升性能


<details>
  <summary>Details</summary>
Motivation: 传统口腔细胞学人工检查速度慢、主观性强且依赖专家病理学家，需要AI辅助的自动化诊断方法

Method: 提出RAA-MIL（区域亲和注意力多实例学习）模型，建模玻片内区域间的空间关系，使用患者级别的弱标签进行训练

Result: RAA-MIL在未见测试集上达到72.7%的平均准确率和0.69的加权F1分数，优于基线模型

Conclusion: 建立了首个口腔细胞学患者级别弱监督基准，推动了可靠的AI辅助数字病理学发展

Abstract: Cytology is a valuable tool for early detection of oral squamous cell carcinoma (OSCC). However, manual examination of cytology whole slide images (WSIs) is slow, subjective, and depends heavily on expert pathologists. To address this, we introduce the first weakly supervised deep learning framework for patient-level diagnosis of oral cytology whole slide images, leveraging the newly released Oral Cytology Dataset [1], which provides annotated cytology WSIs from ten medical centres across India. Each patient case is represented as a bag of cytology patches and assigned a diagnosis label (Healthy, Benign, Oral Potentially Malignant Disorders (OPMD), OSCC) by an in-house expert pathologist. These patient-level weak labels form a new extension to the dataset. We evaluate a baseline multiple-instance learning (MIL) model and a proposed Region-Affinity Attention MIL (RAA-MIL) that models spatial relationships between regions within each slide. The RAA-MIL achieves an average accuracy of 72.7%, weighted F1-score of 0.69 on an unseen test set, outperforming the baseline. This study establishes the first patient-level weakly supervised benchmark for oral cytology and moves toward reliable AI-assisted digital pathology.

</details>


### [483] [MTMed3D: A Multi-Task Transformer-Based Model for 3D Medical Imaging](https://arxiv.org/abs/2511.12373)
*Fan Li,Arun Iyengar,Lanyu Xu*

Main category: eess.IV

TL;DR: 提出了MTMed3D，一种基于Transformer的多任务模型，在3D医学影像中同时执行检测、分割和分类任务，相比单任务模型显著降低了计算成本并提高了推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像AI技术主要使用单任务模型，忽略了任务间的共享信息，导致实际应用效率低下。

Method: 使用Transformer作为共享编码器生成多尺度特征，然后使用CNN作为任务特定解码器，在BraTS 2018和2019数据集上进行评估。

Result: 在所有三个任务上都取得了有希望的结果，特别是在检测任务上优于先前工作；多任务模型相比单任务变体显著降低了计算成本并保持了可比性能。

Conclusion: 这是首个在3D医学影像中利用Transformer同时处理检测、分割和分类任务的多任务学习工作，展示了提升诊断流程的潜力。

Abstract: In the field of medical imaging, AI-assisted techniques such as object detection, segmentation, and classification are widely employed to alleviate the workload of physicians and doctors. However, single-task models are predominantly used, overlooking the shared information across tasks. This oversight leads to inefficiencies in real-life applications. In this work, we propose MTMed3D, a novel end-to-end Multi-task Transformer-based model to address the limitations of single-task models by jointly performing 3D detection, segmentation, and classification in medical imaging. Our model uses a Transformer as the shared encoder to generate multi-scale features, followed by CNN-based task-specific decoders. The proposed framework was evaluated on the BraTS 2018 and 2019 datasets, achieving promising results across all three tasks, especially in detection, where our method achieves better results than prior works. Additionally, we compare our multi-task model with equivalent single-task variants trained separately. Our multi-task model significantly reduces computational costs and achieves faster inference speed while maintaining comparable performance to the single-task models, highlighting its efficiency advantage. To the best of our knowledge, this is the first work to leverage Transformers for multi-task learning that simultaneously covers detection, segmentation, and classification tasks in 3D medical imaging, presenting its potential to enhance diagnostic processes. The code is available at https://github.com/fanlimua/MTMed3D.git.

</details>


### [484] [DEMIST: \underline{DE}coupled \underline{M}ulti-stream latent d\underline{I}ffusion for Quantitative Myelin Map \underline{S}yn\underline{T}hesis](https://arxiv.org/abs/2511.12396)
*Jiacheng Wang,Hao Li,Xing Yao,Ahmad Toubasi,Taegan Vinarsky,Caroline Gheen,Joy Derwenskus,Chaoyang Jin,Richard Dortch,Junzhong Xu,Francesca Bagnato,Ipek Oguz*

Main category: eess.IV

TL;DR: DEMIST方法使用3D潜在扩散模型从标准T1w和FLAIR图像合成定量磁化转移(qMT)的池大小比(PSR)图，避免了专门的20-30分钟qMT扫描。


<details>
  <summary>Details</summary>
Motivation: qMT成像提供髓鞘敏感的生物标志物，但需要专门的长时间扫描。目标是开发从标准MRI序列合成PSR图的方法。

Method: 两阶段方法：首先训练PSR和解剖图像的自编码器学习对齐潜在表示；然后在潜在空间训练条件扩散模型，使用三种互补条件机制（语义标记、空间残差提示、自适应LoRA调制）。

Result: 在163个扫描的5折交叉验证中，方法优于VAE、GAN和扩散基线，产生更清晰的边界和更好的定量一致性。

Conclusion: DEMIST能够从标准MRI序列有效合成qMT参数图，为多发性硬化评估提供实用的替代方案。

Abstract: Quantitative magnetization transfer (qMT) imaging provides myelin-sensitive biomarkers, such as the pool size ratio (PSR), which is valuable for multiple sclerosis (MS) assessment. However, qMT requires specialized 20-30 minute scans. We propose DEMIST to synthesize PSR maps from standard T1w and FLAIR images using a 3D latent diffusion model with three complementary conditioning mechanisms. Our approach has two stages: first, we train separate autoencoders for PSR and anatomical images to learn aligned latent representations. Second, we train a conditional diffusion model in this latent space on top of a frozen diffusion foundation backbone. Conditioning is decoupled into: (i) \textbf{semantic} tokens via cross-attention, (ii) \textbf{spatial} per-scale residual hints via a 3D ControlNet branch, and (iii) \textbf{adaptive} LoRA-modulated attention. We include edge-aware loss terms to preserve lesion boundaries and alignment losses to maintain quantitative consistency, while keeping the number of trainable parameters low and retaining the inductive bias of the pretrained model. We evaluate on 163 scans from 99 subjects using 5-fold cross-validation. Our method outperforms VAE, GAN and diffusion baselines on multiple metrics, producing sharper boundaries and better quantitative agreement with ground truth. Our code is publicly available at https://github.com/MedICL-VU/MS-Synthesis-3DcLDM.

</details>


### [485] [Improving the Generalisation of Learned Reconstruction Frameworks](https://arxiv.org/abs/2511.12730)
*Emilien Valat,Ozan Öktem*

Main category: eess.IV

TL;DR: 本文提出了一种基于图结构的神经网络架构GLM，用于处理CT成像中的逆问题，相比传统CNN在几何泛化性、参数效率和计算资源方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在处理CT投影数据时存在局限性，因为它们将网格卷积应用于本质上线流形的正弦图数据，无法有效处理不同采集几何结构，且参数效率低下。

Method: 引入图数据结构表示CT采集几何和层析数据，提出混合神经网络架构GLM，结合图卷积和网格卷积处理层析数据。

Result: GLM在结构相似性和峰值信噪比方面优于CNN，仅使用少量可训练参数，训练时间和内存需求显著减少，且能很好地泛化到未见过的采集几何变化。

Conclusion: GLM架构为CT成像中的逆问题提供了更有效和泛化性更强的解决方案，特别适用于处理不同采集几何结构的情况。

Abstract: Ensuring proper generalization is a critical challenge in applying data-driven methods for solving inverse problems in imaging, as neural networks reconstructing an image must perform well across varied datasets and acquisition geometries. In X-ray Computed Tomography (CT), convolutional neural networks (CNNs) are widely used to filter the projection data but are ill-suited for this task as they apply grid-based convolutions to the sinogram, which inherently lies on a line manifold, not a regular grid. The CNNs, unaware of the geometry, are implicitly tied to it and require an excessive amount of parameters as they must infer the relations between measurements from the data rather than from prior information.
  The contribution of this paper is twofold. First, we introduce a graph data structure to represent CT acquisition geometries and tomographic data, providing a detailed explanation of the graph's structure for circular, cone-beam geometries. Second, we propose GLM, a hybrid neural network architecture that leverages both graph and grid convolutions to process tomographic data.
  We demonstrate that GLM outperforms CNNs when performance is quantified in terms of structural similarity and peak signal-to-noise ratio, despite the fact that GLM uses only a fraction of the trainable parameters. Compared to CNNs, GLM also requires significantly less training time and memory, and its memory requirements scale better. Crucially, GLM demonstrates robust generalization to unseen variations in the acquisition geometry, like when training only on fully sampled CT data and then testing on sparse-view CT data.

</details>


### [486] [BrainNormalizer: Anatomy-Informed Pseudo-Healthy Brain Reconstruction from Tumor MRI via Edge-Guided ControlNet](https://arxiv.org/abs/2511.12853)
*Min Gu Kwak,Yeonju Lee,Hairong Wang,Jing Li*

Main category: eess.IV

TL;DR: BrainNormalizer是一个基于扩散模型的框架，能够从肿瘤脑部MRI直接重建伪健康MRI，通过边界引导的生成过程实现解剖学上合理的重建，无需配对的非肿瘤扫描。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤会导致显著的解剖结构变形，但在临床实践中无法获得无肿瘤影响的个体特异性参考图像，这给诊断、治疗规划和手术导航带来困难。

Method: 采用两阶段训练策略：首先通过修复式微调在肿瘤和非肿瘤扫描上适应预训练扩散模型，然后训练边缘图引导的ControlNet分支注入精细解剖轮廓。推理时使用故意错配策略，将肿瘤输入与非肿瘤提示和镜像对侧边缘图配对。

Result: 在BraTS2020数据集上，BrainNormalizer实现了强大的定量性能，在肿瘤影响区域产生解剖学上合理的重建，同时保持整体结构一致性。

Conclusion: BrainNormalizer为治疗规划提供临床可靠的解剖参考，并支持反事实建模和肿瘤诱导变形分析的新研究方向。

Abstract: Brain tumors are among the most clinically significant neurological diseases and remain a major cause of morbidity and mortality due to their aggressive growth and structural heterogeneity. As tumors expand, they induce substantial anatomical deformation that disrupts both local tissue organization and global brain architecture, complicating diagnosis, treatment planning, and surgical navigation. Yet a subject-specific reference of how the brain would appear without tumor-induced changes is fundamentally unobtainable in clinical practice. We present BrainNormalizer, an anatomy-informed diffusion framework that reconstructs pseudo-healthy MRIs directly from tumorous scans by conditioning the generative process on boundary cues extracted from the subject's own anatomy. This boundary-guided conditioning enables anatomically plausible pseudo-healthy reconstruction without requiring paired non-tumorous and tumorous scans. BrainNormalizer employs a two-stage training strategy. The pretrained diffusion model is first adapted through inpainting-based fine-tuning on tumorous and non-tumorous scans. Next, an edge-map-guided ControlNet branch is trained to inject fine-grained anatomical contours into the frozen decoder while preserving learned priors. During inference, a deliberate misalignment strategy pairs tumorous inputs with non-tumorous prompts and mirrored contralateral edge maps, leveraging hemispheric correspondence to guide reconstruction. On the BraTS2020 dataset, BrainNormalizer achieves strong quantitative performance and qualitatively produces anatomically plausible reconstructions in tumor-affected regions while retaining overall structural coherence. BrainNormalizer provides clinically reliable anatomical references for treatment planning and supports new research directions in counterfactual modeling and tumor-induced deformation analysis.

</details>


### [487] [Inertia-Informed Orientation Priors for Event-Based Optical Flow Estimation](https://arxiv.org/abs/2511.12961)
*Pritam P. Karmokar,William J. Beksi*

Main category: eess.IV

TL;DR: 提出了一种基于对比度最大化的生物启发混合方法，结合视觉和惯性运动线索来估计事件相机的光流，使用方向图作为先验来引导优化过程。


<details>
  <summary>Details</summary>
Motivation: 事件相机直接编码场景运动，但事件的时间密集性和空间稀疏性给光流估计带来挑战。对比度最大化方法虽然有效，但仍然是高度非凸优化问题。

Method: 提出混合对比度最大化方法，使用从相机3D速度导出的方向图作为先验来引导对比度最大化过程，约束运动轨迹空间。

Result: 在MVSEC、DSEC和ECD数据集上的评估显示，该方法在精度上优于现有最先进方法。

Conclusion: 方向图引导的公式化方法提高了事件光流估计的鲁棒性和收敛性。

Abstract: Event cameras, by virtue of their working principle, directly encode motion within a scene. Many learning-based and model-based methods exist that estimate event-based optical flow, however the temporally dense yet spatially sparse nature of events poses significant challenges. To address these issues, contrast maximization (CM) is a prominent model-based optimization methodology that estimates the motion trajectories of events within an event volume by optimally warping them. Since its introduction, the CM framework has undergone a series of refinements by the computer vision community. Nonetheless, it remains a highly non-convex optimization problem. In this paper, we introduce a novel biologically-inspired hybrid CM method for event-based optical flow estimation that couples visual and inertial motion cues. Concretely, we propose the use of orientation maps, derived from camera 3D velocities, as priors to guide the CM process. The orientation maps provide directional guidance and constrain the space of estimated motion trajectories. We show that this orientation-guided formulation leads to improved robustness and convergence in event-based optical flow estimation. The evaluation of our approach on the MVSEC, DSEC, and ECD datasets yields superior accuracy scores over the state of the art.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [488] [Asymptotic analysis of cooperative censoring policies in sensor networks](https://arxiv.org/abs/2511.13492)
*Jesus Fernandez-Bes,Rocío Arroyo-Valles,Jesús Cid-Sueiro*

Main category: cs.MA

TL;DR: 本文分析了电池供电多跳传感器网络中的协作数据筛选问题，提出了一种基于联合马尔可夫决策过程的最优筛选策略，通过重要性分级来节省能量，并开发了集中式阈值计算算法。


<details>
  <summary>Details</summary>
Motivation: 在电池供电的多跳传感器网络中，节点生成的消息具有不同重要性，筛选掉不太重要的消息可以节省能量用于后续通信，延长网络寿命。

Method: 使用联合马尔可夫决策过程对整个网络动态进行建模，找到理论上的最优筛选策略，并提出集中式阈值计算算法来近似最优策略。

Result: 实验模拟表明，协作筛选策略具有高能效，优于其他非协作方案，且在某些条件下最优策略可以通过有限个恒定阈值规则来近似。

Conclusion: 协作数据筛选是电池供电传感器网络中有效的能量管理策略，通过重要性分级和阈值控制可以显著提高网络能效。

Abstract: The problem of cooperative data censoring in battery-powered multihop sensor networks is analyzed in this paper. We are interested in scenarios where nodes generate messages (which are related to the sensor measurements) that can be graded with some importance value. Less important messages can be censored in order to save energy for later communications. The problem is modeled using a joint Markov Decision Process of the whole network dynamics, and a theoretically optimal censoring policy, which maximizes a long-term reward, is found. Though the optimal censoring rules are computationally prohibitive, our analysis suggests that, under some conditions, they can be approximated by a finite collection of constant-threshold rules. A centralized algorithm for the computation of these thresholds is proposed. The experimental simulations show that cooperative censoring policies are energy-efficient, and outperform other non-cooperative schemes.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [489] [EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation](https://arxiv.org/abs/2511.11635)
*Rui Jia,Min Zhang,Fengrui Liu,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.CY

TL;DR: 提出了EduAgentQG，一个多智能体协作框架，用于生成高质量、多样化的个性化问题，通过五个专业智能体的迭代反馈循环提升问题质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 手动设计高质量个性化问题库耗时且难以满足多样化学习需求，现有自动问题生成方法质量不稳定、多样性有限且与教育目标对齐不足。

Method: 包含五个专业智能体：规划器生成结构化设计计划，编写器基于计划生成候选问题，求解器和教育者进行多维度二元评分，检查器进行最终验证，通过迭代反馈循环优化问题质量。

Result: 在两个数学问题数据集上的实验表明，EduAgentQG在问题多样性、目标一致性和整体质量方面优于现有单智能体和多智能体方法。

Conclusion: 多智能体协作和迭代反馈循环能够生成高质量、多样化且与教育目标一致的问题，有效支持自适应学习和个性化评估。

Abstract: High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.

</details>


### [490] [Automatic generation of DRI Statements](https://arxiv.org/abs/2511.11655)
*Maurice Flechtner*

Main category: cs.CY

TL;DR: 本文提出了一种自动化生成审议理由指数(DRI)陈述的创新方法，利用自然语言处理和大型语言模型显著减少人工工作量。


<details>
  <summary>Details</summary>
Motivation: 评估群体审议质量对于理解审议过程至关重要，但现有的DRI指标实施受到复杂耗时的陈述生成过程的限制。

Method: 开发了一个系统化框架，利用先进的自然语言处理和大型语言模型来自动化生成DRI陈述。

Result: 该方法显著降低了进行综合审议过程评估的门槛，为将生成式人工智能整合到社会科学研究方法中提供了可复制的模板。

Conclusion: 自动化DRI陈述生成方法能够有效解决传统方法的效率问题，推动审议过程评估的广泛应用。

Abstract: Assessing the quality of group deliberation is essential for improving our understanding of deliberative processes. The Deliberative Reason Index (DRI) offers a sophisticated metric for evaluating group reasoning, but its implementation has been constrained by the complex and time-consuming process of statement generation. This thesis introduces an innovative, automated approach to DRI statement generation that leverages advanced natural language processing (NLP) and large language models (LLMs) to substantially reduce the human effort involved in survey preparation. Key contributions are a systematic framework for automated DRI statement generation and a methodological innovation that significantly lowers the barrier to conducting comprehensive deliberative process assessments. In addition, the findings provide a replicable template for integrating generative artificial intelligence into social science research methodologies.

</details>


### [491] [Generative AI as a Linguistic Equalizer in Global Science](https://arxiv.org/abs/2511.11687)
*Dragan Filimonovic,Christian Rutzer,Jeffrey Macher,Rolf Weder*

Main category: cs.CY

TL;DR: 生成式AI（特别是ChatGPT）正在帮助非英语国家的科研人员缩小与英语母语者在科学写作风格上的差距，促进全球科学交流的语言平等。


<details>
  <summary>Details</summary>
Motivation: 英语在全球科学中的主导地位长期阻碍了非母语者的参与，生成式AI的出现为解决这一不平等问题提供了技术可能性。

Method: 分析2021-2024年间的565万篇科学论文，使用SciBERT文本嵌入模型测量非英语国家作者在GenAI辅助前后的论文与美国作者科学写作风格的相似度。

Result: ChatGPT发布后，GenAI辅助的论文显示出显著且不断增强的语言风格趋同性，特别是对于与英语语言距离较远的国家的国内合作团队。

Conclusion: 生成式AI正在通过减少语言障碍来重塑全球科学交流，为非英语国家科研人员提供了语言平等的工具。

Abstract: For decades, the dominance of English has created a substantial barrier in global science, disadvantaging non-native speakers. The recent rise of generative AI (GenAI) offers a potential technological response to this long-standing inequity. We provide the first large-scale evidence testing whether GenAI acts as a linguistic equalizer in global science. Drawing on 5.65 million scientific articles published from 2021 to 2024, we compare GenAI-assisted and non-assisted publications from authors in non-English-speaking countries. Using text embeddings derived from a pretrained large language model (SciBERT), we measure each publication's linguistic similarity to a benchmark of scientific writing from U.S.-based authors and track stylistic convergence over time. We find significant and growing convergence for GenAI-assisted publications after the release of ChatGPT in late 2022. The effect is strongest for domestic coauthor teams from countries linguistically distant from English. These findings provide large-scale evidence that GenAI is beginning to reshape global science communication by reducing language barriers in research.

</details>


### [492] [Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles](https://arxiv.org/abs/2511.12010)
*Palakorn Achananuparp,Connie Xu,Yao Lu,Xavier Jayaraj Siddarth Ashok,Ee-Peng Lim*

Main category: cs.CY

TL;DR: 基于在线简历数据的大规模分析显示，公司内部职位变动对职业上升最有利，女性和非裔大学毕业生从工作变动中获得的回报显著低于男性和白人同行。


<details>
  <summary>Details</summary>
Motivation: 研究职业变动如何影响职业上升流动性，以及这种流动性结果在性别和种族方面的差异。

Method: 使用在线简历数据，开发基于大语言模型的FewSOC职业分类方法，分析228,710条职业轨迹，进行多层次敏感性分析。

Result: 公司内部职位变动对职业上升最有利，女性和非裔大学毕业生从工作变动中获得的回报显著较低，这些差异对集群异质性具有稳健性。

Conclusion: 职业变动类型对职业上升有不同影响，存在显著的性别和种族不平等，需要针对性政策干预。

Abstract: We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.

</details>


### [493] [Understanding the Representation of Older Adults in Motion Capture Locomotion Datasets](https://arxiv.org/abs/2511.11713)
*Yunkai Yu,Yingying Wang,Rong Zheng*

Main category: cs.CY

TL;DR: 该论文调查了41个公开运动捕捉数据集，发现老年人参与度低，且模拟的老年风格行走动作无法真实反映年龄相关差异，提出了定量评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有运动捕捉数据集中老年人代表性不足，且模拟的老年风格行走动作的真实性未得到充分验证，这在医疗健康应用中尤为重要。

Method: 调查41个公开数据集，识别包含老年人动作的数据集；引入定量指标评估老年风格行走动作的保真度，使用对年龄敏感、抗噪声且适应数据稀缺的步态参数。

Result: 老年人仅占总体参与者的小部分，提供老年人全身运动数据的数据集很少；老年风格行走动作常表现出过度控制的模式，无法真实表征衰老特征。

Conclusion: 需要改进运动数据集中老年人的代表性，并建立了定量评估老年风格行走动作质量的方法。

Abstract: The Internet of Things (IoT) sensors have been widely employed to capture human locomotions to enable applications such as activity recognition, human pose estimation, and fall detection. Motion capture (MoCap) systems are frequently used to generate ground truth annotations for human poses when training models with data from wearable or ambient sensors, and have been shown to be effective to synthesize data in these modalities. However, the representation of older adults, an increasingly important demographic in healthcare, in existing MoCap locomotion datasets has not been thoroughly examined. This work surveyed 41 publicly available datasets, identifying eight that include older adult motions and four that contain motions performed by younger actors annotated as old style. Older adults represent a small portion of participants overall, and few datasets provide full-body motion data for this group. To assess the fidelity of old-style walking motions, quantitative metrics are introduced, defining high fidelity as the ability to capture age-related differences relative to normative walking. Using gait parameters that are age-sensitive, robust to noise, and resilient to data scarcity, we found that old-style walking motions often exhibit overly controlled patterns and fail to faithfully characterize aging. These findings highlight the need for improved representation of older adults in motion datasets and establish a method to quantitatively evaluate the quality of old-style walking motions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [494] [Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data](https://arxiv.org/abs/2511.12788)
*Rubén Darío Guerrero*

Main category: cs.LG

TL;DR: 提出了一种物理约束自适应学习框架，通过可学习参数自动校准电磁近似，在极紫外光刻优化中实现亚纳米精度，仅需少量训练样本即可实现跨几何泛化。


<details>
  <summary>Details</summary>
Motivation: 半导体行业在极紫外光刻优化中面临计算危机，传统方法消耗数十亿CPU小时却无法达到亚纳米精度，需要解决学术物理信息神经网络与工业部署需求之间的关键差距。

Method: 集成可微分模块处理菲涅尔衍射、材料吸收、光学点扩散函数模糊、相移效应和对比度调制，通过物理约束学习自动校准电磁近似参数，结合直接几何图案匹配目标。

Result: 在15个代表性图案上实现一致的亚纳米EPE性能（0.664-2.536 nm范围），仅需每个图案50个训练样本，比无物理约束的CNN基线平均提升69.9%，推理速度显著优于严格电磁求解器。

Conclusion: 物理约束自适应学习为实时半导体制造优化建立了基础方法，通过联合物理校准和制造精度目标，填补了学术研究与工业部署之间的关键空白。

Abstract: The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\boldsymbolθ = \{θ_d, θ_a, θ_b, θ_p, θ_c\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.

</details>


### [495] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: 开发了一个基于Triton DSL的跨平台LLM推理系统，通过paged attention内核在NVIDIA和AMD GPU上实现最先进性能，将通用Triton注意力内核性能从19.7%提升到105.9%。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理平台在硬件架构间的可移植性问题，消除低级别手动调优需求，同时保持最佳效率。

Method: 使用Triton领域特定语言开发paged attention内核，结合算法和系统级改进、参数自动调优，并集成到流行推理服务器中。

Result: 在NVIDIA和AMD GPU上实现最先进性能，将通用Triton注意力内核性能从19.7%提升到105.9%。

Conclusion: 开源领域特定语言可以用于解锁模型在不同GPU厂商间的可移植性，证明便携高效的跨平台LLM推理是可行的。

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>


### [496] [Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge](https://arxiv.org/abs/2511.11585)
*Kabir Khan,Manju Sarkar,Anita Kar,Suresh Ghosh*

Main category: cs.LG

TL;DR: FedGen-Edge是一个联邦学习框架，通过解耦预训练全局主干网络和轻量级客户端适配器，仅联邦化适配器来降低通信开销，支持异构边缘设备上的个性化生成AI。


<details>
  <summary>Details</summary>
Motivation: 大型生成模型在跨设备联邦学习中面临计算通信负担重、统计/系统异构性等问题，需要找到资源高效且支持个性化的解决方案。

Method: 使用LoRA（低秩适应）将客户端更新约束到紧凑子空间，仅联邦化轻量级适配器而非完整模型，减少99%以上的上行流量。

Result: 在语言建模（PTB）和图像生成（CIFAR-10）任务中，FedGen-Edge相比强基线实现了更低的困惑度/FID和更快的收敛速度。

Conclusion: FedGen-Edge为异构边缘设备上的隐私保护、资源感知和个性化生成AI提供了一条实用路径。

Abstract: Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.

</details>


### [497] [Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data](https://arxiv.org/abs/2511.11714)
*Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del Rio,Oleksii Sliusarenko,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 使用Sherpa.ai联邦学习平台，多个医院在不共享数据的情况下协作训练肺炎检测模型，在非独立同分布数据上实现了90%准确率和96.6% ROC-AUC，相比单医院模型性能提升47.5-50%。


<details>
  <summary>Details</summary>
Motivation: 解决医疗数据隐私法规限制下，全球分布式数据无法集中化训练的问题，同时应对医院间数据异质性和图像传输成本高的挑战。

Method: 采用联邦学习框架，使用儿科肺炎胸部X光数据集模拟跨医院协作，在非独立同分布数据条件下进行隐私保护训练。

Result: 联邦学习模型达到0.900准确率和0.966 ROC-AUC，相比单医院模型（0.610准确率，0.644 ROC-AUC）性能显著提升47.5%和50%。

Conclusion: 联邦学习能够在保持数据本地化的前提下，实现高性能、可泛化且安全的肺炎检测，特别适用于罕见疾病的多机构协作诊断和治疗开发。

Abstract: Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.

</details>


### [498] [A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.12695)
*Minghui Chen,Hrad Ghoukasian,Ruinan Jin,Zehua Wang,Sai Praneeth Karimireddy,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 将集中式学习中的LP-FT策略（线性探测后全微调）应用于联邦学习，有效平衡个性化与泛化能力，解决联邦特征扭曲问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据下难以平衡全局泛化与本地个性化，现有PFT方法容易过拟合或无法应对领域偏移。

Method: 将LP-FT策略适配到联邦学习设置，先线性探测再全微调，通过分阶段参数更新缓解特征扭曲。

Result: 在7个数据集和6种PFT变体上的系统评估显示LP-FT在平衡个性化与泛化方面表现优越，理论分析证实其能缓解联邦特征扭曲。

Conclusion: LP-FT为联邦学习提供了稳健的个性化方案，在部分特征重叠和协变量-概念偏移等条件下优于标准微调方法。

Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.

</details>


### [499] [On the Fundamental Limits of LLMs at Scale](https://arxiv.org/abs/2511.12869)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zeeshan Memon,Muhammad Ibtsaam Qadir,Sagnik Bhattacharya,Hassan Rizwan,Abhiram R. Gorle,Maahe Zehra Kazmi,Ayesha Mohsin,Muhammad Usman Rafique,Zihao He,Pulkit Mehta,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的理论框架，从计算理论、信息论和统计学习角度形式化分析了LLM扩展的五个基本理论极限：幻觉、上下文压缩、推理退化、检索脆弱性和多模态不对齐。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM扩展限制的描述多停留在经验层面，缺乏将这些现象与计算、信息和学习的基本理论限制相连接的严谨理论综合。本文旨在填补这一空白。

Method: 构建了一个基于证明的统一框架，结合计算理论（可计算性与不可计算性）、信息论（有限描述长度）和统计学习理论，通过定理证明和实证证据分析LLM扩展的理论天花板。

Result: 证明了：1）不可计算性导致不可避免的错误残留；2）信息论约束限制了可达到的准确性；3）几何和计算效应导致上下文压缩；4）基于似然的训练偏好模式完成而非推理；5）检索和多模态扩展存在固有局限。

Conclusion: LLM扩展在某些领域有收益，但在某些领域会饱和甚至无法进展。论文提供了理论基础和实践缓解路径，如有限预言检索、位置课程学习和稀疏/分层注意力机制。

Abstract: Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.

</details>


### [500] [MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity](https://arxiv.org/abs/2511.13061)
*Vladimír Macko,Vladimír Boža*

Main category: cs.LG

TL;DR: MACKO-SpMV是一种GPU优化的稀疏矩阵向量乘法格式和内核，专门为处理大型语言模型中的非结构化稀疏性而设计，在50%稀疏度下实现1.5倍内存减少和1.2-1.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有SpMV方法在处理LLM剪枝后常见的低且非结构化稀疏性（30-90%）时性能不佳，非结构化剪枝只能提供有限的内存减少和加速效果。

Method: 提出MACKO-SpMV，这是一种GPU优化的格式和内核协同设计，减少存储开销同时保持与GPU执行模型的兼容性，无需专用硬件单元或格式特定的预计算。

Result: 在50%稀疏度下，MACKO是首个实现显著1.5倍内存减少和1.2-1.5倍加速的方法；相比其他SpMV基线：比cuSPARSE快2.8-13.0倍，比Sputnik快1.9-2.6倍，比DASP快2.2-2.5倍。应用于Llama2-7B模型，在fp16精度下实现1.5倍内存减少和1.5倍推理加速。

Conclusion: MACKO使得在现实世界LLM工作负载中，50%稀疏度的非结构化剪枝变得合理可行。

Abstract: Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.

</details>


### [501] [Decoupling Positional and Symbolic Attention Behavior in Transformers](https://arxiv.org/abs/2511.11579)
*Felipe Urrutia,Jorge Salas,Alexander Kozachinskiy,Cristian Buc Calderon,Hector Pasten,Cristobal Rojas*

Main category: cs.LG

TL;DR: 本文深入分析了Transformer中RoPE位置编码的位置信息与语义信息编码机制，证明了注意力头的位置行为和符号行为是互斥的，并开发了量化指标。研究发现注意力头行为与频率使用有强相关性，通过控制频率访问可以控制模型性能。


<details>
  <summary>Details</summary>
Motivation: 理解RoPE位置编码成功的原因，特别是其如何分别使用大频率和小频率编码位置信息和语义信息，深入探究注意力头的位置行为与符号行为二分法。

Method: 从理论和实证层面分析注意力头行为，定义位置行为和符号行为的一般定义，证明二者互斥性，开发量化指标，分析基于RoPE的Transformer LLMs，设计纯位置和纯符号的典型任务。

Result: 发现所有注意力头的行为与频率使用存在强对应关系，通过控制注意力头可访问的频率可以控制Transformer的性能表现。

Conclusion: 研究提供了对RoPE的详细理解，揭示了其特性与模型行为之间的关系，证明了频率使用在位置和语义信息编码中的关键作用。

Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.

</details>


### [502] [Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models](https://arxiv.org/abs/2511.11622)
*Alexis Roger,Gwen Legate,Kashif Rasul,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型中分词器设计（缩放和量化策略）与迁移学习对模型性能的影响，发现分词器配置主要控制模型的表示能力和稳定性，而迁移学习影响优化效率和对齐效果。


<details>
  <summary>Details</summary>
Motivation: 研究分词器设计和迁移学习在构建最先进时间序列基础模型中的关键作用，特别是在预测任务中。

Method: 通过经验训练实验和理论分析相结合的方法，系统研究分词器配置（缩放和量化策略）与预训练对模型性能的影响。

Result: 预训练模型能更有效地利用设计良好的分词器，特别是在较小词汇量时；而错位的分词会削弱甚至逆转预训练的好处。

Conclusion: 在时间序列建模中，精心设计的分词器至关重要，将小型高效词汇表与预训练权重结合在多模态预测环境中特别有利。

Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.

</details>


### [503] [H-Model: Dynamic Neural Architectures for Adaptive Processing](https://arxiv.org/abs/2511.11669)
*Dmytro Hospodarchuk*

Main category: cs.LG

TL;DR: 提出了一种能够根据输入数据动态调整内部结构的神经网络架构，通过路由机制实现迭代和自适应计算，探索可适应和更可解释的网络新方向。


<details>
  <summary>Details</summary>
Motivation: 受思维过程和动态推理启发，旨在设计能够根据数据和内部状态调节信息流的网络，探索学习计算结构本身的可能性，而非追求现有基准的性能优化。

Method: 引入路由机制，使每层能够影响其输出在网络中的传播方式，实现迭代和自适应计算，构建概念原型架构框架。

Result: 由于计算资源和数据的实际限制，本研究仍为初步探索，但初步观察显示出潜力，需要未来在更有利的计算条件下进行完整评估。

Conclusion: 该工作提出了一个可适应且可能更可解释的网络架构概念原型，为探索动态计算结构开辟了新方向，但需要进一步实验来验证其全部潜力。

Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.

</details>


### [504] [Reasoning: From Reflection to Solution](https://arxiv.org/abs/2511.11712)
*Zixi Li*

Main category: cs.LG

TL;DR: 论文提出推理是状态空间中迭代算子应用收敛到固定点的过程，并通过OpenLM架构在OpenXOR问题上实现76%准确率，而现有LLMs为0%。


<details>
  <summary>Details</summary>
Motivation: 在LLMs在GSM8K和HumanEval等基准上取得超人表现的时代，需要区分系统是学会了推理还是仅仅在推理轨迹上进行模式匹配。

Method: 提出推理作为状态空间中迭代算子应用收敛到固定点的定义，并开发OpenLM架构来验证这一理论。

Result: OpenLM在OpenXOR问题上达到76%准确率，而现有最先进LLMs完全失败（0%准确率）。

Conclusion: 论文不仅批评现有系统，更重要的是理解推理的要求并构建能够提供真正推理能力的架构。

Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.

</details>


### [505] [Better LLM Reasoning via Dual-Play](https://arxiv.org/abs/2511.11881)
*Zhengxin Zhang,Chengyu Huang,Aochong Oliver Li,Claire Cardie*

Main category: cs.LG

TL;DR: PasoDoble是一个无监督的LLM双角色对抗训练框架，通过提议者生成有挑战性的问题和求解者解答问题的方式，提升语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练严重依赖外部监督，而对抗学习特别是双角色对抗训练能够减少对外部监督的依赖，但现有方法存在奖励破解和训练不稳定的问题。

Method: 从同一基础模型初始化两个模型：提议者生成带有真实答案的挑战性问题，求解者尝试解答；提议者从预训练数据集中获取知识确保问题质量；通过联合更新避免奖励破解；引入可选离线范式增强训练稳定性。

Result: 实验结果表明PasoDoble能够提升LLM的推理性能。

Conclusion: PasoDoble框架成功实现了无监督的LLM对抗训练，有效提升了模型推理能力，为减少外部监督依赖提供了可行方案。

Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

</details>


### [506] [Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms](https://arxiv.org/abs/2511.13238)
*Patrick Parschan,Charlott Jakob*

Main category: cs.LG

TL;DR: 本文对无监督和半监督的基于文本的理想点估计算法进行了首次系统性综述，分析了从词频模型到大型语言模型的四类方法，并提出了评估框架和实用指南。


<details>
  <summary>Details</summary>
Motivation: 文本理想点估计算法在政治学等领域广泛应用，但领域发展碎片化，缺乏系统比较和应用指导。

Method: 通过系统性文献回顾识别25种算法，进行内容分析，并建立区分文本方差生成、捕获和聚合的概念框架。

Result: 识别出词频、主题建模、词嵌入和LLM四类方法家族，分析了各自假设、可解释性、扩展性和局限性。

Conclusion: 提供了算法发展的结构化综合，为应用研究者提供实用指导，强调算法差异本身具有信息价值，需要系统性基准测试。

Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.

</details>


### [507] [P1: Mastering Physics Olympiads with Reinforcement Learning](https://arxiv.org/abs/2511.13612)
*Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.LG

TL;DR: P1系列模型通过强化学习训练，在物理推理方面取得突破性进展，其中P1-235B-A22B在IPhO 2025获得金牌表现，并在13个国际物理竞赛中赢得12枚金牌。


<details>
  <summary>Details</summary>
Motivation: 推动大语言模型从解决谜题向科学级推理发展，物理学作为连接符号与现实的基础学科，是检验这一转变的最佳测试场。

Method: 完全通过强化学习训练开源物理推理模型家族P1，并配备代理框架PhysicsMinions。

Result: P1-235B-A22B在IPhO 2025获得金牌表现，在13个国际物理竞赛中赢得12枚金牌；P1-30B-A3B获得银牌；配备PhysicsMinions后获得IPhO 2025总分第一。

Conclusion: P1系列模型不仅在物理推理方面表现卓越，在数学和编程等其他推理任务上也展现出强大的泛化能力。

Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

</details>


### [508] [Learning with Preserving for Continual Multitask Learning](https://arxiv.org/abs/2511.11676)
*Hanchen David Wang,Siwoo Bae,Zirong Chen,Meiyi Ma*

Main category: cs.LG

TL;DR: 提出了Learning with Preserving (LwP)框架，通过保持共享表示空间的几何结构来解决持续多任务学习中的灾难性遗忘问题，无需重放缓冲区。


<details>
  <summary>Details</summary>
Motivation: 关键领域如自动驾驶和医学影像分析中的AI系统需要持续学习新任务，但现有持续学习方法在持续多任务学习(CMTL)场景中表现不佳，因为它们学习到的任务特定特征会相互干扰。

Method: 引入LwP框架，核心是动态加权距离保持(DWDP)损失函数，通过正则化潜在数据表示之间的成对距离来防止表示漂移，保持底层几何结构。

Result: 在时间序列和图像基准测试中，LwP不仅缓解了灾难性遗忘，而且持续优于最先进的基线方法，是唯一超越强单任务学习基线的方法。

Conclusion: LwP方法在真实世界动态环境中表现出色，具有对分布偏移的鲁棒性，适用于隐私敏感应用。

Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.

</details>


### [509] [A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications](https://arxiv.org/abs/2511.11679)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.LG

TL;DR: 提出SBN-Opt框架，通过神经代理模型SBN嵌入LSQC能量，优化自由边界微分同胚映射，解决表面映射中边界无约束和局部双射性保持的难题。


<details>
  <summary>Details</summary>
Motivation: 自由边界微分同胚优化在表面映射中至关重要，但传统数值算法需要地标条件且无法应用于基于梯度的优化，限制了其应用范围。

Method: 提出Spectral Beltrami Network (SBN)作为神经代理，将LSQC能量嵌入多尺度网格谱架构；然后提出SBN-Opt框架优化自由边界微分同胚。

Result: 在密度均衡映射和不一致表面配准上的大量实验表明，SBN-Opt优于传统数值算法。

Conclusion: SBN-Opt框架成功解决了自由边界微分同胚优化问题，能够显式控制局部几何畸变，在表面映射任务中表现出优越性能。

Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.

</details>


### [510] [Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP](https://arxiv.org/abs/2511.11680)
*Udaya Bhasker Cheerala,Varun Teja Chirukuri,Venkata Akhil Kumar Gummadi,Jintu Moni Bhuyan,Praveen Damacharla*

Main category: cs.LG

TL;DR: 本研究使用随机森林算法结合可解释人工智能(SHAP)开发了加利福尼亚州野火风险地图，识别了不同生态系统的关键风险驱动因素，并评估了模型的空间和时间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 野火对全球生态系统构成严重威胁，加利福尼亚州由于气候、地形、植被和人类活动等因素经常发生火灾。需要开发综合的野火风险评估方法来支持决策制定和风险缓解策略。

Method: 采用随机森林算法，结合可解释人工智能(SHAP)来解释模型预测，使用空间和时间验证策略评估模型性能。

Result: RF模型表现出强大的预测性能，草地和森林的AUC分别达到0.996和0.997。空间交叉验证显示中等可转移性，时间分割验证显示更好的泛化能力。SHAP分析识别出生态系统特定的关键驱动因素：森林中土壤有机碳、树木覆盖和NDVI最重要，草地中地表温度、海拔和植被健康指数占主导。

Conclusion: RF-SHAP框架提供了一个稳健、可理解和适应性强的方法来评估野火风险，能够支持明智决策并制定有针对性的风险缓解策略。

Abstract: Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.

</details>


### [511] [MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation](https://arxiv.org/abs/2511.11681)
*Penghui Niu,Jiashuai She,Taotao Cai,Yajuan Zhang,Ping Zhang,Junhua Gu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了MPCM-Net网络，结合部分注意力卷积和Mamba架构，用于地面云图像分割，在CSRC数据集上实现了分割精度和推理速度的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法存在三个主要局限：依赖空洞卷积缺乏通道间互操作性；注意力机制忽视精度-吞吐量平衡；解码器修改未能建立层次局部特征的全局依赖关系。

Method: 编码器包含MPAC模块（MPC块和MPA块），实现多尺度云形成的全局空间交互和低计算复杂度特征提取；解码器使用M2B通过SSHD保持线性复杂度，实现跨空间和尺度维度的深度特征聚合。

Result: 在CSRC数据集上的广泛实验表明，MPCM-Net优于最先进方法，实现了分割精度和推理速度的最佳平衡。

Conclusion: MPCM-Net通过集成部分注意力卷积和Mamba架构，有效解决了现有方法的局限性，同时发布了CSRC数据集作为清晰标签的细粒度分割基准。

Abstract: Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.

</details>


### [512] [Stratified Knowledge-Density Super-Network for Scalable Vision Transformers](https://arxiv.org/abs/2511.11683)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.LG

TL;DR: 提出WPAC和PIAD方法，将预训练ViT转换为分层知识密度超网络，实现灵活的子网络提取以适应不同资源约束。


<details>
  <summary>Details</summary>
Motivation: 训练和部署多个ViT模型用于不同资源约束成本高且效率低，需要一种方法能够从单个预训练模型灵活提取不同大小的子网络。

Method: WPAC通过加权PCA压缩注意力层知识到关键权重；PIAD通过渐进重要性感知dropout促进知识分层组织。

Result: WPAC在知识集中方面优于现有剪枝标准，与PIAD结合为模型压缩和扩展提供了强大的替代方案。

Conclusion: 该方法能够高效地从单个预训练ViT中提取不同大小的子网络，在模型压缩和扩展任务中表现优异。

Abstract: Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \textbf{W}eighted \textbf{P}CA for \textbf{A}ttention \textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \textbf{P}rogressive \textbf{I}mportance-\textbf{A}ware \textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.

</details>


### [513] [Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling](https://arxiv.org/abs/2511.11688)
*Aihua Zhu,Rui Su,Qinglin Zhao,Li Feng,Meng Shen,Shibo He*

Main category: cs.LG

TL;DR: HSO是一种用于加速扩散模型采样的训练无关方法，通过双层优化框架在极低函数评估次数下实现最优时间步分布，显著提升采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散概率模型采样过程缓慢，传统调度优化方法难以同时满足有效性、自适应性、实用鲁棒性和计算效率四个核心原则。

Method: 提出分层调度优化器(HSO)，采用双层优化框架：上层全局搜索最优初始化策略，下层局部优化调度细化，结合中点误差代理和间距惩罚适应度函数。

Result: 在极低NFE(如5次)下，HSO在Stable Diffusion v2.1上达到11.94的FID，优化成本仅需不到8秒。

Conclusion: HSO为扩散模型加速提供了一种高效实用的训练无关解决方案，在极低函数评估次数下实现了最先进的采样性能。

Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.

</details>


### [514] [Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2511.11690)
*Fei Song,Yi Li,Rui Wang,Jiahuan Zhou,Changwen Zheng,Jiangmeng Li*

Main category: cs.LG

TL;DR: 提出双重去偏测试时提示调优方法，通过动态检索增强调制和可靠性感知提示优化来缓解提示优化偏差问题


<details>
  <summary>Details</summary>
Motivation: 测试时提示调优在零样本设置下表现优异，但仅基于未标记测试数据调优可能导致提示优化偏差，影响下游任务性能

Method: 双重去偏测试时提示调优：动态检索增强调制模块从动态知识库检索高置信度知识；可靠性感知提示优化模块采用置信度加权集成和跨模态一致性蒸馏

Result: 在15个基准数据集上的实验表明，该方法在自然分布偏移和跨数据集泛化场景下均优于基线方法

Conclusion: 所提方法能有效缓解提示优化偏差，提高视觉语言模型的测试时提示调优性能

Abstract: Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.

</details>


### [515] [AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation](https://arxiv.org/abs/2511.11692)
*Jiayin Zhu,Linlin Yang,Yicong Li,Angela Yao*

Main category: cs.LG

TL;DR: 本文提出AnchorDS方法，通过将文本到3D优化重新表述为动态源分布到固定目标分布的映射，解决了传统SDS方法中语义过度平滑的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的文本到3D方法将2D生成模型的指导视为静态，忽略了源动态，导致语义线索被抑制或合并，产生语义过度平滑的伪影。

Method: 将问题转化为双条件潜在空间，基于文本提示和中间渲染图像进行条件化。引入AnchorDS机制，提供状态锚定指导，并设计了轻量级过滤策略和微调策略来优化锚点。

Result: AnchorDS产生了更精细的细节、更自然的颜色和更强的语义一致性，特别是在复杂提示下，同时保持了效率。

Conclusion: 该方法在质量和效率上都超越了先前的方法，为文本到3D生成提供了更稳定和高质量的解决方案。

Abstract: Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.

</details>


### [516] [Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL](https://arxiv.org/abs/2511.11696)
*Xun Shao,Aoba Otani,Yuto Hirasuka,Runji Cai,Seng W. Loke*

Main category: cs.LG

TL;DR: 本文提出下一代老年人监护系统，从跌倒检测扩展到日常活动识别(ADL)，旨在开发隐私保护、边缘部署和联邦学习系统，支持老年人独立生活。


<details>
  <summary>Details</summary>
Motivation: 应对老龄化社会需求，从单一跌倒检测转向全面日常活动识别，支持老年人保持独立性和尊严。

Method: 使用SISFall数据集及其GAN增强变体进行可行性验证，将跌倒检测作为代理任务，在非IID条件下进行联邦学习实验，并在Jetson Orin Nano设备上部署。

Result: 展示了在边缘设备和联邦学习环境下的初步可行性，为全面ADL监测提供了早期证据。

Conclusion: 提出了从单任务检测向全面日常活动识别的转型路线图，指出了领域偏移、数据稀缺和隐私风险等开放挑战，为可持续、以人为本的老年人护理AI指明了方向。

Abstract: This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.

</details>


### [517] [Simple Vision-Language Math Reasoning via Rendered Text](https://arxiv.org/abs/2511.11704)
*Matvey Skripkin,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.LG

TL;DR: 提出一种轻量级但有效的训练流程，通过将LaTeX公式渲染为图像并配以结构化思维链提示，使紧凑的多模态架构在数学问题求解上达到最先进的推理精度。


<details>
  <summary>Details</summary>
Motivation: 解决数学问题需要结合视觉和语言理解，现有方法要么过于复杂，要么在数学推理上表现不佳。本文旨在开发一种简单但高效的方法，利用文本到视觉的增强来提升多模态模型在数学问题上的表现。

Method: 将LaTeX编码的数学公式渲染成图像，并与结构化的思维链提示配对，使用紧凑的多模态架构进行训练。通过系统消融实验分析渲染保真度和提示设计对性能的影响。

Result: 该方法在广泛使用的基准测试中持续匹配或超越开源和专有的数学视觉语言求解器，同时在MMMU、ChartQA和DocVQA等任务上获得高达20%的性能提升，并保持广泛的通用领域能力。

Conclusion: 尽管方法简单，但通过渲染保真度和精心设计的提示，紧凑的多模态架构能够实现最先进的数学推理性能，证明了文本到视觉增强的有效性。

Abstract: We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.

</details>


### [518] [Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs](https://arxiv.org/abs/2511.11705)
*Arya Narang*

Main category: cs.LG

TL;DR: 研究探讨了在卡路里估算中，添加菜品名称文本信息相比仅使用图像能带来多大改进。使用多模态CNN模型相比仅图像CNN模型，平均绝对误差从84.76千卡降低到83.70千卡，提升了1.25%。


<details>
  <summary>Details</summary>
Motivation: 探索短文本输入（菜品名称）能否显著改善卡路里估算的准确性，并与仅使用图像的基线模型进行比较。

Method: 使用TensorFlow库和Nutrition5k数据集，训练了仅图像的CNN模型和接受文本与图像输入的多模态CNN模型。

Result: 多模态模型的卡路里估算平均绝对误差从84.76千卡降低到83.70千卡，减少了1.06千卡，相对改进为1.25%。

Conclusion: 添加菜品名称文本信息确实能略微改善卡路里估算准确性，但改进幅度相对较小。

Abstract: This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.

</details>


### [519] [Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling](https://arxiv.org/abs/2511.11706)
*Julia Peters,Karin Mora,Miguel D. Mahecha,Chaonan Ji,David Montero,Clemens Mosig,Guido Kraemer*

Main category: cs.LG

TL;DR: 提出了一个统一的地球观测表示学习框架，能够在高时空分辨率下整合不同遥感模态数据，解决现有模型固定尺度限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测基础模型通常在固定空间或时间尺度上运行，限制了需要精细空间细节和高时间保真度的生态分析应用。

Method: 采用两阶段设计：首先独立建模各传感器特征，然后将表征融合到共享模型中。使用Sentinel-1和Sentinel-2数据作为代表模态，生成10米分辨率的统一特征空间。

Result: 学习到的嵌入在异质景观中表现出高空间和语义一致性，在总初级生产力建模中编码了生态意义模式并保持了足够的时间保真度。

Conclusion: 该框架为需要不同时空分辨率的环境应用提供了灵活、分析就绪的表示学习方法。

Abstract: Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.

</details>


### [520] [Fast 3D Surrogate Modeling for Data Center Thermal Management](https://arxiv.org/abs/2511.11722)
*Soumyendu Sarkar,Antonio Guillen-Perez,Zachariah J Carmichael,Avisek Naug,Refik Mert Cam,Vineet Gundecha,Ashwin Ramesh Babu,Sahand Ghorbanpour,Ricardo Luna Gutierrez*

Main category: cs.LG

TL;DR: 开发基于视觉的替代建模框架，直接在3D体素化数据中心上运行，实现实时温度预测，相比传统CFD求解器加速20000倍，节省7%能源消耗。


<details>
  <summary>Details</summary>
Motivation: 减少数据中心能耗和碳排放，传统热CFD求解器计算成本高且需要专家构建网格，不适合实时应用。

Method: 评估多种架构：3D CNN U-Net变体、3D傅里叶神经算子和3D视觉变换器，将热输入映射到高保真热图。

Result: 替代模型在不同数据中心配置中泛化良好，实现20000倍加速（数百毫秒vs数小时），准确估计热点和温度分布。

Conclusion: 快速准确的温度估计支持实时冷却控制和负载重新分配，实现显著节能和碳足迹减少。

Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.

</details>


### [521] [Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm](https://arxiv.org/abs/2511.11727)
*Tongda Xu*

Main category: cs.LG

TL;DR: 本文证明在扩散模型中优化条件输入会破坏去噪分数匹配与精确分数匹配的等价性，导致分数范数偏置，影响多个领域的研究工作。


<details>
  <summary>Details</summary>
Motivation: 许多近期工作使用去噪分数匹配来优化扩散模型的条件输入，但作者发现这种方法存在理论偏差，需要深入分析其对不同应用领域的影响。

Method: 通过理论分析和实验验证，展示了在扩散模型中优化条件输入时去噪分数匹配与精确分数匹配不等价的问题，并观察了数据分布优化时的类似偏置。

Result: 发现这种偏置会导致更高的分数范数，并影响包括自回归生成的MAR、图像压缩的PerCo和文本到3D生成的DreamFusion在内的多个领域工作。

Conclusion: 扩散模型中优化条件输入的去噪分数匹配方法存在理论偏置，这种偏置具有广泛影响，需要在相关应用中予以重视。

Abstract: Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.

</details>


### [522] [Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain](https://arxiv.org/abs/2511.11753)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.LG

TL;DR: 本文提出了一种混合GraphSAGE网络(H-GSN)用于供应链物流管理的多任务预测，包括货物类型、物流状态、交通状态、物流ID和物流延迟等目标，在三个不同数据集上取得了97.8%-100%的准确率。


<details>
  <summary>Details</summary>
Motivation: 系统化物流、运输设施和仓储信息对供应链盈利发展至关重要。需要自动预测货物类型、物流延迟和交通状态的方法来提高供应链管理效率，增强供应链韧性。

Method: 提出混合GraphSAGE网络(H-GSN)进行多任务物流管理预测，使用三个Kaggle上的供应链物流数据库(DataCo、Shipping和Smart Logistics)进行验证。

Result: 在Smart Logistics数据集上，10种物流ID预测准确率97.8%，3种交通状态预测准确率100%；在DataCo数据集上货物类型预测准确率98.7%；在Shipping数据集上物流延迟预测准确率99.4%。

Conclusion: 所提方法在不同物流场景下的评估指标证实了其有效性，能够提高供应链的韧性和可持续性。

Abstract: Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.

</details>


### [523] [Coordinate Descent for Network Linearization](https://arxiv.org/abs/2511.11781)
*Vlad Rakhlin,Amir Jevnisek,Shai Avidan*

Main category: cs.LG

TL;DR: 本文提出了一种基于坐标下降的离散优化方法，直接减少ResNet网络中ReLU激活函数的数量，以降低私有推理的延迟。


<details>
  <summary>Details</summary>
Motivation: ReLU激活函数是基于ResNet网络的私有推理中的主要瓶颈，因为它们会导致显著的推理延迟。现有的平滑近似方法在最后硬阈值步骤中通常会造成较大的性能损失。

Method: 采用坐标下降作为优化框架，直接在离散域中工作，通过设计产生稀疏解。

Result: 通过大量实验证明，该方法在常见基准测试中达到了最先进的性能。

Conclusion: 提出的基于坐标下降的离散优化方法能够有效减少ReLU数量，在私有推理中实现更好的性能。

Abstract: ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.

</details>


### [524] [Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production](https://arxiv.org/abs/2511.11880)
*David Montero,Miguel D. Mahecha,Francesco Martinuzzi,César Aybar,Anne Klosterhalfen,Alexander Knohl,Jesús Anaya,Clemens Mosig,Sebastian Wieneke*

Main category: cs.LG

TL;DR: 比较GPT-2和LSTM两种深度学习模型在预测森林CO₂吸收(GPP)方面的表现，发现LSTM整体精度更高，但GPT-2在极端事件中表现更好，且LSTM需要更短的输入窗口即可达到相似精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统遥感方法难以捕捉GPP复杂时间动态的问题，利用深度学习和多模态数据融合来改进森林碳吸收的监测能力。

Method: 使用GPT-2(Transformer架构)和LSTM(循环神经网络)两种代表性深度学习模型，结合多变量输入数据进行GPP预测，并分析时间上下文长度的影响。

Result: 两种模型达到相似精度，但LSTM整体表现更好，GPT-2在极端事件中更优；LSTM仅需较短输入窗口即可达到GPT-2的精度水平；辐射是最重要预测因子，其次是Sentinel-2、MODIS地表温度和Sentinel-1数据。

Conclusion: 模型架构、上下文长度和多模态输入共同决定GPP预测性能，为开发监测陆地碳动态的深度学习框架提供指导。

Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.

</details>


### [525] [A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts](https://arxiv.org/abs/2511.11934)
*C. César Claros Olivares,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: 对CLIP分层机制下的OOD检测方法进行系统比较，发现特征空间学习决定检测效果，概率得分在误分类检测中占优，几何感知得分在CNN中表现更好，而ViT中GradNorm和KPCA重构误差保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 系统比较不同OOD检测方法在CLIP分层机制下的表现，为分布偏移下的方法选择提供统计基础指导。

Method: 使用AURC和AUGRC作为主要指标，比较CNN从头训练和ViT微调两种表示范式，在CIFAR-10/100、SuperCIFAR-100和TinyImageNet上进行实验，采用多重比较控制的基于排名的流程。

Result: 学习到的特征空间很大程度上决定了OOD检测效果，概率得分在误分类检测中占主导地位，在强分布偏移下，几何感知得分在CNN中表现更好，而ViT中GradNorm和KPCA重构误差保持竞争力。

Conclusion: 结果支持OOD检测的表示中心视角，并为分布偏移下的方法选择提供了统计基础指导。

Abstract: We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.

</details>


### [526] [Selecting Fine-Tuning Examples by Quizzing VLMs](https://arxiv.org/abs/2511.12002)
*Tenghao Ji,Eytan Adar*

Main category: cs.LG

TL;DR: QZLoRA是一个通过QuizRank方法自动选择高质量训练图像来进行LoRA微调的框架，能够用更少样本生成更对齐、更逼真的图像。


<details>
  <summary>Details</summary>
Motivation: 在微调文本到图像扩散模型时，从质量参差不齐的图像集（如维基共享资源）中选择训练样本往往会产生较差的结果，需要选择能代表目标概念的高质量图像。

Method: 提出QZLoRA框架，利用QuizRank方法将图像视为'教育干预'并通过VLM进行'测验'来自动排名图像，用于低秩适应(LoRA)微调。

Result: QZLoRA能够用更少样本生成更对齐、更逼真的图像，并且这些微调后的模型也能生成具有代表性的风格化图像（如插画）。

Conclusion: 将自动视觉推理与参数高效微调相结合，在主题自适应生成建模方面具有广阔前景。

Abstract: A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.

</details>


### [527] [Variation-Bounded Loss for Noise-Tolerant Learning](https://arxiv.org/abs/2511.12143)
*Jialiang Wang,Xiong Zhou,Xianming Liu,Gangfeng Hu,Deming Zhai,Junjun Jiang,Haoliang Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的鲁棒损失函数属性——变异比，并基于此构建了变异有界损失函数家族，通过理论分析和实验验证了其在噪声标签下的有效性。


<details>
  <summary>Details</summary>
Motivation: 缓解噪声标签对监督学习的负面影响是一个长期问题，鲁棒损失函数是解决该问题的流行方法。本文旨在通过引入变异比这一新属性来改进损失函数的鲁棒性。

Method: 提出变异比作为损失函数鲁棒性的新属性，构建变异有界损失函数家族，理论分析变异比与鲁棒性的关系，并将常用损失函数重新表述为变异有界形式。

Result: 在各种数据集上的实验表明，该方法在噪声标签环境下具有有效性和灵活性。

Conclusion: 变异比提供了一个可行的放松对称条件的方法，并为实现非对称条件提供了更简洁的路径，变异有界损失函数在噪声标签环境下表现出良好的鲁棒性。

Abstract: Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.

</details>


### [528] [Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks](https://arxiv.org/abs/2511.12265)
*Rui Wang,Zeming Wei,Xiyue Zhang,Meng Sun*

Main category: cs.LG

TL;DR: 提出了一种名为校准对抗采样(CAS)的高效微调方法，通过多臂老虎机框架动态设计奖励并平衡探索与利用，以提升DNN对多种攻击类型的整体鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗训练框架主要关注单一或有限攻击类型，导致DNN在面对实际中可能遇到但训练时未处理的攻击类型时仍然脆弱。

Method: CAS方法从多臂老虎机优化视角出发，考虑多个鲁棒性维度的动态和相互依赖特性，动态设计奖励并平衡探索与利用。

Result: 在基准数据集上的实验表明，CAS实现了优越的整体鲁棒性，同时保持了较高的干净准确率。

Conclusion: CAS为DNN的鲁棒泛化提供了新的范式。

Abstract: Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.

</details>


### [529] [BSO: Binary Spiking Online Optimization Algorithm](https://arxiv.org/abs/2511.12502)
*Yu Liang,Yu Yang,Wenjie Wei,Ammar Belatreche,Shuai Wang,Malu Zhang,Yang Yang*

Main category: cs.LG

TL;DR: 提出了BSO和T-BSO两种在线训练算法，显著减少二元脉冲神经网络的训练内存开销，无需存储潜在权重，通过翻转信号直接更新权重。


<details>
  <summary>Details</summary>
Motivation: 二元脉冲神经网络在资源受限计算中具有效率优势，但其训练算法通常需要大量内存来存储潜在权重和处理时序需求。

Method: BSO算法通过翻转信号直接更新权重，当梯度动量与权重的乘积超过阈值时触发翻转；T-BSO是时序感知变体，利用BSNN的时序动态特性，通过捕获跨时间步的梯度信息进行自适应阈值调整。

Result: 理论分析证明了BSO和T-BSO的收敛保证和遗憾边界；实验表明两种算法相比现有BSNN训练方法实现了更优的优化性能。

Conclusion: BSO和T-BSO是高效的在线训练算法，显著降低了二元脉冲神经网络的训练内存需求，同时保持了良好的优化性能。

Abstract: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.

</details>


### [530] [Linear time small coresets for k-mean clustering of segments with applications](https://arxiv.org/abs/2511.12564)
*David Denisov,Shlomi Dolev,Dan Felmdan,Michael Segal*

Main category: cs.LG

TL;DR: 提出了首个能够处理任意输入线段的核心集构造方法，用于线段k-means聚类问题，核心集大小为O(log²n)，计算时间为O(nd)，在实验中展示了显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 研究线段集合的k-means聚类问题，旨在找到k个中心点最小化所有线段到最近中心点的总距离积分。需要构建核心集来支持高效的流式、分布式或并行计算。

Method: 提出了一种核心集构造方法，能够处理任意输入线段。对于常数k和ε，该方法生成大小为O(log²n)的核心集，计算时间为O(nd)。

Result: 实验验证了该方法在包括实时视频跟踪在内的应用中能够实现显著加速，同时聚类精度损失最小，证实了方法的实用效率和理论保证。

Conclusion: 该工作首次提供了能够处理任意线段输入的核心集构造，为线段k-means聚类问题提供了高效的近似解决方案，具有理论和实践意义。

Abstract: We study the $k$-means problem for a set $\mathcal{S} \subseteq \mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \subseteq \mathbb{R}^d$ that minimize
  $D(\mathcal{S},X) := \sum_{S \in \mathcal{S}} \min_{x \in X} D(S,x)$, where $D(S,x) := \int_{p \in S} |p - x| dp$
  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\varepsilon > 0$, an $\varepsilon$-coreset is a weighted subset $C \subseteq \mathbb{R}^d$ that approximates $D(\mathcal{S},X)$ within a factor of $1 \pm \varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\varepsilon$, it produces a coreset of size $O(\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.

</details>


### [531] [Functional Mean Flow in Hilbert Space](https://arxiv.org/abs/2511.12898)
*Zhiqi Li,Yuchen Sun,Greg Turk,Bo Zhu*

Main category: cs.LG

TL;DR: FMF是一种在无限维希尔伯特空间中定义的一步生成模型，将Mean Flow框架扩展到函数域，提供理论公式和实际实现，并引入x₁预测变体以提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 将一步Mean Flow框架扩展到函数域，为函数数据生成任务提供实用的Flow Matching方法。

Method: 在无限维希尔伯特空间中定义Functional Mean Flow，提供Functional Flow Matching的理论公式和实际实现，引入x₁预测变体。

Result: 开发出适用于时间序列、图像、PDE和3D几何等多种函数数据生成任务的实用一步Flow Matching框架。

Conclusion: FMF是一个实用的函数域一步生成模型，能够高效处理各种函数数据生成任务。

Abstract: We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.

</details>


### [532] [Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks](https://arxiv.org/abs/2511.12985)
*Minsoo Jo,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 提出了一种针对双曲网络的几何感知对抗攻击方法，通过在双曲空间的切空间中分解梯度并仅使用角度方向进行扰动，生成更有效的对抗样本。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法如FGSM和PGD未考虑双曲网络的基础几何结构，可能导致效率低下或几何不一致的攻击，需要在非欧几何中重新评估攻击策略。

Method: 在双曲空间的切空间中计算损失函数梯度，将其分解为径向（深度）和角度（语义）分量，仅使用角度方向进行扰动生成对抗样本。

Result: 在图像分类、跨模态检索任务和网络架构上的实验表明，该方法比传统对抗攻击获得更高的欺骗率，同时产生具有高影响力的扰动。

Conclusion: 这项工作强调了在弯曲表示空间中几何感知对抗策略的重要性，并为攻击分层嵌入提供了原则性框架。

Abstract: Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.

</details>


### [533] [Real-time prediction of breast cancer sites using deformation-aware graph neural network](https://arxiv.org/abs/2511.13082)
*Kyunghyun Lee,Yong-Min Shin,Minwoo Shin,Jihun Kim,Sunghwan Lim,Won-Yong Shin,Kyungho Yoon*

Main category: cs.LG

TL;DR: 开发基于图神经网络的实时变形预测模型，用于乳腺癌活检中准确预测肿瘤位置变形，实现毫米级精度和4000倍计算加速。


<details>
  <summary>Details</summary>
Motivation: 解决间接MRI引导活检中实时变形乳腺模型精度不足的问题，克服直接MRI引导活检时间长、成本高的限制。

Method: 结合个体特异性有限元模型和GNN，利用MRI图像结构信息处理表面位移和距离图数据，预测组织整体位移。

Result: 在幻影和真实患者数据验证中，癌症节点位移精度达0.2mm RMSE，空间重叠DSC为0.977，计算速度比传统FE模拟快4000倍。

Conclusion: 该变形感知GNN模型为乳腺癌活检提供了高精度实时肿瘤位移预测方案，有望显著提升诊断精度和效率。

Abstract: Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.

</details>


### [534] [Uncovering and Mitigating Transient Blindness in Multimodal Model Editing](https://arxiv.org/abs/2511.13243)
*Xiaoqi Han,Ru Li,Ran Yi,Hongye Tan,Zhuomin Liang,Víctor Gutiérrez-Basulto,Jeff Z. Pan*

Main category: cs.LG

TL;DR: 提出全面的多模态模型编辑评估框架，解决现有方法因依赖低相似度输入而夸大成功率的问题，揭示瞬态盲现象，并提出对抗性损失方法改善跨模态表示平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本模型编辑的多模态评估方法通过依赖低相似度或随机输入夸大成功，掩盖过拟合问题，需要更全面的评估框架来准确评估多模态编辑效果。

Method: 提出包含三个关键维度（随机图像局部性、无图像局部性、一致图像局部性）的局部性评估框架，通过七种数据类型进行结构化分析；引入De-VQA动态评估，并提出局部性感知对抗性损失来平衡跨模态表示。

Result: 经验结果表明，该方法持续优于现有基线，平均减少17%的瞬态盲现象并改善局部性，令牌分析显示编辑对文本令牌影响不成比例。

Conclusion: 提出的评估框架和对抗性损失方法有效解决了多模态模型编辑中的瞬态盲问题，显著改善了编辑的局部性和跨模态表示平衡。

Abstract: Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.

</details>


### [535] [Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning](https://arxiv.org/abs/2511.13654)
*Pascal Zimmer,Ghassan Karame*

Main category: cs.LG

TL;DR: 本文首次系统分析了优化超参数（学习率、权重衰减、动量、批大小）对迁移攻击和查询攻击鲁棒性的影响，发现学习率对两种攻击存在相反作用，并探索了联合增强两种鲁棒性的超参数设计空间。


<details>
  <summary>Details</summary>
Motivation: 研究优化超参数如何影响对抗攻击鲁棒性，特别是在不同部署场景下（集中训练、集成学习、分布式训练）对迁移攻击和查询攻击的防御效果。

Method: 通过理论分析和实验验证，系统测试不同优化超参数在各种实际部署设置下的鲁棒性表现，包括集中训练、集成学习和分布式训练。

Result: 发现学习率对两种攻击存在显著二分现象：降低学习率可增强迁移攻击鲁棒性达64%，而提高学习率可增强查询攻击鲁棒性达28%。分布式模型通过超参数调优能最有效地同时缓解两种攻击。

Conclusion: 优化超参数是提升对抗鲁棒性的关键因素，分布式训练模型通过适当的超参数调优能够实现最佳的鲁棒性权衡，同时有效防御迁移攻击和查询攻击。

Abstract: In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [536] [How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer](https://arxiv.org/abs/2511.12285)
*Minu Kim,Ji Sub Um,Hoirin Kim*

Main category: eess.AS

TL;DR: 该研究探讨了自监督学习语音模型在四种复杂声调语言（缅甸语、泰语、老挝语、越南语）中的声调感知能力，发现下游任务会显著影响模型的声调转移表现。


<details>
  <summary>Details</summary>
Motivation: 声调在许多语言中至关重要，但在自监督学习语音模型中研究不足，特别是对于非汉语的复杂声调系统。

Method: 通过探针分析和梯度分析，研究微调后的SSL模型在不同下游任务中的声调感知表现，并与基线声调线索时间跨度进行比较。

Result: 自动语音识别微调使模型的时间跨度与语言特定的声调线索对齐，而韵律和语音相关任务则使模型偏向过长的时间跨度。

Conclusion: 声调转移受到下游任务的显著影响，这突显了任务效应对声调建模中时间焦点的影响。

Abstract: Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.

</details>


### [537] [VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing](https://arxiv.org/abs/2511.12347)
*Zhisheng Zheng,Puyuan Peng,Anuj Diwan,Cong Phuoc Huynh,Xiaohang Sun,Zhu Liu,Vimal Bhat,David Harwath*

Main category: eess.AS

TL;DR: VoiceCraft-X是一个统一多语言语音编辑和零样本文本转语音合成的自回归神经编解码语言模型，支持11种语言，无需音素处理，通过新颖的令牌重排序机制实现高质量语音生成。


<details>
  <summary>Details</summary>
Motivation: 解决多语言语音编辑和文本转语音合成任务的分割问题，提供一个统一的框架来处理不同语言的复杂语音应用，即使在每种语言数据有限的情况下也能实现稳健性能。

Method: 基于Qwen3大语言模型进行无音素跨语言文本处理，采用时间对齐的文本和语音令牌的新颖令牌重排序机制，将两个任务统一为单一序列生成问题。

Result: 模型生成高质量、自然发音的语音，能够无缝创建新音频或编辑现有录音，在多样化的语言环境中表现出稳健性能。

Conclusion: 统一的自回归方法在推进复杂、真实世界的多语言语音应用方面具有强大潜力，即使每种语言数据有限也能实现良好性能。

Abstract: We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [538] [TR-Gaussians: High-fidelity Real-time Rendering of Planar Transmission and Reflection with 3D Gaussian Splatting](https://arxiv.org/abs/2511.13009)
*Yong Liu,Keyang Ye,Tianjia Shao,Kun Zhou*

Main category: cs.GR

TL;DR: 提出TR-Gaussians方法，使用3D高斯和可学习反射平面来建模平面透射和反射，实现室内场景的高保真实时渲染。


<details>
  <summary>Details</summary>
Motivation: 室内场景中普遍存在平面透射和反射现象，现有方法难以对这些复杂外观效果进行高保真渲染。

Method: 结合3D高斯和可学习反射平面，透射分量由3D高斯建模，反射分量由镜像高斯建模，基于菲涅尔权重混合，采用多阶段优化框架。

Result: 在不同数据集上实验表明，TR-Gaussians在平面透射反射场景中实现实时高保真新视角合成，定量和定性均优于现有方法。

Conclusion: TR-Gaussians能够有效建模平面透射和反射，实现高质量的实时渲染，在复杂外观效果合成方面表现出色。

Abstract: We propose Transmission-Reflection Gaussians (TR-Gaussians), a novel 3D-Gaussian-based representation for high-fidelity rendering of planar transmission and reflection, which are ubiquitous in indoor scenes. Our method combines 3D Gaussians with learnable reflection planes that explicitly model the glass planes with view-dependent reflectance strengths. Real scenes and transmission components are modeled by 3D Gaussians and the reflection components are modeled by the mirrored Gaussians with respect to the reflection plane. The transmission and reflection components are blended according to a Fresnel-based, view-dependent weighting scheme, allowing for faithful synthesis of complex appearance effects under varying viewpoints. To effectively optimize TR-Gaussians, we develop a multi-stage optimization framework incorporating color and geometry constraints and an opacity perturbation mechanism. Experiments on different datasets demonstrate that TR-Gaussians achieve real-time, high-fidelity novel view synthesis in scenes with planar transmission and reflection, and outperform state-of-the-art approaches both quantitatively and qualitatively.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [539] [Scalable Vision-Guided Crop Yield Estimation](https://arxiv.org/abs/2511.12999)
*Harrison H. Li,Medhanie Irgau,Nabil Janmohamed,Karen Solveig Rieckmann,David B. Lobell*

Main category: stat.AP

TL;DR: 提出基于预测驱动推断(PPI)的方法，利用田间照片补充传统作物切割测量，提高作物产量估计精度和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统作物切割测量方法耗时，需要更高效的方式来精确估计作物产量，以支持农业监测和决策。

Method: 训练计算机视觉模型从照片预测作物产量，学习控制函数利用空间坐标重新校准预测，结合PPI方法提高区域平均产量估计精度。

Result: 在撒哈拉以南非洲近20,000个真实作物切割和照片数据集上验证，有效样本量增加73%(水稻)和12-23%(玉米)，置信区间更短且覆盖率高。

Conclusion: 低成本图像技术可显著提高作物产量估计效率，使基于区域的作物保险更经济可行，促进可持续农业实践投资。

Abstract: Precise estimation and uncertainty quantification for average crop yields are critical for agricultural monitoring and decision making. Existing data collection methods, such as crop cuts in randomly sampled fields at harvest time, are relatively time-consuming. Thus, we propose an approach based on prediction-powered inference (PPI) to supplement these crop cuts with less time-consuming field photos. After training a computer vision model to predict the ground truth crop cut yields from the photos, we learn a ``control function" that recalibrates these predictions with the spatial coordinates of each field. This enables fields with photos but not crop cuts to be leveraged to improve the precision of zone-wide average yield estimates. Our control function is learned by training on a dataset of nearly 20,000 real crop cuts and photos of rice and maize fields in sub-Saharan Africa. To improve precision, we pool training observations across different zones within the same first-level subdivision of each country. Our final PPI-based point estimates of the average yield are provably asymptotically unbiased and cannot increase the asymptotic variance beyond that of the natural baseline estimator -- the sample average of the crop cuts -- as the number of fields grows. We also propose a novel bias-corrected and accelerated (BCa) bootstrap to construct accompanying confidence intervals. Even in zones with as few as 20 fields, the point estimates show significant empirical improvement over the baseline, increasing the effective sample size by as much as 73% for rice and by 12-23% for maize. The confidence intervals are accordingly shorter at minimal cost to empirical finite-sample coverage. This demonstrates the potential for relatively low-cost images to make area-based crop insurance more affordable and thus spur investment into sustainable agricultural practices.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [540] [Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture](https://arxiv.org/abs/2511.11845)
*K. A. I. N Jayarathne,R. M. N. M. Rathnayaka,D. P. S. S. Peiris*

Main category: cs.RO

TL;DR: 提出了一种结合SLAM与Soar认知架构的自主水下认知系统(AUCS)，通过多传感器数据融合和认知推理模块，实现复杂海洋环境中的自适应导航。


<details>
  <summary>Details</summary>
Motivation: 深海探索面临迷失方向、通信中断和动态水下环境中导航失败等重大挑战，需要更智能的自适应导航系统。

Method: 集成SLAM与Soar认知架构，融合SONAR、LiDAR、IMU和DVL等多传感器数据，结合感知、注意力、规划和学习等认知推理模块。

Result: 系统能够区分动态和静态物体，减少错误闭环检测，增强长期地图一致性，实现了完整的感知-认知-行动-学习循环。

Conclusion: 为下一代认知潜水系统奠定了基础，提高了深海探索的安全性、可靠性和自主性。

Abstract: Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.

</details>


### [541] [Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding](https://arxiv.org/abs/2511.11634)
*Michikuni Eguchi,Takekazu Kitagishi,Yuichi Hiroi,Takefumi Hiraki*

Main category: cs.RO

TL;DR: 提出基于机械臂的系统收集完整服装的触觉数据，通过模拟指尖滑动测量创建带运动标签的多模态触觉数据库，证明运动相关参数能提高识别精度。


<details>
  <summary>Details</summary>
Motivation: 服装的触觉感受对穿着舒适度至关重要，需要系统收集滑动运动中的触觉数据来揭示影响舒适度的物理特性。

Method: 使用机械臂系统进行滑动测量，模拟指尖触感，精确控制速度和方向，创建带运动标签的多模态触觉数据库。

Result: 机器学习评估显示，包含运动相关参数提高了音频和加速度数据的识别准确率，证明了运动相关标签在表征服装触感方面的有效性。

Conclusion: 该系统提供了一种可扩展、非破坏性的服装触觉数据采集方法，有助于未来织物感知和再现的研究。

Abstract: The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.

</details>


### [542] [Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature](https://arxiv.org/abs/2511.11639)
*Jie Fan,Francesco Visentin,Barbara Mazzolai,Emanuela Del Dottore*

Main category: cs.RO

TL;DR: 提出了一种基于图像的3D几何建模方法，用于分析卷须在不同部位受到机械刺激时的形状变化，发现卷须顶端区域响应性更高。


<details>
  <summary>Details</summary>
Motivation: 研究攀援植物卷须形状变化与机械刺激触发事件及接触位置之间的关系，目前仍具挑战性。

Method: 采用基于3D分段回旋曲线的几何方法重建卷须在机械摩擦后的构型，通过图像分析实现形状变化的时空分析。

Result: 重建方法具有高鲁棒性和可靠性（R2 > 0.99），相比深度学习方法具有数据需求少、计算成本低和可解释性强的优势，发现卷须顶端区域响应性更高。

Conclusion: 该方法为植物生物力学研究提供了新工具，并为受攀援植物启发的智能机器人系统设计奠定了基础。

Abstract: Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.

</details>


### [543] [Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review](https://arxiv.org/abs/2511.11777)
*Vinit Mehta,Charu Sharma,Karthick Thiyagarajan*

Main category: cs.RO

TL;DR: 这篇综述论文分析了大型语言模型与3D视觉在机器人感知技术中的融合，涵盖了方法论、应用场景、挑战以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和机器人技术的快速发展，将大型语言模型与3D视觉相结合，有望显著提升机器人在复杂环境中的感知、推理和交互能力，弥合语言智能与空间感知之间的鸿沟。

Method: 论文首先介绍了LLMs和3D数据表示的基础原理，深入分析了机器人技术中的3D感知技术，探讨了场景理解、文本到3D生成、物体定位和具身智能体等关键进展，包括零样本3D分割、动态场景合成和语言引导操作等前沿技术。

Result: 论文系统梳理了多模态LLMs整合3D数据与触觉、听觉和热输入的方法，增强了环境理解和机器人决策能力，并整理了专门用于3D-语言和视觉任务的基准数据集和评估指标。

Conclusion: 论文识别了关键挑战和未来研究方向，包括自适应模型架构、增强跨模态对齐和实时处理能力，这些将为更智能、上下文感知和自主的机器人感知系统铺平道路。

Abstract: With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.

</details>


### [544] [PIGEON: VLM-Driven Object Navigation via Points of Interest Selection](https://arxiv.org/abs/2511.13207)
*Cheng Peng,Zhenzhe Zhang,Cheng Chi,Xiaobao Wei,Yanhao Zhang,Heng Wang,Pengwei Wang,Zhongyuan Wang,Jing Liu,Shanghang Zhang*

Main category: cs.RO

TL;DR: PIGEON是一种基于兴趣点引导的物体导航方法，使用视觉语言模型选择探索过程中的兴趣点，通过低级规划器输出动作，提高决策频率并实现零样本迁移的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在未知环境中导航到指定物体时，难以平衡决策频率与智能性，导致决策缺乏前瞻性或动作不连续。

Method: 使用视觉语言模型PIGEON-VL选择探索过程中形成的兴趣点，维护轻量级语义对齐的快照记忆作为探索策略的语义输入，通过低级规划器输出动作。

Result: 在经典物体导航基准测试中，零样本迁移方法达到最先进性能，RLVR进一步增强模型的语义引导能力。

Conclusion: PIGEON方法通过兴趣点引导的探索策略，实现了高效的物体导航，RLVR技术能够增强语义推理能力。

Abstract: Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [545] [Attention Grounded Enhancement for Visual Document Retrieval](https://arxiv.org/abs/2511.13415)
*Wanqing Cui,Wei Huang,Yazhi Guo,Yibo Hu,Meiguang Jin,Junfeng Ma,Keping Bi*

Main category: cs.IR

TL;DR: AGREE框架通过利用多模态大语言模型的跨模态注意力作为局部监督信号，结合全局监督共同优化检索器，提升视觉文档检索的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有检索器仅使用粗粒度的全局相关标签进行训练，无法识别支持匹配的具体文档区域，导致依赖表面线索且难以处理非抽取式查询。

Method: 提出AGREE框架，利用多模态大语言模型的跨模态注意力作为代理局部监督，指导识别相关文档区域，并结合全局信号联合优化检索器。

Result: 在ViDoRe V2基准测试中，AGREE显著优于仅使用全局监督的基线方法，实现了更准确和可解释的检索。

Conclusion: AGREE通过局部和全局监督的联合优化，促进了查询术语与文档区域的深度对齐，超越了表面级匹配，实现了更准确和可解释的视觉文档检索。

Abstract: Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.

</details>


### [546] [Exploring Multi-Table Retrieval Through Iterative Search](https://arxiv.org/abs/2511.13418)
*Allaa Boutaleb,Bernd Amann,Rafael Angarita,Hubert Naacke*

Main category: cs.IR

TL;DR: 提出了一种迭代式多表检索框架，通过贪心连接感知检索算法在相关性、覆盖率和可连接性之间取得平衡，相比MIP方法在保持竞争力的同时速度提升4-400倍。


<details>
  <summary>Details</summary>
Motivation: 解决数据湖中开放域问答的多表检索挑战，需要在语义相关性和结构连贯性（如可连接性）之间取得平衡，而现有方法要么计算复杂度过高，要么无法找到连贯的可连接表集。

Method: 将多表检索构建为迭代搜索过程，提出贪心连接感知检索算法，综合考虑相关性、覆盖率和可连接性进行优化。

Result: 在5个NL2SQL基准测试中，迭代方法相比MIP方法在保持竞争力的检索性能的同时，速度提升4-400倍（取决于基准和搜索空间设置）。

Conclusion: 迭代启发式方法在实用、可扩展和组合感知检索方面具有巨大潜力。

Abstract: Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.

</details>
